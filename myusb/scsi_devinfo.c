# 1 "drivers/scsi/scsi_devinfo.c"
# 1 "<built-in>" 1
# 1 "<built-in>" 3
# 379 "<built-in>" 3
# 1 "<command line>" 1
# 1 "<built-in>" 2
# 1 "././include/linux/compiler-version.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */






/*
 * This header exists to force full rebuild when the compiler is upgraded.
 *
 * When fixdep scans this, it will find this string "CONFIG_CC_VERSION_TEXT"
 * and add dependency on include/config/CC_VERSION_TEXT, which is touched
 * by Kconfig when the version string from the compiler changes.
 */
# 2 "<built-in>" 2
# 1 "././include/linux/kconfig.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/generated/autoconf.h" 1
/*
 * Automatically generated file; DO NOT EDIT.
 * Linux/arm64 6.1.0 Kernel Configuration
 */
# 6 "././include/linux/kconfig.h" 2
# 16 "././include/linux/kconfig.h"
/*
 * The use of "&&" / "||" is limited in certain expressions.
 * The following enable to calculate "and" / "or" with macro expansion only.
 */
# 28 "././include/linux/kconfig.h"
/*
 * Helper macros to use CONFIG_ options in C/CPP expressions. Note that
 * these only work with boolean and tristate options.
 */

/*
 * Getting something that works in C and CPP for an arg that may or may
 * not be defined is tricky.  Here, if we have "#define CONFIG_BOOGER 1"
 * we match on the placeholder define, insert the "0," for arg1 and generate
 * the triplet (0, 1, 0).  Then the last step cherry picks the 2nd arg (a one).
 * When CONFIG_BOOGER is not defined, we generate a (... 1, 0) pair, and when
 * the last step cherry picks the 2nd arg, we get a zero.
 */




/*
 * IS_BUILTIN(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'y', 0
 * otherwise. For boolean options, this is equivalent to
 * IS_ENABLED(CONFIG_FOO).
 */


/*
 * IS_MODULE(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'm', 0
 * otherwise.  CONFIG_FOO=m results in "#define CONFIG_FOO_MODULE 1" in
 * autoconf.h.
 */


/*
 * IS_REACHABLE(CONFIG_FOO) evaluates to 1 if the currently compiled
 * code can call a function defined in code compiled based on CONFIG_FOO.
 * This is similar to IS_ENABLED(), but returns false when invoked from
 * built-in code when CONFIG_FOO is set to 'm'.
 */



/*
 * IS_ENABLED(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'y' or 'm',
 * 0 otherwise.  Note that CONFIG_FOO=y results in "#define CONFIG_FOO 1" in
 * autoconf.h, while CONFIG_FOO=m results in "#define CONFIG_FOO_MODULE 1".
 */
# 3 "<built-in>" 2
# 1 "././include/linux/compiler_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * Skipped when running bindgen due to a libclang issue;
 * see https://github.com/rust-lang/rust-bindgen/issues/2244.
 */







/* sparse defines __CHECKER__; see Documentation/dev-tools/sparse.rst */
# 43 "././include/linux/compiler_types.h"
/* address spaces */
# 56 "././include/linux/compiler_types.h"
/* context/locking */







/* other */
# 73 "././include/linux/compiler_types.h"
/* Indirect macros required for expanded argument pasting, eg. __LINE__. */





/* Attributes */
# 1 "./include/linux/compiler_attributes.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * The attributes in this file are unconditionally defined and they directly
 * map to compiler attribute(s), unless one of the compilers does not support
 * the attribute. In that case, __has_attribute is used to check for support
 * and the reason is stated in its comment ("Optional: ...").
 *
 * Any other "attributes" (i.e. those that depend on a configuration option,
 * on a compiler, on an architecture, on plugins, on other attributes...)
 * should be defined elsewhere (e.g. compiler_types.h or compiler-*.h).
 * The intention is to keep this file as simple as possible, as well as
 * compiler- and version-agnostic (e.g. avoiding GCC_VERSION checks).
 *
 * This file is meant to be sorted (by actual attribute name,
 * not by #define identifier). Use the __attribute__((__name__)) syntax
 * (i.e. with underscores) to avoid future collisions with other macros.
 * Provide links to the documentation of each supported compiler, if it exists.
 */

/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-alias-function-attribute
 */


/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-aligned-function-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.html#index-aligned-type-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-aligned-variable-attribute
 */



/*
 * Note: do not use this directly. Instead, use __alloc_size() since it is conditionally
 * available and includes other attributes. For GCC < 9.1, __alloc_size__ gets undefined
 * in compiler-gcc.h, due to misbehaviors.
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-alloc_005fsize-function-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#alloc-size
 */


/*
 * Note: users of __always_inline currently do not write "inline" themselves,
 * which seems to be required by gcc to apply the attribute according
 * to its docs (and also "warning: always_inline function might not be
 * inlinable [-Wattributes]" is emitted).
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-always_005finline-function-attribute
 * clang: mentioned
 */


/*
 * The second argument is optional (default 0), so we use a variadic macro
 * to make the shorthand.
 *
 * Beware: Do not apply this to functions which may return
 * ERR_PTRs. Also, it is probably unwise to apply it to functions
 * returning extra information in the low bits (but in that case the
 * compiler should see some alignment anyway, when the return value is
 * massaged by 'flags = ptr & 3; ptr &= ~3;').
 *
 * Optional: not supported by icc
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-assume_005faligned-function-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#assume-aligned
 */






/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-cold-function-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Label-Attributes.html#index-cold-label-attribute
 */


/*
 * Note the long name.
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-const-function-attribute
 */


/*
 * Optional: only supported since gcc >= 9
 * Optional: not supported by clang
 * Optional: not supported by icc
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-copy-function-attribute
 */






/*
 * Optional: not supported by gcc
 * Optional: only supported since clang >= 14.0
 * Optional: not supported by icc
 *
 * clang: https://clang.llvm.org/docs/AttributeReference.html#diagnose_as_builtin
 */






/*
 * Don't. Just don't. See commit 771c035372a0 ("deprecate the '__deprecated'
 * attribute warnings entirely and for good") for more information.
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-deprecated-function-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.html#index-deprecated-type-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-deprecated-variable-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Enumerator-Attributes.html#index-deprecated-enumerator-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#deprecated
 */


/*
 * Optional: not supported by clang
 * Optional: not supported by icc
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.html#index-designated_005finit-type-attribute
 */






/*
 * Optional: only supported since clang >= 14.0
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-error-function-attribute
 */






/*
 * Optional: not supported by clang
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-externally_005fvisible-function-attribute
 */






/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-format-function-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#format
 */



/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-gnu_005finline-function-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#gnu-inline
 */


/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-malloc-function-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#malloc
 */


/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.html#index-mode-type-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-mode-variable-attribute
 */


/*
 * Optional: only supported since gcc >= 7
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/x86-Function-Attributes.html#index-no_005fcaller_005fsaved_005fregisters-function-attribute_002c-x86
 * clang: https://clang.llvm.org/docs/AttributeReference.html#no-caller-saved-registers
 */






/*
 * Optional: not supported by clang
 *
 *  gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noclone-function-attribute
 */






/*
 * Add the pseudo keyword 'fallthrough' so case statement blocks
 * must end with any of these keywords:
 *   break;
 *   fallthrough;
 *   continue;
 *   goto <label>;
 *   return [expression];
 *
 *  gcc: https://gcc.gnu.org/onlinedocs/gcc/Statement-Attributes.html#Statement-Attributes
 */






/*
 * gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#Common-Function-Attributes
 * clang: https://clang.llvm.org/docs/AttributeReference.html#flatten
 */


/*
 * Note the missing underscores.
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noinline-function-attribute
 * clang: mentioned
 */


/*
 * Optional: only supported since gcc >= 8
 * Optional: not supported by clang
 * Optional: not supported by icc
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
 */






/*
 * Optional: only supported since GCC >= 7.1, clang >= 13.0.
 *
 *      gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-no_005fprofile_005finstrument_005ffunction-function-attribute
 *    clang: https://clang.llvm.org/docs/AttributeReference.html#no-profile-instrument-function
 */






/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
 * clang: https://clang.llvm.org/docs/AttributeReference.html#id1
 */


/*
 * Optional: not supported by gcc.
 * Optional: not supported by icc.
 *
 * clang: https://clang.llvm.org/docs/AttributeReference.html#overloadable
 */






/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.html#index-packed-type-attribute
 * clang: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-packed-variable-attribute
 */


/*
 * Note: the "type" argument should match any __builtin_object_size(p, type) usage.
 *
 * Optional: not supported by gcc.
 * Optional: not supported by icc.
 *
 * clang: https://clang.llvm.org/docs/AttributeReference.html#pass-object-size-pass-dynamic-object-size
 */






/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-pure-function-attribute
 */


/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-section-function-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-section-variable-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#section-declspec-allocate
 */


/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-unused-function-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.html#index-unused-type-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-unused-variable-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Label-Attributes.html#index-unused-label-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#maybe-unused-unused
 */



/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-used-function-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-used-variable-attribute
 */


/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-warn_005funused_005fresult-function-attribute
 * clang: https://clang.llvm.org/docs/AttributeReference.html#nodiscard-warn-unused-result
 */


/*
 * Optional: only supported since clang >= 14.0
 *
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-warning-function-attribute
 */






/*
 * Optional: only supported since clang >= 14.0
 *
 * clang: https://clang.llvm.org/docs/AttributeReference.html#disable-sanitizer-instrumentation
 *
 * disable_sanitizer_instrumentation is not always similar to
 * no_sanitize((<sanitizer-name>)): the latter may still let specific sanitizers
 * insert code into functions to prevent false positives. Unlike that,
 * disable_sanitizer_instrumentation prevents all kinds of instrumentation to
 * functions with the attribute.
 */







/*
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-weak-function-attribute
 *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-weak-variable-attribute
 */


/*
 * Used by functions that use '__builtin_return_address'. These function
 * don't want to be splited or made inline, which can make
 * the '__builtin_return_address' get unexpected address.
 */
# 81 "././include/linux/compiler_types.h" 2

/* Builtins */

/*
 * __has_builtin is supported on gcc >= 10, clang >= 3 and icc >= 21.
 * In the meantime, to support gcc < 10, we implement __has_builtin
 * by hand.
 */




/* Compiler specific macros. */

# 1 "./include/linux/compiler-clang.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




/* Compiler specific definitions for Clang compiler */

/* same as gcc, this was present in clang-2.6 so we can assume it works
 * with any version that can compile the kernel
 */


/* all clang versions usable with the kernel support KASAN ABI version 5 */


/*
 * Note: Checking __has_feature(*_sanitizer) is only true if the feature is
 * enabled. Therefore it is not required to additionally check defined(CONFIG_*)
 * to avoid adding redundant attributes in other configurations.
 */
# 77 "./include/linux/compiler-clang.h"
/*
 * Support for __has_feature(coverage_sanitizer) was added in Clang 13 together
 * with no_sanitize("coverage"). Prior versions of Clang support coverage
 * instrumentation, but cannot be queried for support by the preprocessor.
 */
# 97 "./include/linux/compiler-clang.h"
/*
 * Turn individual warnings and errors on and off locally, depending
 * on version.
 */



/* Severity used in pragma directives */
# 96 "././include/linux/compiler_types.h" 2
# 105 "././include/linux/compiler_types.h"
/*
 * Some architectures need to provide custom definitions of macros provided
 * by linux/compiler-*.h, and can do so using asm/compiler.h. We include that
 * conditionally rather than using an asm-generic wrapper in order to avoid
 * build failures if any C compilation, which will include this file via an
 * -include argument in c_flags, occurs prior to the asm-generic wrappers being
 * generated.
 */

# 1 "./arch/arm64/include/asm/compiler.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./arch/arm64/include/asm/compiler.h"
/*
 * The EL0/EL1 pointer bits used by a pointer authentication code.
 * This is dependent on TBI0/TBI1 being enabled, or bits 63:56 would also apply.
 */



/* Valid for EL0 TTBR0 and EL1 TTBR1 instruction pointers */
# 115 "././include/linux/compiler_types.h" 2


struct ftrace_branch_data {
 const char *func;
 const char *file;
 unsigned line;
 union {
  struct {
   unsigned long correct;
   unsigned long incorrect;
  };
  struct {
   unsigned long miss;
   unsigned long hit;
  };
  unsigned long miss_hit[2];
 };
};

struct ftrace_likely_data {
 struct ftrace_branch_data data;
 unsigned long constant;
};
# 147 "././include/linux/compiler_types.h"
/*
 * it doesn't make sense on ARM (currently the only user of __naked)
 * to trace naked functions because then mcount is called without
 * stack and frame pointer being set up and there is no chance to
 * restore the lr register to the value before mcount was called.
 */


/*
 * Prefer gnu_inline, so that extern inline functions do not emit an
 * externally visible function. This makes extern inline behave as per gnu89
 * semantics rather than c99. This prevents multiple symbol definition errors
 * of extern inline functions at link time.
 * A lot of inline functions can cause havoc with function tracing.
 */


/*
 * gcc provides both __inline__ and __inline as alternate spellings of
 * the inline keyword, though the latter is undocumented. New kernel
 * code should only use the inline spelling, but some existing code
 * uses __inline__. Since we #define inline above, to ensure
 * __inline__ has the same semantics, we need this #define.
 *
 * However, the spelling __inline is strictly reserved for referring
 * to the bare keyword.
 */


/*
 * GCC does not warn about unused static inline functions for -Wunused-function.
 * Suppress the warning in clang as well by using __maybe_unused, but enable it
 * for W=1 build. This will allow clang to find unused functions. Remove the
 * __inline_maybe_unused entirely after fixing most of -Wunused-function warnings.
 */






/*
 * Rather then using noinline to prevent stack consumption, use
 * noinline_for_stack instead.  For documentation reasons.
 */


/*
 * Sanitizer helper attributes: Because using __always_inline and
 * __no_sanitize_* conflict, provide helper attributes that will either expand
 * to __no_sanitize_* in compilation units where instrumentation is enabled
 * (__SANITIZE_*__), or __always_inline in compilation units without
 * instrumentation (__SANITIZE_*__ undefined).
 */
# 234 "././include/linux/compiler_types.h"
/* Section for code which can't be instrumented at all */
# 244 "././include/linux/compiler_types.h"
/*
 * The below symbols may be defined for one or more, but not ALL, of the above
 * compilers. We don't consider that to be an error, so set them to nothing.
 * For example, some of them are for compiler specific plugins.
 */
# 274 "././include/linux/compiler_types.h"
/*
 * Any place that could be marked with the "alloc_size" attribute is also
 * a place to be marked with the "malloc" attribute, except those that may
 * be performing a _reallocation_, as that may alias the existing pointer.
 * For these, use __realloc_size().
 */
# 298 "././include/linux/compiler_types.h"
/* Are two types/vars the same type (ignoring qualifiers)? */


/*
 * __unqual_scalar_typeof(x) - Declare an unqualified scalar type, leaving
 *			       non-scalar types unchanged.
 */
/*
 * Prefer C11 _Generic for better compile-times and simpler code. Note: 'char'
 * is not type-compatible with 'signed char', and we define a separate case.
 */
# 323 "././include/linux/compiler_types.h"
/* Is this type a native word size -- useful for atomic operations */
# 348 "././include/linux/compiler_types.h"
/**
 * compiletime_assert - break build and emit msg if condition is false
 * @condition: a compile-time constant condition to check
 * @msg:       a message to emit if condition is false
 *
 * In tradition of POSIX assert, this macro will break the build if the
 * supplied condition is *false*, emitting the supplied error message if the
 * compiler has support to do so.
 */







/* Helpers for emitting diagnostics in pragmas. */
# 4 "<built-in>" 2
# 1 "drivers/scsi/scsi_devinfo.c" 2
// SPDX-License-Identifier: GPL-2.0

# 1 "./include/linux/blkdev.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Portions Copyright (C) 1992 Drew Eckhardt
 */



# 1 "./include/linux/types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/uapi/linux/types.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 1 "./include/uapi/asm-generic/types.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */


/*
 * int-ll64 is used everywhere now.
 */
# 1 "./include/asm-generic/int-ll64.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * asm-generic/int-ll64.h
 *
 * Integer declarations for architectures which use "long long"
 * for 64-bit types.
 */



# 1 "./include/uapi/asm-generic/int-ll64.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * asm-generic/int-ll64.h
 *
 * Integer declarations for architectures which use "long long"
 * for 64-bit types.
 */




# 1 "./arch/arm64/include/uapi/asm/bitsperlong.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */





# 1 "./include/asm-generic/bitsperlong.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/asm-generic/bitsperlong.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/*
 * There seems to be no way of detecting this automatically from user
 * space, so 64 bit architectures should override this in their
 * bitsperlong.h. In particular, an architecture that supports
 * both 32 and 64 bit user space must not rely on CONFIG_64BIT
 * to decide it, but rather check a compiler provided macro.
 */
# 6 "./include/asm-generic/bitsperlong.h" 2








/*
 * FIXME: The check currently breaks x86-64 build, so it's
 * temporarily disabled. Please fix x86-64 and reenable
 */
# 26 "./include/asm-generic/bitsperlong.h"
/*
 * small_const_nbits(n) is true precisely when it is known at compile-time
 * that BITMAP_SIZE(n) is 1, i.e. 1 <= n <= BITS_PER_LONG. This allows
 * various bit/bitmap APIs to provide a fast inline implementation. Bitmaps
 * of size 0 are very rare, and a compile-time-known-size 0 is most likely
 * a sign of error. They will be handled correctly by the bit/bitmap APIs,
 * but using the out-of-line functions, so that the inline implementations
 * can unconditionally dereference the pointer(s).
 */
# 23 "./arch/arm64/include/uapi/asm/bitsperlong.h" 2
# 13 "./include/uapi/asm-generic/int-ll64.h" 2


/*
 * __xx is ok: it doesn't pollute the POSIX namespace. Use these in the
 * header files exported to user space
 */

typedef __signed__ char __s8;
typedef unsigned char __u8;

typedef __signed__ short __s16;
typedef unsigned short __u16;

typedef __signed__ int __s32;
typedef unsigned int __u32;


__extension__ typedef __signed__ long long __s64;
__extension__ typedef unsigned long long __u64;
# 12 "./include/asm-generic/int-ll64.h" 2




typedef __s8 s8;
typedef __u8 u8;
typedef __s16 s16;
typedef __u16 u16;
typedef __s32 s32;
typedef __u32 u32;
typedef __s64 s64;
typedef __u64 u64;
# 8 "./include/uapi/asm-generic/types.h" 2
# 2 "./arch/arm64/include/generated/uapi/asm/types.h" 2
# 6 "./include/uapi/linux/types.h" 2








# 1 "./include/uapi/linux/posix_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./include/linux/stddef.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/stddef.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 11 "./include/uapi/linux/stddef.h"
/**
 * __struct_group() - Create a mirrored named and anonyomous struct
 *
 * @TAG: The tag name for the named sub-struct (usually empty)
 * @NAME: The identifier name of the mirrored sub-struct
 * @ATTRS: Any struct attributes (usually empty)
 * @MEMBERS: The member declarations for the mirrored structs
 *
 * Used to create an anonymous union of two structs with identical layout
 * and size: one anonymous and one named. The former's members can be used
 * normally without sub-struct naming, and the latter can be used to
 * reason about the start, end, and size of the group of struct members.
 * The named struct can also be explicitly tagged for layer reuse, as well
 * as both having struct attributes appended.
 */






/**
 * __DECLARE_FLEX_ARRAY() - Declare a flexible array usable in a union
 *
 * @TYPE: The type of each flexible array element
 * @NAME: The name of the flexible array member
 *
 * In order to have a flexible array member in a union or alone in a
 * struct, it needs to be wrapped in an anonymous struct with at least 1
 * named member, but that member can be empty.
 */
# 6 "./include/linux/stddef.h" 2




enum {
 false = 0,
 true = 1
};




/**
 * sizeof_field() - Report the size of a struct field in bytes
 *
 * @TYPE: The structure containing the field of interest
 * @MEMBER: The field to return the size of
 */


/**
 * offsetofend() - Report the offset of a struct field within the struct
 *
 * @TYPE: The type of the structure
 * @MEMBER: The member within the structure to get the end offset of
 */



/**
 * struct_group() - Wrap a set of declarations in a mirrored struct
 *
 * @NAME: The identifier name of the mirrored sub-struct
 * @MEMBERS: The member declarations for the mirrored structs
 *
 * Used to create an anonymous union of two structs with identical
 * layout and size: one anonymous and one named. The former can be
 * used normally without sub-struct naming, and the latter can be
 * used to reason about the start, end, and size of the group of
 * struct members.
 */



/**
 * struct_group_attr() - Create a struct_group() with trailing attributes
 *
 * @NAME: The identifier name of the mirrored sub-struct
 * @ATTRS: Any struct attributes to apply
 * @MEMBERS: The member declarations for the mirrored structs
 *
 * Used to create an anonymous union of two structs with identical
 * layout and size: one anonymous and one named. The former can be
 * used normally without sub-struct naming, and the latter can be
 * used to reason about the start, end, and size of the group of
 * struct members. Includes structure attributes argument.
 */



/**
 * struct_group_tagged() - Create a struct_group with a reusable tag
 *
 * @TAG: The tag name for the named sub-struct
 * @NAME: The identifier name of the mirrored sub-struct
 * @MEMBERS: The member declarations for the mirrored structs
 *
 * Used to create an anonymous union of two structs with identical
 * layout and size: one anonymous and one named. The former can be
 * used normally without sub-struct naming, and the latter can be
 * used to reason about the start, end, and size of the group of
 * struct members. Includes struct tag argument for the named copy,
 * so the specified layout can be reused later.
 */



/**
 * DECLARE_FLEX_ARRAY() - Declare a flexible array usable in a union
 *
 * @TYPE: The type of each flexible array element
 * @NAME: The name of the flexible array member
 *
 * In order to have a flexible array member in a union or alone in a
 * struct, it needs to be wrapped in an anonymous struct with at least 1
 * named member, but that member can be empty.
 */
# 6 "./include/uapi/linux/posix_types.h" 2

/*
 * This allows for 1024 file descriptors: if NR_OPEN is ever grown
 * beyond that you'll have to change this too. But 1024 fd's seem to be
 * enough even for such "real" unices like OSF/1, so hopefully this is
 * one limit that doesn't have to be changed [again].
 *
 * Note that POSIX wants the FD_CLEAR(fd,fdsetp) defines to be in
 * <sys/time.h> (and thus <linux/time.h>) - but this is a more logical
 * place for them. Solved by having dummy defines in <sys/time.h>.
 */

/*
 * This macro may have been defined in <gnu/types.h>. But we always
 * use the one here.
 */



typedef struct {
 unsigned long fds_bits[1024 / (8 * sizeof(long))];
} __kernel_fd_set;

/* Type of a signal handler.  */
typedef void (*__kernel_sighandler_t)(int);

/* Type of a SYSV IPC key.  */
typedef int __kernel_key_t;
typedef int __kernel_mqd_t;

# 1 "./arch/arm64/include/uapi/asm/posix_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



typedef unsigned short __kernel_old_uid_t;
typedef unsigned short __kernel_old_gid_t;


# 1 "./include/uapi/asm-generic/posix_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */




/*
 * This file is generally used by user-level software, so you need to
 * be a little careful about namespace pollution etc.
 *
 * First the types that are often defined in different ways across
 * architectures, so that you can override them.
 */


typedef long __kernel_long_t;
typedef unsigned long __kernel_ulong_t;



typedef __kernel_ulong_t __kernel_ino_t;



typedef unsigned int __kernel_mode_t;



typedef int __kernel_pid_t;



typedef int __kernel_ipc_pid_t;



typedef unsigned int __kernel_uid_t;
typedef unsigned int __kernel_gid_t;



typedef __kernel_long_t __kernel_suseconds_t;



typedef int __kernel_daddr_t;



typedef unsigned int __kernel_uid32_t;
typedef unsigned int __kernel_gid32_t;
# 59 "./include/uapi/asm-generic/posix_types.h"
typedef unsigned int __kernel_old_dev_t;


/*
 * Most 32 bit architectures use "unsigned int" size_t,
 * and all 64 bit architectures use "unsigned long" size_t.
 */






typedef __kernel_ulong_t __kernel_size_t;
typedef __kernel_long_t __kernel_ssize_t;
typedef __kernel_long_t __kernel_ptrdiff_t;




typedef struct {
 int val[2];
} __kernel_fsid_t;


/*
 * anything below here should be completely generic
 */
typedef __kernel_long_t __kernel_off_t;
typedef long long __kernel_loff_t;
typedef __kernel_long_t __kernel_old_time_t;



typedef long long __kernel_time64_t;
typedef __kernel_long_t __kernel_clock_t;
typedef int __kernel_timer_t;
typedef int __kernel_clockid_t;
typedef char * __kernel_caddr_t;
typedef unsigned short __kernel_uid16_t;
typedef unsigned short __kernel_gid16_t;
# 10 "./arch/arm64/include/uapi/asm/posix_types.h" 2
# 37 "./include/uapi/linux/posix_types.h" 2
# 15 "./include/uapi/linux/types.h" 2


/*
 * Below are truly Linux-specific types that should never collide with
 * any application/library that wants linux/types.h.
 */

/* sparse defines __CHECKER__; see Documentation/dev-tools/sparse.rst */






/* The kernel doesn't use this legacy form, but user space does */


typedef __u16 __le16;
typedef __u16 __be16;
typedef __u32 __le32;
typedef __u32 __be32;
typedef __u64 __le64;
typedef __u64 __be64;

typedef __u16 __sum16;
typedef __u32 __wsum;

/*
 * aligned_u64 should be used in defining kernel<->userspace ABIs to avoid
 * common 32/64-bit compat problems.
 * 64-bit values align to 4-byte boundaries on x86_32 (and possibly other
 * architectures) and to 8-byte boundaries on 64-bit architectures.  The new
 * aligned_64 type enforces 8-byte alignment so that structs containing
 * aligned_64 values have the same alignment on 32-bit and 64-bit architectures.
 * No conversions are necessary between 32-bit user-space and a 64-bit kernel.
 */




typedef unsigned __poll_t;
# 7 "./include/linux/types.h" 2






typedef u32 __kernel_dev_t;

typedef __kernel_fd_set fd_set;
typedef __kernel_dev_t dev_t;
typedef __kernel_ulong_t ino_t;
typedef __kernel_mode_t mode_t;
typedef unsigned short umode_t;
typedef u32 nlink_t;
typedef __kernel_off_t off_t;
typedef __kernel_pid_t pid_t;
typedef __kernel_daddr_t daddr_t;
typedef __kernel_key_t key_t;
typedef __kernel_suseconds_t suseconds_t;
typedef __kernel_timer_t timer_t;
typedef __kernel_clockid_t clockid_t;
typedef __kernel_mqd_t mqd_t;

typedef _Bool bool;

typedef __kernel_uid32_t uid_t;
typedef __kernel_gid32_t gid_t;
typedef __kernel_uid16_t uid16_t;
typedef __kernel_gid16_t gid16_t;

typedef unsigned long uintptr_t;


/* This is defined by include/asm-{arch}/posix_types.h */
typedef __kernel_old_uid_t old_uid_t;
typedef __kernel_old_gid_t old_gid_t;



typedef __kernel_loff_t loff_t;


/*
 * The following typedefs are also protected by individual ifdefs for
 * historical reasons:
 */


typedef __kernel_size_t size_t;




typedef __kernel_ssize_t ssize_t;




typedef __kernel_ptrdiff_t ptrdiff_t;




typedef __kernel_clock_t clock_t;




typedef __kernel_caddr_t caddr_t;


/* bsd */
typedef unsigned char u_char;
typedef unsigned short u_short;
typedef unsigned int u_int;
typedef unsigned long u_long;

/* sysv */
typedef unsigned char unchar;
typedef unsigned short ushort;
typedef unsigned int uint;
typedef unsigned long ulong;




typedef u8 u_int8_t;
typedef s8 int8_t;
typedef u16 u_int16_t;
typedef s16 int16_t;
typedef u32 u_int32_t;
typedef s32 int32_t;



typedef u8 uint8_t;
typedef u16 uint16_t;
typedef u32 uint32_t;


typedef u64 uint64_t;
typedef u64 u_int64_t;
typedef s64 int64_t;


/* this is a special 64bit data type that is 8-byte aligned */




/**
 * The type used for indexing onto a disc or disc partition.
 *
 * Linux always considers sectors to be 512 bytes long independently
 * of the devices real block size.
 *
 * blkcnt_t is the type of the inode's block count.
 */
typedef u64 sector_t;
typedef u64 blkcnt_t;

/*
 * The type of an index into the pagecache.
 */


/*
 * A dma_addr_t can hold any valid DMA address, i.e., any address returned
 * by the DMA API.
 *
 * If the DMA API only uses 32-bit addresses, dma_addr_t need only be 32
 * bits wide.  Bus addresses, e.g., PCI BARs, may be wider than 32 bits,
 * but drivers do memory-mapped I/O to ioremapped kernel virtual addresses,
 * so they don't care about the size of the actual bus addresses.
 */

typedef u64 dma_addr_t;




typedef unsigned int gfp_t;
typedef unsigned int slab_flags_t;
typedef unsigned int fmode_t;


typedef u64 phys_addr_t;




typedef phys_addr_t resource_size_t;

/*
 * This type is the placeholder for a hardware interrupt number. It has to be
 * big enough to enclose whatever representation is used by a given platform.
 */
typedef unsigned long irq_hw_number_t;

typedef struct {
 int counter;
} atomic_t;




typedef struct {
 s64 counter;
} atomic64_t;


struct list_head {
 struct list_head *next, *prev;
};

struct hlist_head {
 struct hlist_node *first;
};

struct hlist_node {
 struct hlist_node *next, **pprev;
};

struct ustat {
 __kernel_daddr_t f_tfree;



 unsigned long f_tinode;

 char f_fname[6];
 char f_fpack[6];
};

/**
 * struct callback_head - callback structure for use with RCU and task_work
 * @next: next update requests in a list
 * @func: actual update function to call after the grace period.
 *
 * The struct is aligned to size of pointer. On most architectures it happens
 * naturally due ABI requirements, but some architectures (like CRIS) have
 * weird ABI and we need to ask it explicitly.
 *
 * The alignment is required to guarantee that bit 0 of @next will be
 * clear under normal conditions -- as long as we use call_rcu() or
 * call_srcu() to queue the callback.
 *
 * This guarantee is important for few reasons:
 *  - future call_rcu_lazy() will make use of lower bits in the pointer;
 *  - the structure shares storage space in struct page with @compound_head,
 *    which encode PageTail() in bit 0. The guarantee is needed to avoid
 *    false-positive PageTail().
 */
struct callback_head {
 struct callback_head *next;
 void (*func)(struct callback_head *head);
} __attribute__((aligned(sizeof(void *))));


typedef void (*rcu_callback_t)(struct callback_head *head);
typedef void (*call_rcu_func_t)(struct callback_head *head, rcu_callback_t func);

typedef void (*swap_r_func_t)(void *a, void *b, int size, const void *priv);
typedef void (*swap_func_t)(void *a, void *b, int size);

typedef int (*cmp_r_func_t)(const void *a, const void *b, const void *priv);
typedef int (*cmp_func_t)(const void *a, const void *b);
# 9 "./include/linux/blkdev.h" 2
# 1 "./include/linux/blk_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Block data types and constants.  Directly include this file only to
 * break include dependency loop.
 */




# 1 "./include/linux/bvec.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * bvec iterator
 *
 * Copyright (C) 2001 Ming Lei <ming.lei@canonical.com>
 */



# 1 "./include/linux/highmem.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/fs.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/linkage.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/stringify.h" 1



/* Indirect stringification.  Doing two levels allows the parameter to be a
 * macro itself.  For example, compile with -DFOO=bar, __stringify(FOO)
 * converts to "bar".
 */
# 7 "./include/linux/linkage.h" 2
# 1 "./include/linux/export.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */





/*
 * Export symbols from the kernel to modules.  Forked from module.h
 * to reduce the amount of pointless cruft we feed to gcc when only
 * exporting a simple symbol or two.
 *
 * Try not to add #includes here.  It slows compilation and makes kernel
 * hackers place grumpy comments in header files.
 */

/*
 * This comment block is used by fixdep. Please do not remove.
 *
 * When CONFIG_MODVERSIONS is changed from n to y, all source files having
 * EXPORT_SYMBOL variants must be re-compiled because genksyms is run as a
 * side effect of the *.o build rule.
 */
# 33 "./include/linux/export.h"
# 1 "./include/linux/compiler.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./include/linux/compiler.h"
/*
 * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
 * to disable branch tracing on a per file basis.
 */
# 83 "./include/linux/compiler.h"
/* Optimization barrier */

/* The "volatile" is due to gcc bugs */




/*
 * This version is i.e. to prevent dead stores elimination on @ptr
 * where gcc and llvm may behave differently when otherwise using
 * normal barrier(): while gcc behavior gets along with a normal
 * barrier(), llvm needs an explicit input variable to be assumed
 * clobbered. The issue is as follows: while the inline asm might
 * access any memory it wants, the compiler could have fit all of
 * @ptr into memory registers instead, and since @ptr never escaped
 * from that, it proved that the inline asm wasn't touching any of
 * it. This version works well with both compilers, i.e. we're telling
 * the compiler that the inline asm absolutely may see the contents
 * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
 */



/* workaround for GCC PR82365 if needed */




/* Unreachable code */
# 143 "./include/linux/compiler.h"
/*
 * KENTRY - kernel entry point
 * This can be used to annotate symbols (functions or data) that are used
 * without their linker symbol being referenced explicitly. For example,
 * interrupt vector handlers, or functions in the kernel image that are found
 * programatically.
 *
 * Not required for symbols exported with EXPORT_SYMBOL, or initcalls. Those
 * are handled in their own way (with KEEP() in linker scripts).
 *
 * KENTRY can be avoided if the symbols in question are marked as KEEP() in the
 * linker script. For example an architecture could KEEP() its entire
 * boot/exception vector code rather than annotate each function and data.
 */
# 176 "./include/linux/compiler.h"
/* Make the optimizer believe the variable can be manipulated arbitrarily. */




/* Not-quite-unique ID. */




/**
 * data_race - mark an expression as containing intentional data races
 *
 * This data_race() macro is useful for situations in which data races
 * should be forgiven.  One example is diagnostic code that accesses
 * shared variables but is not a part of the core synchronization design.
 *
 * This macro *does not* affect normal code generation, but is a hint
 * to tooling that data races here are to be ignored.
 */
# 208 "./include/linux/compiler.h"
/*
 * Force the compiler to emit 'sym' as a symbol, so that we can reference
 * it from inline assembler. Necessary in case 'sym' could be inlined
 * otherwise, or eliminated entirely due to lack of references that are
 * visible to the compiler.
 */






/**
 * offset_to_ptr - convert a relative memory offset to an absolute pointer
 * @off:	the address of the 32-bit offset value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *offset_to_ptr(const int *off)
{
 return (void *)((unsigned long)off + *off);
}



/* &a[0] degrades to a pointer: a different type from an array */


/*
 * Whether 'type' is a signed type or an unsigned type. Supports scalar types,
 * bool and also pointer types.
 */



/*
 * This is needed in functions which generate the stack canary, see
 * arch/x86/kernel/smpboot.c::start_secondary() for an example.
 */


# 1 "./arch/arm64/include/asm/rwonce.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright (C) 2020 Google LLC.
 */
# 71 "./arch/arm64/include/asm/rwonce.h"
# 1 "./include/asm-generic/rwonce.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Prevent the compiler from merging or refetching reads or writes. The
 * compiler is also forbidden from reordering successive instances of
 * READ_ONCE and WRITE_ONCE, but only when the compiler is aware of some
 * particular ordering. One way to make the compiler aware of ordering is to
 * put the two invocations of READ_ONCE or WRITE_ONCE in different C
 * statements.
 *
 * These two macros will also work on aggregate data types like structs or
 * unions.
 *
 * Their two major use cases are: (1) Mediating communication between
 * process-level code and irq/NMI handlers, all running on the same CPU,
 * and (2) Ensuring that the compiler does not fold, spindle, or otherwise
 * mutilate accesses that either do not require ordering or that interact
 * with an explicit memory barrier or atomic instruction that provides the
 * required ordering.
 */






# 1 "./include/linux/kasan-checks.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * The annotations present in this file are only relevant for the software
 * KASAN modes that rely on compiler instrumentation, and will be optimized
 * away for the hardware tag-based KASAN mode. Use kasan_check_byte() instead.
 */

/*
 * __kasan_check_*: Always available when KASAN is enabled. This may be used
 * even in compilation units that selectively disable KASAN, but must use KASAN
 * to validate access to an address.   Never use these in header files!
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __kasan_check_read(const volatile void *p, unsigned int size)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __kasan_check_write(const volatile void *p, unsigned int size)
{
 return true;
}


/*
 * kasan_check_*: Only available when the particular compilation unit has KASAN
 * instrumentation enabled. May be used in header files.
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kasan_check_read(const volatile void *p, unsigned int size)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kasan_check_write(const volatile void *p, unsigned int size)
{
 return true;
}
# 27 "./include/asm-generic/rwonce.h" 2
# 1 "./include/linux/kcsan-checks.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * KCSAN access checks and modifiers. These can be used to explicitly check
 * uninstrumented accesses, or change KCSAN checking behaviour of accesses.
 *
 * Copyright (C) 2019, Google LLC.
 */




/* Note: Only include what is already included by compiler.h. */



/* Access types -- if KCSAN_ACCESS_WRITE is not set, the access is a read. */



/* The following are special, and never due to compiler instrumentation. */



/*
 * __kcsan_*: Always calls into the runtime when KCSAN is enabled. This may be used
 * even in compilation units that selectively disable KCSAN, but must use KCSAN
 * to validate access to an address. Never use these in header files!
 */
# 189 "./include/linux/kcsan-checks.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kcsan_check_access(const volatile void *ptr, size_t size,
     int type) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kcsan_mb(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kcsan_wmb(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kcsan_rmb(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kcsan_release(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_disable_current(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_enable_current(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_enable_current_nowarn(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_nestable_atomic_begin(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_nestable_atomic_end(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_flat_atomic_begin(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_flat_atomic_end(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_atomic_next(int n) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_set_access_mask(unsigned long mask) { }

struct kcsan_scoped_access { };

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kcsan_scoped_access *
kcsan_begin_scoped_access(const volatile void *ptr, size_t size, int type,
     struct kcsan_scoped_access *sa) { return sa; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_end_scoped_access(struct kcsan_scoped_access *sa) { }
# 229 "./include/linux/kcsan-checks.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_check_access(const volatile void *ptr, size_t size,
          int type) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kcsan_enable_current(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kcsan_disable_current(void) { }
# 270 "./include/linux/kcsan-checks.h"
/**
 * __kcsan_check_read - check regular read access for races
 *
 * @ptr: address of access
 * @size: size of access
 */


/**
 * __kcsan_check_write - check regular write access for races
 *
 * @ptr: address of access
 * @size: size of access
 */



/**
 * __kcsan_check_read_write - check regular read-write access for races
 *
 * @ptr: address of access
 * @size: size of access
 */



/**
 * kcsan_check_read - check regular read access for races
 *
 * @ptr: address of access
 * @size: size of access
 */


/**
 * kcsan_check_write - check regular write access for races
 *
 * @ptr: address of access
 * @size: size of access
 */



/**
 * kcsan_check_read_write - check regular read-write access for races
 *
 * @ptr: address of access
 * @size: size of access
 */



/*
 * Check for atomic accesses: if atomic accesses are not ignored, this simply
 * aliases to kcsan_check_access(), otherwise becomes a no-op.
 */
# 339 "./include/linux/kcsan-checks.h"
/**
 * ASSERT_EXCLUSIVE_WRITER - assert no concurrent writes to @var
 *
 * Assert that there are no concurrent writes to @var; other readers are
 * allowed. This assertion can be used to specify properties of concurrent code,
 * where violation cannot be detected as a normal data race.
 *
 * For example, if we only have a single writer, but multiple concurrent
 * readers, to avoid data races, all these accesses must be marked; even
 * concurrent marked writes racing with the single writer are bugs.
 * Unfortunately, due to being marked, they are no longer data races. For cases
 * like these, we can use the macro as follows:
 *
 * .. code-block:: c
 *
 *	void writer(void) {
 *		spin_lock(&update_foo_lock);
 *		ASSERT_EXCLUSIVE_WRITER(shared_foo);
 *		WRITE_ONCE(shared_foo, ...);
 *		spin_unlock(&update_foo_lock);
 *	}
 *	void reader(void) {
 *		// update_foo_lock does not need to be held!
 *		... = READ_ONCE(shared_foo);
 *	}
 *
 * Note: ASSERT_EXCLUSIVE_WRITER_SCOPED(), if applicable, performs more thorough
 * checking if a clear scope where no concurrent writes are expected exists.
 *
 * @var: variable to assert on
 */



/*
 * Helper macros for implementation of for ASSERT_EXCLUSIVE_*_SCOPED(). @id is
 * expected to be unique for the scope in which instances of kcsan_scoped_access
 * are declared.
 */
# 387 "./include/linux/kcsan-checks.h"
/**
 * ASSERT_EXCLUSIVE_WRITER_SCOPED - assert no concurrent writes to @var in scope
 *
 * Scoped variant of ASSERT_EXCLUSIVE_WRITER().
 *
 * Assert that there are no concurrent writes to @var for the duration of the
 * scope in which it is introduced. This provides a better way to fully cover
 * the enclosing scope, compared to multiple ASSERT_EXCLUSIVE_WRITER(), and
 * increases the likelihood for KCSAN to detect racing accesses.
 *
 * For example, it allows finding race-condition bugs that only occur due to
 * state changes within the scope itself:
 *
 * .. code-block:: c
 *
 *	void writer(void) {
 *		spin_lock(&update_foo_lock);
 *		{
 *			ASSERT_EXCLUSIVE_WRITER_SCOPED(shared_foo);
 *			WRITE_ONCE(shared_foo, 42);
 *			...
 *			// shared_foo should still be 42 here!
 *		}
 *		spin_unlock(&update_foo_lock);
 *	}
 *	void buggy(void) {
 *		if (READ_ONCE(shared_foo) == 42)
 *			WRITE_ONCE(shared_foo, 1); // bug!
 *	}
 *
 * @var: variable to assert on
 */



/**
 * ASSERT_EXCLUSIVE_ACCESS - assert no concurrent accesses to @var
 *
 * Assert that there are no concurrent accesses to @var (no readers nor
 * writers). This assertion can be used to specify properties of concurrent
 * code, where violation cannot be detected as a normal data race.
 *
 * For example, where exclusive access is expected after determining no other
 * users of an object are left, but the object is not actually freed. We can
 * check that this property actually holds as follows:
 *
 * .. code-block:: c
 *
 *	if (refcount_dec_and_test(&obj->refcnt)) {
 *		ASSERT_EXCLUSIVE_ACCESS(*obj);
 *		do_some_cleanup(obj);
 *		release_for_reuse(obj);
 *	}
 *
 * Note:
 *
 * 1. ASSERT_EXCLUSIVE_ACCESS_SCOPED(), if applicable, performs more thorough
 *    checking if a clear scope where no concurrent accesses are expected exists.
 *
 * 2. For cases where the object is freed, `KASAN <kasan.html>`_ is a better
 *    fit to detect use-after-free bugs.
 *
 * @var: variable to assert on
 */



/**
 * ASSERT_EXCLUSIVE_ACCESS_SCOPED - assert no concurrent accesses to @var in scope
 *
 * Scoped variant of ASSERT_EXCLUSIVE_ACCESS().
 *
 * Assert that there are no concurrent accesses to @var (no readers nor writers)
 * for the entire duration of the scope in which it is introduced. This provides
 * a better way to fully cover the enclosing scope, compared to multiple
 * ASSERT_EXCLUSIVE_ACCESS(), and increases the likelihood for KCSAN to detect
 * racing accesses.
 *
 * @var: variable to assert on
 */



/**
 * ASSERT_EXCLUSIVE_BITS - assert no concurrent writes to subset of bits in @var
 *
 * Bit-granular variant of ASSERT_EXCLUSIVE_WRITER().
 *
 * Assert that there are no concurrent writes to a subset of bits in @var;
 * concurrent readers are permitted. This assertion captures more detailed
 * bit-level properties, compared to the other (word granularity) assertions.
 * Only the bits set in @mask are checked for concurrent modifications, while
 * ignoring the remaining bits, i.e. concurrent writes (or reads) to ~mask bits
 * are ignored.
 *
 * Use this for variables, where some bits must not be modified concurrently,
 * yet other bits are expected to be modified concurrently.
 *
 * For example, variables where, after initialization, some bits are read-only,
 * but other bits may still be modified concurrently. A reader may wish to
 * assert that this is true as follows:
 *
 * .. code-block:: c
 *
 *	ASSERT_EXCLUSIVE_BITS(flags, READ_ONLY_MASK);
 *	foo = (READ_ONCE(flags) & READ_ONLY_MASK) >> READ_ONLY_SHIFT;
 *
 * Note: The access that immediately follows ASSERT_EXCLUSIVE_BITS() is assumed
 * to access the masked bits only, and KCSAN optimistically assumes it is
 * therefore safe, even in the presence of data races, and marking it with
 * READ_ONCE() is optional from KCSAN's point-of-view. We caution, however, that
 * it may still be advisable to do so, since we cannot reason about all compiler
 * optimizations when it comes to bit manipulations (on the reader and writer
 * side). If you are sure nothing can go wrong, we can write the above simply
 * as:
 *
 * .. code-block:: c
 *
 *	ASSERT_EXCLUSIVE_BITS(flags, READ_ONLY_MASK);
 *	foo = (flags & READ_ONLY_MASK) >> READ_ONLY_SHIFT;
 *
 * Another example, where this may be used, is when certain bits of @var may
 * only be modified when holding the appropriate lock, but other bits may still
 * be modified concurrently. Writers, where other bits may change concurrently,
 * could use the assertion as follows:
 *
 * .. code-block:: c
 *
 *	spin_lock(&foo_lock);
 *	ASSERT_EXCLUSIVE_BITS(flags, FOO_MASK);
 *	old_flags = flags;
 *	new_flags = (old_flags & ~FOO_MASK) | (new_foo << FOO_SHIFT);
 *	if (cmpxchg(&flags, old_flags, new_flags) != old_flags) { ... }
 *	spin_unlock(&foo_lock);
 *
 * @var: variable to assert on
 * @mask: only check for modifications to bits set in @mask
 */
# 28 "./include/asm-generic/rwonce.h" 2

/*
 * Yes, this permits 64-bit accesses on 32-bit architectures. These will
 * actually be atomic in some cases (namely Armv7 + LPAE), but for others we
 * rely on the access being split into 2x32-bit accesses for a 32-bit quantity
 * (e.g. a virtual address) and a strong prevailing wind.
 */




/*
 * Use __READ_ONCE() instead of READ_ONCE() if you do not require any
 * atomicity. Note that this may result in tears!
 */
# 64 "./include/asm-generic/rwonce.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__))
unsigned long __read_once_word_nocheck(const void *addr)
{
 return (*(const volatile typeof( _Generic((*(unsigned long *)addr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*(unsigned long *)addr))) *)&(*(unsigned long *)addr));
}

/*
 * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need to load a
 * word from memory atomically but without telling KASAN/KCSAN. This is
 * usually used by unwinding code when walking the stack of a running process.
 */







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__))
unsigned long read_word_at_a_time(const void *addr)
{
 kasan_check_read(addr, 1);
 return *(unsigned long *)addr;
}
# 72 "./arch/arm64/include/asm/rwonce.h" 2
# 248 "./include/linux/compiler.h" 2
# 34 "./include/linux/export.h" 2
/*
 * Emit the ksymtab entry as a pair of relative references: this reduces
 * the size by half on 64-bit architectures, and eliminates the need for
 * absolute relocations that require runtime processing on relocatable
 * kernels.
 */
# 50 "./include/linux/export.h"
struct kernel_symbol {
 int value_offset;
 int name_offset;
 int namespace_offset;
};
# 75 "./include/linux/export.h"
/*
 * For every exported symbol, do the following:
 *
 * - Put the name of the symbol and namespace (empty string "" for none) in
 *   __ksymtab_strings.
 * - Place a struct kernel_symbol entry in the __ksymtab section.
 *
 * note on .section use: we specify progbits since usage of the "M" (SHF_MERGE)
 * section flag requires it. Use '%progbits' instead of '@progbits' since the
 * former apparently works on all arches according to the binutils source.
 */
# 8 "./include/linux/linkage.h" 2
# 1 "./arch/arm64/include/asm/linkage.h" 1
# 11 "./arch/arm64/include/asm/linkage.h"
/*
 * When using in-kernel BTI we need to ensure that PCS-conformant
 * assembly functions have suitable annotations.  Override
 * SYM_FUNC_START to insert a BTI landing pad at the start of
 * everything, the override is done unconditionally so we're more
 * likely to notice any drift from the overridden definitions.
 */
# 9 "./include/linux/linkage.h" 2

/* Some toolchains use other characters (e.g. '`') to mark new line in macro */
# 42 "./include/linux/linkage.h"
/*
 * For assembly routines.
 *
 * Note when using these that you must specify the appropriate
 * alignment directives yourself
 */



/*
 * This is used by architectures to keep arguments on the stack
 * untouched by the compiler by keeping them live until the end.
 * The argument stack may be owned by the assembly-language
 * caller, not the callee, and gcc doesn't always understand
 * that.
 *
 * We have the return value, and a maximum of six arguments.
 *
 * This should always be followed by a "return ret" for the
 * protection to work (ie no more work that the compiler might
 * end up needing stack temporaries for).
 */
/* Assembly files may be compiled with -traditional .. */
# 6 "./include/linux/fs.h" 2
# 1 "./include/linux/wait_bit.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Linux wait-bit related types and methods:
 */
# 1 "./include/linux/wait.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/*
 * Linux wait queue related types and methods
 */
# 1 "./include/linux/list.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/container_of.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/build_bug.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/build_bug.h"
/*
 * Force a compilation error if condition is true, but also produce a
 * result (of value 0 and type int), so the expression can be used
 * e.g. in a structure initializer (or where-ever else comma expressions
 * aren't permitted).
 */



/* Force a compilation error if a constant expression is not a power of 2 */





/*
 * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the
 * expression but avoids the generation of any code, even if that expression
 * has side-effects.
 */


/**
 * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied
 *		      error message.
 * @condition: the condition which the compiler should know is false.
 *
 * See BUILD_BUG_ON for description.
 */


/**
 * BUILD_BUG_ON - break compile if a condition is true.
 * @condition: the condition which the compiler should know is false.
 *
 * If you have some code which relies on certain constants being equal, or
 * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to
 * detect if someone changes it.
 */



/**
 * BUILD_BUG - break compile if used.
 *
 * If you have some code that you expect the compiler to eliminate at
 * build time, you should use BUILD_BUG to detect if it is
 * unexpectedly used.
 */


/**
 * static_assert - check integer constant expression at build time
 *
 * static_assert() is a wrapper for the C11 _Static_assert, with a
 * little macro magic to make the message optional (defaulting to the
 * stringification of the tested expression).
 *
 * Contrary to BUILD_BUG_ON(), static_assert() can be used at global
 * scope, but requires the expression to be an integer constant
 * expression (i.e., it is not enough that __builtin_constant_p() is
 * true for expr).
 *
 * Also note that BUILD_BUG_ON() fails the build if the condition is
 * true, while static_assert() fails the build if the expression is
 * false.
 */
# 6 "./include/linux/container_of.h" 2
# 1 "./include/linux/err.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






# 1 "./arch/arm64/include/generated/uapi/asm/errno.h" 1
# 1 "./include/uapi/asm-generic/errno.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./include/uapi/asm-generic/errno-base.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 6 "./include/uapi/asm-generic/errno.h" 2





/*
 * This error code is special: arch syscall entry code will return
 * -ENOSYS if users try to call a syscall that doesn't exist.  To keep
 * failures of syscalls that really do exist distinguishable from
 * failures due to attempts to use a nonexistent syscall, syscall
 * implementations should refrain from returning -ENOSYS.
 */
# 115 "./include/uapi/asm-generic/errno.h"
/* for robust mutexes */
# 2 "./arch/arm64/include/generated/uapi/asm/errno.h" 2
# 9 "./include/linux/err.h" 2

/*
 * Kernel pointers have redundant information, so we can use a
 * scheme where we can return either an error code or a normal
 * pointer with the same return value.
 *
 * This should be a per-architecture thing, to allow different
 * error and pointer decisions.
 */






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void * __attribute__((__warn_unused_result__)) ERR_PTR(long error)
{
 return (void *) error;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long __attribute__((__warn_unused_result__)) PTR_ERR( const void *ptr)
{
 return (long) ptr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __attribute__((__warn_unused_result__)) IS_ERR( const void *ptr)
{
 return __builtin_expect(!!((unsigned long)(void *)((unsigned long)ptr) >= (unsigned long)-4095), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __attribute__((__warn_unused_result__)) IS_ERR_OR_NULL( const void *ptr)
{
 return __builtin_expect(!!(!ptr), 0) || __builtin_expect(!!((unsigned long)(void *)((unsigned long)ptr) >= (unsigned long)-4095), 0);
}

/**
 * ERR_CAST - Explicitly cast an error-valued pointer to another pointer type
 * @ptr: The pointer to cast.
 *
 * Explicitly cast an error-valued pointer to another pointer type in such a
 * way as to make it clear that's what's going on.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void * __attribute__((__warn_unused_result__)) ERR_CAST( const void *ptr)
{
 /* cast away the const */
 return (void *) ptr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) PTR_ERR_OR_ZERO( const void *ptr)
{
 if (IS_ERR(ptr))
  return PTR_ERR(ptr);
 else
  return 0;
}
# 7 "./include/linux/container_of.h" 2



/**
 * container_of - cast a member of a structure out to the containing structure
 * @ptr:	the pointer to the member.
 * @type:	the type of the container struct this is embedded in.
 * @member:	the name of the member within the struct.
 *
 */







/**
 * container_of_safe - cast a member of a structure out to the containing structure
 * @ptr:	the pointer to the member.
 * @type:	the type of the container struct this is embedded in.
 * @member:	the name of the member within the struct.
 *
 * If IS_ERR_OR_NULL(ptr), ptr is returned unchanged.
 */
# 6 "./include/linux/list.h" 2


# 1 "./include/linux/poison.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/********** include/linux/list.h **********/

/*
 * Architectures might want to move the poison pointer offset
 * into some well-recognized area such as 0xdead000000000000,
 * that is also not mappable by user-space exploits:
 */






/*
 * These are non-NULL pointers that will result in page faults
 * under normal circumstances, used to verify that nobody uses
 * non-initialized list entries.
 */



/********** include/linux/timer.h **********/


/********** mm/page_poison.c **********/


/********** mm/page_alloc.c ************/



/********** mm/slab.c **********/
/*
 * Magic nums for obj red zoning.
 * Placed in the first word before and the first word after an obj.
 */






/* ...and for poisoning */




/********** arch/$ARCH/mm/init.c **********/


/********** arch/ia64/hp/common/sba_iommu.c **********/
/*
 * arch/ia64/hp/common/sba_iommu.c uses a 16-byte poison string with a
 * value of "SBAIOMMU POISON\0" for spill-over poisoning.
 */

/********** fs/jbd/journal.c **********/



/********** drivers/base/dmapool.c **********/



/********** drivers/atm/ **********/



/********** kernel/mutexes **********/




/********** security/ **********/


/********** net/core/page_pool.c **********/


/********** kernel/bpf/ **********/
# 9 "./include/linux/list.h" 2
# 1 "./include/linux/const.h" 1



# 1 "./include/vdso/const.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/const.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/* const.h: Macros for dealing with constants.  */




/* Some constant macros are used in both assembler and
 * C code.  Therefore we cannot annotate them always with
 * 'UL' and other type specifiers unilaterally.  We
 * use the following macros to deal with this.
 *
 * Similarly, _AT() will cast an expression with a type in C, but
 * leave it unchanged in asm.
 */
# 6 "./include/vdso/const.h" 2
# 5 "./include/linux/const.h" 2

/*
 * This returns a constant expression while determining if an argument is
 * a constant expression, most importantly without evaluating the argument.
 * Glory to Martin Uecker <Martin.Uecker@med.uni-goettingen.de>
 */
# 10 "./include/linux/list.h" 2

# 1 "./arch/arm64/include/asm/barrier.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/barrier.h
 *
 * Copyright (C) 2012 ARM Ltd.
 */
# 33 "./arch/arm64/include/asm/barrier.h"
/*
 * Data Gathering Hint:
 * This instruction prevents merging memory accesses with Normal-NC or
 * Device-GRE attributes before the hint instruction with any memory accesses
 * appearing after the hint instruction.
 */
# 76 "./arch/arm64/include/asm/barrier.h"
/*
 * Generate a mask for array_index__nospec() that is ~0UL when 0 <= idx < sz
 * and 0 otherwise.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long array_index_mask_nospec(unsigned long idx,
          unsigned long sz)
{
 unsigned long mask;

 asm volatile(
 "	cmp	%1, %2\n"
 "	sbc	%0, xzr, xzr\n"
 : "=r" (mask)
 : "r" (idx), "Ir" (sz)
 : "cc");

 asm volatile("hint #20" : : : "memory");
 return mask;
}

/*
 * Ensure that reads of the counter are treated the same as memory reads
 * for the purposes of ordering by subsequent memory barriers.
 *
 * This insanity brought to you by speculative system register reads,
 * out-of-order memory accesses, sequence locks and Thomas Gleixner.
 *
 * https://lore.kernel.org/r/alpine.DEB.2.21.1902081950260.1662@nanos.tec.linutronix.de/
 */
# 212 "./arch/arm64/include/asm/barrier.h"
# 1 "./include/asm-generic/barrier.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * Generic barrier definitions.
 *
 * It should be possible to use these on really simple architectures,
 * but it serves more as a starting point for new ports.
 *
 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 */
# 24 "./include/asm-generic/barrier.h"
/*
 * Architectures that want generic instrumentation can define __ prefixed
 * variants of all barriers.
 */
# 53 "./include/asm-generic/barrier.h"
/*
 * Force strict CPU ordering. And yes, this is required on UP too when we're
 * talking to devices.
 *
 * Fall back to compiler barriers if nothing better is provided.
 */
# 214 "./include/asm-generic/barrier.h"
/* Barriers for virtual machine guests when talking to an SMP host */
# 224 "./include/asm-generic/barrier.h"
/**
 * smp_acquire__after_ctrl_dep() - Provide ACQUIRE ordering after a control dependency
 *
 * A control dependency provides a LOAD->STORE order, the additional RMB
 * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
 * aka. (load)-ACQUIRE.
 *
 * Architectures that do not do load speculation can have this be barrier().
 */




/**
 * smp_cond_load_relaxed() - (Spin) wait for cond with no ordering guarantees
 * @ptr: pointer to the variable to wait on
 * @cond: boolean expression to wait for
 *
 * Equivalent to using READ_ONCE() on the condition variable.
 *
 * Due to C lacking lambda expressions we load the value of *ptr into a
 * pre-named variable @VAL to be used in @cond.
 */
# 261 "./include/asm-generic/barrier.h"
/**
 * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
 * @ptr: pointer to the variable to wait on
 * @cond: boolean expression to wait for
 *
 * Equivalent to using smp_load_acquire() on the condition variable but employs
 * the control dependency of the wait to reduce the barrier on many platforms.
 */
# 278 "./include/asm-generic/barrier.h"
/*
 * pmem_wmb() ensures that all stores for which the modification
 * are written to persistent storage by preceding instructions have
 * updated persistent storage before any data  access or data transfer
 * caused by subsequent instructions is initiated.
 */




/*
 * ioremap_wc() maps I/O memory as memory with write-combining attributes. For
 * this kind of memory accesses, the CPU may wait for prior accesses to be
 * merged with subsequent ones. In some situation, such wait is bad for the
 * performance. io_stop_wc() can be used to prevent the merging of
 * write-combining memory accesses before this macro with those after it.
 */
# 213 "./arch/arm64/include/asm/barrier.h" 2
# 12 "./include/linux/list.h" 2

/*
 * Circular doubly linked list implementation.
 *
 * Some of the internal functions ("__xxx") are useful when
 * manipulating whole lists rather than single entries, as
 * sometimes we already know the next/prev entries and we can
 * generate better code by using them directly rather than
 * using the generic single-entry routines.
 */






/**
 * INIT_LIST_HEAD - Initialize a list_head structure
 * @list: list_head structure to be initialized.
 *
 * Initializes the list_head to point to itself.  If it is a list header,
 * the result is an empty list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void INIT_LIST_HEAD(struct list_head *list)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_0(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(list->next) == sizeof(char) || sizeof(list->next) == sizeof(short) || sizeof(list->next) == sizeof(int) || sizeof(list->next) == sizeof(long)) || sizeof(list->next) == sizeof(long long))) __compiletime_assert_0(); } while (0); do { *(volatile typeof(list->next) *)&(list->next) = (list); } while (0); } while (0);
# 38 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_1(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(list->prev) == sizeof(char) || sizeof(list->prev) == sizeof(short) || sizeof(list->prev) == sizeof(int) || sizeof(list->prev) == sizeof(long)) || sizeof(list->prev) == sizeof(long long))) __compiletime_assert_1(); } while (0); do { *(volatile typeof(list->prev) *)&(list->prev) = (list); } while (0); } while (0);
# 39 "./include/linux/list.h"
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __list_add_valid(struct list_head *new,
    struct list_head *prev,
    struct list_head *next)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __list_del_entry_valid(struct list_head *entry)
{
 return true;
}


/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_add(struct list_head *new,
         struct list_head *prev,
         struct list_head *next)
{
 if (!__list_add_valid(new, prev, next))
  return;

 next->prev = new;
 new->next = next;
 new->prev = prev;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_2(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(prev->next) == sizeof(char) || sizeof(prev->next) == sizeof(short) || sizeof(prev->next) == sizeof(int) || sizeof(prev->next) == sizeof(long)) || sizeof(prev->next) == sizeof(long long))) __compiletime_assert_2(); } while (0); do { *(volatile typeof(prev->next) *)&(prev->next) = (new); } while (0); } while (0);
# 76 "./include/linux/list.h"
}

/**
 * list_add - add a new entry
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_add(struct list_head *new, struct list_head *head)
{
 __list_add(new, head, head->next);
}


/**
 * list_add_tail - add a new entry
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_add_tail(struct list_head *new, struct list_head *head)
{
 __list_add(new, head->prev, head);
}

/*
 * Delete a list entry by making the prev/next entries
 * point to each other.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_del(struct list_head * prev, struct list_head * next)
{
 next->prev = prev;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_3(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(prev->next) == sizeof(char) || sizeof(prev->next) == sizeof(short) || sizeof(prev->next) == sizeof(int) || sizeof(prev->next) == sizeof(long)) || sizeof(prev->next) == sizeof(long long))) __compiletime_assert_3(); } while (0); do { *(volatile typeof(prev->next) *)&(prev->next) = (next); } while (0); } while (0);
# 116 "./include/linux/list.h"
}

/*
 * Delete a list entry and clear the 'prev' pointer.
 *
 * This is a special-purpose list clearing method used in the networking code
 * for lists allocated as per-cpu, where we don't want to incur the extra
 * WRITE_ONCE() overhead of a regular list_del_init(). The code that uses this
 * needs to check the node 'prev' pointer instead of calling list_empty().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_del_clearprev(struct list_head *entry)
{
 __list_del(entry->prev, entry->next);
 entry->prev = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_del_entry(struct list_head *entry)
{
 if (!__list_del_entry_valid(entry))
  return;

 __list_del(entry->prev, entry->next);
}

/**
 * list_del - deletes entry from list.
 * @entry: the element to delete from the list.
 * Note: list_empty() on entry does not return true after this, the entry is
 * in an undefined state.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_del(struct list_head *entry)
{
 __list_del_entry(entry);
 entry->next = ((void *) 0x100 + (0xdead000000000000UL));
 entry->prev = ((void *) 0x122 + (0xdead000000000000UL));
}

/**
 * list_replace - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * If @old was empty, it will be overwritten.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_replace(struct list_head *old,
    struct list_head *new)
{
 new->next = old->next;
 new->next->prev = new;
 new->prev = old->prev;
 new->prev->next = new;
}

/**
 * list_replace_init - replace old entry by new one and initialize the old one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * If @old was empty, it will be overwritten.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_replace_init(struct list_head *old,
         struct list_head *new)
{
 list_replace(old, new);
 INIT_LIST_HEAD(old);
}

/**
 * list_swap - replace entry1 with entry2 and re-add entry1 at entry2's position
 * @entry1: the location to place entry2
 * @entry2: the location to place entry1
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_swap(struct list_head *entry1,
        struct list_head *entry2)
{
 struct list_head *pos = entry2->prev;

 list_del(entry2);
 list_replace(entry1, entry2);
 if (pos == entry1)
  pos = entry2;
 list_add(entry1, pos);
}

/**
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_del_init(struct list_head *entry)
{
 __list_del_entry(entry);
 INIT_LIST_HEAD(entry);
}

/**
 * list_move - delete from one list and add as another's head
 * @list: the entry to move
 * @head: the head that will precede our entry
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_move(struct list_head *list, struct list_head *head)
{
 __list_del_entry(list);
 list_add(list, head);
}

/**
 * list_move_tail - delete from one list and add as another's tail
 * @list: the entry to move
 * @head: the head that will follow our entry
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_move_tail(struct list_head *list,
      struct list_head *head)
{
 __list_del_entry(list);
 list_add_tail(list, head);
}

/**
 * list_bulk_move_tail - move a subsection of a list to its tail
 * @head: the head that will follow our entry
 * @first: first entry to move
 * @last: last entry to move, can be the same as first
 *
 * Move all entries between @first and including @last before @head.
 * All three entries must belong to the same linked list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_bulk_move_tail(struct list_head *head,
           struct list_head *first,
           struct list_head *last)
{
 first->prev->next = last->next;
 last->next->prev = first->prev;

 head->prev->next = first;
 first->prev = head->prev;

 last->next = head;
 head->prev = last;
}

/**
 * list_is_first -- tests whether @list is the first entry in list @head
 * @list: the entry to test
 * @head: the head of the list
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int list_is_first(const struct list_head *list, const struct list_head *head)
{
 return list->prev == head;
}

/**
 * list_is_last - tests whether @list is the last entry in list @head
 * @list: the entry to test
 * @head: the head of the list
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int list_is_last(const struct list_head *list, const struct list_head *head)
{
 return list->next == head;
}

/**
 * list_is_head - tests whether @list is the list @head
 * @list: the entry to test
 * @head: the head of the list
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int list_is_head(const struct list_head *list, const struct list_head *head)
{
 return list == head;
}

/**
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int list_empty(const struct list_head *head)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_4(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head->next) == sizeof(char) || sizeof(head->next) == sizeof(short) || sizeof(head->next) == sizeof(int) || sizeof(head->next) == sizeof(long)) || sizeof(head->next) == sizeof(long long))) __compiletime_assert_4(); } while (0); (*(const volatile typeof( _Generic((head->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head->next))) *)&(head->next)); }) == head;
# 293 "./include/linux/list.h"
}

/**
 * list_del_init_careful - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 *
 * This is the same as list_del_init(), except designed to be used
 * together with list_empty_careful() in a way to guarantee ordering
 * of other memory operations.
 *
 * Any memory operations done before a list_del_init_careful() are
 * guaranteed to be visible after a list_empty_careful() test.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_del_init_careful(struct list_head *entry)
{
 __list_del_entry(entry);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_5(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(entry->prev) == sizeof(char) || sizeof(entry->prev) == sizeof(short) || sizeof(entry->prev) == sizeof(int) || sizeof(entry->prev) == sizeof(long)) || sizeof(entry->prev) == sizeof(long long))) __compiletime_assert_5(); } while (0); do { *(volatile typeof(entry->prev) *)&(entry->prev) = (entry); } while (0); } while (0);
# 310 "./include/linux/list.h"
 do { do { } while (0); do { typeof(&entry->next) __p = (&entry->next); union { typeof( _Generic((*&entry->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&entry->next))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&entry->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&entry->next)))) (entry) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_6(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&entry->next) == sizeof(char) || sizeof(*&entry->next) == sizeof(short) || sizeof(*&entry->next) == sizeof(int) || sizeof(*&entry->next) == sizeof(long)))) __compiletime_assert_6(); } while (0); kasan_check_write(__p, sizeof(*&entry->next)); switch (sizeof(*&entry->next)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0);
# 311 "./include/linux/list.h"
}

/**
 * list_empty_careful - tests whether a list is empty and not being modified
 * @head: the list to test
 *
 * Description:
 * tests whether a list is empty _and_ checks that no other CPU might be
 * in the process of modifying either member (next or prev)
 *
 * NOTE: using list_empty_careful() without synchronization
 * can only be safe if the only activity that can happen
 * to the list entry is list_del_init(). Eg. it cannot be used
 * if another CPU could re-list_add() it.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int list_empty_careful(const struct list_head *head)
{
 struct list_head *next = ({ union { typeof( _Generic((*&head->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&head->next))) __val; char __c[1]; } __u; typeof(&head->next) __p = (&head->next); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_7(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&head->next) == sizeof(char) || sizeof(*&head->next) == sizeof(short) || sizeof(*&head->next) == sizeof(int) || sizeof(*&head->next) == sizeof(long)))) __compiletime_assert_7(); } while (0); kasan_check_read(__p, sizeof(*&head->next)); switch (sizeof(*&head->next)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&head->next))__u.__val; });
# 329 "./include/linux/list.h"
 return list_is_head(next, head) && (next == ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_8(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head->prev) == sizeof(char) || sizeof(head->prev) == sizeof(short) || sizeof(head->prev) == sizeof(int) || sizeof(head->prev) == sizeof(long)) || sizeof(head->prev) == sizeof(long long))) __compiletime_assert_8(); } while (0); (*(const volatile typeof( _Generic((head->prev), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head->prev))) *)&(head->prev)); }));
# 330 "./include/linux/list.h"
}

/**
 * list_rotate_left - rotate the list to the left
 * @head: the head of the list
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_rotate_left(struct list_head *head)
{
 struct list_head *first;

 if (!list_empty(head)) {
  first = head->next;
  list_move_tail(first, head);
 }
}

/**
 * list_rotate_to_front() - Rotate list to specific item.
 * @list: The desired new front of the list.
 * @head: The head of the list.
 *
 * Rotates list so that @list becomes the new front of the list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_rotate_to_front(struct list_head *list,
     struct list_head *head)
{
 /*
	 * Deletes the list head from the list denoted by @head and
	 * places it as the tail of @list, this effectively rotates the
	 * list so that @list is at the front.
	 */
 list_move_tail(head, list);
}

/**
 * list_is_singular - tests whether a list has just one entry.
 * @head: the list to test.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int list_is_singular(const struct list_head *head)
{
 return !list_empty(head) && (head->next == head->prev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_cut_position(struct list_head *list,
  struct list_head *head, struct list_head *entry)
{
 struct list_head *new_first = entry->next;
 list->next = head->next;
 list->next->prev = list;
 list->prev = entry;
 entry->next = list;
 head->next = new_first;
 new_first->prev = head;
}

/**
 * list_cut_position - cut a list into two
 * @list: a new list to add all removed entries
 * @head: a list with entries
 * @entry: an entry within head, could be the head itself
 *	and if so we won't cut the list
 *
 * This helper moves the initial part of @head, up to and
 * including @entry, from @head to @list. You should
 * pass on @entry an element you know is on @head. @list
 * should be an empty list or a list you do not care about
 * losing its data.
 *
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_cut_position(struct list_head *list,
  struct list_head *head, struct list_head *entry)
{
 if (list_empty(head))
  return;
 if (list_is_singular(head) && !list_is_head(entry, head) && (entry != head->next))
  return;
 if (list_is_head(entry, head))
  INIT_LIST_HEAD(list);
 else
  __list_cut_position(list, head, entry);
}

/**
 * list_cut_before - cut a list into two, before given entry
 * @list: a new list to add all removed entries
 * @head: a list with entries
 * @entry: an entry within head, could be the head itself
 *
 * This helper moves the initial part of @head, up to but
 * excluding @entry, from @head to @list.  You should pass
 * in @entry an element you know is on @head.  @list should
 * be an empty list or a list you do not care about losing
 * its data.
 * If @entry == @head, all entries on @head are moved to
 * @list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_cut_before(struct list_head *list,
       struct list_head *head,
       struct list_head *entry)
{
 if (head->next == entry) {
  INIT_LIST_HEAD(list);
  return;
 }
 list->next = head->next;
 list->next->prev = list;
 list->prev = entry->prev;
 list->prev->next = list;
 head->next = entry;
 entry->prev = head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_splice(const struct list_head *list,
     struct list_head *prev,
     struct list_head *next)
{
 struct list_head *first = list->next;
 struct list_head *last = list->prev;

 first->prev = prev;
 prev->next = first;

 last->next = next;
 next->prev = last;
}

/**
 * list_splice - join two lists, this is designed for stacks
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_splice(const struct list_head *list,
    struct list_head *head)
{
 if (!list_empty(list))
  __list_splice(list, head, head->next);
}

/**
 * list_splice_tail - join two lists, each list being a queue
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_splice_tail(struct list_head *list,
    struct list_head *head)
{
 if (!list_empty(list))
  __list_splice(list, head->prev, head);
}

/**
 * list_splice_init - join two lists and reinitialise the emptied list.
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * The list at @list is reinitialised
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_splice_init(struct list_head *list,
        struct list_head *head)
{
 if (!list_empty(list)) {
  __list_splice(list, head, head->next);
  INIT_LIST_HEAD(list);
 }
}

/**
 * list_splice_tail_init - join two lists and reinitialise the emptied list
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * Each of the lists is a queue.
 * The list at @list is reinitialised
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_splice_tail_init(struct list_head *list,
      struct list_head *head)
{
 if (!list_empty(list)) {
  __list_splice(list, head->prev, head);
  INIT_LIST_HEAD(list);
 }
}

/**
 * list_entry - get the struct for this entry
 * @ptr:	the &struct list_head pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_first_entry - get the first element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note, that list is expected to be not empty.
 */



/**
 * list_last_entry - get the last element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note, that list is expected to be not empty.
 */



/**
 * list_first_entry_or_null - get the first element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note that if the list is empty, it returns NULL.
 */






/**
 * list_next_entry - get the next element in list
 * @pos:	the type * to cursor
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_next_entry_circular - get the next element in list
 * @pos:	the type * to cursor.
 * @head:	the list head to take the element from.
 * @member:	the name of the list_head within the struct.
 *
 * Wraparound if pos is the last element (return the first element).
 * Note, that list is expected to be not empty.
 */




/**
 * list_prev_entry - get the prev element in list
 * @pos:	the type * to cursor
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_prev_entry_circular - get the prev element in list
 * @pos:	the type * to cursor.
 * @head:	the list head to take the element from.
 * @member:	the name of the list_head within the struct.
 *
 * Wraparound if pos is the first element (return the last element).
 * Note, that list is expected to be not empty.
 */




/**
 * list_for_each	-	iterate over a list
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */



/**
 * list_for_each_rcu - Iterate over a list in an RCU-safe fashion
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */





/**
 * list_for_each_continue - continue iteration over a list
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 *
 * Continue to iterate over a list, continuing after the current position.
 */



/**
 * list_for_each_prev	-	iterate over a list backwards
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */



/**
 * list_for_each_safe - iterate over a list safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */





/**
 * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */





/**
 * list_entry_is_head - test if the entry points to the head of the list
 * @pos:	the type * to cursor
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_for_each_entry	-	iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */





/**
 * list_for_each_entry_reverse - iterate backwards over list of given type.
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */





/**
 * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()
 * @pos:	the type * to use as a start point
 * @head:	the head of the list
 * @member:	the name of the list_head within the struct.
 *
 * Prepares a pos entry for use as a start point in list_for_each_entry_continue().
 */



/**
 * list_for_each_entry_continue - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */





/**
 * list_for_each_entry_continue_reverse - iterate backwards from the given point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Start to iterate over list of given type backwards, continuing after
 * the current position.
 */





/**
 * list_for_each_entry_from - iterate over list of given type from the current point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type, continuing from current position.
 */




/**
 * list_for_each_entry_from_reverse - iterate backwards over list of given type
 *                                    from the current point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate backwards over list of given type, continuing from current position.
 */




/**
 * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */






/**
 * list_for_each_entry_safe_continue - continue list iteration safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type, continuing after current point,
 * safe against removal of list entry.
 */






/**
 * list_for_each_entry_safe_from - iterate over list from current point safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type from current point, safe against
 * removal of list entry.
 */





/**
 * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate backwards over list of given type, safe against removal
 * of list entry.
 */






/**
 * list_safe_reset_next - reset a stale list_for_each_entry_safe loop
 * @pos:	the loop cursor used in the list_for_each_entry_safe loop
 * @n:		temporary storage used in list_for_each_entry_safe
 * @member:	the name of the list_head within the struct.
 *
 * list_safe_reset_next is not safe to use in general if the list may be
 * modified concurrently (eg. the lock is dropped in the loop body). An
 * exception to this is if the cursor element (pos) is pinned in the list,
 * and list_safe_reset_next is called after re-taking the lock and before
 * completing the current iteration of the loop body.
 */



/*
 * Double linked lists with a single pointer list head.
 * Mostly useful for hash tables where the two pointer list head is
 * too wasteful.
 * You lose the ability to access the tail in O(1).
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void INIT_HLIST_NODE(struct hlist_node *h)
{
 h->next = ((void *)0);
 h->pprev = ((void *)0);
}

/**
 * hlist_unhashed - Has node been removed from list and reinitialized?
 * @h: Node to be checked
 *
 * Not that not all removal functions will leave a node in unhashed
 * state.  For example, hlist_nulls_del_init_rcu() does leave the
 * node in unhashed state, but hlist_nulls_del() does not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int hlist_unhashed(const struct hlist_node *h)
{
 return !h->pprev;
}

/**
 * hlist_unhashed_lockless - Version of hlist_unhashed for lockless use
 * @h: Node to be checked
 *
 * This variant of hlist_unhashed() must be used in lockless contexts
 * to avoid potential load-tearing.  The READ_ONCE() is paired with the
 * various WRITE_ONCE() in hlist helpers that are defined below.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int hlist_unhashed_lockless(const struct hlist_node *h)
{
 return !({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_9(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->pprev) == sizeof(char) || sizeof(h->pprev) == sizeof(short) || sizeof(h->pprev) == sizeof(int) || sizeof(h->pprev) == sizeof(long)) || sizeof(h->pprev) == sizeof(long long))) __compiletime_assert_9(); } while (0); (*(const volatile typeof( _Generic((h->pprev), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (h->pprev))) *)&(h->pprev)); });
# 868 "./include/linux/list.h"
}

/**
 * hlist_empty - Is the specified hlist_head structure an empty hlist?
 * @h: Structure to check.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int hlist_empty(const struct hlist_head *h)
{
 return !({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_10(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->first) == sizeof(char) || sizeof(h->first) == sizeof(short) || sizeof(h->first) == sizeof(int) || sizeof(h->first) == sizeof(long)) || sizeof(h->first) == sizeof(long long))) __compiletime_assert_10(); } while (0); (*(const volatile typeof( _Generic((h->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (h->first))) *)&(h->first)); });
# 877 "./include/linux/list.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __hlist_del(struct hlist_node *n)
{
 struct hlist_node *next = n->next;
 struct hlist_node **pprev = n->pprev;

 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_11(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pprev) == sizeof(char) || sizeof(*pprev) == sizeof(short) || sizeof(*pprev) == sizeof(int) || sizeof(*pprev) == sizeof(long)) || sizeof(*pprev) == sizeof(long long))) __compiletime_assert_11(); } while (0); do { *(volatile typeof(*pprev) *)&(*pprev) = (next); } while (0); } while (0);
# 885 "./include/linux/list.h"
 if (next)
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_12(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(next->pprev) == sizeof(char) || sizeof(next->pprev) == sizeof(short) || sizeof(next->pprev) == sizeof(int) || sizeof(next->pprev) == sizeof(long)) || sizeof(next->pprev) == sizeof(long long))) __compiletime_assert_12(); } while (0); do { *(volatile typeof(next->pprev) *)&(next->pprev) = (pprev); } while (0); } while (0);
# 887 "./include/linux/list.h"
}

/**
 * hlist_del - Delete the specified hlist_node from its list
 * @n: Node to delete.
 *
 * Note that this function leaves the node in hashed state.  Use
 * hlist_del_init() or similar instead to unhash @n.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_del(struct hlist_node *n)
{
 __hlist_del(n);
 n->next = ((void *) 0x100 + (0xdead000000000000UL));
 n->pprev = ((void *) 0x122 + (0xdead000000000000UL));
}

/**
 * hlist_del_init - Delete the specified hlist_node from its list and initialize
 * @n: Node to delete.
 *
 * Note that this function leaves the node in unhashed state.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_del_init(struct hlist_node *n)
{
 if (!hlist_unhashed(n)) {
  __hlist_del(n);
  INIT_HLIST_NODE(n);
 }
}

/**
 * hlist_add_head - add a new entry at the beginning of the hlist
 * @n: new entry to be added
 * @h: hlist head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
{
 struct hlist_node *first = h->first;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_13(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next) == sizeof(char) || sizeof(n->next) == sizeof(short) || sizeof(n->next) == sizeof(int) || sizeof(n->next) == sizeof(long)) || sizeof(n->next) == sizeof(long long))) __compiletime_assert_13(); } while (0); do { *(volatile typeof(n->next) *)&(n->next) = (first); } while (0); } while (0);
# 929 "./include/linux/list.h"
 if (first)
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_14(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(first->pprev) == sizeof(char) || sizeof(first->pprev) == sizeof(short) || sizeof(first->pprev) == sizeof(int) || sizeof(first->pprev) == sizeof(long)) || sizeof(first->pprev) == sizeof(long long))) __compiletime_assert_14(); } while (0); do { *(volatile typeof(first->pprev) *)&(first->pprev) = (&n->next); } while (0); } while (0);
# 931 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_15(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->first) == sizeof(char) || sizeof(h->first) == sizeof(short) || sizeof(h->first) == sizeof(int) || sizeof(h->first) == sizeof(long)) || sizeof(h->first) == sizeof(long long))) __compiletime_assert_15(); } while (0); do { *(volatile typeof(h->first) *)&(h->first) = (n); } while (0); } while (0);
# 932 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_16(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_16(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&h->first); } while (0); } while (0);
# 933 "./include/linux/list.h"
}

/**
 * hlist_add_before - add a new entry before the one specified
 * @n: new entry to be added
 * @next: hlist node to add it before, which must be non-NULL
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_before(struct hlist_node *n,
        struct hlist_node *next)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_17(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_17(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (next->pprev); } while (0); } while (0);
# 944 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_18(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next) == sizeof(char) || sizeof(n->next) == sizeof(short) || sizeof(n->next) == sizeof(int) || sizeof(n->next) == sizeof(long)) || sizeof(n->next) == sizeof(long long))) __compiletime_assert_18(); } while (0); do { *(volatile typeof(n->next) *)&(n->next) = (next); } while (0); } while (0);
# 945 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_19(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(next->pprev) == sizeof(char) || sizeof(next->pprev) == sizeof(short) || sizeof(next->pprev) == sizeof(int) || sizeof(next->pprev) == sizeof(long)) || sizeof(next->pprev) == sizeof(long long))) __compiletime_assert_19(); } while (0); do { *(volatile typeof(next->pprev) *)&(next->pprev) = (&n->next); } while (0); } while (0);
# 946 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_20(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(n->pprev)) == sizeof(char) || sizeof(*(n->pprev)) == sizeof(short) || sizeof(*(n->pprev)) == sizeof(int) || sizeof(*(n->pprev)) == sizeof(long)) || sizeof(*(n->pprev)) == sizeof(long long))) __compiletime_assert_20(); } while (0); do { *(volatile typeof(*(n->pprev)) *)&(*(n->pprev)) = (n); } while (0); } while (0);
# 947 "./include/linux/list.h"
}

/**
 * hlist_add_behind - add a new entry after the one specified
 * @n: new entry to be added
 * @prev: hlist node to add it after, which must be non-NULL
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_behind(struct hlist_node *n,
        struct hlist_node *prev)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_21(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next) == sizeof(char) || sizeof(n->next) == sizeof(short) || sizeof(n->next) == sizeof(int) || sizeof(n->next) == sizeof(long)) || sizeof(n->next) == sizeof(long long))) __compiletime_assert_21(); } while (0); do { *(volatile typeof(n->next) *)&(n->next) = (prev->next); } while (0); } while (0);
# 958 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_22(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(prev->next) == sizeof(char) || sizeof(prev->next) == sizeof(short) || sizeof(prev->next) == sizeof(int) || sizeof(prev->next) == sizeof(long)) || sizeof(prev->next) == sizeof(long long))) __compiletime_assert_22(); } while (0); do { *(volatile typeof(prev->next) *)&(prev->next) = (n); } while (0); } while (0);
# 959 "./include/linux/list.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_23(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_23(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&prev->next); } while (0); } while (0);
# 961 "./include/linux/list.h"
 if (n->next)
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_24(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next->pprev) == sizeof(char) || sizeof(n->next->pprev) == sizeof(short) || sizeof(n->next->pprev) == sizeof(int) || sizeof(n->next->pprev) == sizeof(long)) || sizeof(n->next->pprev) == sizeof(long long))) __compiletime_assert_24(); } while (0); do { *(volatile typeof(n->next->pprev) *)&(n->next->pprev) = (&n->next); } while (0); } while (0);
# 963 "./include/linux/list.h"
}

/**
 * hlist_add_fake - create a fake hlist consisting of a single headless node
 * @n: Node to make a fake list out of
 *
 * This makes @n appear to be its own predecessor on a headless hlist.
 * The point of this is to allow things like hlist_del() to work correctly
 * in cases where there is no list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_fake(struct hlist_node *n)
{
 n->pprev = &n->next;
}

/**
 * hlist_fake: Is this node a fake hlist?
 * @h: Node to check for being a self-referential fake hlist.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool hlist_fake(struct hlist_node *h)
{
 return h->pprev == &h->next;
}

/**
 * hlist_is_singular_node - is node the only element of the specified hlist?
 * @n: Node to check for singularity.
 * @h: Header for potentially singular list.
 *
 * Check whether the node is the only node of the head without
 * accessing head, thus avoiding unnecessary cache misses.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
hlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)
{
 return !n->next && n->pprev == &h->first;
}

/**
 * hlist_move_list - Move an hlist
 * @old: hlist_head for old list.
 * @new: hlist_head for new list.
 *
 * Move a list from one list head to another. Fixup the pprev
 * reference of the first entry if it exists.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_move_list(struct hlist_head *old,
       struct hlist_head *new)
{
 new->first = old->first;
 if (new->first)
  new->first->pprev = &new->first;
 old->first = ((void *)0);
}
# 1032 "./include/linux/list.h"
/**
 * hlist_for_each_entry	- iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */





/**
 * hlist_for_each_entry_continue - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */





/**
 * hlist_for_each_entry_from - iterate over a hlist continuing from current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */




/**
 * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		a &struct hlist_node to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */
# 8 "./include/linux/wait.h" 2

# 1 "./include/linux/spinlock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




/*
 * include/linux/spinlock.h - generic spinlock/rwlock declarations
 *
 * here's the role of the various spinlock/rwlock related include files:
 *
 * on SMP builds:
 *
 *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the
 *                        initializers
 *
 *  linux/spinlock_types_raw:
 *			  The raw types and initializers
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel
 *                        implementations, mostly inline assembly code
 *
 *   (also included on UP-debug builds:)
 *
 *  linux/spinlock_api_smp.h:
 *                        contains the prototypes for the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 *
 * on UP builds:
 *
 *  linux/spinlock_type_up.h:
 *                        contains the generic, simplified UP spinlock type.
 *                        (which is an empty structure on non-debug builds)
 *
 *  linux/spinlock_types_raw:
 *			  The raw RT types and initializers
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  linux/spinlock_up.h:
 *                        contains the arch_spin_*()/etc. version of UP
 *                        builds. (which are NOPs on non-debug, non-preempt
 *                        builds)
 *
 *   (included on UP-non-debug builds:)
 *
 *  linux/spinlock_api_up.h:
 *                        builds the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 */

# 1 "./include/linux/typecheck.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Check at compile time that something is of a particular type.
 * Always evaluates to 1 so you may use it easily in comparisons.
 */







/*
 * Check at compile time that 'function' is a certain type, or is a pointer
 * to that type (needs to use typedef for the function type.)
 */





/*
 * Check at compile time that something is a pointer type.
 */
# 56 "./include/linux/spinlock.h" 2
# 1 "./include/linux/preempt.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * include/linux/preempt.h - macros for accessing and manipulating
 * preempt_count (used for kernel preemption, interrupt count, etc.)
 */




/*
 * We put the hardirq and softirq counter into the preemption
 * counter. The bitmask has the following meaning:
 *
 * - bits 0-7 are the preemption count (max preemption depth: 256)
 * - bits 8-15 are the softirq count (max # of softirqs: 256)
 *
 * The hardirq count could in theory be the same as the number of
 * interrupts in the system, but we run all interrupt handlers with
 * interrupts disabled, so we cannot have nesting interrupts. Though
 * there are a few palaeontologic drivers which reenable interrupts in
 * the handler, so we need more than one bit here.
 *
 *         PREEMPT_MASK:	0x000000ff
 *         SOFTIRQ_MASK:	0x0000ff00
 *         HARDIRQ_MASK:	0x000f0000
 *             NMI_MASK:	0x00f00000
 * PREEMPT_NEED_RESCHED:	0x80000000
 */
# 58 "./include/linux/preempt.h"
/*
 * Disable preemption until the scheduler is running -- use an unconditional
 * value so that it also works on !PREEMPT_COUNT kernels.
 *
 * Reset by start_kernel()->sched_init()->init_idle()->init_idle_preempt_count().
 */


/*
 * Initial preempt_count value; reflects the preempt_count schedule invariant
 * which states that during context switches:
 *
 *    preempt_count() == 2*PREEMPT_DISABLE_OFFSET
 *
 * Note: PREEMPT_DISABLE_OFFSET is 0 for !PREEMPT_COUNT kernels.
 * Note: See finish_task_switch().
 */


/* preempt_count() and related functions, depends on PREEMPT_NEED_RESCHED */
# 1 "./arch/arm64/include/asm/preempt.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/jump_label.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Jump label support
 *
 * Copyright (C) 2009-2012 Jason Baron <jbaron@redhat.com>
 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra
 *
 * DEPRECATED API:
 *
 * The use of 'struct static_key' directly, is now DEPRECATED. In addition
 * static_key_{true,false}() is also DEPRECATED. IE DO NOT use the following:
 *
 * struct static_key false = STATIC_KEY_INIT_FALSE;
 * struct static_key true = STATIC_KEY_INIT_TRUE;
 * static_key_true()
 * static_key_false()
 *
 * The updated API replacements are:
 *
 * DEFINE_STATIC_KEY_TRUE(key);
 * DEFINE_STATIC_KEY_FALSE(key);
 * DEFINE_STATIC_KEY_ARRAY_TRUE(keys, count);
 * DEFINE_STATIC_KEY_ARRAY_FALSE(keys, count);
 * static_branch_likely()
 * static_branch_unlikely()
 *
 * Jump labels provide an interface to generate dynamic branches using
 * self-modifying code. Assuming toolchain and architecture support, if we
 * define a "key" that is initially false via "DEFINE_STATIC_KEY_FALSE(key)",
 * an "if (static_branch_unlikely(&key))" statement is an unconditional branch
 * (which defaults to false - and the true block is placed out of line).
 * Similarly, we can define an initially true key via
 * "DEFINE_STATIC_KEY_TRUE(key)", and use it in the same
 * "if (static_branch_unlikely(&key))", in which case we will generate an
 * unconditional branch to the out-of-line true branch. Keys that are
 * initially true or false can be using in both static_branch_unlikely()
 * and static_branch_likely() statements.
 *
 * At runtime we can change the branch target by setting the key
 * to true via a call to static_branch_enable(), or false using
 * static_branch_disable(). If the direction of the branch is switched by
 * these calls then we run-time modify the branch target via a
 * no-op -> jump or jump -> no-op conversion. For example, for an
 * initially false key that is used in an "if (static_branch_unlikely(&key))"
 * statement, setting the key to true requires us to patch in a jump
 * to the out-of-line of true branch.
 *
 * In addition to static_branch_{enable,disable}, we can also reference count
 * the key or branch direction via static_branch_{inc,dec}. Thus,
 * static_branch_inc() can be thought of as a 'make more true' and
 * static_branch_dec() as a 'make more false'.
 *
 * Since this relies on modifying code, the branch modifying functions
 * must be considered absolute slow paths (machine wide synchronization etc.).
 * OTOH, since the affected branches are unconditional, their runtime overhead
 * will be absolutely minimal, esp. in the default (off) case where the total
 * effect is a single NOP of appropriate size. The on case will patch in a jump
 * to the out-of-line block.
 *
 * When the control is directly exposed to userspace, it is prudent to delay the
 * decrement to avoid high frequency code modifications which can (and do)
 * cause significant performance degradation. Struct static_key_deferred and
 * static_key_slow_dec_deferred() provide for this.
 *
 * Lacking toolchain and or architecture support, static keys fall back to a
 * simple conditional branch.
 *
 * Additional babbling in: Documentation/staging/static-keys.rst
 */






extern bool static_key_initialized;





struct static_key {
 atomic_t enabled;

/*
 * Note:
 *   To make anonymous unions work with old compilers, the static
 *   initialization of them requires brackets. This creates a dependency
 *   on the order of the struct with the initializers. If any fields
 *   are added, STATIC_KEY_INIT_TRUE and STATIC_KEY_INIT_FALSE may need
 *   to be modified.
 *
 * bit 0 => 1 if key is initially true
 *	    0 if initially false
 * bit 1 => 1 if points to struct static_key_mod
 *	    0 if points to struct jump_entry
 */
 union {
  unsigned long type;
  struct jump_entry *entries;
  struct static_key_mod *next;
 };

};




# 1 "./arch/arm64/include/asm/jump_label.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2013 Huawei Ltd.
 * Author: Jiang Liu <liuj97@gmail.com>
 *
 * Based on arch/arm/include/asm/jump_label.h
 */






# 1 "./arch/arm64/include/asm/insn.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2013 Huawei Ltd.
 * Author: Jiang Liu <liuj97@gmail.com>
 *
 * Copyright (C) 2014 Zi Shen Lim <zlim.lnx@gmail.com>
 */





# 1 "./arch/arm64/include/asm/insn-def.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */




# 1 "./arch/arm64/include/asm/brk-imm.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */




/*
 * #imm16 values used for BRK instruction generation
 * 0x004: for installing kprobes
 * 0x005: for installing uprobes
 * 0x006: for kprobe software single-step
 * Allowed values for kgdb are 0x400 - 0x7ff
 * 0x100: for triggering a fault on purpose (reserved)
 * 0x400: for dynamic BRK instruction
 * 0x401: for compile time BRK instruction
 * 0x800: kernel-mode BUG() and WARN() traps
 * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
 * 0x8xxx: Control-Flow Integrity traps
 */
# 7 "./arch/arm64/include/asm/insn-def.h" 2

/* A64 instructions are always 32 bits. */


/*
 * BRK instruction encoding
 * The #imm16 value should be placed at bits[20:5] within BRK ins
 */


/*
 * BRK instruction for provoking a fault on purpose
 * Unlike kgdb, #imm16 value with unallocated handler is used for faulting.
 */
# 14 "./arch/arm64/include/asm/insn.h" 2



enum aarch64_insn_hint_cr_op {
 AARCH64_INSN_HINT_NOP = 0x0 << 5,
 AARCH64_INSN_HINT_YIELD = 0x1 << 5,
 AARCH64_INSN_HINT_WFE = 0x2 << 5,
 AARCH64_INSN_HINT_WFI = 0x3 << 5,
 AARCH64_INSN_HINT_SEV = 0x4 << 5,
 AARCH64_INSN_HINT_SEVL = 0x5 << 5,

 AARCH64_INSN_HINT_XPACLRI = 0x07 << 5,
 AARCH64_INSN_HINT_PACIA_1716 = 0x08 << 5,
 AARCH64_INSN_HINT_PACIB_1716 = 0x0A << 5,
 AARCH64_INSN_HINT_AUTIA_1716 = 0x0C << 5,
 AARCH64_INSN_HINT_AUTIB_1716 = 0x0E << 5,
 AARCH64_INSN_HINT_PACIAZ = 0x18 << 5,
 AARCH64_INSN_HINT_PACIASP = 0x19 << 5,
 AARCH64_INSN_HINT_PACIBZ = 0x1A << 5,
 AARCH64_INSN_HINT_PACIBSP = 0x1B << 5,
 AARCH64_INSN_HINT_AUTIAZ = 0x1C << 5,
 AARCH64_INSN_HINT_AUTIASP = 0x1D << 5,
 AARCH64_INSN_HINT_AUTIBZ = 0x1E << 5,
 AARCH64_INSN_HINT_AUTIBSP = 0x1F << 5,

 AARCH64_INSN_HINT_ESB = 0x10 << 5,
 AARCH64_INSN_HINT_PSB = 0x11 << 5,
 AARCH64_INSN_HINT_TSB = 0x12 << 5,
 AARCH64_INSN_HINT_CSDB = 0x14 << 5,
 AARCH64_INSN_HINT_CLEARBHB = 0x16 << 5,

 AARCH64_INSN_HINT_BTI = 0x20 << 5,
 AARCH64_INSN_HINT_BTIC = 0x22 << 5,
 AARCH64_INSN_HINT_BTIJ = 0x24 << 5,
 AARCH64_INSN_HINT_BTIJC = 0x26 << 5,
};

enum aarch64_insn_imm_type {
 AARCH64_INSN_IMM_ADR,
 AARCH64_INSN_IMM_26,
 AARCH64_INSN_IMM_19,
 AARCH64_INSN_IMM_16,
 AARCH64_INSN_IMM_14,
 AARCH64_INSN_IMM_12,
 AARCH64_INSN_IMM_9,
 AARCH64_INSN_IMM_7,
 AARCH64_INSN_IMM_6,
 AARCH64_INSN_IMM_S,
 AARCH64_INSN_IMM_R,
 AARCH64_INSN_IMM_N,
 AARCH64_INSN_IMM_MAX
};

enum aarch64_insn_register_type {
 AARCH64_INSN_REGTYPE_RT,
 AARCH64_INSN_REGTYPE_RN,
 AARCH64_INSN_REGTYPE_RT2,
 AARCH64_INSN_REGTYPE_RM,
 AARCH64_INSN_REGTYPE_RD,
 AARCH64_INSN_REGTYPE_RA,
 AARCH64_INSN_REGTYPE_RS,
};

enum aarch64_insn_register {
 AARCH64_INSN_REG_0 = 0,
 AARCH64_INSN_REG_1 = 1,
 AARCH64_INSN_REG_2 = 2,
 AARCH64_INSN_REG_3 = 3,
 AARCH64_INSN_REG_4 = 4,
 AARCH64_INSN_REG_5 = 5,
 AARCH64_INSN_REG_6 = 6,
 AARCH64_INSN_REG_7 = 7,
 AARCH64_INSN_REG_8 = 8,
 AARCH64_INSN_REG_9 = 9,
 AARCH64_INSN_REG_10 = 10,
 AARCH64_INSN_REG_11 = 11,
 AARCH64_INSN_REG_12 = 12,
 AARCH64_INSN_REG_13 = 13,
 AARCH64_INSN_REG_14 = 14,
 AARCH64_INSN_REG_15 = 15,
 AARCH64_INSN_REG_16 = 16,
 AARCH64_INSN_REG_17 = 17,
 AARCH64_INSN_REG_18 = 18,
 AARCH64_INSN_REG_19 = 19,
 AARCH64_INSN_REG_20 = 20,
 AARCH64_INSN_REG_21 = 21,
 AARCH64_INSN_REG_22 = 22,
 AARCH64_INSN_REG_23 = 23,
 AARCH64_INSN_REG_24 = 24,
 AARCH64_INSN_REG_25 = 25,
 AARCH64_INSN_REG_26 = 26,
 AARCH64_INSN_REG_27 = 27,
 AARCH64_INSN_REG_28 = 28,
 AARCH64_INSN_REG_29 = 29,
 AARCH64_INSN_REG_FP = 29, /* Frame pointer */
 AARCH64_INSN_REG_30 = 30,
 AARCH64_INSN_REG_LR = 30, /* Link register */
 AARCH64_INSN_REG_ZR = 31, /* Zero: as source register */
 AARCH64_INSN_REG_SP = 31 /* Stack pointer: as load/store base reg */
};

enum aarch64_insn_special_register {
 AARCH64_INSN_SPCLREG_SPSR_EL1 = 0xC200,
 AARCH64_INSN_SPCLREG_ELR_EL1 = 0xC201,
 AARCH64_INSN_SPCLREG_SP_EL0 = 0xC208,
 AARCH64_INSN_SPCLREG_SPSEL = 0xC210,
 AARCH64_INSN_SPCLREG_CURRENTEL = 0xC212,
 AARCH64_INSN_SPCLREG_DAIF = 0xDA11,
 AARCH64_INSN_SPCLREG_NZCV = 0xDA10,
 AARCH64_INSN_SPCLREG_FPCR = 0xDA20,
 AARCH64_INSN_SPCLREG_DSPSR_EL0 = 0xDA28,
 AARCH64_INSN_SPCLREG_DLR_EL0 = 0xDA29,
 AARCH64_INSN_SPCLREG_SPSR_EL2 = 0xE200,
 AARCH64_INSN_SPCLREG_ELR_EL2 = 0xE201,
 AARCH64_INSN_SPCLREG_SP_EL1 = 0xE208,
 AARCH64_INSN_SPCLREG_SPSR_INQ = 0xE218,
 AARCH64_INSN_SPCLREG_SPSR_ABT = 0xE219,
 AARCH64_INSN_SPCLREG_SPSR_UND = 0xE21A,
 AARCH64_INSN_SPCLREG_SPSR_FIQ = 0xE21B,
 AARCH64_INSN_SPCLREG_SPSR_EL3 = 0xF200,
 AARCH64_INSN_SPCLREG_ELR_EL3 = 0xF201,
 AARCH64_INSN_SPCLREG_SP_EL2 = 0xF210
};

enum aarch64_insn_variant {
 AARCH64_INSN_VARIANT_32BIT,
 AARCH64_INSN_VARIANT_64BIT
};

enum aarch64_insn_condition {
 AARCH64_INSN_COND_EQ = 0x0, /* == */
 AARCH64_INSN_COND_NE = 0x1, /* != */
 AARCH64_INSN_COND_CS = 0x2, /* unsigned >= */
 AARCH64_INSN_COND_CC = 0x3, /* unsigned < */
 AARCH64_INSN_COND_MI = 0x4, /* < 0 */
 AARCH64_INSN_COND_PL = 0x5, /* >= 0 */
 AARCH64_INSN_COND_VS = 0x6, /* overflow */
 AARCH64_INSN_COND_VC = 0x7, /* no overflow */
 AARCH64_INSN_COND_HI = 0x8, /* unsigned > */
 AARCH64_INSN_COND_LS = 0x9, /* unsigned <= */
 AARCH64_INSN_COND_GE = 0xa, /* signed >= */
 AARCH64_INSN_COND_LT = 0xb, /* signed < */
 AARCH64_INSN_COND_GT = 0xc, /* signed > */
 AARCH64_INSN_COND_LE = 0xd, /* signed <= */
 AARCH64_INSN_COND_AL = 0xe, /* always */
};

enum aarch64_insn_branch_type {
 AARCH64_INSN_BRANCH_NOLINK,
 AARCH64_INSN_BRANCH_LINK,
 AARCH64_INSN_BRANCH_RETURN,
 AARCH64_INSN_BRANCH_COMP_ZERO,
 AARCH64_INSN_BRANCH_COMP_NONZERO,
};

enum aarch64_insn_size_type {
 AARCH64_INSN_SIZE_8,
 AARCH64_INSN_SIZE_16,
 AARCH64_INSN_SIZE_32,
 AARCH64_INSN_SIZE_64,
};

enum aarch64_insn_ldst_type {
 AARCH64_INSN_LDST_LOAD_REG_OFFSET,
 AARCH64_INSN_LDST_STORE_REG_OFFSET,
 AARCH64_INSN_LDST_LOAD_IMM_OFFSET,
 AARCH64_INSN_LDST_STORE_IMM_OFFSET,
 AARCH64_INSN_LDST_LOAD_PAIR_PRE_INDEX,
 AARCH64_INSN_LDST_STORE_PAIR_PRE_INDEX,
 AARCH64_INSN_LDST_LOAD_PAIR_POST_INDEX,
 AARCH64_INSN_LDST_STORE_PAIR_POST_INDEX,
 AARCH64_INSN_LDST_LOAD_EX,
 AARCH64_INSN_LDST_LOAD_ACQ_EX,
 AARCH64_INSN_LDST_STORE_EX,
 AARCH64_INSN_LDST_STORE_REL_EX,
};

enum aarch64_insn_adsb_type {
 AARCH64_INSN_ADSB_ADD,
 AARCH64_INSN_ADSB_SUB,
 AARCH64_INSN_ADSB_ADD_SETFLAGS,
 AARCH64_INSN_ADSB_SUB_SETFLAGS
};

enum aarch64_insn_movewide_type {
 AARCH64_INSN_MOVEWIDE_ZERO,
 AARCH64_INSN_MOVEWIDE_KEEP,
 AARCH64_INSN_MOVEWIDE_INVERSE
};

enum aarch64_insn_bitfield_type {
 AARCH64_INSN_BITFIELD_MOVE,
 AARCH64_INSN_BITFIELD_MOVE_UNSIGNED,
 AARCH64_INSN_BITFIELD_MOVE_SIGNED
};

enum aarch64_insn_data1_type {
 AARCH64_INSN_DATA1_REVERSE_16,
 AARCH64_INSN_DATA1_REVERSE_32,
 AARCH64_INSN_DATA1_REVERSE_64,
};

enum aarch64_insn_data2_type {
 AARCH64_INSN_DATA2_UDIV,
 AARCH64_INSN_DATA2_SDIV,
 AARCH64_INSN_DATA2_LSLV,
 AARCH64_INSN_DATA2_LSRV,
 AARCH64_INSN_DATA2_ASRV,
 AARCH64_INSN_DATA2_RORV,
};

enum aarch64_insn_data3_type {
 AARCH64_INSN_DATA3_MADD,
 AARCH64_INSN_DATA3_MSUB,
};

enum aarch64_insn_logic_type {
 AARCH64_INSN_LOGIC_AND,
 AARCH64_INSN_LOGIC_BIC,
 AARCH64_INSN_LOGIC_ORR,
 AARCH64_INSN_LOGIC_ORN,
 AARCH64_INSN_LOGIC_EOR,
 AARCH64_INSN_LOGIC_EON,
 AARCH64_INSN_LOGIC_AND_SETFLAGS,
 AARCH64_INSN_LOGIC_BIC_SETFLAGS
};

enum aarch64_insn_prfm_type {
 AARCH64_INSN_PRFM_TYPE_PLD,
 AARCH64_INSN_PRFM_TYPE_PLI,
 AARCH64_INSN_PRFM_TYPE_PST,
};

enum aarch64_insn_prfm_target {
 AARCH64_INSN_PRFM_TARGET_L1,
 AARCH64_INSN_PRFM_TARGET_L2,
 AARCH64_INSN_PRFM_TARGET_L3,
};

enum aarch64_insn_prfm_policy {
 AARCH64_INSN_PRFM_POLICY_KEEP,
 AARCH64_INSN_PRFM_POLICY_STRM,
};

enum aarch64_insn_adr_type {
 AARCH64_INSN_ADR_TYPE_ADRP,
 AARCH64_INSN_ADR_TYPE_ADR,
};

enum aarch64_insn_mem_atomic_op {
 AARCH64_INSN_MEM_ATOMIC_ADD,
 AARCH64_INSN_MEM_ATOMIC_CLR,
 AARCH64_INSN_MEM_ATOMIC_EOR,
 AARCH64_INSN_MEM_ATOMIC_SET,
 AARCH64_INSN_MEM_ATOMIC_SWP,
};

enum aarch64_insn_mem_order_type {
 AARCH64_INSN_MEM_ORDER_NONE,
 AARCH64_INSN_MEM_ORDER_ACQ,
 AARCH64_INSN_MEM_ORDER_REL,
 AARCH64_INSN_MEM_ORDER_ACQREL,
};

enum aarch64_insn_mb_type {
 AARCH64_INSN_MB_SY,
 AARCH64_INSN_MB_ST,
 AARCH64_INSN_MB_LD,
 AARCH64_INSN_MB_ISH,
 AARCH64_INSN_MB_ISHST,
 AARCH64_INSN_MB_ISHLD,
 AARCH64_INSN_MB_NSH,
 AARCH64_INSN_MB_NSHST,
 AARCH64_INSN_MB_NSHLD,
 AARCH64_INSN_MB_OSH,
 AARCH64_INSN_MB_OSHST,
 AARCH64_INSN_MB_OSHLD,
};
# 304 "./arch/arm64/include/asm/insn.h"
/*
 * ARM Architecture Reference Manual for ARMv8 Profile-A, Issue A.a
 * Section C3.1 "A64 instruction index by encoding":
 * AArch64 main encoding table
 *  Bit position
 *   28 27 26 25	Encoding Group
 *   0  0  -  -		Unallocated
 *   1  0  0  -		Data processing, immediate
 *   1  0  1  -		Branch, exception generation and system instructions
 *   -  1  -  0		Loads and stores
 *   -  1  0  1		Data processing - register
 *   0  1  1  1		Data processing - SIMD and floating point
 *   1  1  1  1		Data processing - SIMD and floating point
 * "-" means "don't care"
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_class_branch_sys(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_25(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x1c000000) & (0x14000000)"))); if (!(!(~(0x1c000000) & (0x14000000)))) __compiletime_assert_25(); } while (0); return (code & (0x1c000000)) == (0x14000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_class_branch_sys_value(void) { return (0x14000000); }
# 321 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_adr(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_26(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x9F000000) & (0x10000000)"))); if (!(!(~(0x9F000000) & (0x10000000)))) __compiletime_assert_26(); } while (0); return (code & (0x9F000000)) == (0x10000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_adr_value(void) { return (0x10000000); }
# 322 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_adrp(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_27(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x9F000000) & (0x90000000)"))); if (!(!(~(0x9F000000) & (0x90000000)))) __compiletime_assert_27(); } while (0); return (code & (0x9F000000)) == (0x90000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_adrp_value(void) { return (0x90000000); }
# 323 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_prfm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_28(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FC00000) & (0x39800000)"))); if (!(!(~(0x3FC00000) & (0x39800000)))) __compiletime_assert_28(); } while (0); return (code & (0x3FC00000)) == (0x39800000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_prfm_value(void) { return (0x39800000); }
# 324 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_prfm_lit(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_29(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFF000000) & (0xD8000000)"))); if (!(!(~(0xFF000000) & (0xD8000000)))) __compiletime_assert_29(); } while (0); return (code & (0xFF000000)) == (0xD8000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_prfm_lit_value(void) { return (0xD8000000); }
# 325 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_store_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_30(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FC00000) & (0x39000000)"))); if (!(!(~(0x3FC00000) & (0x39000000)))) __compiletime_assert_30(); } while (0); return (code & (0x3FC00000)) == (0x39000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_store_imm_value(void) { return (0x39000000); }
# 326 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_load_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_31(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FC00000) & (0x39400000)"))); if (!(!(~(0x3FC00000) & (0x39400000)))) __compiletime_assert_31(); } while (0); return (code & (0x3FC00000)) == (0x39400000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_load_imm_value(void) { return (0x39400000); }
# 327 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_store_pre(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_32(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FE00C00) & (0x38000C00)"))); if (!(!(~(0x3FE00C00) & (0x38000C00)))) __compiletime_assert_32(); } while (0); return (code & (0x3FE00C00)) == (0x38000C00); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_store_pre_value(void) { return (0x38000C00); }
# 328 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_load_pre(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_33(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FE00C00) & (0x38400C00)"))); if (!(!(~(0x3FE00C00) & (0x38400C00)))) __compiletime_assert_33(); } while (0); return (code & (0x3FE00C00)) == (0x38400C00); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_load_pre_value(void) { return (0x38400C00); }
# 329 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_store_post(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_34(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FE00C00) & (0x38000400)"))); if (!(!(~(0x3FE00C00) & (0x38000400)))) __compiletime_assert_34(); } while (0); return (code & (0x3FE00C00)) == (0x38000400); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_store_post_value(void) { return (0x38000400); }
# 330 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_load_post(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_35(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FE00C00) & (0x38400400)"))); if (!(!(~(0x3FE00C00) & (0x38400400)))) __compiletime_assert_35(); } while (0); return (code & (0x3FE00C00)) == (0x38400400); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_load_post_value(void) { return (0x38400400); }
# 331 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_str_reg(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_36(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FE0EC00) & (0x38206800)"))); if (!(!(~(0x3FE0EC00) & (0x38206800)))) __compiletime_assert_36(); } while (0); return (code & (0x3FE0EC00)) == (0x38206800); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_str_reg_value(void) { return (0x38206800); }
# 332 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_str_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_37(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FC00000) & (0x39000000)"))); if (!(!(~(0x3FC00000) & (0x39000000)))) __compiletime_assert_37(); } while (0); return (code & (0x3FC00000)) == (0x39000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_str_imm_value(void) { return (0x39000000); }
# 333 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldadd(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_38(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F20FC00) & (0x38200000)"))); if (!(!(~(0x3F20FC00) & (0x38200000)))) __compiletime_assert_38(); } while (0); return (code & (0x3F20FC00)) == (0x38200000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldadd_value(void) { return (0x38200000); }
# 334 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldclr(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_39(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F20FC00) & (0x38201000)"))); if (!(!(~(0x3F20FC00) & (0x38201000)))) __compiletime_assert_39(); } while (0); return (code & (0x3F20FC00)) == (0x38201000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldclr_value(void) { return (0x38201000); }
# 335 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldeor(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_40(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F20FC00) & (0x38202000)"))); if (!(!(~(0x3F20FC00) & (0x38202000)))) __compiletime_assert_40(); } while (0); return (code & (0x3F20FC00)) == (0x38202000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldeor_value(void) { return (0x38202000); }
# 336 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldset(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_41(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F20FC00) & (0x38203000)"))); if (!(!(~(0x3F20FC00) & (0x38203000)))) __compiletime_assert_41(); } while (0); return (code & (0x3F20FC00)) == (0x38203000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldset_value(void) { return (0x38203000); }
# 337 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_swp(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_42(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F20FC00) & (0x38208000)"))); if (!(!(~(0x3F20FC00) & (0x38208000)))) __compiletime_assert_42(); } while (0); return (code & (0x3F20FC00)) == (0x38208000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_swp_value(void) { return (0x38208000); }
# 338 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_cas(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_43(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FA07C00) & (0x08A07C00)"))); if (!(!(~(0x3FA07C00) & (0x08A07C00)))) __compiletime_assert_43(); } while (0); return (code & (0x3FA07C00)) == (0x08A07C00); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_cas_value(void) { return (0x08A07C00); }
# 339 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldr_reg(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_44(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FE0EC00) & (0x38606800)"))); if (!(!(~(0x3FE0EC00) & (0x38606800)))) __compiletime_assert_44(); } while (0); return (code & (0x3FE0EC00)) == (0x38606800); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldr_reg_value(void) { return (0x38606800); }
# 340 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldr_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_45(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3FC00000) & (0x39400000)"))); if (!(!(~(0x3FC00000) & (0x39400000)))) __compiletime_assert_45(); } while (0); return (code & (0x3FC00000)) == (0x39400000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldr_imm_value(void) { return (0x39400000); }
# 341 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldr_lit(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_46(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xBF000000) & (0x18000000)"))); if (!(!(~(0xBF000000) & (0x18000000)))) __compiletime_assert_46(); } while (0); return (code & (0xBF000000)) == (0x18000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldr_lit_value(void) { return (0x18000000); }
# 342 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldrsw_lit(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_47(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFF000000) & (0x98000000)"))); if (!(!(~(0xFF000000) & (0x98000000)))) __compiletime_assert_47(); } while (0); return (code & (0xFF000000)) == (0x98000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldrsw_lit_value(void) { return (0x98000000); }
# 343 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_exclusive(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_48(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F800000) & (0x08000000)"))); if (!(!(~(0x3F800000) & (0x08000000)))) __compiletime_assert_48(); } while (0); return (code & (0x3F800000)) == (0x08000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_exclusive_value(void) { return (0x08000000); }
# 344 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_load_ex(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_49(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F400000) & (0x08400000)"))); if (!(!(~(0x3F400000) & (0x08400000)))) __compiletime_assert_49(); } while (0); return (code & (0x3F400000)) == (0x08400000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_load_ex_value(void) { return (0x08400000); }
# 345 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_store_ex(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_50(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x3F400000) & (0x08000000)"))); if (!(!(~(0x3F400000) & (0x08000000)))) __compiletime_assert_50(); } while (0); return (code & (0x3F400000)) == (0x08000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_store_ex_value(void) { return (0x08000000); }
# 346 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_stp(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_51(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FC00000) & (0x29000000)"))); if (!(!(~(0x7FC00000) & (0x29000000)))) __compiletime_assert_51(); } while (0); return (code & (0x7FC00000)) == (0x29000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_stp_value(void) { return (0x29000000); }
# 347 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldp(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_52(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FC00000) & (0x29400000)"))); if (!(!(~(0x7FC00000) & (0x29400000)))) __compiletime_assert_52(); } while (0); return (code & (0x7FC00000)) == (0x29400000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldp_value(void) { return (0x29400000); }
# 348 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_stp_post(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_53(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FC00000) & (0x28800000)"))); if (!(!(~(0x7FC00000) & (0x28800000)))) __compiletime_assert_53(); } while (0); return (code & (0x7FC00000)) == (0x28800000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_stp_post_value(void) { return (0x28800000); }
# 349 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldp_post(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_54(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FC00000) & (0x28C00000)"))); if (!(!(~(0x7FC00000) & (0x28C00000)))) __compiletime_assert_54(); } while (0); return (code & (0x7FC00000)) == (0x28C00000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldp_post_value(void) { return (0x28C00000); }
# 350 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_stp_pre(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_55(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FC00000) & (0x29800000)"))); if (!(!(~(0x7FC00000) & (0x29800000)))) __compiletime_assert_55(); } while (0); return (code & (0x7FC00000)) == (0x29800000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_stp_pre_value(void) { return (0x29800000); }
# 351 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ldp_pre(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_56(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FC00000) & (0x29C00000)"))); if (!(!(~(0x7FC00000) & (0x29C00000)))) __compiletime_assert_56(); } while (0); return (code & (0x7FC00000)) == (0x29C00000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ldp_pre_value(void) { return (0x29C00000); }
# 352 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_add_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_57(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x11000000)"))); if (!(!(~(0x7F000000) & (0x11000000)))) __compiletime_assert_57(); } while (0); return (code & (0x7F000000)) == (0x11000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_add_imm_value(void) { return (0x11000000); }
# 353 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_adds_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_58(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x31000000)"))); if (!(!(~(0x7F000000) & (0x31000000)))) __compiletime_assert_58(); } while (0); return (code & (0x7F000000)) == (0x31000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_adds_imm_value(void) { return (0x31000000); }
# 354 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_sub_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_59(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x51000000)"))); if (!(!(~(0x7F000000) & (0x51000000)))) __compiletime_assert_59(); } while (0); return (code & (0x7F000000)) == (0x51000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_sub_imm_value(void) { return (0x51000000); }
# 355 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_subs_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_60(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x71000000)"))); if (!(!(~(0x7F000000) & (0x71000000)))) __compiletime_assert_60(); } while (0); return (code & (0x7F000000)) == (0x71000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_subs_imm_value(void) { return (0x71000000); }
# 356 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_movn(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_61(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x12800000)"))); if (!(!(~(0x7F800000) & (0x12800000)))) __compiletime_assert_61(); } while (0); return (code & (0x7F800000)) == (0x12800000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_movn_value(void) { return (0x12800000); }
# 357 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_sbfm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_62(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x13000000)"))); if (!(!(~(0x7F800000) & (0x13000000)))) __compiletime_assert_62(); } while (0); return (code & (0x7F800000)) == (0x13000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_sbfm_value(void) { return (0x13000000); }
# 358 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_bfm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_63(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x33000000)"))); if (!(!(~(0x7F800000) & (0x33000000)))) __compiletime_assert_63(); } while (0); return (code & (0x7F800000)) == (0x33000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_bfm_value(void) { return (0x33000000); }
# 359 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_movz(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_64(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x52800000)"))); if (!(!(~(0x7F800000) & (0x52800000)))) __compiletime_assert_64(); } while (0); return (code & (0x7F800000)) == (0x52800000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_movz_value(void) { return (0x52800000); }
# 360 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ubfm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_65(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x53000000)"))); if (!(!(~(0x7F800000) & (0x53000000)))) __compiletime_assert_65(); } while (0); return (code & (0x7F800000)) == (0x53000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ubfm_value(void) { return (0x53000000); }
# 361 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_movk(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_66(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x72800000)"))); if (!(!(~(0x7F800000) & (0x72800000)))) __compiletime_assert_66(); } while (0); return (code & (0x7F800000)) == (0x72800000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_movk_value(void) { return (0x72800000); }
# 362 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_add(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_67(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x0B000000)"))); if (!(!(~(0x7F200000) & (0x0B000000)))) __compiletime_assert_67(); } while (0); return (code & (0x7F200000)) == (0x0B000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_add_value(void) { return (0x0B000000); }
# 363 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_adds(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_68(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x2B000000)"))); if (!(!(~(0x7F200000) & (0x2B000000)))) __compiletime_assert_68(); } while (0); return (code & (0x7F200000)) == (0x2B000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_adds_value(void) { return (0x2B000000); }
# 364 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_sub(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_69(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x4B000000)"))); if (!(!(~(0x7F200000) & (0x4B000000)))) __compiletime_assert_69(); } while (0); return (code & (0x7F200000)) == (0x4B000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_sub_value(void) { return (0x4B000000); }
# 365 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_subs(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_70(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x6B000000)"))); if (!(!(~(0x7F200000) & (0x6B000000)))) __compiletime_assert_70(); } while (0); return (code & (0x7F200000)) == (0x6B000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_subs_value(void) { return (0x6B000000); }
# 366 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_madd(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_71(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE08000) & (0x1B000000)"))); if (!(!(~(0x7FE08000) & (0x1B000000)))) __compiletime_assert_71(); } while (0); return (code & (0x7FE08000)) == (0x1B000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_madd_value(void) { return (0x1B000000); }
# 367 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_msub(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_72(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE08000) & (0x1B008000)"))); if (!(!(~(0x7FE08000) & (0x1B008000)))) __compiletime_assert_72(); } while (0); return (code & (0x7FE08000)) == (0x1B008000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_msub_value(void) { return (0x1B008000); }
# 368 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_udiv(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_73(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE0FC00) & (0x1AC00800)"))); if (!(!(~(0x7FE0FC00) & (0x1AC00800)))) __compiletime_assert_73(); } while (0); return (code & (0x7FE0FC00)) == (0x1AC00800); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_udiv_value(void) { return (0x1AC00800); }
# 369 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_sdiv(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_74(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE0FC00) & (0x1AC00C00)"))); if (!(!(~(0x7FE0FC00) & (0x1AC00C00)))) __compiletime_assert_74(); } while (0); return (code & (0x7FE0FC00)) == (0x1AC00C00); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_sdiv_value(void) { return (0x1AC00C00); }
# 370 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_lslv(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_75(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE0FC00) & (0x1AC02000)"))); if (!(!(~(0x7FE0FC00) & (0x1AC02000)))) __compiletime_assert_75(); } while (0); return (code & (0x7FE0FC00)) == (0x1AC02000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_lslv_value(void) { return (0x1AC02000); }
# 371 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_lsrv(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_76(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE0FC00) & (0x1AC02400)"))); if (!(!(~(0x7FE0FC00) & (0x1AC02400)))) __compiletime_assert_76(); } while (0); return (code & (0x7FE0FC00)) == (0x1AC02400); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_lsrv_value(void) { return (0x1AC02400); }
# 372 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_asrv(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_77(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE0FC00) & (0x1AC02800)"))); if (!(!(~(0x7FE0FC00) & (0x1AC02800)))) __compiletime_assert_77(); } while (0); return (code & (0x7FE0FC00)) == (0x1AC02800); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_asrv_value(void) { return (0x1AC02800); }
# 373 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_rorv(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_78(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE0FC00) & (0x1AC02C00)"))); if (!(!(~(0x7FE0FC00) & (0x1AC02C00)))) __compiletime_assert_78(); } while (0); return (code & (0x7FE0FC00)) == (0x1AC02C00); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_rorv_value(void) { return (0x1AC02C00); }
# 374 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_rev16(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_79(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FFFFC00) & (0x5AC00400)"))); if (!(!(~(0x7FFFFC00) & (0x5AC00400)))) __compiletime_assert_79(); } while (0); return (code & (0x7FFFFC00)) == (0x5AC00400); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_rev16_value(void) { return (0x5AC00400); }
# 375 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_rev32(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_80(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FFFFC00) & (0x5AC00800)"))); if (!(!(~(0x7FFFFC00) & (0x5AC00800)))) __compiletime_assert_80(); } while (0); return (code & (0x7FFFFC00)) == (0x5AC00800); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_rev32_value(void) { return (0x5AC00800); }
# 376 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_rev64(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_81(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FFFFC00) & (0x5AC00C00)"))); if (!(!(~(0x7FFFFC00) & (0x5AC00C00)))) __compiletime_assert_81(); } while (0); return (code & (0x7FFFFC00)) == (0x5AC00C00); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_rev64_value(void) { return (0x5AC00C00); }
# 377 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_and(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_82(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x0A000000)"))); if (!(!(~(0x7F200000) & (0x0A000000)))) __compiletime_assert_82(); } while (0); return (code & (0x7F200000)) == (0x0A000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_and_value(void) { return (0x0A000000); }
# 378 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_bic(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_83(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x0A200000)"))); if (!(!(~(0x7F200000) & (0x0A200000)))) __compiletime_assert_83(); } while (0); return (code & (0x7F200000)) == (0x0A200000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_bic_value(void) { return (0x0A200000); }
# 379 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_orr(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_84(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x2A000000)"))); if (!(!(~(0x7F200000) & (0x2A000000)))) __compiletime_assert_84(); } while (0); return (code & (0x7F200000)) == (0x2A000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_orr_value(void) { return (0x2A000000); }
# 380 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_mov_reg(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_85(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FE0FFE0) & (0x2A0003E0)"))); if (!(!(~(0x7FE0FFE0) & (0x2A0003E0)))) __compiletime_assert_85(); } while (0); return (code & (0x7FE0FFE0)) == (0x2A0003E0); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_mov_reg_value(void) { return (0x2A0003E0); }
# 381 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_orn(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_86(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x2A200000)"))); if (!(!(~(0x7F200000) & (0x2A200000)))) __compiletime_assert_86(); } while (0); return (code & (0x7F200000)) == (0x2A200000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_orn_value(void) { return (0x2A200000); }
# 382 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_eor(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_87(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x4A000000)"))); if (!(!(~(0x7F200000) & (0x4A000000)))) __compiletime_assert_87(); } while (0); return (code & (0x7F200000)) == (0x4A000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_eor_value(void) { return (0x4A000000); }
# 383 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_eon(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_88(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x4A200000)"))); if (!(!(~(0x7F200000) & (0x4A200000)))) __compiletime_assert_88(); } while (0); return (code & (0x7F200000)) == (0x4A200000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_eon_value(void) { return (0x4A200000); }
# 384 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ands(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_89(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x6A000000)"))); if (!(!(~(0x7F200000) & (0x6A000000)))) __compiletime_assert_89(); } while (0); return (code & (0x7F200000)) == (0x6A000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ands_value(void) { return (0x6A000000); }
# 385 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_bics(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_90(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F200000) & (0x6A200000)"))); if (!(!(~(0x7F200000) & (0x6A200000)))) __compiletime_assert_90(); } while (0); return (code & (0x7F200000)) == (0x6A200000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_bics_value(void) { return (0x6A200000); }
# 386 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_and_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_91(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x12000000)"))); if (!(!(~(0x7F800000) & (0x12000000)))) __compiletime_assert_91(); } while (0); return (code & (0x7F800000)) == (0x12000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_and_imm_value(void) { return (0x12000000); }
# 387 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_orr_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_92(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x32000000)"))); if (!(!(~(0x7F800000) & (0x32000000)))) __compiletime_assert_92(); } while (0); return (code & (0x7F800000)) == (0x32000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_orr_imm_value(void) { return (0x32000000); }
# 388 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_eor_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_93(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x52000000)"))); if (!(!(~(0x7F800000) & (0x52000000)))) __compiletime_assert_93(); } while (0); return (code & (0x7F800000)) == (0x52000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_eor_imm_value(void) { return (0x52000000); }
# 389 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ands_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_94(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F800000) & (0x72000000)"))); if (!(!(~(0x7F800000) & (0x72000000)))) __compiletime_assert_94(); } while (0); return (code & (0x7F800000)) == (0x72000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ands_imm_value(void) { return (0x72000000); }
# 390 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_extr(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_95(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7FA00000) & (0x13800000)"))); if (!(!(~(0x7FA00000) & (0x13800000)))) __compiletime_assert_95(); } while (0); return (code & (0x7FA00000)) == (0x13800000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_extr_value(void) { return (0x13800000); }
# 391 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_b(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_96(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFC000000) & (0x14000000)"))); if (!(!(~(0xFC000000) & (0x14000000)))) __compiletime_assert_96(); } while (0); return (code & (0xFC000000)) == (0x14000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_b_value(void) { return (0x14000000); }
# 392 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_bl(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_97(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFC000000) & (0x94000000)"))); if (!(!(~(0xFC000000) & (0x94000000)))) __compiletime_assert_97(); } while (0); return (code & (0xFC000000)) == (0x94000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_bl_value(void) { return (0x94000000); }
# 393 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_cbz(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_98(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x34000000)"))); if (!(!(~(0x7F000000) & (0x34000000)))) __compiletime_assert_98(); } while (0); return (code & (0x7F000000)) == (0x34000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_cbz_value(void) { return (0x34000000); }
# 394 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_cbnz(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_99(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x35000000)"))); if (!(!(~(0x7F000000) & (0x35000000)))) __compiletime_assert_99(); } while (0); return (code & (0x7F000000)) == (0x35000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_cbnz_value(void) { return (0x35000000); }
# 395 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_tbz(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_100(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x36000000)"))); if (!(!(~(0x7F000000) & (0x36000000)))) __compiletime_assert_100(); } while (0); return (code & (0x7F000000)) == (0x36000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_tbz_value(void) { return (0x36000000); }
# 396 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_tbnz(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_101(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0x7F000000) & (0x37000000)"))); if (!(!(~(0x7F000000) & (0x37000000)))) __compiletime_assert_101(); } while (0); return (code & (0x7F000000)) == (0x37000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_tbnz_value(void) { return (0x37000000); }
# 397 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_bcond(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_102(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFF000010) & (0x54000000)"))); if (!(!(~(0xFF000010) & (0x54000000)))) __compiletime_assert_102(); } while (0); return (code & (0xFF000010)) == (0x54000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_bcond_value(void) { return (0x54000000); }
# 398 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_svc(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_103(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFE0001F) & (0xD4000001)"))); if (!(!(~(0xFFE0001F) & (0xD4000001)))) __compiletime_assert_103(); } while (0); return (code & (0xFFE0001F)) == (0xD4000001); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_svc_value(void) { return (0xD4000001); }
# 399 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_hvc(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_104(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFE0001F) & (0xD4000002)"))); if (!(!(~(0xFFE0001F) & (0xD4000002)))) __compiletime_assert_104(); } while (0); return (code & (0xFFE0001F)) == (0xD4000002); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_hvc_value(void) { return (0xD4000002); }
# 400 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_smc(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_105(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFE0001F) & (0xD4000003)"))); if (!(!(~(0xFFE0001F) & (0xD4000003)))) __compiletime_assert_105(); } while (0); return (code & (0xFFE0001F)) == (0xD4000003); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_smc_value(void) { return (0xD4000003); }
# 401 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_brk(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_106(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFE0001F) & (0xD4200000)"))); if (!(!(~(0xFFE0001F) & (0xD4200000)))) __compiletime_assert_106(); } while (0); return (code & (0xFFE0001F)) == (0xD4200000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_brk_value(void) { return (0xD4200000); }
# 402 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_exception(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_107(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFF000000) & (0xD4000000)"))); if (!(!(~(0xFF000000) & (0xD4000000)))) __compiletime_assert_107(); } while (0); return (code & (0xFF000000)) == (0xD4000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_exception_value(void) { return (0xD4000000); }
# 403 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_hint(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_108(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFF01F) & (0xD503201F)"))); if (!(!(~(0xFFFFF01F) & (0xD503201F)))) __compiletime_assert_108(); } while (0); return (code & (0xFFFFF01F)) == (0xD503201F); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_hint_value(void) { return (0xD503201F); }
# 404 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_br(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_109(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFC1F) & (0xD61F0000)"))); if (!(!(~(0xFFFFFC1F) & (0xD61F0000)))) __compiletime_assert_109(); } while (0); return (code & (0xFFFFFC1F)) == (0xD61F0000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_br_value(void) { return (0xD61F0000); }
# 405 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_br_auth(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_110(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFEFFF800) & (0xD61F0800)"))); if (!(!(~(0xFEFFF800) & (0xD61F0800)))) __compiletime_assert_110(); } while (0); return (code & (0xFEFFF800)) == (0xD61F0800); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_br_auth_value(void) { return (0xD61F0800); }
# 406 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_blr(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_111(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFC1F) & (0xD63F0000)"))); if (!(!(~(0xFFFFFC1F) & (0xD63F0000)))) __compiletime_assert_111(); } while (0); return (code & (0xFFFFFC1F)) == (0xD63F0000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_blr_value(void) { return (0xD63F0000); }
# 407 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_blr_auth(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_112(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFEFFF800) & (0xD63F0800)"))); if (!(!(~(0xFEFFF800) & (0xD63F0800)))) __compiletime_assert_112(); } while (0); return (code & (0xFEFFF800)) == (0xD63F0800); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_blr_auth_value(void) { return (0xD63F0800); }
# 408 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ret(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_113(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFC1F) & (0xD65F0000)"))); if (!(!(~(0xFFFFFC1F) & (0xD65F0000)))) __compiletime_assert_113(); } while (0); return (code & (0xFFFFFC1F)) == (0xD65F0000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ret_value(void) { return (0xD65F0000); }
# 409 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ret_auth(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_114(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFBFF) & (0xD65F0BFF)"))); if (!(!(~(0xFFFFFBFF) & (0xD65F0BFF)))) __compiletime_assert_114(); } while (0); return (code & (0xFFFFFBFF)) == (0xD65F0BFF); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ret_auth_value(void) { return (0xD65F0BFF); }
# 410 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_eret(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_115(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFFFF) & (0xD69F03E0)"))); if (!(!(~(0xFFFFFFFF) & (0xD69F03E0)))) __compiletime_assert_115(); } while (0); return (code & (0xFFFFFFFF)) == (0xD69F03E0); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_eret_value(void) { return (0xD69F03E0); }
# 411 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_eret_auth(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_116(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFBFF) & (0xD69F0BFF)"))); if (!(!(~(0xFFFFFBFF) & (0xD69F0BFF)))) __compiletime_assert_116(); } while (0); return (code & (0xFFFFFBFF)) == (0xD69F0BFF); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_eret_auth_value(void) { return (0xD69F0BFF); }
# 412 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_mrs(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_117(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFF00000) & (0xD5300000)"))); if (!(!(~(0xFFF00000) & (0xD5300000)))) __compiletime_assert_117(); } while (0); return (code & (0xFFF00000)) == (0xD5300000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_mrs_value(void) { return (0xD5300000); }
# 413 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_msr_imm(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_118(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFF8F01F) & (0xD500401F)"))); if (!(!(~(0xFFF8F01F) & (0xD500401F)))) __compiletime_assert_118(); } while (0); return (code & (0xFFF8F01F)) == (0xD500401F); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_msr_imm_value(void) { return (0xD500401F); }
# 414 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_msr_reg(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_119(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFF00000) & (0xD5100000)"))); if (!(!(~(0xFFF00000) & (0xD5100000)))) __compiletime_assert_119(); } while (0); return (code & (0xFFF00000)) == (0xD5100000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_msr_reg_value(void) { return (0xD5100000); }
# 415 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_dmb(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_120(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFF0FF) & (0xD50330BF)"))); if (!(!(~(0xFFFFF0FF) & (0xD50330BF)))) __compiletime_assert_120(); } while (0); return (code & (0xFFFFF0FF)) == (0xD50330BF); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_dmb_value(void) { return (0xD50330BF); }
# 416 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_dsb_base(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_121(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFF0FF) & (0xD503309F)"))); if (!(!(~(0xFFFFF0FF) & (0xD503309F)))) __compiletime_assert_121(); } while (0); return (code & (0xFFFFF0FF)) == (0xD503309F); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_dsb_base_value(void) { return (0xD503309F); }
# 417 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_dsb_nxs(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_122(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFF3FF) & (0xD503323F)"))); if (!(!(~(0xFFFFF3FF) & (0xD503323F)))) __compiletime_assert_122(); } while (0); return (code & (0xFFFFF3FF)) == (0xD503323F); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_dsb_nxs_value(void) { return (0xD503323F); }
# 418 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_isb(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_123(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFF0FF) & (0xD50330DF)"))); if (!(!(~(0xFFFFF0FF) & (0xD50330DF)))) __compiletime_assert_123(); } while (0); return (code & (0xFFFFF0FF)) == (0xD50330DF); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_isb_value(void) { return (0xD50330DF); }
# 419 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_sb(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_124(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFFFF) & (0xD50330FF)"))); if (!(!(~(0xFFFFFFFF) & (0xD50330FF)))) __compiletime_assert_124(); } while (0); return (code & (0xFFFFFFFF)) == (0xD50330FF); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_sb_value(void) { return (0xD50330FF); }
# 420 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_clrex(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_125(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFF0FF) & (0xD503305F)"))); if (!(!(~(0xFFFFF0FF) & (0xD503305F)))) __compiletime_assert_125(); } while (0); return (code & (0xFFFFF0FF)) == (0xD503305F); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_clrex_value(void) { return (0xD503305F); }
# 421 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_ssbb(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_126(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFFFF) & (0xD503309F)"))); if (!(!(~(0xFFFFFFFF) & (0xD503309F)))) __compiletime_assert_126(); } while (0); return (code & (0xFFFFFFFF)) == (0xD503309F); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_ssbb_value(void) { return (0xD503309F); }
# 422 "./arch/arm64/include/asm/insn.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_pssbb(u32 code) { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_127(void) __attribute__((__error__("BUILD_BUG_ON failed: " "~(0xFFFFFFFF) & (0xD503349F)"))); if (!(!(~(0xFFFFFFFF) & (0xD503349F)))) __compiletime_assert_127(); } while (0); return (code & (0xFFFFFFFF)) == (0xD503349F); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_get_pssbb_value(void) { return (0xD503349F); }static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_steppable_hint(u32 insn)
{
 if (!aarch64_insn_is_hint(insn))
  return false;

 switch (insn & 0xFE0) {
 case AARCH64_INSN_HINT_XPACLRI:
 case AARCH64_INSN_HINT_PACIA_1716:
 case AARCH64_INSN_HINT_PACIB_1716:
 case AARCH64_INSN_HINT_PACIAZ:
 case AARCH64_INSN_HINT_PACIASP:
 case AARCH64_INSN_HINT_PACIBZ:
 case AARCH64_INSN_HINT_PACIBSP:
 case AARCH64_INSN_HINT_BTI:
 case AARCH64_INSN_HINT_BTIC:
 case AARCH64_INSN_HINT_BTIJ:
 case AARCH64_INSN_HINT_BTIJC:
 case AARCH64_INSN_HINT_NOP:
  return true;
 default:
  return false;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_branch(u32 insn)
{
 /* b, bl, cb*, tb*, ret*, b.cond, br*, blr* */

 return aarch64_insn_is_b(insn) ||
        aarch64_insn_is_bl(insn) ||
        aarch64_insn_is_cbz(insn) ||
        aarch64_insn_is_cbnz(insn) ||
        aarch64_insn_is_tbz(insn) ||
        aarch64_insn_is_tbnz(insn) ||
        aarch64_insn_is_ret(insn) ||
        aarch64_insn_is_ret_auth(insn) ||
        aarch64_insn_is_br(insn) ||
        aarch64_insn_is_br_auth(insn) ||
        aarch64_insn_is_blr(insn) ||
        aarch64_insn_is_blr_auth(insn) ||
        aarch64_insn_is_bcond(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_branch_imm(u32 insn)
{
 return aarch64_insn_is_b(insn) ||
        aarch64_insn_is_bl(insn) ||
        aarch64_insn_is_tbz(insn) ||
        aarch64_insn_is_tbnz(insn) ||
        aarch64_insn_is_cbz(insn) ||
        aarch64_insn_is_cbnz(insn) ||
        aarch64_insn_is_bcond(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_adr_adrp(u32 insn)
{
 return aarch64_insn_is_adr(insn) ||
        aarch64_insn_is_adrp(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_dsb(u32 insn)
{
 return aarch64_insn_is_dsb_base(insn) ||
        aarch64_insn_is_dsb_nxs(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_barrier(u32 insn)
{
 return aarch64_insn_is_dmb(insn) ||
        aarch64_insn_is_dsb(insn) ||
        aarch64_insn_is_isb(insn) ||
        aarch64_insn_is_sb(insn) ||
        aarch64_insn_is_clrex(insn) ||
        aarch64_insn_is_ssbb(insn) ||
        aarch64_insn_is_pssbb(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_store_single(u32 insn)
{
 return aarch64_insn_is_store_imm(insn) ||
        aarch64_insn_is_store_pre(insn) ||
        aarch64_insn_is_store_post(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_store_pair(u32 insn)
{
 return aarch64_insn_is_stp(insn) ||
        aarch64_insn_is_stp_pre(insn) ||
        aarch64_insn_is_stp_post(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_load_single(u32 insn)
{
 return aarch64_insn_is_load_imm(insn) ||
        aarch64_insn_is_load_pre(insn) ||
        aarch64_insn_is_load_post(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_is_load_pair(u32 insn)
{
 return aarch64_insn_is_ldp(insn) ||
        aarch64_insn_is_ldp_pre(insn) ||
        aarch64_insn_is_ldp_post(insn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool aarch64_insn_uses_literal(u32 insn)
{
 /* ldr/ldrsw (literal), prfm */

 return aarch64_insn_is_ldr_lit(insn) ||
        aarch64_insn_is_ldrsw_lit(insn) ||
        aarch64_insn_is_adr_adrp(insn) ||
        aarch64_insn_is_prfm_lit(insn);
}

enum aarch64_insn_encoding_class aarch64_get_insn_class(u32 insn);
u64 aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn);
u32 aarch64_insn_encode_immediate(enum aarch64_insn_imm_type type,
      u32 insn, u64 imm);
u32 aarch64_insn_decode_register(enum aarch64_insn_register_type type,
      u32 insn);
u32 aarch64_insn_gen_branch_imm(unsigned long pc, unsigned long addr,
    enum aarch64_insn_branch_type type);
u32 aarch64_insn_gen_comp_branch_imm(unsigned long pc, unsigned long addr,
         enum aarch64_insn_register reg,
         enum aarch64_insn_variant variant,
         enum aarch64_insn_branch_type type);
u32 aarch64_insn_gen_cond_branch_imm(unsigned long pc, unsigned long addr,
         enum aarch64_insn_condition cond);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32
aarch64_insn_gen_hint(enum aarch64_insn_hint_cr_op op)
{
 return aarch64_insn_get_hint_value() | op;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 aarch64_insn_gen_nop(void)
{
 return aarch64_insn_gen_hint(AARCH64_INSN_HINT_NOP);
}

u32 aarch64_insn_gen_branch_reg(enum aarch64_insn_register reg,
    enum aarch64_insn_branch_type type);
u32 aarch64_insn_gen_load_store_reg(enum aarch64_insn_register reg,
        enum aarch64_insn_register base,
        enum aarch64_insn_register offset,
        enum aarch64_insn_size_type size,
        enum aarch64_insn_ldst_type type);
u32 aarch64_insn_gen_load_store_imm(enum aarch64_insn_register reg,
        enum aarch64_insn_register base,
        unsigned int imm,
        enum aarch64_insn_size_type size,
        enum aarch64_insn_ldst_type type);
u32 aarch64_insn_gen_load_literal(unsigned long pc, unsigned long addr,
      enum aarch64_insn_register reg,
      bool is64bit);
u32 aarch64_insn_gen_load_store_pair(enum aarch64_insn_register reg1,
         enum aarch64_insn_register reg2,
         enum aarch64_insn_register base,
         int offset,
         enum aarch64_insn_variant variant,
         enum aarch64_insn_ldst_type type);
u32 aarch64_insn_gen_load_store_ex(enum aarch64_insn_register reg,
       enum aarch64_insn_register base,
       enum aarch64_insn_register state,
       enum aarch64_insn_size_type size,
       enum aarch64_insn_ldst_type type);
u32 aarch64_insn_gen_add_sub_imm(enum aarch64_insn_register dst,
     enum aarch64_insn_register src,
     int imm, enum aarch64_insn_variant variant,
     enum aarch64_insn_adsb_type type);
u32 aarch64_insn_gen_adr(unsigned long pc, unsigned long addr,
    enum aarch64_insn_register reg,
    enum aarch64_insn_adr_type type);
u32 aarch64_insn_gen_bitfield(enum aarch64_insn_register dst,
         enum aarch64_insn_register src,
         int immr, int imms,
         enum aarch64_insn_variant variant,
         enum aarch64_insn_bitfield_type type);
u32 aarch64_insn_gen_movewide(enum aarch64_insn_register dst,
         int imm, int shift,
         enum aarch64_insn_variant variant,
         enum aarch64_insn_movewide_type type);
u32 aarch64_insn_gen_add_sub_shifted_reg(enum aarch64_insn_register dst,
      enum aarch64_insn_register src,
      enum aarch64_insn_register reg,
      int shift,
      enum aarch64_insn_variant variant,
      enum aarch64_insn_adsb_type type);
u32 aarch64_insn_gen_data1(enum aarch64_insn_register dst,
      enum aarch64_insn_register src,
      enum aarch64_insn_variant variant,
      enum aarch64_insn_data1_type type);
u32 aarch64_insn_gen_data2(enum aarch64_insn_register dst,
      enum aarch64_insn_register src,
      enum aarch64_insn_register reg,
      enum aarch64_insn_variant variant,
      enum aarch64_insn_data2_type type);
u32 aarch64_insn_gen_data3(enum aarch64_insn_register dst,
      enum aarch64_insn_register src,
      enum aarch64_insn_register reg1,
      enum aarch64_insn_register reg2,
      enum aarch64_insn_variant variant,
      enum aarch64_insn_data3_type type);
u32 aarch64_insn_gen_logical_shifted_reg(enum aarch64_insn_register dst,
      enum aarch64_insn_register src,
      enum aarch64_insn_register reg,
      int shift,
      enum aarch64_insn_variant variant,
      enum aarch64_insn_logic_type type);
u32 aarch64_insn_gen_move_reg(enum aarch64_insn_register dst,
         enum aarch64_insn_register src,
         enum aarch64_insn_variant variant);
u32 aarch64_insn_gen_logical_immediate(enum aarch64_insn_logic_type type,
           enum aarch64_insn_variant variant,
           enum aarch64_insn_register Rn,
           enum aarch64_insn_register Rd,
           u64 imm);
u32 aarch64_insn_gen_extr(enum aarch64_insn_variant variant,
     enum aarch64_insn_register Rm,
     enum aarch64_insn_register Rn,
     enum aarch64_insn_register Rd,
     u8 lsb);

u32 aarch64_insn_gen_atomic_ld_op(enum aarch64_insn_register result,
      enum aarch64_insn_register address,
      enum aarch64_insn_register value,
      enum aarch64_insn_size_type size,
      enum aarch64_insn_mem_atomic_op op,
      enum aarch64_insn_mem_order_type order);
u32 aarch64_insn_gen_cas(enum aarch64_insn_register result,
    enum aarch64_insn_register address,
    enum aarch64_insn_register value,
    enum aarch64_insn_size_type size,
    enum aarch64_insn_mem_order_type order);
# 683 "./arch/arm64/include/asm/insn.h"
u32 aarch64_insn_gen_dmb(enum aarch64_insn_mb_type type);

s32 aarch64_get_branch_offset(u32 insn);
u32 aarch64_set_branch_offset(u32 insn, s32 offset);

s32 aarch64_insn_adrp_get_offset(u32 insn);
u32 aarch64_insn_adrp_set_offset(u32 insn, s32 offset);

bool aarch32_insn_is_wide(u32 insn);





u32 aarch64_insn_extract_system_reg(u32 insn);
u32 aarch32_insn_extract_reg_num(u32 insn, int offset);
u32 aarch32_insn_mcr_extract_opc2(u32 insn);
u32 aarch32_insn_mcr_extract_crm(u32 insn);

typedef bool (pstate_check_t)(unsigned long);
extern pstate_check_t * const aarch32_opcode_cond_checks[16];
# 15 "./arch/arm64/include/asm/jump_label.h" 2



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool arch_static_branch(struct static_key * const key,
            const bool branch)
{
 asm goto("1:	nop					\n\t" "	.pushsection	__jump_table, \"aw\"	\n\t" "	.align		3			\n\t" "	.long		1b - ., %l[l_yes] - .	\n\t" "	.quad		%c0 - .			\n\t" "	.popsection				\n\t" : : "i"(&((char *)key)[branch]) : : l_yes);
# 30 "./arch/arm64/include/asm/jump_label.h"
 return false;
l_yes:
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool arch_static_branch_jump(struct static_key * const key,
          const bool branch)
{
 asm goto("1:	b		%l[l_yes]		\n\t" "	.pushsection	__jump_table, \"aw\"	\n\t" "	.align		3			\n\t" "	.long		1b - ., %l[l_yes] - .	\n\t" "	.quad		%c0 - .			\n\t" "	.popsection				\n\t" : : "i"(&((char *)key)[branch]) : : l_yes);
# 47 "./arch/arm64/include/asm/jump_label.h"
 return false;
l_yes:
 return true;
}
# 113 "./include/linux/jump_label.h" 2




struct jump_entry {
 s32 code;
 s32 target;
 long key; // key may be far away from the core kernel under KASLR
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long jump_entry_code(const struct jump_entry *entry)
{
 return (unsigned long)&entry->code + entry->code;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long jump_entry_target(const struct jump_entry *entry)
{
 return (unsigned long)&entry->target + entry->target;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct static_key *jump_entry_key(const struct jump_entry *entry)
{
 long offset = entry->key & ~3L;

 return (struct static_key *)((unsigned long)&entry->key + offset);
}
# 159 "./include/linux/jump_label.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool jump_entry_is_branch(const struct jump_entry *entry)
{
 return (unsigned long)entry->key & 1UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool jump_entry_is_init(const struct jump_entry *entry)
{
 return (unsigned long)entry->key & 2UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void jump_entry_set_init(struct jump_entry *entry, bool set)
{
 if (set)
  entry->key |= 2;
 else
  entry->key &= ~2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int jump_entry_size(struct jump_entry *entry)
{

 return 4;



}






enum jump_label_type {
 JUMP_LABEL_NOP = 0,
 JUMP_LABEL_JMP,
};

struct module;
# 205 "./include/linux/jump_label.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool static_key_false(struct static_key *key)
{
 return arch_static_branch(key, false);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool static_key_true(struct static_key *key)
{
 return !arch_static_branch(key, true);
}

extern struct jump_entry __start___jump_table[];
extern struct jump_entry __stop___jump_table[];

extern void jump_label_init(void);
extern void jump_label_lock(void);
extern void jump_label_unlock(void);
extern void arch_jump_label_transform(struct jump_entry *entry,
          enum jump_label_type type);
extern bool arch_jump_label_transform_queue(struct jump_entry *entry,
         enum jump_label_type type);
extern void arch_jump_label_transform_apply(void);
extern int jump_label_text_reserved(void *start, void *end);
extern bool static_key_slow_inc(struct static_key *key);
extern bool static_key_fast_inc_not_disabled(struct static_key *key);
extern void static_key_slow_dec(struct static_key *key);
extern bool static_key_slow_inc_cpuslocked(struct static_key *key);
extern void static_key_slow_dec_cpuslocked(struct static_key *key);
extern int static_key_count(struct static_key *key);
extern void static_key_enable(struct static_key *key);
extern void static_key_disable(struct static_key *key);
extern void static_key_enable_cpuslocked(struct static_key *key);
extern void static_key_disable_cpuslocked(struct static_key *key);
extern enum jump_label_type jump_label_init_type(struct jump_entry *entry);

/*
 * We should be using ATOMIC_INIT() for initializing .enabled, but
 * the inclusion of atomic.h is problematic for inclusion of jump_label.h
 * in 'low-level' headers. Thus, we are initializing .enabled with a
 * raw value, but have added a BUILD_BUG_ON() to catch any issues in
 * jump_label_init() see: kernel/jump_label.c.
 */
# 350 "./include/linux/jump_label.h"
/* -------------------------------------------------------------------------- */

/*
 * Two type wrappers around static_key, such that we can use compile time
 * type differentiation to emit the right code.
 *
 * All the below code is macros in order to play type games.
 */

struct static_key_true {
 struct static_key key;
};

struct static_key_false {
 struct static_key key;
};
# 413 "./include/linux/jump_label.h"
extern bool ____wrong_branch_error(void);
# 426 "./include/linux/jump_label.h"
/*
 * Combine the right initial value (type) with the right branch order
 * to generate the desired result.
 *
 *
 * type\branch|	likely (1)	      |	unlikely (0)
 * -----------+-----------------------+------------------
 *            |                       |
 *  true (1)  |	   ...		      |	   ...
 *            |    NOP		      |	   JMP L
 *            |    <br-stmts>	      |	1: ...
 *            |	L: ...		      |
 *            |			      |
 *            |			      |	L: <br-stmts>
 *            |			      |	   jmp 1b
 *            |                       |
 * -----------+-----------------------+------------------
 *            |                       |
 *  false (0) |	   ...		      |	   ...
 *            |    JMP L	      |	   NOP
 *            |    <br-stmts>	      |	1: ...
 *            |	L: ...		      |
 *            |			      |
 *            |			      |	L: <br-stmts>
 *            |			      |	   jmp 1b
 *            |                       |
 * -----------+-----------------------+------------------
 *
 * The initial value is encoded in the LSB of static_key::entries,
 * type: 0 = false, 1 = true.
 *
 * The branch type is encoded in the LSB of jump_entry::key,
 * branch: 0 = unlikely, 1 = likely.
 *
 * This gives the following logic table:
 *
 *	enabled	type	branch	  instuction
 * -----------------------------+-----------
 *	0	0	0	| NOP
 *	0	0	1	| JMP
 *	0	1	0	| NOP
 *	0	1	1	| JMP
 *
 *	1	0	0	| JMP
 *	1	0	1	| NOP
 *	1	1	0	| JMP
 *	1	1	1	| NOP
 *
 * Which gives the following functions:
 *
 *   dynamic: instruction = enabled ^ branch
 *   static:  instruction = type ^ branch
 *
 * See jump_label_type() / jump_label_init_type().
 */
# 517 "./include/linux/jump_label.h"
/*
 * Advanced usage; refcount, branch is enabled when: count != 0
 */






/*
 * Normal usage; boolean enable/disable.
 */
# 6 "./arch/arm64/include/asm/preempt.h" 2
# 1 "./include/linux/thread_info.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/* thread_info.h: common low-level thread information accessors
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds
 */





# 1 "./include/linux/limits.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/limits.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 6 "./include/linux/limits.h" 2

# 1 "./include/vdso/limits.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 8 "./include/linux/limits.h" 2
# 13 "./include/linux/thread_info.h" 2
# 1 "./include/linux/bug.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/asm/bug.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2015  ARM Limited
 * Author: Dave Martin <Dave.Martin@arm.com>
 */






# 1 "./arch/arm64/include/asm/asm-bug.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */

/*
 * Copyright (C) 2017  ARM Limited
 */
# 13 "./arch/arm64/include/asm/bug.h" 2
# 26 "./arch/arm64/include/asm/bug.h"
# 1 "./include/asm-generic/bug.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/instrumentation.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 7 "./include/asm-generic/bug.h" 2
# 1 "./include/linux/once_lite.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/* Call a function once. Similar to DO_ONCE(), but does not use jump label
 * patching via static keys.
 */
# 8 "./include/asm-generic/bug.h" 2
# 21 "./include/asm-generic/bug.h"
# 1 "./include/linux/panic.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






struct pt_regs;

extern long (*panic_blink)(int state);
__attribute__((__format__(printf, 1, 2)))
void panic(const char *fmt, ...) __attribute__((__noreturn__)) __attribute__((__cold__));
void nmi_panic(struct pt_regs *regs, const char *msg);
void check_panic_on_warn(const char *origin);
extern void oops_enter(void);
extern void oops_exit(void);
extern bool oops_may_print(void);

extern int panic_timeout;
extern unsigned long panic_print;
extern int panic_on_oops;
extern int panic_on_unrecovered_nmi;
extern int panic_on_io_nmi;
extern int panic_on_warn;

extern unsigned long panic_on_taint;
extern bool panic_on_taint_nousertaint;

extern int sysctl_panic_on_rcu_stall;
extern int sysctl_max_rcu_stall_to_panic;
extern int sysctl_panic_on_stackoverflow;

extern bool crash_kexec_post_notifiers;

/*
 * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It
 * holds a CPU number which is executing panic() currently. A value of
 * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().
 */
extern atomic_t panic_cpu;


/*
 * Only to be used by arch init code. If the user over-wrote the default
 * CONFIG_PANIC_TIMEOUT, honor it.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_arch_panic_timeout(int timeout, int arch_default_timeout)
{
 if (panic_timeout == arch_default_timeout)
  panic_timeout = timeout;
}

/* This cannot be an enum because some may be used in assembly source. */
# 76 "./include/linux/panic.h"
struct taint_flag {
 char c_true; /* character printed when tainted */
 char c_false; /* character printed when not tainted */
 bool module; /* also show as a per-module taint flag */
};

extern const struct taint_flag taint_flags[19];

enum lockdep_ok {
 LOCKDEP_STILL_OK,
 LOCKDEP_NOW_UNRELIABLE,
};

extern const char *print_tainted(void);
extern void add_taint(unsigned flag, enum lockdep_ok);
extern int test_taint(unsigned flag);
extern unsigned long get_taint(void);
# 22 "./include/asm-generic/bug.h" 2
# 1 "./include/linux/printk.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/stdarg.h" 1
// SPDX-License-Identifier: GPL-2.0-or-later



typedef __builtin_va_list va_list;
# 6 "./include/linux/printk.h" 2
# 1 "./include/linux/init.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/init.h"
/* Built-in __init functions needn't be compiled with retpoline */






/* These macros are used to mark some functions or
 * initialized data (doesn't apply to uninitialized data)
 * as `initialization' functions. The kernel can take this
 * as hint that the function is used only during the initialization
 * phase and free up used memory resources after
 *
 * Usage:
 * For functions:
 *
 * You should add __init immediately before the function name, like:
 *
 * static void __init initme(int x, int y)
 * {
 *    extern int z; z = x * y;
 * }
 *
 * If the function has a prototype somewhere, you can also add
 * __init between closing brace of the prototype and semicolon:
 *
 * extern int initialize_foobar_device(int, int, int) __init;
 *
 * For initialized data:
 * You should insert __initdata or __initconst between the variable name
 * and equal sign followed by value, e.g.:
 *
 * static int init_variable __initdata = 0;
 * static const char linux_logo[] __initconst = { 0x32, 0x36, ... };
 *
 * Don't forget to initialize data not at file scope, i.e. within a function,
 * as gcc otherwise puts the data into the bss section and not into the init
 * section.
 */

/* These are for everybody (although not all archs will actually
   discard it in modules) */






/*
 * modpost check for section mismatches during the kernel build.
 * A section mismatch happens when there are references from a
 * code or data section to an init section (both code or data).
 * The init sections are (for most archs) discarded by the kernel
 * when early init has completed so all such references are potential bugs.
 * For exit sections the same issue exists.
 *
 * The following markers are used for the cases where the reference to
 * the *init / *exit section (code or data) is valid and will teach
 * modpost not to issue a warning.  Intended semantics is that a code or
 * data tagged __ref* can reference code or data from init section without
 * producing a warning (of course, no warning does not mean code is
 * correct, so optimally document why the __ref is needed and why it's OK).
 *
 * The markers follow same syntax rules as __init / __initdata.
 */
# 87 "./include/linux/init.h"
/* Used for MEMORY_HOTPLUG */
# 96 "./include/linux/init.h"
/* For assembly routines */
# 109 "./include/linux/init.h"
/* silence warnings when references are OK */





/*
 * Used for initialization calls..
 */
typedef int (*initcall_t)(void);
typedef void (*exitcall_t)(void);


typedef int initcall_entry_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) initcall_t initcall_from_entry(initcall_entry_t *entry)
{
 return offset_to_ptr(entry);
}
# 137 "./include/linux/init.h"
extern initcall_entry_t __con_initcall_start[], __con_initcall_end[];

/* Used for constructor calls. */
typedef void (*ctor_fn_t)(void);

struct file_system_type;

/* Defined in init/main.c */
extern int do_one_initcall(initcall_t fn);
extern char __attribute__((__section__(".init.data"))) boot_command_line[];
extern char *saved_command_line;
extern unsigned int saved_command_line_len;
extern unsigned int reset_devices;

/* used by init/main.c */
void setup_arch(char **);
void prepare_namespace(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) init_rootfs(void);
extern struct file_system_type rootfs_fs_type;


extern bool rodata_enabled;


void mark_rodata_ro(void);


extern void (*late_time_init)(void);

extern bool initcall_debug;







/*
 * initcalls are now grouped by functionality into separate
 * subsections. Ordering inside the subsections is determined
 * by link order.
 * For backwards compatibility, initcall() puts the call in
 * the device init subsection.
 *
 * The `id' arg to __define_initcall() is needed so that multiple initcalls
 * can point at the same handler without causing duplicate-symbol build errors.
 *
 * Initcalls are run by placing pointers in initcall sections that the
 * kernel iterates at runtime. The linker can do dead code / data elimination
 * and remove that completely, so the initcall sections have to be marked
 * as KEEP() in the linker script.
 */

/* Format: <modname>__<counter>_<line>_<fn> */
# 199 "./include/linux/init.h"
/* Format: __<prefix>__<iid><id> */
# 267 "./include/linux/init.h"
/*
 * Early initcalls run before initializing SMP.
 *
 * Only for built-in code, not modules.
 */


/*
 * A "pure" initcall has no dependencies on anything else, and purely
 * initializes variables that couldn't be statically initialized.
 *
 * This only exists for built-in code, not for modules.
 * Keep main.c:initcall_level_names[] in sync.
 */
# 306 "./include/linux/init.h"
struct obs_kernel_param {
 const char *str;
 int (*setup_func)(char *);
 int early;
};

/*
 * Only for really core code.  See moduleparam.h for the normal way.
 *
 * Force the alignment so the compiler doesn't space elements of the
 * obs_kernel_param "array" too far apart in .init.setup.
 */
# 326 "./include/linux/init.h"
/*
 * NOTE: __setup functions return values:
 * @fn returns 1 (or non-zero) if the option argument is "handled"
 * and returns 0 if the option argument is "not handled".
 */



/*
 * NOTE: @fn is as per module_param, not __setup!
 * I.e., @fn returns 0 for no error or non-zero for error
 * (possibly @fn returns a -errno value, but it does not matter).
 * Emits warning if @fn returns non-zero.
 */
# 361 "./include/linux/init.h"
/* Relies on boot_command_line being set */
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) parse_early_param(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) parse_early_options(char *cmdline);
# 372 "./include/linux/init.h"
/* Data marked not to be saved by software suspend */
# 7 "./include/linux/printk.h" 2
# 1 "./include/linux/kern_levels.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 19 "./include/linux/kern_levels.h"
/*
 * Annotation for a "continued" line of log printout (only done after a
 * line that had no enclosing \n). Only to be used by core/arch code
 * during early bootup (a continued line is not SMP-safe otherwise).
 */


/* integer equivalents of KERN_<LEVEL> */
# 8 "./include/linux/printk.h" 2

# 1 "./include/linux/ratelimit_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/bits.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/vdso/bits.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 7 "./include/linux/bits.h" 2
# 16 "./include/linux/bits.h"
/*
 * Create a contiguous bitmask starting at bit position @l and ending at
 * position @h. For example
 * GENMASK_ULL(39, 21) gives us the 64bit vector 0x000000ffffe00000.
 */
# 6 "./include/linux/ratelimit_types.h" 2
# 1 "./include/uapi/linux/param.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./arch/arm64/include/uapi/asm/param.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */





# 1 "./include/asm-generic/param.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/asm-generic/param.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 6 "./include/asm-generic/param.h" 2
# 23 "./arch/arm64/include/uapi/asm/param.h" 2
# 6 "./include/uapi/linux/param.h" 2
# 7 "./include/linux/ratelimit_types.h" 2
# 1 "./include/linux/spinlock_types_raw.h" 1






# 1 "./arch/arm64/include/asm/spinlock_types.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */







# 1 "./include/asm-generic/qspinlock_types.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * Queued spinlock
 *
 * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.
 *
 * Authors: Waiman Long <waiman.long@hp.com>
 */





typedef struct qspinlock {
 union {
  atomic_t val;

  /*
		 * By using the whole 2nd least significant byte for the
		 * pending bit, we can allow better optimization of the lock
		 * acquisition for the pending bit holder.
		 */

  struct {
   u8 locked;
   u8 pending;
  };
  struct {
   u16 locked_pending;
   u16 tail;
  };
# 43 "./include/asm-generic/qspinlock_types.h"
 };
} arch_spinlock_t;

/*
 * Initializier
 */


/*
 * Bitfields in the atomic value:
 *
 * When NR_CPUS < 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-15: not used
 * 16-17: tail index
 * 18-31: tail cpu (+1)
 *
 * When NR_CPUS >= 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-10: tail index
 * 11-31: tail cpu (+1)
 */
# 13 "./arch/arm64/include/asm/spinlock_types.h" 2
# 1 "./include/asm-generic/qrwlock_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/uapi/asm/byteorder.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */






# 1 "./include/linux/byteorder/little_endian.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/byteorder/little_endian.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 14 "./include/uapi/linux/byteorder/little_endian.h"
# 1 "./include/linux/swab.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/swab.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */






# 1 "./arch/arm64/include/generated/uapi/asm/swab.h" 1
# 1 "./include/uapi/asm-generic/swab.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





/*
 * 32 bit architectures typically (but not always) want to
 * set __SWAB_64_THRU_32__. In user space, this is only
 * valid if the compiler supports 64 bit data types.
 */
# 2 "./arch/arm64/include/generated/uapi/asm/swab.h" 2
# 9 "./include/uapi/linux/swab.h" 2

/*
 * casts are necessary for constants, because we never know how for sure
 * how U/UL/ULL map to __u16, __u32, __u64. At least not in a portable way.
 */
# 42 "./include/uapi/linux/swab.h"
/*
 * Implement the following as inlines, but define the interface using
 * macros to allow constant folding when possible:
 * ___swab16, ___swab32, ___swab64, ___swahw32, ___swahb32
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__const__)) __u16 __fswab16(__u16 val)
{



 return ((__u16)( (((__u16)(val) & (__u16)0x00ffU) << 8) | (((__u16)(val) & (__u16)0xff00U) >> 8)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__const__)) __u32 __fswab32(__u32 val)
{



 return ((__u32)( (((__u32)(val) & (__u32)0x000000ffUL) << 24) | (((__u32)(val) & (__u32)0x0000ff00UL) << 8) | (((__u32)(val) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(val) & (__u32)0xff000000UL) >> 24)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__const__)) __u64 __fswab64(__u64 val)
{







 return ((__u64)( (((__u64)(val) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(val) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(val) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(val) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(val) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(val) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(val) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(val) & (__u64)0xff00000000000000ULL) >> 56)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__const__)) __u32 __fswahw32(__u32 val)
{



 return ((__u32)( (((__u32)(val) & (__u32)0x0000ffffUL) << 16) | (((__u32)(val) & (__u32)0xffff0000UL) >> 16)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__const__)) __u32 __fswahb32(__u32 val)
{



 return ((__u32)( (((__u32)(val) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(val) & (__u32)0xff00ff00UL) >> 8)));

}

/**
 * __swab16 - return a byteswapped 16-bit value
 * @x: value to byteswap
 */
# 110 "./include/uapi/linux/swab.h"
/**
 * __swab32 - return a byteswapped 32-bit value
 * @x: value to byteswap
 */
# 123 "./include/uapi/linux/swab.h"
/**
 * __swab64 - return a byteswapped 64-bit value
 * @x: value to byteswap
 */
# 136 "./include/uapi/linux/swab.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __swab(const unsigned long y)
{

 return (__u64)(__builtin_constant_p(y) ? ((__u64)( (((__u64)(y) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(y) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(y) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(y) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(y) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(y) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(y) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(y) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(y));



}

/**
 * __swahw32 - return a word-swapped 32-bit value
 * @x: value to wordswap
 *
 * __swahw32(0x12340000) is 0x00001234
 */





/**
 * __swahb32 - return a high and low byte-swapped 32-bit value
 * @x: value to byteswap
 *
 * __swahb32(0x12345678) is 0x34127856
 */





/**
 * __swab16p - return a byteswapped 16-bit value from a pointer
 * @p: pointer to a naturally-aligned 16-bit value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u16 __swab16p(const __u16 *p)
{



 return (__u16)(__builtin_constant_p(*p) ? ((__u16)( (((__u16)(*p) & (__u16)0x00ffU) << 8) | (((__u16)(*p) & (__u16)0xff00U) >> 8))) : __fswab16(*p));

}

/**
 * __swab32p - return a byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u32 __swab32p(const __u32 *p)
{



 return (__u32)(__builtin_constant_p(*p) ? ((__u32)( (((__u32)(*p) & (__u32)0x000000ffUL) << 24) | (((__u32)(*p) & (__u32)0x0000ff00UL) << 8) | (((__u32)(*p) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(*p) & (__u32)0xff000000UL) >> 24))) : __fswab32(*p));

}

/**
 * __swab64p - return a byteswapped 64-bit value from a pointer
 * @p: pointer to a naturally-aligned 64-bit value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u64 __swab64p(const __u64 *p)
{



 return (__u64)(__builtin_constant_p(*p) ? ((__u64)( (((__u64)(*p) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(*p) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(*p) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(*p) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(*p) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(*p) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(*p) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(*p) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(*p));

}

/**
 * __swahw32p - return a wordswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u32 __swahw32p(const __u32 *p)
{



 return (__builtin_constant_p((__u32)(*p)) ? ((__u32)( (((__u32)(*p) & (__u32)0x0000ffffUL) << 16) | (((__u32)(*p) & (__u32)0xffff0000UL) >> 16))) : __fswahw32(*p));

}

/**
 * __swahb32p - return a high and low byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high/low byteswapping.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u32 __swahb32p(const __u32 *p)
{



 return (__builtin_constant_p((__u32)(*p)) ? ((__u32)( (((__u32)(*p) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(*p) & (__u32)0xff00ff00UL) >> 8))) : __fswahb32(*p));

}

/**
 * __swab16s - byteswap a 16-bit value in-place
 * @p: pointer to a naturally-aligned 16-bit value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __swab16s(__u16 *p)
{



 *p = __swab16p(p);

}
/**
 * __swab32s - byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __swab32s(__u32 *p)
{



 *p = __swab32p(p);

}

/**
 * __swab64s - byteswap a 64-bit value in-place
 * @p: pointer to a naturally-aligned 64-bit value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __swab64s(__u64 *p)
{



 *p = __swab64p(p);

}

/**
 * __swahw32s - wordswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __swahw32s(__u32 *p)
{



 *p = __swahw32p(p);

}

/**
 * __swahb32s - high and low byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high and low byte swapping
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __swahb32s(__u32 *p)
{



 *p = __swahb32p(p);

}
# 6 "./include/linux/swab.h" 2
# 24 "./include/linux/swab.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void swab16_array(u16 *buf, unsigned int words)
{
 while (words--) {
  __swab16s(buf);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void swab32_array(u32 *buf, unsigned int words)
{
 while (words--) {
  __swab32s(buf);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void swab64_array(u64 *buf, unsigned int words)
{
 while (words--) {
  __swab64s(buf);
  buf++;
 }
}
# 15 "./include/uapi/linux/byteorder/little_endian.h" 2
# 45 "./include/uapi/linux/byteorder/little_endian.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le64 __cpu_to_le64p(const __u64 *p)
{
 return ( __le64)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u64 __le64_to_cpup(const __le64 *p)
{
 return ( __u64)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le32 __cpu_to_le32p(const __u32 *p)
{
 return ( __le32)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u32 __le32_to_cpup(const __le32 *p)
{
 return ( __u32)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le16 __cpu_to_le16p(const __u16 *p)
{
 return ( __le16)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u16 __le16_to_cpup(const __le16 *p)
{
 return ( __u16)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be64 __cpu_to_be64p(const __u64 *p)
{
 return ( __be64)__swab64p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u64 __be64_to_cpup(const __be64 *p)
{
 return __swab64p((__u64 *)p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be32 __cpu_to_be32p(const __u32 *p)
{
 return ( __be32)__swab32p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u32 __be32_to_cpup(const __be32 *p)
{
 return __swab32p((__u32 *)p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be16 __cpu_to_be16p(const __u16 *p)
{
 return ( __be16)__swab16p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u16 __be16_to_cpup(const __be16 *p)
{
 return __swab16p((__u16 *)p);
}
# 6 "./include/linux/byteorder/little_endian.h" 2





# 1 "./include/linux/byteorder/generic.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * linux/byteorder/generic.h
 * Generic Byte-reordering support
 *
 * The "... p" macros, like le64_to_cpup, can be used with pointers
 * to unaligned data, but there will be a performance penalty on
 * some architectures.  Use get_unaligned for unaligned data.
 *
 * Francois-Rene Rideau <fare@tunes.org> 19970707
 *    gathered all the good ideas from all asm-foo/byteorder.h into one file,
 *    cleaned them up.
 *    I hope it is compliant with non-GCC compilers.
 *    I decided to put __BYTEORDER_HAS_U64__ in byteorder.h,
 *    because I wasn't sure it would be ok to put it in types.h
 *    Upgraded it to 2.1.43
 * Francois-Rene Rideau <fare@tunes.org> 19971012
 *    Upgraded it to 2.1.57
 *    to please Linus T., replaced huge #ifdef's between little/big endian
 *    by nestedly #include'd files.
 * Francois-Rene Rideau <fare@tunes.org> 19971205
 *    Made it to 2.1.71; now a facelift:
 *    Put files under include/linux/byteorder/
 *    Split swab from generic support.
 *
 * TODO:
 *   = Regular kernel maintainers could also replace all these manual
 *    byteswap macros that remain, disseminated among drivers,
 *    after some grep or the sources...
 *   = Linus might want to rename all these macros and files to fit his taste,
 *    to fit his personal naming scheme.
 *   = it seems that a few drivers would also appreciate
 *    nybble swapping support...
 *   = every architecture could add their byteswap macro in asm/byteorder.h
 *    see how some architectures already do (i386, alpha, ppc, etc)
 *   = cpu_to_beXX and beXX_to_cpu might some day need to be well
 *    distinguished throughout the kernel. This is not the case currently,
 *    since little endian, big endian, and pdp endian machines needn't it.
 *    But this might be the case for, say, a port of Linux to 20/21 bit
 *    architectures (and F21 Linux addict around?).
 */

/*
 * The following macros are to be defined by <asm/byteorder.h>:
 *
 * Conversion of long and short int between network and host format
 *	ntohl(__u32 x)
 *	ntohs(__u16 x)
 *	htonl(__u32 x)
 *	htons(__u16 x)
 * It seems that some programs (which? where? or perhaps a standard? POSIX?)
 * might like the above to be functions, not macros (why?).
 * if that's true, then detect them, and take measures.
 * Anyway, the measure is: define only ___ntohl as a macro instead,
 * and in a separate file, have
 * unsigned long inline ntohl(x){return ___ntohl(x);}
 *
 * The same for constant arguments
 *	__constant_ntohl(__u32 x)
 *	__constant_ntohs(__u16 x)
 *	__constant_htonl(__u32 x)
 *	__constant_htons(__u16 x)
 *
 * Conversion of XX-bit integers (16- 32- or 64-)
 * between native CPU format and little/big endian format
 * 64-bit stuff only defined for proper architectures
 *	cpu_to_[bl]eXX(__uXX x)
 *	[bl]eXX_to_cpu(__uXX x)
 *
 * The same, but takes a pointer to the value to convert
 *	cpu_to_[bl]eXXp(__uXX x)
 *	[bl]eXX_to_cpup(__uXX x)
 *
 * The same, but change in situ
 *	cpu_to_[bl]eXXs(__uXX x)
 *	[bl]eXX_to_cpus(__uXX x)
 *
 * See asm-foo/byteorder.h for examples of how to provide
 * architecture-optimized versions
 *
 */
# 123 "./include/linux/byteorder/generic.h"
/*
 * They have to be macros in order to do the constant folding
 * correctly - if the argument passed into a inline function
 * it is no longer constant according to gcc..
 */
# 144 "./include/linux/byteorder/generic.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void le16_add_cpu(__le16 *var, u16 val)
{
 *var = (( __le16)(__u16)((( __u16)(__le16)(*var)) + val));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void le32_add_cpu(__le32 *var, u32 val)
{
 *var = (( __le32)(__u32)((( __u32)(__le32)(*var)) + val));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void le64_add_cpu(__le64 *var, u64 val)
{
 *var = (( __le64)(__u64)((( __u64)(__le64)(*var)) + val));
}

/* XXX: this stuff can be optimized */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void le32_to_cpu_array(u32 *buf, unsigned int words)
{
 while (words--) {
  do { (void)(buf); } while (0);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpu_to_le32_array(u32 *buf, unsigned int words)
{
 while (words--) {
  do { (void)(buf); } while (0);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void be16_add_cpu(__be16 *var, u16 val)
{
 *var = (( __be16)(__u16)(__builtin_constant_p(((__u16)(__builtin_constant_p(( __u16)(__be16)(*var)) ? ((__u16)( (((__u16)(( __u16)(__be16)(*var)) & (__u16)0x00ffU) << 8) | (((__u16)(( __u16)(__be16)(*var)) & (__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val)) ? ((__u16)( (((__u16)(((__u16)(__builtin_constant_p(( __u16)(__be16)(*var)) ? ((__u16)( (((__u16)(( __u16)(__be16)(*var)) & (__u16)0x00ffU) << 8) | (((__u16)(( __u16)(__be16)(*var)) & (__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val)) & (__u16)0x00ffU) << 8) | (((__u16)(((__u16)(__builtin_constant_p(( __u16)(__be16)(*var)) ? ((__u16)( (((__u16)(( __u16)(__be16)(*var)) & (__u16)0x00ffU) << 8) | (((__u16)(( __u16)(__be16)(*var)) & (__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val)) & (__u16)0xff00U) >> 8))) : __fswab16(((__u16)(__builtin_constant_p(( __u16)(__be16)(*var)) ? ((__u16)( (((__u16)(( __u16)(__be16)(*var)) & (__u16)0x00ffU) << 8) | (((__u16)(( __u16)(__be16)(*var)) & (__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void be32_add_cpu(__be32 *var, u32 val)
{
 *var = (( __be32)(__u32)(__builtin_constant_p(((__u32)(__builtin_constant_p(( __u32)(__be32)(*var)) ? ((__u32)( (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) ? ((__u32)( (((__u32)(((__u32)(__builtin_constant_p(( __u32)(__be32)(*var)) ? ((__u32)( (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0x000000ffUL) << 24) | (((__u32)(((__u32)(__builtin_constant_p(( __u32)(__be32)(*var)) ? ((__u32)( (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(((__u32)(__builtin_constant_p(( __u32)(__be32)(*var)) ? ((__u32)( (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(((__u32)(__builtin_constant_p(( __u32)(__be32)(*var)) ? ((__u32)( (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0xff000000UL) >> 24))) : __fswab32(((__u32)(__builtin_constant_p(( __u32)(__be32)(*var)) ? ((__u32)( (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(*var)) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void be64_add_cpu(__be64 *var, u64 val)
{
 *var = (( __be64)(__u64)(__builtin_constant_p(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) ? ((__u64)( (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(((__u64)(__builtin_constant_p(( __u64)(__be64)(*var)) ? ((__u64)( (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(*var)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpu_to_be32_array(__be32 *dst, const u32 *src, size_t len)
{
 size_t i;

 for (i = 0; i < len; i++)
  dst[i] = (( __be32)(__u32)(__builtin_constant_p((src[i])) ? ((__u32)( (((__u32)((src[i])) & (__u32)0x000000ffUL) << 24) | (((__u32)((src[i])) & (__u32)0x0000ff00UL) << 8) | (((__u32)((src[i])) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((src[i])) & (__u32)0xff000000UL) >> 24))) : __fswab32((src[i]))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void be32_to_cpu_array(u32 *dst, const __be32 *src, size_t len)
{
 size_t i;

 for (i = 0; i < len; i++)
  dst[i] = (__u32)(__builtin_constant_p(( __u32)(__be32)(src[i])) ? ((__u32)( (((__u32)(( __u32)(__be32)(src[i])) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(src[i])) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(src[i])) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(src[i])) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(src[i])));
}
# 12 "./include/linux/byteorder/little_endian.h" 2
# 24 "./arch/arm64/include/uapi/asm/byteorder.h" 2
# 7 "./include/asm-generic/qrwlock_types.h" 2
# 1 "./arch/arm64/include/asm/spinlock_types.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 8 "./include/asm-generic/qrwlock_types.h" 2

/*
 * The queued read/write lock data structure
 */

typedef struct qrwlock {
 union {
  atomic_t cnts;
  struct {

   u8 wlocked; /* Locked for write? */
   u8 __lstate[3];




  };
 };
 arch_spinlock_t wait_lock;
} arch_rwlock_t;
# 14 "./arch/arm64/include/asm/spinlock_types.h" 2
# 8 "./include/linux/spinlock_types_raw.h" 2




# 1 "./include/linux/lockdep_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
 *
 * see Documentation/locking/lockdep-design.rst for more details.
 */







enum lockdep_wait_type {
 LD_WAIT_INV = 0, /* not checked, catch all */

 LD_WAIT_FREE, /* wait free, rcu etc.. */
 LD_WAIT_SPIN, /* spin loops, raw_spinlock_t etc.. */




 LD_WAIT_CONFIG = LD_WAIT_SPIN,

 LD_WAIT_SLEEP, /* sleeping locks, mutex_t etc.. */

 LD_WAIT_MAX, /* must be last */
};

enum lockdep_lock_type {
 LD_LOCK_NORMAL = 0, /* normal, catch all */
 LD_LOCK_PERCPU, /* percpu */
 LD_LOCK_MAX,
};
# 194 "./include/linux/lockdep_types.h"
/*
 * The class key takes no space if lockdep is disabled:
 */
struct lock_class_key { };

/*
 * The lockdep_map takes no space if lockdep is disabled:
 */
struct lockdep_map { };

struct pin_cookie { };
# 13 "./include/linux/spinlock_types_raw.h" 2

typedef struct raw_spinlock {
 arch_spinlock_t raw_lock;







} raw_spinlock_t;
# 8 "./include/linux/ratelimit_types.h" 2




/* issue num suppressed message on exit */


struct ratelimit_state {
 raw_spinlock_t lock; /* protect the state */

 int interval;
 int burst;
 int printed;
 int missed;
 unsigned long begin;
 unsigned long flags;
};
# 44 "./include/linux/ratelimit_types.h"
extern int ___ratelimit(struct ratelimit_state *rs, const char *func);
# 10 "./include/linux/printk.h" 2


extern const char linux_banner[];
extern const char linux_proc_banner[];

extern int oops_in_progress; /* If set, an oops, panic(), BUG() or die() is in progress */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int printk_get_level(const char *buffer)
{
 if (buffer[0] == '\001' && buffer[1]) {
  switch (buffer[1]) {
  case '0' ... '7':
  case 'c': /* KERN_CONT */
   return buffer[1];
  }
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *printk_skip_level(const char *buffer)
{
 if (printk_get_level(buffer))
  return buffer + 2;

 return buffer;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *printk_skip_headers(const char *buffer)
{
 while (printk_get_level(buffer))
  buffer = printk_skip_level(buffer);

 return buffer;
}



/* printk's without a loglevel use this.. */


/* We show everything that is MORE important than this.. */





/*
 * Default used to be hard-coded at 7, quiet used to be hardcoded at 4,
 * we're now allowing both to be set from kernel config.
 */



extern int console_printk[];






extern void console_verbose(void);

/* strlen("ratelimit") + 1 */

extern char devkmsg_log_str[];
struct ctl_table;

extern int suppress_printk;

struct va_format {
 const char *fmt;
 va_list *va;
};

/*
 * FW_BUG
 * Add this to a message where you are sure the firmware is buggy or behaves
 * really stupid or out of spec. Be aware that the responsible BIOS developer
 * should be able to fix this issue or at least get a concrete idea of the
 * problem by reading your message without the need of looking at the kernel
 * code.
 *
 * Use it for definite and high priority BIOS bugs.
 *
 * FW_WARN
 * Use it for not that clear (e.g. could the kernel messed up things already?)
 * and medium priority BIOS bugs.
 *
 * FW_INFO
 * Use this one if you want to tell the user or vendor about something
 * suspicious, but generally harmless related to the firmware.
 *
 * Use it for information or very low priority BIOS bugs.
 */




/*
 * HW_ERR
 * Add this to a message for hardware errors, so that user can report
 * it to hardware vendor instead of LKML or software vendor.
 */


/*
 * DEPRECATED
 * Add this to a message whenever you want to warn user space about the use
 * of a deprecated aspect of an API so they can stop using it
 */


/*
 * Dummy printk for disabled debugging statements to use whilst maintaining
 * gcc's format checking.
 */
# 139 "./include/linux/printk.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__format__(printf, 1, 2))) __attribute__((__cold__))
void early_printk(const char *s, ...) { }


struct dev_printk_info;


           __attribute__((__format__(printf, 4, 0)))
int vprintk_emit(int facility, int level,
   const struct dev_printk_info *dev_info,
   const char *fmt, va_list args);

           __attribute__((__format__(printf, 1, 0)))
int vprintk(const char *fmt, va_list args);

           __attribute__((__format__(printf, 1, 2))) __attribute__((__cold__))
int _printk(const char *fmt, ...);

/*
 * Special printk facility for scheduler/timekeeping use only, _DO_NOT_USE_ !
 */
__attribute__((__format__(printf, 1, 2))) __attribute__((__cold__)) int _printk_deferred(const char *fmt, ...);

extern void __printk_safe_enter(void);
extern void __printk_safe_exit(void);
/*
 * The printk_deferred_enter/exit macros are available only as a hack for
 * some code paths that need to defer all printk console printing. Interrupts
 * must be disabled for the deferred duration.
 */



/*
 * Please don't use printk_ratelimit(), because it shares ratelimiting state
 * with all other unrelated printk_ratelimit() callsites.  Instead use
 * printk_ratelimited() or plain old __ratelimit().
 */
extern int __printk_ratelimit(const char *func);

extern bool printk_timed_ratelimit(unsigned long *caller_jiffies,
       unsigned int interval_msec);

extern int printk_delay_msec;
extern int dmesg_restrict;

extern void wake_up_klogd(void);

char *log_buf_addr_get(void);
u32 log_buf_len_get(void);
void log_buf_vmcoreinfo_setup(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) setup_log_buf(int early);
__attribute__((__format__(printf, 1, 2))) void dump_stack_set_arch_desc(const char *fmt, ...);
void dump_stack_print_info(const char *log_lvl);
void show_regs_print_info(const char *log_lvl);
extern void dump_stack_lvl(const char *log_lvl) __attribute__((__cold__));
extern void dump_stack(void) __attribute__((__cold__));
void printk_trigger_flush(void);
# 279 "./include/linux/printk.h"
extern int __printk_cpu_sync_try_get(void);
extern void __printk_cpu_sync_wait(void);
extern void __printk_cpu_sync_put(void);
# 290 "./include/linux/printk.h"
/**
 * printk_cpu_sync_get_irqsave() - Disable interrupts and acquire the printk
 *                                 cpu-reentrant spinning lock.
 * @flags: Stack-allocated storage for saving local interrupt state,
 *         to be passed to printk_cpu_sync_put_irqrestore().
 *
 * If the lock is owned by another CPU, spin until it becomes available.
 * Interrupts are restored while spinning.
 *
 * CAUTION: This function must be used carefully. It does not behave like a
 * typical lock. Here are important things to watch out for...
 *
 *     * This function is reentrant on the same CPU. Therefore the calling
 *       code must not assume exclusive access to data if code accessing the
 *       data can run reentrant or within NMI context on the same CPU.
 *
 *     * If there exists usage of this function from NMI context, it becomes
 *       unsafe to perform any type of locking or spinning to wait for other
 *       CPUs after calling this function from any context. This includes
 *       using spinlocks or any other busy-waiting synchronization methods.
 */
# 320 "./include/linux/printk.h"
/**
 * printk_cpu_sync_put_irqrestore() - Release the printk cpu-reentrant spinning
 *                                    lock and restore interrupts.
 * @flags: Caller's saved interrupt state, from printk_cpu_sync_get_irqsave().
 */






extern int kptr_restrict;

/**
 * pr_fmt - used by the pr_*() macros to generate the printk format string
 * @fmt: format string passed from a pr_*() macro
 *
 * This macro can be used to generate a unified format string for pr_*()
 * macros. A common use is to prefix all pr_*() messages in a file with a common
 * string. For example, defining this at the top of a source file:
 *
 *        #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 *
 * would prefix all pr_info, pr_emerg... messages in the file with the module
 * name.
 */




struct module;
# 406 "./include/linux/printk.h"
/*
 * Some subsystems have their own custom printk that applies a va_format to a
 * generic format, for example, to include a device number or other metadata
 * alongside the format supplied by the caller.
 *
 * In order to store these in the way they would be emitted by the printk
 * infrastructure, the subsystem provides us with the start, fixed string, and
 * any subsequent text in the format string.
 *
 * We take a variable argument list as pr_fmt/dev_fmt/etc are sometimes passed
 * as multiple arguments (eg: `"%s: ", "blah"`), and we must only take the
 * first one.
 *
 * subsys_fmt_prefix must be known at compile time, or compilation will fail
 * (since this is a mistake). If fmt or level is not known at compile time, no
 * index entry will be made (since this can legitimately happen).
 */
# 433 "./include/linux/printk.h"
/**
 * printk - print a kernel message
 * @fmt: format string
 *
 * This is printk(). It can be called from any context. We want it to work.
 *
 * If printk indexing is enabled, _printk() is called from printk_index_wrap.
 * Otherwise, printk is simply #defined to _printk.
 *
 * We try to grab the console_lock. If we succeed, it's easy - we log the
 * output and call the console drivers.  If we fail to get the semaphore, we
 * place the output into the log buffer and return. The current holder of
 * the console_sem will notice the new output in console_unlock(); and will
 * send it to the consoles before releasing the lock.
 *
 * One effect of this deferred printing is that code which calls printk() and
 * then changes console_loglevel may break. This is because console_loglevel
 * is inspected when the actual printing occurs.
 *
 * See also:
 * printf(3)
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */




/**
 * pr_emerg - Print an emergency-level message
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_EMERG loglevel. It uses pr_fmt() to
 * generate the format string.
 */


/**
 * pr_alert - Print an alert-level message
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_ALERT loglevel. It uses pr_fmt() to
 * generate the format string.
 */


/**
 * pr_crit - Print a critical-level message
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_CRIT loglevel. It uses pr_fmt() to
 * generate the format string.
 */


/**
 * pr_err - Print an error-level message
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_ERR loglevel. It uses pr_fmt() to
 * generate the format string.
 */


/**
 * pr_warn - Print a warning-level message
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_WARNING loglevel. It uses pr_fmt()
 * to generate the format string.
 */


/**
 * pr_notice - Print a notice-level message
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_NOTICE loglevel. It uses pr_fmt() to
 * generate the format string.
 */


/**
 * pr_info - Print an info-level message
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_INFO loglevel. It uses pr_fmt() to
 * generate the format string.
 */



/**
 * pr_cont - Continues a previous log message in the same line.
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_CONT loglevel. It should only be
 * used when continuing a log message with no newline ('\n') enclosed. Otherwise
 * it defaults back to KERN_DEFAULT loglevel.
 */



/**
 * pr_devel - Print a debug-level message conditionally
 * @fmt: format string
 * @...: arguments for the format string
 *
 * This macro expands to a printk with KERN_DEBUG loglevel if DEBUG is
 * defined. Otherwise it does nothing.
 *
 * It uses pr_fmt() to generate the format string.
 */
# 563 "./include/linux/printk.h"
/* If you are writing a driver, please use dev_dbg instead */
# 590 "./include/linux/printk.h"
/*
 * Print a one-time message (analogous to WARN_ONCE() et al):
 */
# 620 "./include/linux/printk.h"
/* no pr_cont_once, don't do that... */
# 630 "./include/linux/printk.h"
/* If you are writing a driver, please use dev_dbg instead */
# 639 "./include/linux/printk.h"
/*
 * ratelimited messages with local ratelimit_state,
 * no local ratelimit_state used in the !PRINTK case
 */
# 672 "./include/linux/printk.h"
/* no pr_cont_ratelimited, don't do that... */
# 682 "./include/linux/printk.h"
/* If you are writing a driver, please use dev_dbg instead */
# 704 "./include/linux/printk.h"
extern const struct file_operations kmsg_fops;

enum {
 DUMP_PREFIX_NONE,
 DUMP_PREFIX_ADDRESS,
 DUMP_PREFIX_OFFSET
};
extern int hex_dump_to_buffer(const void *buf, size_t len, int rowsize,
         int groupsize, char *linebuf, size_t linebuflen,
         bool ascii);

extern void print_hex_dump(const char *level, const char *prefix_str,
      int prefix_type, int rowsize, int groupsize,
      const void *buf, size_t len, bool ascii);
# 743 "./include/linux/printk.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void print_hex_dump_debug(const char *prefix_str, int prefix_type,
     int rowsize, int groupsize,
     const void *buf, size_t len, bool ascii)
{
}


/**
 * print_hex_dump_bytes - shorthand form of print_hex_dump() with default params
 * @prefix_str: string to prefix each line with;
 *  caller supplies trailing spaces for alignment if desired
 * @prefix_type: controls whether prefix of an offset, address, or none
 *  is printed (%DUMP_PREFIX_OFFSET, %DUMP_PREFIX_ADDRESS, %DUMP_PREFIX_NONE)
 * @buf: data blob to dump
 * @len: number of bytes in the @buf
 *
 * Calls print_hex_dump(), with log level of KERN_DEBUG,
 * rowsize of 16, groupsize of 1, and ASCII output included.
 */
# 23 "./include/asm-generic/bug.h" 2

struct warn_args;
struct pt_regs;

void __warn(const char *file, int line, void *caller, unsigned taint,
     struct pt_regs *regs, struct warn_args *args);




struct bug_entry {



 signed int bug_addr_disp;





 signed int file_disp;

 unsigned short line;

 unsigned short flags;
};


/*
 * Don't use BUG() or BUG_ON() unless there's really no way out; one
 * example might be detecting data structure corruption in the middle
 * of an operation that can't be backed out of.  If the (sub)system
 * can somehow continue operating, perhaps with reduced functionality,
 * it's probably not BUG-worthy.
 *
 * If you're tempted to BUG(), think again:  is completely giving up
 * really the *only* solution?  There are usually better options, where
 * users don't need to reboot ASAP and can mostly shut down cleanly.
 */
# 74 "./include/asm-generic/bug.h"
/*
 * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report
 * significant kernel issues that need prompt attention if they should ever
 * appear at runtime.
 *
 * Do not use these macros when checking for invalid external inputs
 * (e.g. invalid system call arguments, or invalid data coming from
 * network/devices), and on transient conditions like ENOMEM or EAGAIN.
 * These macros should be used for recoverable kernel issues only.
 * For invalid external inputs, transient conditions, etc use
 * pr_err[_once/_ratelimited]() followed by dump_stack(), if necessary.
 * Do not include "BUG"/"WARNING" in format strings manually to make these
 * conditions distinguishable from kernel issues.
 *
 * Use the versions with printk format strings to provide better diagnostics.
 */
# 101 "./include/asm-generic/bug.h"
extern __attribute__((__format__(printf, 1, 2))) void __warn_printk(const char *fmt, ...);
# 118 "./include/asm-generic/bug.h"
/* used internally by panic.c */
# 187 "./include/asm-generic/bug.h"
/*
 * WARN_ON_SMP() is for cases that the warning is either
 * meaningless for !SMP or may even cause failures.
 * It can also be used with values that are only defined
 * on SMP:
 *
 * struct foo {
 *  [...]
 * #ifdef CONFIG_SMP
 *	int bar;
 * #endif
 * };
 *
 * void func(struct foo *zoot)
 * {
 *	WARN_ON_SMP(!zoot->bar);
 *
 * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),
 * and should be a nop and return false for uniprocessor.
 *
 * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set
 * and x is true.
 */
# 27 "./arch/arm64/include/asm/bug.h" 2
# 6 "./include/linux/bug.h" 2



enum bug_trap_type {
 BUG_TRAP_TYPE_NONE = 0,
 BUG_TRAP_TYPE_WARN = 1,
 BUG_TRAP_TYPE_BUG = 2,
};

struct pt_regs;
# 34 "./include/linux/bug.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_warning_bug(const struct bug_entry *bug)
{
 return bug->flags & (1 << 0);
}

void bug_get_file_line(struct bug_entry *bug, const char **file,
         unsigned int *line);

struct bug_entry *find_bug(unsigned long bugaddr);

enum bug_trap_type report_bug(unsigned long bug_addr, struct pt_regs *regs);

/* These are defined by the architecture */
int is_valid_bugaddr(unsigned long addr);

void generic_bug_clear_once(void);
# 76 "./include/linux/bug.h"
/*
 * Since detected data corruption should stop operation on the affected
 * structures. Return value must be checked and sanely acted on by caller.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool check_data_corruption(bool v) { return v; }
# 14 "./include/linux/thread_info.h" 2
# 1 "./include/linux/restart_block.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Common syscall restarting data
 */





# 1 "./include/linux/time64.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/math64.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/math.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/generated/asm/div64.h" 1
# 1 "./include/asm-generic/div64.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/*
 * Copyright (C) 2003 Bernardo Innocenti <bernie@develer.com>
 * Based on former asm-ppc/div64.h and asm-m68knommu/div64.h
 *
 * Optimization for constant divisors on 32-bit machines:
 * Copyright (C) 2006-2015 Nicolas Pitre
 *
 * The semantics of do_div() is, in C++ notation, observing that the name
 * is a function-like macro and the n parameter has the semantics of a C++
 * reference:
 *
 * uint32_t do_div(uint64_t &n, uint32_t base)
 * {
 * 	uint32_t remainder = n % base;
 * 	n = n / base;
 * 	return remainder;
 * }
 *
 * NOTE: macro parameter n is evaluated multiple times,
 *       beware of side effects!
 */






/**
 * do_div - returns 2 values: calculate remainder and update new dividend
 * @n: uint64_t dividend (will be updated)
 * @base: uint32_t divisor
 *
 * Summary:
 * ``uint32_t remainder = n % base;``
 * ``n = n / base;``
 *
 * Return: (uint32_t)remainder
 *
 * NOTE: macro parameter @n is evaluated multiple times,
 * beware of side effects!
 */
# 2 "./arch/arm64/include/generated/asm/div64.h" 2
# 7 "./include/linux/math.h" 2
# 1 "./include/uapi/linux/kernel.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./include/uapi/linux/sysinfo.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */






struct sysinfo {
 __kernel_long_t uptime; /* Seconds since boot */
 __kernel_ulong_t loads[3]; /* 1, 5, and 15 minute load averages */
 __kernel_ulong_t totalram; /* Total usable main memory size */
 __kernel_ulong_t freeram; /* Available memory size */
 __kernel_ulong_t sharedram; /* Amount of shared memory */
 __kernel_ulong_t bufferram; /* Memory used by buffers */
 __kernel_ulong_t totalswap; /* Total swap space size */
 __kernel_ulong_t freeswap; /* swap space still available */
 __u16 procs; /* Number of current processes */
 __u16 pad; /* Explicit padding for m68k */
 __kernel_ulong_t totalhigh; /* Total high memory size */
 __kernel_ulong_t freehigh; /* Available high memory size */
 __u32 mem_unit; /* Memory unit size in bytes */
 char _f[20-2*sizeof(__kernel_ulong_t)-sizeof(__u32)]; /* Padding: libc5 uses this.. */
};
# 6 "./include/uapi/linux/kernel.h" 2
# 8 "./include/linux/math.h" 2

/*
 * This looks more complex than it should be. But we need to
 * get the type for the ~ right in round_down (it needs to be
 * as wide as the result!), and we want to evaluate the macro
 * arguments just once each.
 */


/**
 * round_up - round up to next specified power of 2
 * @x: the value to round
 * @y: multiple to round up to (must be a power of 2)
 *
 * Rounds @x up to next multiple of @y (which must be a power of 2).
 * To perform arbitrary rounding up, use roundup() below.
 */


/**
 * round_down - round down to next specified power of 2
 * @x: the value to round
 * @y: multiple to round down to (must be a power of 2)
 *
 * Rounds @x down to next multiple of @y (which must be a power of 2).
 * To perform arbitrary rounding down, use rounddown() below.
 */
# 51 "./include/linux/math.h"
/**
 * roundup - round up to the next specified multiple
 * @x: the value to up
 * @y: multiple to round up to
 *
 * Rounds @x up to next multiple of @y. If @y will always be a power
 * of 2, consider using the faster round_up().
 */






/**
 * rounddown - round down to next specified multiple
 * @x: the value to round
 * @y: multiple to round down to
 *
 * Rounds @x down to next multiple of @y. If @y will always be a power
 * of 2, consider using the faster round_down().
 */







/*
 * Divide positive or negative dividend by positive or negative divisor
 * and round to closest integer. Result is undefined for negative
 * divisors if the dividend variable type is unsigned and for negative
 * dividends if the divisor variable type is unsigned.
 */
# 97 "./include/linux/math.h"
/*
 * Same as above but for u64 dividends. divisor must be a 32-bit
 * number.
 */
# 115 "./include/linux/math.h"
struct s16_fract { __s16 numerator; __s16 denominator; };
struct u16_fract { __u16 numerator; __u16 denominator; };
struct s32_fract { __s32 numerator; __s32 denominator; };
struct u32_fract { __u32 numerator; __u32 denominator; };


/*
 * Multiplies an integer by a fraction, while avoiding unnecessary
 * overflow or loss of precision.
 */
# 135 "./include/linux/math.h"
/**
 * abs - return absolute value of an argument
 * @x: the value.  If it is unsigned type, it is converted to signed type first.
 *     char is treated as if it was signed (regardless of whether it really is)
 *     but the macro's return type is preserved as char.
 *
 * Return: an absolute value of x.
 */
# 158 "./include/linux/math.h"
/**
 * reciprocal_scale - "scale" a value into range [0, ep_ro)
 * @val: value
 * @ep_ro: right open interval endpoint
 *
 * Perform a "reciprocal multiplication" in order to "scale" a value into
 * range [0, @ep_ro), where the upper interval endpoint is right-open.
 * This is useful, e.g. for accessing a index of an array containing
 * @ep_ro elements, for example. Think of it as sort of modulus, only that
 * the result isn't that of modulo. ;) Note that if initial input is a
 * small value, then result will return 0.
 *
 * Return: a result based on @val in interval [0, @ep_ro).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 reciprocal_scale(u32 val, u32 ep_ro)
{
 return (u32)(((u64) val * ep_ro) >> 32);
}

u64 int_pow(u64 base, unsigned int exp);
unsigned long int_sqrt(unsigned long);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 int_sqrt64(u64 x)
{
 return (u32)int_sqrt(x);
}
# 7 "./include/linux/math64.h" 2
# 1 "./include/vdso/math64.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{
 u32 ret = 0;

 while (dividend >= divisor) {
  /* The following asm() prevents the compiler from
		   optimising this loop into a modulo operation.  */
  asm("" : "+rm"(dividend));

  dividend -= divisor;
  ret++;
 }

 *remainder = dividend;

 return ret;
}
# 8 "./include/linux/math64.h" 2
# 1 "./arch/arm64/include/generated/asm/div64.h" 1
# 9 "./include/linux/math64.h" 2






/**
 * div_u64_rem - unsigned 64bit divide with 32bit divisor with remainder
 * @dividend: unsigned 64bit dividend
 * @divisor: unsigned 32bit divisor
 * @remainder: pointer to unsigned 32bit remainder
 *
 * Return: sets ``*remainder``, then returns dividend / divisor
 *
 * This is commonly provided by 32bit archs to provide an optimized 64bit
 * divide.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
 *remainder = dividend % divisor;
 return dividend / divisor;
}

/**
 * div_s64_rem - signed 64bit divide with 32bit divisor with remainder
 * @dividend: signed 64bit dividend
 * @divisor: signed 32bit divisor
 * @remainder: pointer to signed 32bit remainder
 *
 * Return: sets ``*remainder``, then returns dividend / divisor
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)
{
 *remainder = dividend % divisor;
 return dividend / divisor;
}

/**
 * div64_u64_rem - unsigned 64bit divide with 64bit divisor and remainder
 * @dividend: unsigned 64bit dividend
 * @divisor: unsigned 64bit divisor
 * @remainder: pointer to unsigned 64bit remainder
 *
 * Return: sets ``*remainder``, then returns dividend / divisor
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 div64_u64_rem(u64 dividend, u64 divisor, u64 *remainder)
{
 *remainder = dividend % divisor;
 return dividend / divisor;
}

/**
 * div64_u64 - unsigned 64bit divide with 64bit divisor
 * @dividend: unsigned 64bit dividend
 * @divisor: unsigned 64bit divisor
 *
 * Return: dividend / divisor
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 div64_u64(u64 dividend, u64 divisor)
{
 return dividend / divisor;
}

/**
 * div64_s64 - signed 64bit divide with 64bit divisor
 * @dividend: signed 64bit dividend
 * @divisor: signed 64bit divisor
 *
 * Return: dividend / divisor
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 div64_s64(s64 dividend, s64 divisor)
{
 return dividend / divisor;
}
# 115 "./include/linux/math64.h"
/**
 * div_u64 - unsigned 64bit divide with 32bit divisor
 * @dividend: unsigned 64bit dividend
 * @divisor: unsigned 32bit divisor
 *
 * This is the most common 64bit divide and should be used if possible,
 * as many 32bit archs can optimize this variant better than a full 64bit
 * divide.
 *
 * Return: dividend / divisor
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 div_u64(u64 dividend, u32 divisor)
{
 u32 remainder;
 return div_u64_rem(dividend, divisor, &remainder);
}


/**
 * div_s64 - signed 64bit divide with 32bit divisor
 * @dividend: signed 64bit dividend
 * @divisor: signed 32bit divisor
 *
 * Return: dividend / divisor
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 div_s64(s64 dividend, s32 divisor)
{
 s32 remainder;
 return div_s64_rem(dividend, divisor, &remainder);
}


u32 iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder);


/*
 * Many a GCC version messes this up and generates a 64x64 mult :-(
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 mul_u32_u32(u32 a, u32 b)
{
 return (u64)a * b;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 mul_u64_u32_shr(u64 a, u32 mul, unsigned int shift)
{
 return (u64)(((unsigned __int128)a * mul) >> shift);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 mul_u64_u64_shr(u64 a, u64 mul, unsigned int shift)
{
 return (u64)(((unsigned __int128)a * mul) >> shift);
}
# 243 "./include/linux/math64.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 mul_s64_u64_shr(s64 a, u64 b, unsigned int shift)
{
 u64 ret;

 /*
	 * Extract the sign before the multiplication and put it back
	 * afterwards if needed.
	 */
 ret = mul_u64_u64_shr(__builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed long long) || __builtin_types_compatible_p(typeof(a), unsigned long long), ({ signed long long __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed long) || __builtin_types_compatible_p(typeof(a), unsigned long), ({ signed long __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed int) || __builtin_types_compatible_p(typeof(a), unsigned int), ({ signed int __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed short) || __builtin_types_compatible_p(typeof(a), unsigned short), ({ signed short __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed char) || __builtin_types_compatible_p(typeof(a), unsigned char), ({ signed char __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), char), (char)({ signed char __x = (a); __x<0?-__x:__x; }), ((void)0))))))), b, shift);

 if (a < 0)
  ret = -((s64) ret);

 return ret;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 mul_u64_u32_div(u64 a, u32 mul, u32 divisor)
{
 union {
  u64 ll;
  struct {



   u32 low, high;

  } l;
 } u, rl, rh;

 u.ll = a;
 rl.ll = mul_u32_u32(u.l.low, mul);
 rh.ll = mul_u32_u32(u.l.high, mul) + rl.l.high;

 /* Bits 32-63 of the result will be in rh.l.low. */
 rl.l.high = ({ uint32_t __base = (divisor); uint32_t __rem; __rem = ((uint64_t)(rh.ll)) % __base; (rh.ll) = ((uint64_t)(rh.ll)) / __base; __rem; });

 /* Bits 0-31 of the result will be in rl.l.low.	*/
 ({ uint32_t __base = (divisor); uint32_t __rem; __rem = ((uint64_t)(rl.ll)) % __base; (rl.ll) = ((uint64_t)(rl.ll)) / __base; __rem; });

 rl.l.high = rh.l.low;
 return rl.ll;
}


u64 mul_u64_u64_div_u64(u64 a, u64 mul, u64 div);

/**
 * DIV64_U64_ROUND_UP - unsigned 64bit divide with 64bit divisor rounded up
 * @ll: unsigned 64bit dividend
 * @d: unsigned 64bit divisor
 *
 * Divide unsigned 64bit dividend by unsigned 64bit divisor
 * and round up.
 *
 * Return: dividend / divisor rounded up
 */



/**
 * DIV64_U64_ROUND_CLOSEST - unsigned 64bit divide with 64bit divisor rounded to nearest integer
 * @dividend: unsigned 64bit dividend
 * @divisor: unsigned 64bit divisor
 *
 * Divide unsigned 64bit dividend by unsigned 64bit divisor
 * and round to closest integer.
 *
 * Return: dividend / divisor rounded to nearest integer
 */



/**
 * DIV_U64_ROUND_CLOSEST - unsigned 64bit divide with 32bit divisor rounded to nearest integer
 * @dividend: unsigned 64bit dividend
 * @divisor: unsigned 32bit divisor
 *
 * Divide unsigned 64bit dividend by unsigned 32bit divisor
 * and round to closest integer.
 *
 * Return: dividend / divisor rounded to nearest integer
 */



/**
 * DIV_S64_ROUND_CLOSEST - signed 64bit divide with 32bit divisor rounded to nearest integer
 * @dividend: signed 64bit dividend
 * @divisor: signed 32bit divisor
 *
 * Divide signed 64bit dividend by signed 32bit divisor
 * and round to closest integer.
 *
 * Return: dividend / divisor rounded to nearest integer
 */
# 6 "./include/linux/time64.h" 2
# 1 "./include/vdso/time64.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/* Parameters used to convert the timespec values: */
# 7 "./include/linux/time64.h" 2

typedef __s64 time64_t;
typedef __u64 timeu64_t;

# 1 "./include/uapi/linux/time.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */




# 1 "./include/uapi/linux/time_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





struct __kernel_timespec {
 __kernel_time64_t tv_sec; /* seconds */
 long long tv_nsec; /* nanoseconds */
};

struct __kernel_itimerspec {
 struct __kernel_timespec it_interval; /* timer period */
 struct __kernel_timespec it_value; /* timer expiration */
};

/*
 * legacy timeval structure, only embedded in structures that
 * traditionally used 'timeval' to pass time intervals (not absolute
 * times). Do not add new users. If user space fails to compile
 * here, this is probably because it is not y2038 safe and needs to
 * be changed to use another interface.
 */

struct __kernel_old_timeval {
 __kernel_long_t tv_sec;
 __kernel_long_t tv_usec;
};


struct __kernel_old_timespec {
 __kernel_old_time_t tv_sec; /* seconds */
 long tv_nsec; /* nanoseconds */
};

struct __kernel_old_itimerval {
 struct __kernel_old_timeval it_interval;/* timer interval */
 struct __kernel_old_timeval it_value; /* current value */
};

struct __kernel_sock_timeval {
 __s64 tv_sec;
 __s64 tv_usec;
};
# 7 "./include/uapi/linux/time.h" 2
# 33 "./include/uapi/linux/time.h"
struct timezone {
 int tz_minuteswest; /* minutes west of Greenwich */
 int tz_dsttime; /* type of dst correction */
};

/*
 * Names of the interval timers, and structure
 * defining a timer setting:
 */




/*
 * The IDs of the various system clocks (for POSIX.1b interval timers):
 */
# 59 "./include/uapi/linux/time.h"
/*
 * The driver implementing this got removed. The clock ID is kept as a
 * place holder. Do not reuse!
 */







/*
 * The various flags for setting POSIX.1b interval timers:
 */
# 12 "./include/linux/time64.h" 2

struct timespec64 {
 time64_t tv_sec; /* seconds */
 long tv_nsec; /* nanoseconds */
};

struct itimerspec64 {
 struct timespec64 it_interval;
 struct timespec64 it_value;
};

/* Parameters used to convert the timespec values: */


/* Located here for timespec[64]_valid_strict */
# 35 "./include/linux/time64.h"
/*
 * Limits for settimeofday():
 *
 * To prevent setting the time close to the wraparound point time setting
 * is limited so a reasonable uptime can be accomodated. Uptime of 30 years
 * should be really sufficient, which means the cutoff is 2232. At that
 * point the cutoff is just a small part of the larger problem.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int timespec64_equal(const struct timespec64 *a,
       const struct timespec64 *b)
{
 return (a->tv_sec == b->tv_sec) && (a->tv_nsec == b->tv_nsec);
}

/*
 * lhs < rhs:  return <0
 * lhs == rhs: return 0
 * lhs > rhs:  return >0
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int timespec64_compare(const struct timespec64 *lhs, const struct timespec64 *rhs)
{
 if (lhs->tv_sec < rhs->tv_sec)
  return -1;
 if (lhs->tv_sec > rhs->tv_sec)
  return 1;
 return lhs->tv_nsec - rhs->tv_nsec;
}

extern void set_normalized_timespec64(struct timespec64 *ts, time64_t sec, s64 nsec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct timespec64 timespec64_add(struct timespec64 lhs,
      struct timespec64 rhs)
{
 struct timespec64 ts_delta;
 set_normalized_timespec64(&ts_delta, lhs.tv_sec + rhs.tv_sec,
    lhs.tv_nsec + rhs.tv_nsec);
 return ts_delta;
}

/*
 * sub = lhs - rhs, in normalized form
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct timespec64 timespec64_sub(struct timespec64 lhs,
      struct timespec64 rhs)
{
 struct timespec64 ts_delta;
 set_normalized_timespec64(&ts_delta, lhs.tv_sec - rhs.tv_sec,
    lhs.tv_nsec - rhs.tv_nsec);
 return ts_delta;
}

/*
 * Returns true if the timespec64 is norm, false if denorm:
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool timespec64_valid(const struct timespec64 *ts)
{
 /* Dates before 1970 are bogus */
 if (ts->tv_sec < 0)
  return false;
 /* Can't have more nanoseconds then a second */
 if ((unsigned long)ts->tv_nsec >= 1000000000L)
  return false;
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool timespec64_valid_strict(const struct timespec64 *ts)
{
 if (!timespec64_valid(ts))
  return false;
 /* Disallow values that could overflow ktime_t */
 if ((unsigned long long)ts->tv_sec >= (((s64)~((u64)1 << 63)) / 1000000000L))
  return false;
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool timespec64_valid_settod(const struct timespec64 *ts)
{
 if (!timespec64_valid(ts))
  return false;
 /* Disallow values which cause overflow issues vs. CLOCK_REALTIME */
 if ((unsigned long long)ts->tv_sec >= ((((s64)~((u64)1 << 63)) / 1000000000L) - (30LL * 365 * 24 *3600)))
  return false;
 return true;
}

/**
 * timespec64_to_ns - Convert timespec64 to nanoseconds
 * @ts:		pointer to the timespec64 variable to be converted
 *
 * Returns the scalar nanosecond representation of the timespec64
 * parameter.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 timespec64_to_ns(const struct timespec64 *ts)
{
 /* Prevent multiplication overflow / underflow */
 if (ts->tv_sec >= (((s64)~((u64)1 << 63)) / 1000000000L))
  return ((s64)~((u64)1 << 63));

 if (ts->tv_sec <= ((-((s64)~((u64)1 << 63)) - 1) / 1000000000L))
  return (-((s64)~((u64)1 << 63)) - 1);

 return ((s64) ts->tv_sec * 1000000000L) + ts->tv_nsec;
}

/**
 * ns_to_timespec64 - Convert nanoseconds to timespec64
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timespec64 representation of the nsec parameter.
 */
extern struct timespec64 ns_to_timespec64(s64 nsec);

/**
 * timespec64_add_ns - Adds nanoseconds to a timespec64
 * @a:		pointer to timespec64 to be incremented
 * @ns:		unsigned nanoseconds value to be added
 *
 * This must always be inlined because its used from the x86-64 vdso,
 * which cannot call other kernel functions.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void timespec64_add_ns(struct timespec64 *a, u64 ns)
{
 a->tv_sec += __iter_div_u64_rem(a->tv_nsec + ns, 1000000000L, &ns);
 a->tv_nsec = ns;
}

/*
 * timespec64_add_safe assumes both values are positive and checks for
 * overflow. It will return TIME64_MAX in case of overflow.
 */
extern struct timespec64 timespec64_add_safe(const struct timespec64 lhs,
      const struct timespec64 rhs);
# 11 "./include/linux/restart_block.h" 2

struct timespec;
struct old_timespec32;
struct pollfd;

enum timespec_type {
 TT_NONE = 0,
 TT_NATIVE = 1,
 TT_COMPAT = 2,
};

/*
 * System call restart block.
 */
struct restart_block {
 unsigned long arch_data;
 long (*fn)(struct restart_block *);
 union {
  /* For futex_wait and futex_wait_requeue_pi */
  struct {
   u32 /* nothing */ *uaddr;
   u32 val;
   u32 flags;
   u32 bitset;
   u64 time;
   u32 /* nothing */ *uaddr2;
  } futex;
  /* For nanosleep */
  struct {
   clockid_t clockid;
   enum timespec_type type;
   union {
    struct __kernel_timespec /* nothing */ *rmtp;
    struct old_timespec32 /* nothing */ *compat_rmtp;
   };
   u64 expires;
  } nanosleep;
  /* For poll */
  struct {
   struct pollfd /* nothing */ *ufds;
   int nfds;
   int has_timeout;
   unsigned long tv_sec;
   unsigned long tv_nsec;
  } poll;
 };
};

extern long do_no_restart_syscall(struct restart_block *parm);
# 15 "./include/linux/thread_info.h" 2
# 1 "./include/linux/errno.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/errno.h" 1
# 1 "./arch/arm64/include/generated/uapi/asm/errno.h" 1
# 2 "./include/uapi/linux/errno.h" 2
# 6 "./include/linux/errno.h" 2


/*
 * These should never be seen by user programs.  To return one of ERESTART*
 * codes, signal_pending() MUST be set.  Note that ptrace can observe these
 * at syscall exit tracing, but they will never be left for the debugged user
 * process to see.
 */
# 23 "./include/linux/errno.h"
/* Defined for the NFSv3 protocol */
# 16 "./include/linux/thread_info.h" 2


/*
 * For CONFIG_THREAD_INFO_IN_TASK kernels we need <asm/current.h> for the
 * definition of current, but for !CONFIG_THREAD_INFO_IN_TASK kernels,
 * including <asm/current.h> can cause a circular dependency on some platforms.
 */
# 1 "./arch/arm64/include/asm/current.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







struct task_struct;

/*
 * We don't use read_sysreg() as we want the compiler to cache the value where
 * possible.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct task_struct *get_current(void)
{
 unsigned long sp_el0;

 asm ("mrs %0, sp_el0" : "=r" (sp_el0));

 return (struct task_struct *)sp_el0;
}
# 24 "./include/linux/thread_info.h" 2



# 1 "./include/linux/bitops.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 6 "./include/linux/bitops.h" 2





/* Set bits in the first 'n' bytes when loaded from memory */
# 24 "./include/linux/bitops.h"
extern unsigned int __sw_hweight8(unsigned int w);
extern unsigned int __sw_hweight16(unsigned int w);
extern unsigned int __sw_hweight32(unsigned int w);
extern unsigned long __sw_hweight64(__u64 w);

/*
 * Defined here because those may be needed by architecture-specific static
 * inlines.
 */

# 1 "./include/asm-generic/bitops/generic-non-atomic.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
# 13 "./include/asm-generic/bitops/generic-non-atomic.h"
/*
 * Generic definitions for bit operations, should not be used in regular code
 * directly.
 */

/**
 * generic___set_bit - Set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * Unlike set_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
generic___set_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

 *p |= mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
generic___clear_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

 *p &= ~mask;
}

/**
 * generic___change_bit - Toggle a bit in memory
 * @nr: the bit to change
 * @addr: the address to start counting from
 *
 * Unlike change_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
generic___change_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

 *p ^= mask;
}

/**
 * generic___test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
generic___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 unsigned long old = *p;

 *p = old | mask;
 return (old & mask) != 0;
}

/**
 * generic___test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
generic___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 unsigned long old = *p;

 *p = old & ~mask;
 return (old & mask) != 0;
}

/* WARNING: non atomic and it can be reordered! */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
generic___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 unsigned long old = *p;

 *p = old ^ mask;
 return (old & mask) != 0;
}

/**
 * generic_test_bit - Determine whether a bit is set
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
generic_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
 /*
	 * Unlike the bitops with the '__' prefix above, this one *is* atomic,
	 * so `volatile` must always stay here with no cast-aways. See
	 * `Documentation/atomic_bitops.txt` for the details.
	 */
 return 1UL & (addr[((nr) / 64)] >> (nr & (64 -1)));
}

/**
 * generic_test_bit_acquire - Determine, with acquire semantics, whether a bit is set
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
generic_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
{
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 return 1UL & (({ union { typeof( _Generic((*p), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*p))) __val; char __c[1]; } __u; typeof(p) __p = (p); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_128(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*p) == sizeof(char) || sizeof(*p) == sizeof(short) || sizeof(*p) == sizeof(int) || sizeof(*p) == sizeof(long)))) __compiletime_assert_128(); } while (0); kasan_check_read(__p, sizeof(*p)); switch (sizeof(*p)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*p))__u.__val; }) >> (nr & (64 -1)));
# 141 "./include/asm-generic/bitops/generic-non-atomic.h"
}

/*
 * const_*() definitions provide good compile-time optimizations when
 * the passed arguments can be resolved at compile time.
 */
# 155 "./include/asm-generic/bitops/generic-non-atomic.h"
/**
 * const_test_bit - Determine whether a bit is set
 * @nr: bit number to test
 * @addr: Address to start counting from
 *
 * A version of generic_test_bit() which discards the `volatile` qualifier to
 * allow a compiler to optimize code harder. Non-atomic and to be called only
 * for testing compile-time constants, e.g. by the corresponding macros, not
 * directly from "regular" code.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
const_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
 const unsigned long *p = (const unsigned long *)addr + ((nr) / 64);
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long val = *p;

 return !!(val & mask);
}
# 35 "./include/linux/bitops.h" 2

/*
 * Many architecture-specific non-atomic bitops contain inline asm code and due
 * to that the compiler can't optimize them to compile-time expressions or
 * constants. In contrary, generic_*() helpers are defined in pure C and
 * compilers optimize them just well.
 * Therefore, to make `unsigned long foo = 0; __set_bit(BAR, &foo)` effectively
 * equal to `unsigned long foo = BIT(BAR)`, pick the generic C alternative when
 * the arguments can be resolved at compile time. That expression itself is a
 * constant and doesn't bring any functional changes to the rest of cases.
 * The casts to `uintptr_t` are needed to mitigate `-Waddress` warnings when
 * passing a bitmap from .bss or .data (-> `!!addr` is always true).
 */
# 64 "./include/linux/bitops.h"
/*
 * Include this here because some architectures need generic_ffs/fls in
 * scope
 */
# 1 "./arch/arm64/include/asm/bitops.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 14 "./arch/arm64/include/asm/bitops.h"
# 1 "./include/asm-generic/bitops/builtin-__ffs.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/**
 * __ffs - find first bit in word.
 * @word: The word to search
 *
 * Undefined if no bit exists, so code should check against 0 first.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __ffs(unsigned long word)
{
 return __builtin_ctzl(word);
}
# 15 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/builtin-ffs.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/**
 * ffs - find first bit set
 * @x: the word to search
 *
 * This is defined the same way as
 * the libc and compiler builtin ffs routines, therefore
 * differs in spirit from ffz (man ffs).
 */
# 16 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/builtin-__fls.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/**
 * __fls - find last (most-significant) set bit in a long word
 * @word: the word to search
 *
 * Undefined if no set bit exists, so code should check against 0 first.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __fls(unsigned long word)
{
 return (sizeof(word) * 8) - 1 - __builtin_clzl(word);
}
# 17 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/builtin-fls.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/**
 * fls - find last (most-significant) bit set
 * @x: the word to search
 *
 * This is defined the same way as ffs.
 * Note fls(0) = 0, fls(1) = 1, fls(0x80000000) = 32.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int fls(unsigned int x)
{
 return x ? sizeof(x) * 8 - __builtin_clz(x) : 0;
}
# 18 "./arch/arm64/include/asm/bitops.h" 2

# 1 "./include/asm-generic/bitops/ffz.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * ffz - find first zero in word.
 * @word: The word to search
 *
 * Undefined if no zero exists, so code should check against ~0UL first.
 */
# 20 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/fls64.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 6 "./include/asm-generic/bitops/fls64.h" 2

/**
 * fls64 - find last set bit in a 64-bit word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffsll, but returns the position of the most significant set bit.
 *
 * fls64(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 64.
 */
# 27 "./include/asm-generic/bitops/fls64.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int fls64(__u64 x)
{
 if (x == 0)
  return 0;
 return __fls(x) + 1;
}
# 21 "./arch/arm64/include/asm/bitops.h" 2

# 1 "./include/asm-generic/bitops/sched.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 7 "./include/asm-generic/bitops/sched.h" 2

/*
 * Every architecture must define this function. It's the fastest
 * way of searching a 100-bit bitmap.  It's guaranteed that at least
 * one of the 100 bits is cleared.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sched_find_first_bit(const unsigned long *b)
{

 if (b[0])
  return __ffs(b[0]);
 return __ffs(b[1]) + 64;
# 30 "./include/asm-generic/bitops/sched.h"
}
# 23 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/hweight.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/asm-generic/bitops/arch_hweight.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 6 "./include/asm-generic/bitops/arch_hweight.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __arch_hweight32(unsigned int w)
{
 return __sw_hweight32(w);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __arch_hweight16(unsigned int w)
{
 return __sw_hweight16(w);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __arch_hweight8(unsigned int w)
{
 return __sw_hweight8(w);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __arch_hweight64(__u64 w)
{
 return __sw_hweight64(w);
}
# 6 "./include/asm-generic/bitops/hweight.h" 2
# 1 "./include/asm-generic/bitops/const_hweight.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Compile time versions of __arch_hweightN()
 */
# 23 "./include/asm-generic/bitops/const_hweight.h"
/*
 * Generic interface.
 */





/*
 * Interface for known constant arguments
 */





/*
 * Type invariant interface to the compile time constant hweight functions.
 */
# 7 "./include/asm-generic/bitops/hweight.h" 2
# 24 "./arch/arm64/include/asm/bitops.h" 2

# 1 "./include/asm-generic/bitops/atomic.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/atomic.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/* Atomic operations usable in machine independent code */




# 1 "./arch/arm64/include/asm/atomic.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/atomic.h
 *
 * Copyright (C) 1996 Russell King.
 * Copyright (C) 2002 Deep Blue Solutions Ltd.
 * Copyright (C) 2012 ARM Ltd.
 */







# 1 "./arch/arm64/include/asm/cmpxchg.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/cmpxchg.h
 *
 * Copyright (C) 2012 ARM Ltd.
 */







# 1 "./arch/arm64/include/asm/lse.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/asm/atomic_ll_sc.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/atomic.h
 *
 * Copyright (C) 1996 Russell King.
 * Copyright (C) 2002 Deep Blue Solutions Ltd.
 * Copyright (C) 2012 ARM Ltd.
 */
# 19 "./arch/arm64/include/asm/atomic_ll_sc.h"
/*
 * AArch64 UP and SMP safe atomic ops.  We use load exclusive and
 * store exclusive to ensure that these are atomic.  We may loop
 * to ensure that the update happens.
 */
# 95 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic_add(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "add" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%w0, %2\n" "	" "add" "	%w0, %w0, %w3\n" "	stxr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_add_return(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "add" "_return" "" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%w0, %2\n" "	" "add" "	%w0, %w0, %w3\n" "	st" "l" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_add_return_relaxed(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "add" "_return" "_relaxed" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%w0, %2\n" "	" "add" "	%w0, %w0, %w3\n" "	st" "" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_add_return_acquire(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "add" "_return" "_acquire" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr	%w0, %2\n" "	" "add" "	%w0, %w0, %w3\n" "	st" "" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_add_return_release(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "add" "_return" "_release" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%w0, %2\n" "	" "add" "	%w0, %w0, %w3\n" "	st" "l" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_add(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "add" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "add" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_add_relaxed(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "add" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "add" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_add_acquire(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "add" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%w0, %3\n" "	" "add" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_add_release(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "add" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "add" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic_sub(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "sub" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%w0, %2\n" "	" "sub" "	%w0, %w0, %w3\n" "	stxr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_sub_return(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "sub" "_return" "" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%w0, %2\n" "	" "sub" "	%w0, %w0, %w3\n" "	st" "l" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_sub_return_relaxed(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "sub" "_return" "_relaxed" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%w0, %2\n" "	" "sub" "	%w0, %w0, %w3\n" "	st" "" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_sub_return_acquire(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "sub" "_return" "_acquire" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr	%w0, %2\n" "	" "sub" "	%w0, %w0, %w3\n" "	st" "" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_sub_return_release(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "sub" "_return" "_release" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%w0, %2\n" "	" "sub" "	%w0, %w0, %w3\n" "	st" "l" "xr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_sub(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "sub" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "sub" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_sub_relaxed(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "sub" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "sub" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_sub_acquire(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "sub" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%w0, %3\n" "	" "sub" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_sub_release(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "sub" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "sub" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; }
# 106 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic_and(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "and" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%w0, %2\n" "	" "and" "	%w0, %w0, %w3\n" "	stxr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_and(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "and" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "and" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_and_relaxed(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "and" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "and" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_and_acquire(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "and" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%w0, %3\n" "	" "and" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_and_release(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "and" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "and" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic_or(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "or" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%w0, %2\n" "	" "orr" "	%w0, %w0, %w3\n" "	stxr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_or(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "or" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "orr" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_or_relaxed(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "or" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "orr" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_or_acquire(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "or" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%w0, %3\n" "	" "orr" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_or_release(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "or" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "orr" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic_xor(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "xor" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%w0, %2\n" "	" "eor" "	%w0, %w0, %w3\n" "	stxr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_xor(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "xor" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "eor" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_xor_relaxed(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "xor" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "eor" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_xor_acquire(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "xor" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%w0, %3\n" "	" "eor" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_xor_release(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "xor" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "eor" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "K" "r" (i) : "memory"); return result; }
/*
 * GAS converts the mysterious and undocumented BIC (immediate) alias to
 * an AND (immediate) instruction with the immediate inverted. We don't
 * have a constraint for this, so fall back to register.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic_andnot(int i, atomic_t *v) { unsigned long tmp; int result; asm volatile("// atomic_" "andnot" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%w0, %2\n" "	" "bic" "	%w0, %w0, %w3\n" "	stxr	%w1, %w0, %2\n" "	cbnz	%w1, 1b\n" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_andnot(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "andnot" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "bic" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_andnot_relaxed(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "andnot" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "bic" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_andnot_acquire(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "andnot" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%w0, %3\n" "	" "bic" "	%w1, %w0, %w4\n" "	st" "" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ll_sc_atomic_fetch_andnot_release(int i, atomic_t *v) { unsigned long tmp; int val, result; asm volatile("// atomic_fetch_" "andnot" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%w0, %3\n" "	" "bic" "	%w1, %w0, %w4\n" "	st" "l" "xr	%w2, %w1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : "memory"); return result; }
# 191 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic64_add(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "add" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%0, %2\n" "	" "add" "	%0, %0, %3\n" "	stxr	%w1, %0, %2\n" "	cbnz	%w1, 1b" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_add_return(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "add" "_return" "" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%0, %2\n" "	" "add" "	%0, %0, %3\n" "	st" "l" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_add_return_relaxed(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "add" "_return" "_relaxed" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%0, %2\n" "	" "add" "	%0, %0, %3\n" "	st" "" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_add_return_acquire(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "add" "_return" "_acquire" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr	%0, %2\n" "	" "add" "	%0, %0, %3\n" "	st" "" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_add_return_release(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "add" "_return" "_release" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%0, %2\n" "	" "add" "	%0, %0, %3\n" "	st" "l" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_add(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "add" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "add" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_add_relaxed(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "add" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "add" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_add_acquire(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "add" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%0, %3\n" "	" "add" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_add_release(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "add" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "add" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "I" "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic64_sub(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "sub" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%0, %2\n" "	" "sub" "	%0, %0, %3\n" "	stxr	%w1, %0, %2\n" "	cbnz	%w1, 1b" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_sub_return(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "sub" "_return" "" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%0, %2\n" "	" "sub" "	%0, %0, %3\n" "	st" "l" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_sub_return_relaxed(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "sub" "_return" "_relaxed" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%0, %2\n" "	" "sub" "	%0, %0, %3\n" "	st" "" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_sub_return_acquire(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "sub" "_return" "_acquire" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr	%0, %2\n" "	" "sub" "	%0, %0, %3\n" "	st" "" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_sub_return_release(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "sub" "_return" "_release" "\n" "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr	%0, %2\n" "	" "sub" "	%0, %0, %3\n" "	st" "l" "xr	%w1, %0, %2\n" "	cbnz	%w1, 1b\n" "	" "" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_sub(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "sub" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "sub" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "sub" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "sub" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_sub_acquire(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "sub" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%0, %3\n" "	" "sub" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_sub_release(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "sub" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "sub" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "J" "r" (i) : "memory"); return result; }
# 202 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic64_and(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "and" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%0, %2\n" "	" "and" "	%0, %0, %3\n" "	stxr	%w1, %0, %2\n" "	cbnz	%w1, 1b" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_and(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "and" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "and" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_and_relaxed(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "and" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "and" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_and_acquire(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "and" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%0, %3\n" "	" "and" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_and_release(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "and" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "and" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic64_or(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "or" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%0, %2\n" "	" "orr" "	%0, %0, %3\n" "	stxr	%w1, %0, %2\n" "	cbnz	%w1, 1b" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_or(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "or" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "orr" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_or_relaxed(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "or" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "orr" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_or_acquire(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "or" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%0, %3\n" "	" "orr" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_or_release(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "or" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "orr" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic64_xor(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "xor" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%0, %2\n" "	" "eor" "	%0, %0, %3\n" "	stxr	%w1, %0, %2\n" "	cbnz	%w1, 1b" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_xor(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "xor" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "eor" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "xor" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "eor" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_xor_acquire(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "xor" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%0, %3\n" "	" "eor" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_xor_release(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "xor" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "eor" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "L" "r" (i) : "memory"); return result; }
/*
 * GAS converts the mysterious and undocumented BIC (immediate) alias to
 * an AND (immediate) instruction with the immediate inverted. We don't
 * have a constraint for this, so fall back to register.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ll_sc_atomic64_andnot(s64 i, atomic64_t *v) { s64 result; unsigned long tmp; asm volatile("// atomic64_" "andnot" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxr	%0, %2\n" "	" "bic" "	%0, %0, %3\n" "	stxr	%w1, %0, %2\n" "	cbnz	%w1, 1b" : "=&r" (result), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_andnot(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "andnot" "" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "bic" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "dmb ish" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "andnot" "_relaxed" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "bic" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : ); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "andnot" "_acquire" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "a" "xr	%0, %3\n" "	" "bic" "	%1, %0, %4\n" "	st" "" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : "memory"); return result; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc_atomic64_fetch_andnot_release(s64 i, atomic64_t *v) { s64 result, val; unsigned long tmp; asm volatile("// atomic64_fetch_" "andnot" "_release" "\n" "	prfm	pstl1strm, %3\n" "1:	ld" "" "xr	%0, %3\n" "	" "bic" "	%1, %0, %4\n" "	st" "l" "xr	%w2, %1, %3\n" "	cbnz	%w2, 1b\n" "	" "" : "=&r" (result), "=&r" (val), "=&r" (tmp), "+Q" (v->counter) : "" "r" (i) : "memory"); return result; }






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
__ll_sc_atomic64_dec_if_positive(atomic64_t *v)
{
 s64 result;
 unsigned long tmp;

 asm volatile("// atomic64_dec_if_positive\n"
 "	prfm	pstl1strm, %2\n"
 "1:	ldxr	%0, %2\n"
 "	subs	%0, %0, #1\n"
 "	b.lt	2f\n"
 "	stlxr	%w1, %0, %2\n"
 "	cbnz	%w1, 1b\n"
 "	dmb	ish\n"
 "2:"
 : "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
 :
 : "cc", "memory");

 return result;
}
# 273 "./arch/arm64/include/asm/atomic_ll_sc.h"
/*
 * Earlier versions of GCC (no later than 8.1.0) appear to incorrectly
 * handle the 'K' constraint for the value 4294967295 - thus we use no
 * constraint for 32 bit operations.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __ll_sc__cmpxchg_case_8(volatile void *ptr, unsigned long old, u8 new) { unsigned long tmp; u8 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (8 < 32) old = (u8)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "b" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "" "xr" "b" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u8 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : ); return oldval; }
# 279 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __ll_sc__cmpxchg_case_16(volatile void *ptr, unsigned long old, u16 new) { unsigned long tmp; u16 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (16 < 32) old = (u16)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "h" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "" "xr" "h" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u16 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : ); return oldval; }
# 280 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __ll_sc__cmpxchg_case_32(volatile void *ptr, unsigned long old, u32 new) { unsigned long tmp; u32 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (32 < 32) old = (u32)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "" "xr" "" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u32 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : ); return oldval; }
# 281 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __ll_sc__cmpxchg_case_64(volatile void *ptr, unsigned long old, u64 new) { unsigned long tmp; u64 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (64 < 32) old = (u64)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "" "\t%" "" "[oldval], %[v]\n" "	eor	%" "" "[tmp], %" "" "[oldval], %" "" "[old]\n" "	cbnz	%" "" "[tmp], 2f\n" "	st" "" "xr" "" "\t%w[tmp], %" "" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u64 *)ptr) : [old] "L" "r" (old), [new] "r" (new) : ); return oldval; }
# 282 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __ll_sc__cmpxchg_case_acq_8(volatile void *ptr, unsigned long old, u8 new) { unsigned long tmp; u8 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (8 < 32) old = (u8)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "a" "xr" "b" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "" "xr" "b" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u8 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 283 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __ll_sc__cmpxchg_case_acq_16(volatile void *ptr, unsigned long old, u16 new) { unsigned long tmp; u16 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (16 < 32) old = (u16)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "a" "xr" "h" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "" "xr" "h" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u16 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 284 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __ll_sc__cmpxchg_case_acq_32(volatile void *ptr, unsigned long old, u32 new) { unsigned long tmp; u32 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (32 < 32) old = (u32)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "a" "xr" "" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "" "xr" "" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u32 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 285 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __ll_sc__cmpxchg_case_acq_64(volatile void *ptr, unsigned long old, u64 new) { unsigned long tmp; u64 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (64 < 32) old = (u64)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "a" "xr" "" "\t%" "" "[oldval], %[v]\n" "	eor	%" "" "[tmp], %" "" "[oldval], %" "" "[old]\n" "	cbnz	%" "" "[tmp], 2f\n" "	st" "" "xr" "" "\t%w[tmp], %" "" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u64 *)ptr) : [old] "L" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 286 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __ll_sc__cmpxchg_case_rel_8(volatile void *ptr, unsigned long old, u8 new) { unsigned long tmp; u8 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (8 < 32) old = (u8)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "b" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "l" "xr" "b" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u8 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 287 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __ll_sc__cmpxchg_case_rel_16(volatile void *ptr, unsigned long old, u16 new) { unsigned long tmp; u16 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (16 < 32) old = (u16)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "h" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "l" "xr" "h" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u16 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 288 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __ll_sc__cmpxchg_case_rel_32(volatile void *ptr, unsigned long old, u32 new) { unsigned long tmp; u32 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (32 < 32) old = (u32)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "l" "xr" "" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u32 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 289 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __ll_sc__cmpxchg_case_rel_64(volatile void *ptr, unsigned long old, u64 new) { unsigned long tmp; u64 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (64 < 32) old = (u64)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "" "\t%" "" "[oldval], %[v]\n" "	eor	%" "" "[tmp], %" "" "[oldval], %" "" "[old]\n" "	cbnz	%" "" "[tmp], 2f\n" "	st" "l" "xr" "" "\t%w[tmp], %" "" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u64 *)ptr) : [old] "L" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 290 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __ll_sc__cmpxchg_case_mb_8(volatile void *ptr, unsigned long old, u8 new) { unsigned long tmp; u8 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (8 < 32) old = (u8)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "b" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "l" "xr" "b" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "dmb ish" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u8 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 291 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __ll_sc__cmpxchg_case_mb_16(volatile void *ptr, unsigned long old, u16 new) { unsigned long tmp; u16 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (16 < 32) old = (u16)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "h" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "l" "xr" "h" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "dmb ish" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u16 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 292 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __ll_sc__cmpxchg_case_mb_32(volatile void *ptr, unsigned long old, u32 new) { unsigned long tmp; u32 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (32 < 32) old = (u32)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "" "\t%" "w" "[oldval], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[oldval], %" "w" "[old]\n" "	cbnz	%" "w" "[tmp], 2f\n" "	st" "l" "xr" "" "\t%w[tmp], %" "w" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "dmb ish" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u32 *)ptr) : [old] "K" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 293 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __ll_sc__cmpxchg_case_mb_64(volatile void *ptr, unsigned long old, u64 new) { unsigned long tmp; u64 oldval; /*								\
	 * Sub-word sizes require explicit casting so that the compare  \
	 * part of the cmpxchg doesn't end up interpreting non-zero	\
	 * upper bits of the register containing "old".			\
	 */ if (64 < 32) old = (u64)old; asm volatile( "	prfm	pstl1strm, %[v]\n" "1:	ld" "" "xr" "" "\t%" "" "[oldval], %[v]\n" "	eor	%" "" "[tmp], %" "" "[oldval], %" "" "[old]\n" "	cbnz	%" "" "[tmp], 2f\n" "	st" "l" "xr" "" "\t%w[tmp], %" "" "[new], %[v]\n" "	cbnz	%w[tmp], 1b\n" "	" "dmb ish" "\n" "2:" : [tmp] "=&r" (tmp), [oldval] "=&r" (oldval), [v] "+Q" (*(u64 *)ptr) : [old] "L" "r" (old), [new] "r" (new) : "memory"); return oldval; }
# 325 "./arch/arm64/include/asm/atomic_ll_sc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc__cmpxchg_double(unsigned long old1, unsigned long old2, unsigned long new1, unsigned long new2, volatile void *ptr) { unsigned long tmp, ret; asm volatile("// __cmpxchg_double" "" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxp	%0, %1, %2\n" "	eor	%0, %0, %3\n" "	eor	%1, %1, %4\n" "	orr	%1, %0, %1\n" "	cbnz	%1, 2f\n" "	st" "" "xp	%w0, %5, %6, %2\n" "	cbnz	%w0, 1b\n" "	" "" "\n" "2:" : "=&r" (tmp), "=&r" (ret), "+Q" (*(unsigned long *)ptr) : "r" (old1), "r" (old2), "r" (new1), "r" (new2) : ); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __ll_sc__cmpxchg_double_mb(unsigned long old1, unsigned long old2, unsigned long new1, unsigned long new2, volatile void *ptr) { unsigned long tmp, ret; asm volatile("// __cmpxchg_double" "_mb" "\n" "	prfm	pstl1strm, %2\n" "1:	ldxp	%0, %1, %2\n" "	eor	%0, %0, %3\n" "	eor	%1, %1, %4\n" "	orr	%1, %0, %1\n" "	cbnz	%1, 2f\n" "	st" "l" "xp	%w0, %5, %6, %2\n" "	cbnz	%w0, 1b\n" "	" "dmb ish" "\n" "2:" : "=&r" (tmp), "=&r" (ret), "+Q" (*(unsigned long *)ptr) : "r" (old1), "r" (old2), "r" (new1), "r" (new2) : "memory"); return ret; }
# 6 "./arch/arm64/include/asm/lse.h" 2








# 1 "./arch/arm64/include/asm/alternative.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/asm/alternative-macros.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






# 1 "./arch/arm64/include/generated/asm/cpucaps.h" 1



/* Generated file - do not edit */
# 9 "./arch/arm64/include/asm/alternative-macros.h" 2


/*
 * Binutils 2.27.0 can't handle a 'UL' suffix on constants, so for the assembly
 * macros below we must use we must use `(1 << ARM64_CB_SHIFT)`.
 */
# 40 "./arch/arm64/include/asm/alternative-macros.h"
/*
 * alternative assembly primitive:
 *
 * If any of these .org directive fail, it means that insn1 and insn2
 * don't have the same length. This used to be written as
 *
 * .if ((664b-663b) != (662b-661b))
 * 	.error "Alternatives instruction length mismatch"
 * .endif
 *
 * but most assemblers die if insn1 or insn2 have a .inst. This should
 * be fixed in a binutils release posterior to 2.25.51.0.2 (anything
 * containing commit 4e4d08cf7399b606 or c1baaddf8861).
 *
 * Alternatives with callbacks do not generate replacement instructions.
 */
# 212 "./arch/arm64/include/asm/alternative-macros.h"
/*
 * Usage: asm(ALTERNATIVE(oldinstr, newinstr, feature));
 *
 * Usage: asm(ALTERNATIVE(oldinstr, newinstr, feature, CONFIG_FOO));
 * N.B. If CONFIG_FOO is specified, but not selected, the whole block
 *      will be omitted, including oldinstr.
 */







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
alternative_has_feature_likely(const unsigned long feature)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_129(void) __attribute__((__error__("feature must be < ARM64_NCAPS"))); if (!(feature < 84)) __compiletime_assert_129(); } while (0);
# 232 "./arch/arm64/include/asm/alternative-macros.h"
 asm goto(".if ""1"" == 1\n" "661:\n\t" "b	%l[l_no]" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word " "alt_cb_patch_nops" "- .\n" /* callback */ " .hword " "(1 << 15) | (%[feature])" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" "663:\n\t" "664:\n\t" ".endif\n" : : [feature] "i" (feature) : : l_no);






 return true;
l_no:
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
alternative_has_feature_unlikely(const unsigned long feature)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_130(void) __attribute__((__error__("feature must be < ARM64_NCAPS"))); if (!(feature < 84)) __compiletime_assert_130(); } while (0);
# 250 "./arch/arm64/include/asm/alternative-macros.h"
 asm goto(".if ""1"" == 1\n" "661:\n\t" "nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "%[feature]" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "b	%l[l_yes]" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : [feature] "i" (feature) : : l_yes);






 return false;
l_yes:
 return true;
}
# 6 "./arch/arm64/include/asm/alternative.h" 2







struct alt_instr {
 s32 orig_offset; /* offset to original instruction */
 s32 alt_offset; /* offset to replacement instruction */
 u16 cpufeature; /* cpufeature bit set for replacement */
 u8 orig_len; /* size of original instruction(s) */
 u8 alt_len; /* size of new instruction(s), <= orig_len */
};

typedef void (*alternative_cb_t)(struct alt_instr *alt,
     __le32 *origptr, __le32 *updptr, int nr_inst);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) apply_boot_alternatives(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) apply_alternatives_all(void);
bool alternative_is_applied(u16 cpufeature);


void apply_alternatives_module(void *start, size_t length);
# 15 "./arch/arm64/include/asm/lse.h" 2

# 1 "./arch/arm64/include/asm/atomic_lse.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/atomic.h
 *
 * Copyright (C) 1996 Russell King.
 * Copyright (C) 2002 Deep Blue Solutions Ltd.
 * Copyright (C) 2012 ARM Ltd.
 */
# 24 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic_andnot(int i, atomic_t *v) { asm volatile( ".arch_extension lse\n" "	" "stclr" "	%w[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic_or(int i, atomic_t *v) { asm volatile( ".arch_extension lse\n" "	" "stset" "	%w[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic_xor(int i, atomic_t *v) { asm volatile( ".arch_extension lse\n" "	" "steor" "	%w[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic_add(int i, atomic_t *v) { asm volatile( ".arch_extension lse\n" "	" "stadd" "	%w[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic_sub(int i, atomic_t *v)
{
 __lse_atomic_add(-i, v);
}
# 59 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_andnot_relaxed(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_andnot_acquire(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "a" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_andnot_release(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "l" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_andnot(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "al" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_or_relaxed(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldset" "" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_or_acquire(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldset" "a" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_or_release(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldset" "l" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_or(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldset" "al" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_xor_relaxed(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_xor_acquire(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "a" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_xor_release(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "l" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_xor(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "al" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_add_relaxed(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_add_acquire(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "a" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_add_release(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "l" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_add(int i, atomic_t *v) { int old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "al" "	%w[i], %w[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
# 74 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_sub_relaxed(int i, atomic_t *v) { return __lse_atomic_fetch_add_relaxed(-i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_sub_acquire(int i, atomic_t *v) { return __lse_atomic_fetch_add_acquire(-i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_sub_release(int i, atomic_t *v) { return __lse_atomic_fetch_add_release(-i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_sub(int i, atomic_t *v) { return __lse_atomic_fetch_add(-i, v); }
# 94 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_add_return_relaxed(int i, atomic_t *v) { return __lse_atomic_fetch_add_relaxed(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_sub_return_relaxed(int i, atomic_t *v) { return __lse_atomic_fetch_sub(i, v) - i; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_add_return_acquire(int i, atomic_t *v) { return __lse_atomic_fetch_add_acquire(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_sub_return_acquire(int i, atomic_t *v) { return __lse_atomic_fetch_sub(i, v) - i; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_add_return_release(int i, atomic_t *v) { return __lse_atomic_fetch_add_release(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_sub_return_release(int i, atomic_t *v) { return __lse_atomic_fetch_sub(i, v) - i; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_add_return(int i, atomic_t *v) { return __lse_atomic_fetch_add(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_sub_return(int i, atomic_t *v) { return __lse_atomic_fetch_sub(i, v) - i; }



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic_and(int i, atomic_t *v)
{
 return __lse_atomic_andnot(~i, v);
}
# 113 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_and_relaxed(int i, atomic_t *v) { return __lse_atomic_fetch_andnot_relaxed(~i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_and_acquire(int i, atomic_t *v) { return __lse_atomic_fetch_andnot_acquire(~i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_and_release(int i, atomic_t *v) { return __lse_atomic_fetch_andnot_release(~i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __lse_atomic_fetch_and(int i, atomic_t *v) { return __lse_atomic_fetch_andnot(~i, v); }
# 131 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic64_andnot(s64 i, atomic64_t *v) { asm volatile( ".arch_extension lse\n" "	" "stclr" "	%[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic64_or(s64 i, atomic64_t *v) { asm volatile( ".arch_extension lse\n" "	" "stset" "	%[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic64_xor(s64 i, atomic64_t *v) { asm volatile( ".arch_extension lse\n" "	" "steor" "	%[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic64_add(s64 i, atomic64_t *v) { asm volatile( ".arch_extension lse\n" "	" "stadd" "	%[i], %[v]\n" : [v] "+Q" (v->counter) : [i] "r" (i)); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic64_sub(s64 i, atomic64_t *v)
{
 __lse_atomic64_add(-i, v);
}
# 166 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "a" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_andnot_release(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "l" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_andnot(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldclr" "al" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_or_relaxed(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldset" "" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_or_acquire(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldset" "a" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_or_release(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldset" "l" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_or(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldset" "al" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_xor_acquire(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "a" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_xor_release(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "l" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_xor(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldeor" "al" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_add_relaxed(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : ); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_add_acquire(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "a" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_add_release(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "l" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_add(s64 i, atomic64_t *v) { s64 old; asm volatile( ".arch_extension lse\n" "	" "ldadd" "al" "	%[i], %[old], %[v]" : [v] "+Q" (v->counter), [old] "=r" (old) : [i] "r" (i) : "memory"); return old; }
# 181 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add_relaxed(-i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_sub_acquire(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add_acquire(-i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_sub_release(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add_release(-i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_sub(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add(-i, v); }
# 201 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_add_return_relaxed(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add_relaxed(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_sub_return_relaxed(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_sub_relaxed(i, v) - i; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_add_return_acquire(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add_acquire(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_sub_return_acquire(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_sub_acquire(i, v) - i; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_add_return_release(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add_release(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_sub_return_release(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_sub_release(i, v) - i; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_add_return(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_add(i, v) + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_sub_return(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_sub(i, v) - i; }



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __lse_atomic64_and(s64 i, atomic64_t *v)
{
 return __lse_atomic64_andnot(~i, v);
}
# 220 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_and_relaxed(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_andnot_relaxed(~i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_and_acquire(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_andnot_acquire(~i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_and_release(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_andnot_release(~i, v); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse_atomic64_fetch_and(s64 i, atomic64_t *v) { return __lse_atomic64_fetch_andnot(~i, v); }



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64 __lse_atomic64_dec_if_positive(atomic64_t *v)
{
 unsigned long tmp;

 asm volatile(
 ".arch_extension lse\n"
 "1:	ldr	%x[tmp], %[v]\n"
 "	subs	%[ret], %x[tmp], #1\n"
 "	b.lt	2f\n"
 "	casal	%x[tmp], %[ret], %[v]\n"
 "	sub	%x[tmp], %x[tmp], #1\n"
 "	sub	%x[tmp], %x[tmp], %[ret]\n"
 "	cbnz	%x[tmp], 1b\n"
 "2:"
 : [ret] "+&r" (v), [v] "+Q" (v->counter), [tmp] "=&r" (tmp)
 :
 : "cc", "memory");

 return (long)v;
}
# 272 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __lse__cmpxchg_case_8(volatile void *ptr, u8 old, u8 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u8 x1 asm ("x1") = old; register u8 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "" "b" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u8 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : ); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __lse__cmpxchg_case_16(volatile void *ptr, u16 old, u16 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u16 x1 asm ("x1") = old; register u16 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "" "h" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u16 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : ); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __lse__cmpxchg_case_32(volatile void *ptr, u32 old, u32 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u32 x1 asm ("x1") = old; register u32 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "" "" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u32 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : ); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __lse__cmpxchg_case_64(volatile void *ptr, u64 old, u64 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u64 x1 asm ("x1") = old; register u64 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "x" "[tmp], %" "x" "[old]\n" "	cas" "" "" "\t%" "x" "[tmp], %" "x" "[new], %[v]\n" "	mov	%" "x" "[ret], %" "x" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u64 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : ); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __lse__cmpxchg_case_acq_8(volatile void *ptr, u8 old, u8 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u8 x1 asm ("x1") = old; register u8 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "a" "b" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u8 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __lse__cmpxchg_case_acq_16(volatile void *ptr, u16 old, u16 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u16 x1 asm ("x1") = old; register u16 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "a" "h" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u16 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __lse__cmpxchg_case_acq_32(volatile void *ptr, u32 old, u32 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u32 x1 asm ("x1") = old; register u32 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "a" "" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u32 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __lse__cmpxchg_case_acq_64(volatile void *ptr, u64 old, u64 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u64 x1 asm ("x1") = old; register u64 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "x" "[tmp], %" "x" "[old]\n" "	cas" "a" "" "\t%" "x" "[tmp], %" "x" "[new], %[v]\n" "	mov	%" "x" "[ret], %" "x" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u64 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __lse__cmpxchg_case_rel_8(volatile void *ptr, u8 old, u8 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u8 x1 asm ("x1") = old; register u8 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "l" "b" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u8 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __lse__cmpxchg_case_rel_16(volatile void *ptr, u16 old, u16 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u16 x1 asm ("x1") = old; register u16 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "l" "h" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u16 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __lse__cmpxchg_case_rel_32(volatile void *ptr, u32 old, u32 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u32 x1 asm ("x1") = old; register u32 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "l" "" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u32 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __lse__cmpxchg_case_rel_64(volatile void *ptr, u64 old, u64 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u64 x1 asm ("x1") = old; register u64 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "x" "[tmp], %" "x" "[old]\n" "	cas" "l" "" "\t%" "x" "[tmp], %" "x" "[new], %[v]\n" "	mov	%" "x" "[ret], %" "x" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u64 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 __lse__cmpxchg_case_mb_8(volatile void *ptr, u8 old, u8 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u8 x1 asm ("x1") = old; register u8 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "al" "b" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u8 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 __lse__cmpxchg_case_mb_16(volatile void *ptr, u16 old, u16 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u16 x1 asm ("x1") = old; register u16 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "al" "h" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u16 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __lse__cmpxchg_case_mb_32(volatile void *ptr, u32 old, u32 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u32 x1 asm ("x1") = old; register u32 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "w" "[tmp], %" "w" "[old]\n" "	cas" "al" "" "\t%" "w" "[tmp], %" "w" "[new], %[v]\n" "	mov	%" "w" "[ret], %" "w" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u32 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __lse__cmpxchg_case_mb_64(volatile void *ptr, u64 old, u64 new) { register unsigned long x0 asm ("x0") = (unsigned long)ptr; register u64 x1 asm ("x1") = old; register u64 x2 asm ("x2") = new; unsigned long tmp; asm volatile( ".arch_extension lse\n" "	mov	%" "x" "[tmp], %" "x" "[old]\n" "	cas" "al" "" "\t%" "x" "[tmp], %" "x" "[new], %[v]\n" "	mov	%" "x" "[ret], %" "x" "[tmp]" : [ret] "+r" (x0), [v] "+Q" (*(u64 *)ptr), [tmp] "=&r" (tmp) : [old] "r" (x1), [new] "r" (x2) : "memory"); return x0; }
# 322 "./arch/arm64/include/asm/atomic_lse.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse__cmpxchg_double(unsigned long old1, unsigned long old2, unsigned long new1, unsigned long new2, volatile void *ptr) { unsigned long oldval1 = old1; unsigned long oldval2 = old2; register unsigned long x0 asm ("x0") = old1; register unsigned long x1 asm ("x1") = old2; register unsigned long x2 asm ("x2") = new1; register unsigned long x3 asm ("x3") = new2; register unsigned long x4 asm ("x4") = (unsigned long)ptr; asm volatile( ".arch_extension lse\n" "	casp" "" "\t%[old1], %[old2], %[new1], %[new2], %[v]\n" "	eor	%[old1], %[old1], %[oldval1]\n" "	eor	%[old2], %[old2], %[oldval2]\n" "	orr	%[old1], %[old1], %[old2]" : [old1] "+&r" (x0), [old2] "+&r" (x1), [v] "+Q" (*(unsigned long *)ptr) : [new1] "r" (x2), [new2] "r" (x3), [ptr] "r" (x4), [oldval1] "r" (oldval1), [oldval2] "r" (oldval2) : ); return x0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long __lse__cmpxchg_double_mb(unsigned long old1, unsigned long old2, unsigned long new1, unsigned long new2, volatile void *ptr) { unsigned long oldval1 = old1; unsigned long oldval2 = old2; register unsigned long x0 asm ("x0") = old1; register unsigned long x1 asm ("x1") = old2; register unsigned long x2 asm ("x2") = new1; register unsigned long x3 asm ("x3") = new2; register unsigned long x4 asm ("x4") = (unsigned long)ptr; asm volatile( ".arch_extension lse\n" "	casp" "al" "\t%[old1], %[old2], %[new1], %[new2], %[v]\n" "	eor	%[old1], %[old1], %[oldval1]\n" "	eor	%[old2], %[old2], %[oldval2]\n" "	orr	%[old1], %[old1], %[old2]" : [old1] "+&r" (x0), [old2] "+&r" (x1), [v] "+Q" (*(unsigned long *)ptr) : [new1] "r" (x2), [new2] "r" (x3), [ptr] "r" (x4), [oldval1] "r" (oldval1), [oldval2] "r" (oldval2) : "memory"); return x0; }
# 17 "./arch/arm64/include/asm/lse.h" 2


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_uses_lse_atomics(void)
{
 return alternative_has_feature_likely(27);
}
# 31 "./arch/arm64/include/asm/lse.h"
/* In-line patching at runtime */
# 15 "./arch/arm64/include/asm/cmpxchg.h" 2

/*
 * We need separate acquire parameters for ll/sc and lse, since the full
 * barrier case is generated as release+dmb for the former and
 * acquire+release for the latter.
 */
# 45 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __xchg_case_8(u8 x, volatile void *ptr) { u8 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "b" "\t%" "w" "0, %2\n" "	st" "" "xr" "b" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "" "b" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u8 *)ptr) : "r" (x) : ); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __xchg_case_16(u16 x, volatile void *ptr) { u16 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "h" "\t%" "w" "0, %2\n" "	st" "" "xr" "h" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "" "h" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u16 *)ptr) : "r" (x) : ); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __xchg_case_32(u32 x, volatile void *ptr) { u32 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "" "\t%" "w" "0, %2\n" "	st" "" "xr" "" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "" "" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u32 *)ptr) : "r" (x) : ); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __xchg_case_64(u64 x, volatile void *ptr) { u64 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "" "\t%" "" "0, %2\n" "	st" "" "xr" "" "\t%w1, %" "" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "" "" "\t%" "" "3, %" "" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u64 *)ptr) : "r" (x) : ); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __xchg_case_acq_8(u8 x, volatile void *ptr) { u8 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr" "b" "\t%" "w" "0, %2\n" "	st" "" "xr" "b" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "" "b" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u8 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __xchg_case_acq_16(u16 x, volatile void *ptr) { u16 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr" "h" "\t%" "w" "0, %2\n" "	st" "" "xr" "h" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "" "h" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u16 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __xchg_case_acq_32(u32 x, volatile void *ptr) { u32 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr" "" "\t%" "w" "0, %2\n" "	st" "" "xr" "" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "" "" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u32 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __xchg_case_acq_64(u64 x, volatile void *ptr) { u64 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "a" "xr" "" "\t%" "" "0, %2\n" "	st" "" "xr" "" "\t%w1, %" "" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "" "" "\t%" "" "3, %" "" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u64 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __xchg_case_rel_8(u8 x, volatile void *ptr) { u8 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "b" "\t%" "w" "0, %2\n" "	st" "l" "xr" "b" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "l" "b" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u8 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __xchg_case_rel_16(u16 x, volatile void *ptr) { u16 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "h" "\t%" "w" "0, %2\n" "	st" "l" "xr" "h" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "l" "h" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u16 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __xchg_case_rel_32(u32 x, volatile void *ptr) { u32 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "" "\t%" "w" "0, %2\n" "	st" "l" "xr" "" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "l" "" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u32 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __xchg_case_rel_64(u64 x, volatile void *ptr) { u64 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "" "\t%" "" "0, %2\n" "	st" "l" "xr" "" "\t%w1, %" "" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "" "l" "" "\t%" "" "3, %" "" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u64 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __xchg_case_mb_8(u8 x, volatile void *ptr) { u8 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "b" "\t%" "w" "0, %2\n" "	st" "l" "xr" "b" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "l" "b" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "nop" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u8 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __xchg_case_mb_16(u16 x, volatile void *ptr) { u16 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "h" "\t%" "w" "0, %2\n" "	st" "l" "xr" "h" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "l" "h" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "nop" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u16 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __xchg_case_mb_32(u32 x, volatile void *ptr) { u32 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "" "\t%" "w" "0, %2\n" "	st" "l" "xr" "" "\t%w1, %" "w" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "l" "" "\t%" "w" "3, %" "w" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "nop" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u32 *)ptr) : "r" (x) : "memory"); return ret; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __xchg_case_mb_64(u64 x, volatile void *ptr) { u64 ret; unsigned long tmp; asm volatile(".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "	prfm	pstl1strm, %2\n" "1:	ld" "" "xr" "" "\t%" "" "0, %2\n" "	st" "l" "xr" "" "\t%w1, %" "" "3, %2\n" "	cbnz	%w1, 1b\n" "	" "dmb ish" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "	swp" "a" "l" "" "\t%" "" "3, %" "" "0, %2\n" ".rept	" "3" "\nnop\n.endr\n" "	" "nop" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : "=&r" (ret), "=&r" (tmp), "+Q" (*(u64 *)ptr) : "r" (x) : "memory"); return ret; }
# 85 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __xchg(unsigned long x, volatile void *ptr, int size) { switch (size) { case 1: return __xchg_case_8(x, ptr); case 2: return __xchg_case_16(x, ptr); case 4: return __xchg_case_32(x, ptr); case 8: return __xchg_case_64(x, ptr); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_131(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_131(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 86 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __xchg_acq(unsigned long x, volatile void *ptr, int size) { switch (size) { case 1: return __xchg_case_acq_8(x, ptr); case 2: return __xchg_case_acq_16(x, ptr); case 4: return __xchg_case_acq_32(x, ptr); case 8: return __xchg_case_acq_64(x, ptr); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_132(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_132(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 87 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __xchg_rel(unsigned long x, volatile void *ptr, int size) { switch (size) { case 1: return __xchg_case_rel_8(x, ptr); case 2: return __xchg_case_rel_16(x, ptr); case 4: return __xchg_case_rel_32(x, ptr); case 8: return __xchg_case_rel_64(x, ptr); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_133(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_133(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 88 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __xchg_mb(unsigned long x, volatile void *ptr, int size) { switch (size) { case 1: return __xchg_case_mb_8(x, ptr); case 2: return __xchg_case_mb_16(x, ptr); case 4: return __xchg_case_mb_32(x, ptr); case 8: return __xchg_case_mb_64(x, ptr); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_134(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_134(); } while (0); } do { ; __builtin_unreachable(); } while (0); }







/* xchg */
# 115 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __cmpxchg_case_8(volatile void *ptr, u8 old, u8 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_8(ptr, old, new) : __ll_sc__cmpxchg_case_8(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __cmpxchg_case_16(volatile void *ptr, u16 old, u16 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_16(ptr, old, new) : __ll_sc__cmpxchg_case_16(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __cmpxchg_case_32(volatile void *ptr, u32 old, u32 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_32(ptr, old, new) : __ll_sc__cmpxchg_case_32(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __cmpxchg_case_64(volatile void *ptr, u64 old, u64 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_64(ptr, old, new) : __ll_sc__cmpxchg_case_64(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __cmpxchg_case_acq_8(volatile void *ptr, u8 old, u8 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_acq_8(ptr, old, new) : __ll_sc__cmpxchg_case_acq_8(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __cmpxchg_case_acq_16(volatile void *ptr, u16 old, u16 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_acq_16(ptr, old, new) : __ll_sc__cmpxchg_case_acq_16(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __cmpxchg_case_acq_32(volatile void *ptr, u32 old, u32 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_acq_32(ptr, old, new) : __ll_sc__cmpxchg_case_acq_32(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __cmpxchg_case_acq_64(volatile void *ptr, u64 old, u64 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_acq_64(ptr, old, new) : __ll_sc__cmpxchg_case_acq_64(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __cmpxchg_case_rel_8(volatile void *ptr, u8 old, u8 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_rel_8(ptr, old, new) : __ll_sc__cmpxchg_case_rel_8(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __cmpxchg_case_rel_16(volatile void *ptr, u16 old, u16 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_rel_16(ptr, old, new) : __ll_sc__cmpxchg_case_rel_16(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __cmpxchg_case_rel_32(volatile void *ptr, u32 old, u32 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_rel_32(ptr, old, new) : __ll_sc__cmpxchg_case_rel_32(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __cmpxchg_case_rel_64(volatile void *ptr, u64 old, u64 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_rel_64(ptr, old, new) : __ll_sc__cmpxchg_case_rel_64(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __cmpxchg_case_mb_8(volatile void *ptr, u8 old, u8 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_mb_8(ptr, old, new) : __ll_sc__cmpxchg_case_mb_8(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __cmpxchg_case_mb_16(volatile void *ptr, u16 old, u16 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_mb_16(ptr, old, new) : __ll_sc__cmpxchg_case_mb_16(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __cmpxchg_case_mb_32(volatile void *ptr, u32 old, u32 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_mb_32(ptr, old, new) : __ll_sc__cmpxchg_case_mb_32(ptr, old, new); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __cmpxchg_case_mb_64(volatile void *ptr, u64 old, u64 new) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_case_mb_64(ptr, old, new) : __ll_sc__cmpxchg_case_mb_64(ptr, old, new); }); }
# 145 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long __cmpxchg_double(unsigned long old1, unsigned long old2, unsigned long new1, unsigned long new2, volatile void *ptr) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_double(old1, old2, new1, new2, ptr) : __ll_sc__cmpxchg_double(old1, old2, new1, new2, ptr); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long __cmpxchg_double_mb(unsigned long old1, unsigned long old2, unsigned long new1, unsigned long new2, volatile void *ptr) { return ({ system_uses_lse_atomics() ? __lse__cmpxchg_double_mb(old1, old2, new1, new2, ptr) : __ll_sc__cmpxchg_double_mb(old1, old2, new1, new2, ptr); }); }
# 172 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __cmpxchg(volatile void *ptr, unsigned long old, unsigned long new, int size) { switch (size) { case 1: return __cmpxchg_case_8(ptr, old, new); case 2: return __cmpxchg_case_16(ptr, old, new); case 4: return __cmpxchg_case_32(ptr, old, new); case 8: return __cmpxchg_case_64(ptr, old, new); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_135(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_135(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 173 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __cmpxchg_acq(volatile void *ptr, unsigned long old, unsigned long new, int size) { switch (size) { case 1: return __cmpxchg_case_acq_8(ptr, old, new); case 2: return __cmpxchg_case_acq_16(ptr, old, new); case 4: return __cmpxchg_case_acq_32(ptr, old, new); case 8: return __cmpxchg_case_acq_64(ptr, old, new); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_136(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_136(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 174 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __cmpxchg_rel(volatile void *ptr, unsigned long old, unsigned long new, int size) { switch (size) { case 1: return __cmpxchg_case_rel_8(ptr, old, new); case 2: return __cmpxchg_case_rel_16(ptr, old, new); case 4: return __cmpxchg_case_rel_32(ptr, old, new); case 8: return __cmpxchg_case_rel_64(ptr, old, new); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_137(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_137(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 175 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __cmpxchg_mb(volatile void *ptr, unsigned long old, unsigned long new, int size) { switch (size) { case 1: return __cmpxchg_case_mb_8(ptr, old, new); case 2: return __cmpxchg_case_mb_16(ptr, old, new); case 4: return __cmpxchg_case_mb_32(ptr, old, new); case 8: return __cmpxchg_case_mb_64(ptr, old, new); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_138(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_138(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 188 "./arch/arm64/include/asm/cmpxchg.h"
/* cmpxchg */






/* cmpxchg64 */






/* cmpxchg_double */
# 250 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __cmpwait_case_8(volatile void *ptr, unsigned long val) { unsigned long tmp; asm volatile( "	sevl\n" "	wfe\n" "	ldxr" "b" "\t%" "w" "[tmp], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	cbnz	%" "w" "[tmp], 1f\n" "	wfe\n" "1:" : [tmp] "=&r" (tmp), [v] "+Q" (*(u8 *)ptr) : [val] "r" (val)); };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __cmpwait_case_16(volatile void *ptr, unsigned long val) { unsigned long tmp; asm volatile( "	sevl\n" "	wfe\n" "	ldxr" "h" "\t%" "w" "[tmp], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	cbnz	%" "w" "[tmp], 1f\n" "	wfe\n" "1:" : [tmp] "=&r" (tmp), [v] "+Q" (*(u16 *)ptr) : [val] "r" (val)); };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __cmpwait_case_32(volatile void *ptr, unsigned long val) { unsigned long tmp; asm volatile( "	sevl\n" "	wfe\n" "	ldxr" "" "\t%" "w" "[tmp], %[v]\n" "	eor	%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	cbnz	%" "w" "[tmp], 1f\n" "	wfe\n" "1:" : [tmp] "=&r" (tmp), [v] "+Q" (*(u32 *)ptr) : [val] "r" (val)); };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __cmpwait_case_64(volatile void *ptr, unsigned long val) { unsigned long tmp; asm volatile( "	sevl\n" "	wfe\n" "	ldxr" "" "\t%" "" "[tmp], %[v]\n" "	eor	%" "" "[tmp], %" "" "[tmp], %" "" "[val]\n" "	cbnz	%" "" "[tmp], 1f\n" "	wfe\n" "1:" : [tmp] "=&r" (tmp), [v] "+Q" (*(u64 *)ptr) : [val] "r" (val)); };
# 278 "./arch/arm64/include/asm/cmpxchg.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __cmpwait(volatile void *ptr, unsigned long val, int size) { switch (size) { case 1: return __cmpwait_case_8(ptr, (u8)val); case 2: return __cmpwait_case_16(ptr, (u16)val); case 4: return __cmpwait_case_32(ptr, val); case 8: return __cmpwait_case_64(ptr, val); default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_139(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_139(); } while (0); } do { ; __builtin_unreachable(); } while (0); }
# 17 "./arch/arm64/include/asm/atomic.h" 2








static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic_andnot(int i, atomic_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic_andnot(i, v) : __ll_sc_atomic_andnot(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic_or(int i, atomic_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic_or(i, v) : __ll_sc_atomic_or(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic_xor(int i, atomic_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic_xor(i, v) : __ll_sc_atomic_xor(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic_add(int i, atomic_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic_add(i, v) : __ll_sc_atomic_add(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic_and(int i, atomic_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic_and(i, v) : __ll_sc_atomic_and(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic_sub(int i, atomic_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic_sub(i, v) : __ll_sc_atomic_sub(i, v); }); }
# 46 "./arch/arm64/include/asm/atomic.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_andnot_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_andnot_relaxed(i, v) : __ll_sc_atomic_fetch_andnot_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_andnot_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_andnot_acquire(i, v) : __ll_sc_atomic_fetch_andnot_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_andnot_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_andnot_release(i, v) : __ll_sc_atomic_fetch_andnot_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_andnot(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_andnot(i, v) : __ll_sc_atomic_fetch_andnot(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_or_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_or_relaxed(i, v) : __ll_sc_atomic_fetch_or_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_or_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_or_acquire(i, v) : __ll_sc_atomic_fetch_or_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_or_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_or_release(i, v) : __ll_sc_atomic_fetch_or_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_or(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_or(i, v) : __ll_sc_atomic_fetch_or(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_xor_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_xor_relaxed(i, v) : __ll_sc_atomic_fetch_xor_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_xor_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_xor_acquire(i, v) : __ll_sc_atomic_fetch_xor_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_xor_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_xor_release(i, v) : __ll_sc_atomic_fetch_xor_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_xor(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_xor(i, v) : __ll_sc_atomic_fetch_xor(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_add_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_add_relaxed(i, v) : __ll_sc_atomic_fetch_add_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_add_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_add_acquire(i, v) : __ll_sc_atomic_fetch_add_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_add_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_add_release(i, v) : __ll_sc_atomic_fetch_add_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_add(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_add(i, v) : __ll_sc_atomic_fetch_add(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_and_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_and_relaxed(i, v) : __ll_sc_atomic_fetch_and_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_and_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_and_acquire(i, v) : __ll_sc_atomic_fetch_and_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_and_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_and_release(i, v) : __ll_sc_atomic_fetch_and_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_and(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_and(i, v) : __ll_sc_atomic_fetch_and(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_sub_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_sub_relaxed(i, v) : __ll_sc_atomic_fetch_sub_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_sub_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_sub_acquire(i, v) : __ll_sc_atomic_fetch_sub_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_sub_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_sub_release(i, v) : __ll_sc_atomic_fetch_sub_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_fetch_sub(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_fetch_sub(i, v) : __ll_sc_atomic_fetch_sub(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_add_return_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_add_return_relaxed(i, v) : __ll_sc_atomic_add_return_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_add_return_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_add_return_acquire(i, v) : __ll_sc_atomic_add_return_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_add_return_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_add_return_release(i, v) : __ll_sc_atomic_add_return_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_add_return(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_add_return(i, v) : __ll_sc_atomic_add_return(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_sub_return_relaxed(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_sub_return_relaxed(i, v) : __ll_sc_atomic_sub_return_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_sub_return_acquire(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_sub_return_acquire(i, v) : __ll_sc_atomic_sub_return_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_sub_return_release(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_sub_return_release(i, v) : __ll_sc_atomic_sub_return_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int arch_atomic_sub_return(int i, atomic_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic_sub_return(i, v) : __ll_sc_atomic_sub_return(i, v); }); }
# 64 "./arch/arm64/include/asm/atomic.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic64_andnot(long i, atomic64_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic64_andnot(i, v) : __ll_sc_atomic64_andnot(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic64_or(long i, atomic64_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic64_or(i, v) : __ll_sc_atomic64_or(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic64_xor(long i, atomic64_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic64_xor(i, v) : __ll_sc_atomic64_xor(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic64_add(long i, atomic64_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic64_add(i, v) : __ll_sc_atomic64_add(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic64_and(long i, atomic64_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic64_and(i, v) : __ll_sc_atomic64_and(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arch_atomic64_sub(long i, atomic64_t *v) { ({ system_uses_lse_atomics() ? __lse_atomic64_sub(i, v) : __ll_sc_atomic64_sub(i, v); }); }
# 85 "./arch/arm64/include/asm/atomic.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_andnot_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_andnot_relaxed(i, v) : __ll_sc_atomic64_fetch_andnot_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_andnot_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_andnot_acquire(i, v) : __ll_sc_atomic64_fetch_andnot_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_andnot_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_andnot_release(i, v) : __ll_sc_atomic64_fetch_andnot_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_andnot(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_andnot(i, v) : __ll_sc_atomic64_fetch_andnot(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_or_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_or_relaxed(i, v) : __ll_sc_atomic64_fetch_or_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_or_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_or_acquire(i, v) : __ll_sc_atomic64_fetch_or_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_or_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_or_release(i, v) : __ll_sc_atomic64_fetch_or_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_or(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_or(i, v) : __ll_sc_atomic64_fetch_or(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_xor_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_xor_relaxed(i, v) : __ll_sc_atomic64_fetch_xor_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_xor_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_xor_acquire(i, v) : __ll_sc_atomic64_fetch_xor_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_xor_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_xor_release(i, v) : __ll_sc_atomic64_fetch_xor_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_xor(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_xor(i, v) : __ll_sc_atomic64_fetch_xor(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_add_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_add_relaxed(i, v) : __ll_sc_atomic64_fetch_add_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_add_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_add_acquire(i, v) : __ll_sc_atomic64_fetch_add_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_add_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_add_release(i, v) : __ll_sc_atomic64_fetch_add_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_add(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_add(i, v) : __ll_sc_atomic64_fetch_add(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_and_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_and_relaxed(i, v) : __ll_sc_atomic64_fetch_and_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_and_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_and_acquire(i, v) : __ll_sc_atomic64_fetch_and_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_and_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_and_release(i, v) : __ll_sc_atomic64_fetch_and_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_and(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_and(i, v) : __ll_sc_atomic64_fetch_and(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_sub_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_sub_relaxed(i, v) : __ll_sc_atomic64_fetch_sub_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_sub_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_sub_acquire(i, v) : __ll_sc_atomic64_fetch_sub_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_sub_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_sub_release(i, v) : __ll_sc_atomic64_fetch_sub_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_fetch_sub(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_fetch_sub(i, v) : __ll_sc_atomic64_fetch_sub(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_add_return_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_add_return_relaxed(i, v) : __ll_sc_atomic64_add_return_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_add_return_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_add_return_acquire(i, v) : __ll_sc_atomic64_add_return_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_add_return_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_add_return_release(i, v) : __ll_sc_atomic64_add_return_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_add_return(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_add_return(i, v) : __ll_sc_atomic64_add_return(i, v); }); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_sub_return_relaxed(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_sub_return_relaxed(i, v) : __ll_sc_atomic64_sub_return_relaxed(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_sub_return_acquire(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_sub_return_acquire(i, v) : __ll_sc_atomic64_sub_return_acquire(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_sub_return_release(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_sub_return_release(i, v) : __ll_sc_atomic64_sub_return_release(i, v); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_sub_return(long i, atomic64_t *v) { return ({ system_uses_lse_atomics() ? __lse_atomic64_sub_return(i, v) : __ll_sc_atomic64_sub_return(i, v); }); }




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long arch_atomic64_dec_if_positive(atomic64_t *v)
{
 return ({ system_uses_lse_atomics() ? __lse_atomic64_dec_if_positive(v) : __ll_sc_atomic64_dec_if_positive(v); });
}
# 165 "./arch/arm64/include/asm/atomic.h"
/*
 * 64-bit arch_atomic operations.
 */
# 8 "./include/linux/atomic.h" 2


/*
 * Relaxed variants of xchg, cmpxchg and some atomic operations.
 *
 * We support four variants:
 *
 * - Fully ordered: The default implementation, no suffix required.
 * - Acquire: Provides ACQUIRE semantics, _acquire suffix.
 * - Release: Provides RELEASE semantics, _release suffix.
 * - Relaxed: No ordering guarantees, _relaxed suffix.
 *
 * For compound atomics performing both a load and a store, ACQUIRE
 * semantics apply only to the load and RELEASE semantics only to the
 * store portion of the operation. Note that a failed cmpxchg_acquire
 * does -not- imply any memory ordering constraints.
 *
 * See Documentation/memory-barriers.txt for ACQUIRE/RELEASE definitions.
 */







/*
 * The idea here is to build acquire/release variants by adding explicit
 * barriers on top of the relaxed variant. In the case where the relaxed
 * variant is already fully ordered, no additional barriers are needed.
 *
 * If an architecture overrides __atomic_acquire_fence() it will probably
 * want to define smp_mb__after_spinlock().
 */
# 80 "./include/linux/atomic.h"
# 1 "./include/linux/atomic/atomic-arch-fallback.h" 1
// SPDX-License-Identifier: GPL-2.0

// Generated by scripts/atomic/gen-atomic-fallback.sh
// DO NOT MODIFY THIS FILE DIRECTLY
# 221 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_read_acquire(const atomic_t *v)
{
 int ret;

 if ((sizeof(atomic_t) == sizeof(char) || sizeof(atomic_t) == sizeof(short) || sizeof(atomic_t) == sizeof(int) || sizeof(atomic_t) == sizeof(long))) {
  ret = ({ union { typeof( _Generic((*&(v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(v)->counter))) __val; char __c[1]; } __u; typeof(&(v)->counter) __p = (&(v)->counter); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_140(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(v)->counter) == sizeof(char) || sizeof(*&(v)->counter) == sizeof(short) || sizeof(*&(v)->counter) == sizeof(int) || sizeof(*&(v)->counter) == sizeof(long)))) __compiletime_assert_140(); } while (0); kasan_check_read(__p, sizeof(*&(v)->counter)); switch (sizeof(*&(v)->counter)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&(v)->counter))__u.__val; });
# 228 "./include/linux/atomic/atomic-arch-fallback.h"
 } else {
  ret = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));
  do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 }

 return ret;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_set_release(atomic_t *v, int i)
{
 if ((sizeof(atomic_t) == sizeof(char) || sizeof(atomic_t) == sizeof(short) || sizeof(atomic_t) == sizeof(int) || sizeof(atomic_t) == sizeof(long))) {
  do { do { } while (0); do { typeof(&(v)->counter) __p = (&(v)->counter); union { typeof( _Generic((*&(v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(v)->counter))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(v)->counter)))) (i) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_141(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(v)->counter) == sizeof(char) || sizeof(*&(v)->counter) == sizeof(short) || sizeof(*&(v)->counter) == sizeof(int) || sizeof(*&(v)->counter) == sizeof(long)))) __compiletime_assert_141(); } while (0); kasan_check_write(__p, sizeof(*&(v)->counter)); switch (sizeof(*&(v)->counter)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0);
# 244 "./include/linux/atomic/atomic-arch-fallback.h"
 } else {
  do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
  do { *(volatile typeof(((v)->counter)) *)&(((v)->counter)) = ((i)); } while (0);
 }
}
# 421 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_inc(atomic_t *v)
{
 arch_atomic_add(1, v);
}
# 437 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_inc_return(atomic_t *v)
{
 return arch_atomic_add_return(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_inc_return_acquire(atomic_t *v)
{
 return arch_atomic_add_return_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_inc_return_release(atomic_t *v)
{
 return arch_atomic_add_return_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_inc_return_relaxed(atomic_t *v)
{
 return arch_atomic_add_return_relaxed(1, v);
}
# 518 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_inc(atomic_t *v)
{
 return arch_atomic_fetch_add(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_inc_acquire(atomic_t *v)
{
 return arch_atomic_fetch_add_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_inc_release(atomic_t *v)
{
 return arch_atomic_fetch_add_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_inc_relaxed(atomic_t *v)
{
 return arch_atomic_fetch_add_relaxed(1, v);
}
# 592 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_dec(atomic_t *v)
{
 arch_atomic_sub(1, v);
}
# 608 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_dec_return(atomic_t *v)
{
 return arch_atomic_sub_return(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_dec_return_acquire(atomic_t *v)
{
 return arch_atomic_sub_return_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_dec_return_release(atomic_t *v)
{
 return arch_atomic_sub_return_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_dec_return_relaxed(atomic_t *v)
{
 return arch_atomic_sub_return_relaxed(1, v);
}
# 689 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_dec(atomic_t *v)
{
 return arch_atomic_fetch_sub(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_dec_acquire(atomic_t *v)
{
 return arch_atomic_fetch_sub_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_dec_release(atomic_t *v)
{
 return arch_atomic_fetch_sub_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_dec_relaxed(atomic_t *v)
{
 return arch_atomic_fetch_sub_relaxed(1, v);
}
# 1070 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_try_cmpxchg(atomic_t *v, int *old, int new)
{
 int r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_mb((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
{
 int r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_acq((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
{
 int r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_rel((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
{
 int r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}
# 1160 "./include/linux/atomic/atomic-arch-fallback.h"
/**
 * arch_atomic_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_sub_and_test(int i, atomic_t *v)
{
 return arch_atomic_sub_return(i, v) == 0;
}




/**
 * arch_atomic_dec_and_test - decrement and test
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_dec_and_test(atomic_t *v)
{
 return arch_atomic_dec_return(v) == 0;
}




/**
 * arch_atomic_inc_and_test - increment and test
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_inc_and_test(atomic_t *v)
{
 return arch_atomic_inc_return(v) == 0;
}




/**
 * arch_atomic_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_add_negative(int i, atomic_t *v)
{
 return arch_atomic_add_return(i, v) < 0;
}




/**
 * arch_atomic_fetch_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as @v was not already @u.
 * Returns original value of @v
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_fetch_add_unless(atomic_t *v, int a, int u)
{
 int c = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));

 do {
  if (__builtin_expect(!!(c == u), 0))
   break;
 } while (!arch_atomic_try_cmpxchg(v, &c, c + a));

 return c;
}




/**
 * arch_atomic_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, if @v was not already @u.
 * Returns true if the addition was done.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_add_unless(atomic_t *v, int a, int u)
{
 return arch_atomic_fetch_add_unless(v, a, u) != u;
}




/**
 * arch_atomic_inc_not_zero - increment unless the number is zero
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1, if @v is non-zero.
 * Returns true if the increment was done.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_inc_not_zero(atomic_t *v)
{
 return arch_atomic_add_unless(v, 1, 0);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_inc_unless_negative(atomic_t *v)
{
 int c = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));

 do {
  if (__builtin_expect(!!(c < 0), 0))
   return false;
 } while (!arch_atomic_try_cmpxchg(v, &c, c + 1));

 return true;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_dec_unless_positive(atomic_t *v)
{
 int c = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));

 do {
  if (__builtin_expect(!!(c > 0), 0))
   return false;
 } while (!arch_atomic_try_cmpxchg(v, &c, c - 1));

 return true;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_atomic_dec_if_positive(atomic_t *v)
{
 int dec, c = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));

 do {
  dec = c - 1;
  if (__builtin_expect(!!(dec < 0), 0))
   break;
 } while (!arch_atomic_try_cmpxchg(v, &c, dec));

 return dec;
}
# 1342 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_read_acquire(const atomic64_t *v)
{
 s64 ret;

 if ((sizeof(atomic64_t) == sizeof(char) || sizeof(atomic64_t) == sizeof(short) || sizeof(atomic64_t) == sizeof(int) || sizeof(atomic64_t) == sizeof(long))) {
  ret = ({ union { typeof( _Generic((*&(v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(v)->counter))) __val; char __c[1]; } __u; typeof(&(v)->counter) __p = (&(v)->counter); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_142(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(v)->counter) == sizeof(char) || sizeof(*&(v)->counter) == sizeof(short) || sizeof(*&(v)->counter) == sizeof(int) || sizeof(*&(v)->counter) == sizeof(long)))) __compiletime_assert_142(); } while (0); kasan_check_read(__p, sizeof(*&(v)->counter)); switch (sizeof(*&(v)->counter)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&(v)->counter))__u.__val; });
# 1349 "./include/linux/atomic/atomic-arch-fallback.h"
 } else {
  ret = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));
  do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 }

 return ret;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic64_set_release(atomic64_t *v, s64 i)
{
 if ((sizeof(atomic64_t) == sizeof(char) || sizeof(atomic64_t) == sizeof(short) || sizeof(atomic64_t) == sizeof(int) || sizeof(atomic64_t) == sizeof(long))) {
  do { do { } while (0); do { typeof(&(v)->counter) __p = (&(v)->counter); union { typeof( _Generic((*&(v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(v)->counter))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(v)->counter)))) (i) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_143(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(v)->counter) == sizeof(char) || sizeof(*&(v)->counter) == sizeof(short) || sizeof(*&(v)->counter) == sizeof(int) || sizeof(*&(v)->counter) == sizeof(long)))) __compiletime_assert_143(); } while (0); kasan_check_write(__p, sizeof(*&(v)->counter)); switch (sizeof(*&(v)->counter)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0);
# 1365 "./include/linux/atomic/atomic-arch-fallback.h"
 } else {
  do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
  do { *(volatile typeof(((v)->counter)) *)&(((v)->counter)) = ((i)); } while (0);
 }
}
# 1542 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic64_inc(atomic64_t *v)
{
 arch_atomic64_add(1, v);
}
# 1558 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_inc_return(atomic64_t *v)
{
 return arch_atomic64_add_return(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_inc_return_acquire(atomic64_t *v)
{
 return arch_atomic64_add_return_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_inc_return_release(atomic64_t *v)
{
 return arch_atomic64_add_return_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_inc_return_relaxed(atomic64_t *v)
{
 return arch_atomic64_add_return_relaxed(1, v);
}
# 1639 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_inc(atomic64_t *v)
{
 return arch_atomic64_fetch_add(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_inc_acquire(atomic64_t *v)
{
 return arch_atomic64_fetch_add_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_inc_release(atomic64_t *v)
{
 return arch_atomic64_fetch_add_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_inc_relaxed(atomic64_t *v)
{
 return arch_atomic64_fetch_add_relaxed(1, v);
}
# 1713 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic64_dec(atomic64_t *v)
{
 arch_atomic64_sub(1, v);
}
# 1729 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_dec_return(atomic64_t *v)
{
 return arch_atomic64_sub_return(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_dec_return_acquire(atomic64_t *v)
{
 return arch_atomic64_sub_return_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_dec_return_release(atomic64_t *v)
{
 return arch_atomic64_sub_return_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_dec_return_relaxed(atomic64_t *v)
{
 return arch_atomic64_sub_return_relaxed(1, v);
}
# 1810 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_dec(atomic64_t *v)
{
 return arch_atomic64_fetch_sub(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_dec_acquire(atomic64_t *v)
{
 return arch_atomic64_fetch_sub_acquire(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_dec_release(atomic64_t *v)
{
 return arch_atomic64_fetch_sub_release(1, v);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_dec_relaxed(atomic64_t *v)
{
 return arch_atomic64_fetch_sub_relaxed(1, v);
}
# 2191 "./include/linux/atomic/atomic-arch-fallback.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
{
 s64 r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_mb((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_try_cmpxchg_acquire(atomic64_t *v, s64 *old, s64 new)
{
 s64 r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_acq((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_try_cmpxchg_release(atomic64_t *v, s64 *old, s64 new)
{
 s64 r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_rel((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_try_cmpxchg_relaxed(atomic64_t *v, s64 *old, s64 new)
{
 s64 r, o = *old;
 r = ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg((&((v)->counter)), (unsigned long)((o)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);
}
# 2281 "./include/linux/atomic/atomic-arch-fallback.h"
/**
 * arch_atomic64_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer of type atomic64_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_sub_and_test(s64 i, atomic64_t *v)
{
 return arch_atomic64_sub_return(i, v) == 0;
}




/**
 * arch_atomic64_dec_and_test - decrement and test
 * @v: pointer of type atomic64_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_dec_and_test(atomic64_t *v)
{
 return arch_atomic64_dec_return(v) == 0;
}




/**
 * arch_atomic64_inc_and_test - increment and test
 * @v: pointer of type atomic64_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_inc_and_test(atomic64_t *v)
{
 return arch_atomic64_inc_return(v) == 0;
}




/**
 * arch_atomic64_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer of type atomic64_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_add_negative(s64 i, atomic64_t *v)
{
 return arch_atomic64_add_return(i, v) < 0;
}




/**
 * arch_atomic64_fetch_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic64_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as @v was not already @u.
 * Returns original value of @v
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
{
 s64 c = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));

 do {
  if (__builtin_expect(!!(c == u), 0))
   break;
 } while (!arch_atomic64_try_cmpxchg(v, &c, c + a));

 return c;
}




/**
 * arch_atomic64_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic64_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, if @v was not already @u.
 * Returns true if the addition was done.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
{
 return arch_atomic64_fetch_add_unless(v, a, u) != u;
}




/**
 * arch_atomic64_inc_not_zero - increment unless the number is zero
 * @v: pointer of type atomic64_t
 *
 * Atomically increments @v by 1, if @v is non-zero.
 * Returns true if the increment was done.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_inc_not_zero(atomic64_t *v)
{
 return arch_atomic64_add_unless(v, 1, 0);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_inc_unless_negative(atomic64_t *v)
{
 s64 c = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));

 do {
  if (__builtin_expect(!!(c < 0), 0))
   return false;
 } while (!arch_atomic64_try_cmpxchg(v, &c, c + 1));

 return true;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic64_dec_unless_positive(atomic64_t *v)
{
 s64 c = (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));

 do {
  if (__builtin_expect(!!(c > 0), 0))
   return false;
 } while (!arch_atomic64_try_cmpxchg(v, &c, c - 1));

 return true;
}
# 2459 "./include/linux/atomic/atomic-arch-fallback.h"
// b5e87bdd5ede61470c29f7a7e4de781af3770f09
# 81 "./include/linux/atomic.h" 2
# 1 "./include/linux/atomic/atomic-long.h" 1
// SPDX-License-Identifier: GPL-2.0

// Generated by scripts/atomic/gen-atomic-long.sh
// DO NOT MODIFY THIS FILE DIRECTLY





# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 11 "./include/linux/atomic/atomic-long.h" 2


typedef atomic64_t atomic_long_t;
# 26 "./include/linux/atomic/atomic-long.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_read(const atomic_long_t *v)
{
 return (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_read_acquire(const atomic_long_t *v)
{
 return arch_atomic64_read_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_set(atomic_long_t *v, long i)
{
 do { *(volatile typeof(((v)->counter)) *)&(((v)->counter)) = ((i)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_set_release(atomic_long_t *v, long i)
{
 arch_atomic64_set_release(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_add(long i, atomic_long_t *v)
{
 arch_atomic64_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_add_return(long i, atomic_long_t *v)
{
 return arch_atomic64_add_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_add_return_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_add_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_add_return_release(long i, atomic_long_t *v)
{
 return arch_atomic64_add_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_add_return_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_add_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_add(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_add_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_add_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_add_release(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_add_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_add_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_add_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_sub(long i, atomic_long_t *v)
{
 arch_atomic64_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_sub_return(long i, atomic_long_t *v)
{
 return arch_atomic64_sub_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_sub_return_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_sub_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_sub_return_release(long i, atomic_long_t *v)
{
 return arch_atomic64_sub_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_sub_return_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_sub_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_sub(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_sub_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_sub_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_sub_release(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_sub_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_sub_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_sub_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_inc(atomic_long_t *v)
{
 arch_atomic64_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_inc_return(atomic_long_t *v)
{
 return arch_atomic64_inc_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_inc_return_acquire(atomic_long_t *v)
{
 return arch_atomic64_inc_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_inc_return_release(atomic_long_t *v)
{
 return arch_atomic64_inc_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_inc_return_relaxed(atomic_long_t *v)
{
 return arch_atomic64_inc_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_inc(atomic_long_t *v)
{
 return arch_atomic64_fetch_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_inc_acquire(atomic_long_t *v)
{
 return arch_atomic64_fetch_inc_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_inc_release(atomic_long_t *v)
{
 return arch_atomic64_fetch_inc_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_inc_relaxed(atomic_long_t *v)
{
 return arch_atomic64_fetch_inc_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_dec(atomic_long_t *v)
{
 arch_atomic64_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_dec_return(atomic_long_t *v)
{
 return arch_atomic64_dec_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_dec_return_acquire(atomic_long_t *v)
{
 return arch_atomic64_dec_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_dec_return_release(atomic_long_t *v)
{
 return arch_atomic64_dec_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_dec_return_relaxed(atomic_long_t *v)
{
 return arch_atomic64_dec_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_dec(atomic_long_t *v)
{
 return arch_atomic64_fetch_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_dec_acquire(atomic_long_t *v)
{
 return arch_atomic64_fetch_dec_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_dec_release(atomic_long_t *v)
{
 return arch_atomic64_fetch_dec_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_dec_relaxed(atomic_long_t *v)
{
 return arch_atomic64_fetch_dec_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_and(long i, atomic_long_t *v)
{
 arch_atomic64_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_and(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_and_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_and_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_and_release(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_and_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_and_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_and_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_andnot(long i, atomic_long_t *v)
{
 arch_atomic64_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_andnot(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_andnot_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_andnot_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_andnot_release(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_andnot_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_andnot_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_andnot_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_or(long i, atomic_long_t *v)
{
 arch_atomic64_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_or(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_or_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_or_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_or_release(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_or_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_or_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_or_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_atomic_long_xor(long i, atomic_long_t *v)
{
 arch_atomic64_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_xor(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_xor_acquire(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_xor_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_xor_release(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_xor_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_xor_relaxed(long i, atomic_long_t *v)
{
 return arch_atomic64_fetch_xor_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_xchg(atomic_long_t *v, long i)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_mb((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_xchg_acquire(atomic_long_t *v, long i)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_acq((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_xchg_release(atomic_long_t *v, long i)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_rel((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_xchg_relaxed(atomic_long_t *v, long i)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_cmpxchg(atomic_long_t *v, long old, long new)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_mb((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_cmpxchg_acquire(atomic_long_t *v, long old, long new)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_acq((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_cmpxchg_release(atomic_long_t *v, long old, long new)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_rel((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_cmpxchg_relaxed(atomic_long_t *v, long old, long new)
{
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_try_cmpxchg(atomic_long_t *v, long *old, long new)
{
 return arch_atomic64_try_cmpxchg(v, (s64 *)old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_try_cmpxchg_acquire(atomic_long_t *v, long *old, long new)
{
 return arch_atomic64_try_cmpxchg_acquire(v, (s64 *)old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_try_cmpxchg_release(atomic_long_t *v, long *old, long new)
{
 return arch_atomic64_try_cmpxchg_release(v, (s64 *)old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_try_cmpxchg_relaxed(atomic_long_t *v, long *old, long new)
{
 return arch_atomic64_try_cmpxchg_relaxed(v, (s64 *)old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_sub_and_test(long i, atomic_long_t *v)
{
 return arch_atomic64_sub_and_test(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_dec_and_test(atomic_long_t *v)
{
 return arch_atomic64_dec_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_inc_and_test(atomic_long_t *v)
{
 return arch_atomic64_inc_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_add_negative(long i, atomic_long_t *v)
{
 return arch_atomic64_add_negative(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_fetch_add_unless(atomic_long_t *v, long a, long u)
{
 return arch_atomic64_fetch_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_add_unless(atomic_long_t *v, long a, long u)
{
 return arch_atomic64_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_inc_not_zero(atomic_long_t *v)
{
 return arch_atomic64_inc_not_zero(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_inc_unless_negative(atomic_long_t *v)
{
 return arch_atomic64_inc_unless_negative(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
arch_atomic_long_dec_unless_positive(atomic_long_t *v)
{
 return arch_atomic64_dec_unless_positive(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
arch_atomic_long_dec_if_positive(atomic_long_t *v)
{
 return arch_atomic64_dec_if_positive(v);
}
# 1014 "./include/linux/atomic/atomic-long.h"
// e8f0e08ff072b74d180eabe2ad001282b38c2c88
# 82 "./include/linux/atomic.h" 2
# 1 "./include/linux/atomic/atomic-instrumented.h" 1
// SPDX-License-Identifier: GPL-2.0

// Generated by scripts/atomic/gen-atomic-instrumented.sh
// DO NOT MODIFY THIS FILE DIRECTLY

/*
 * This file provides wrappers with KASAN instrumentation for atomic operations.
 * To use this functionality an arch's atomic.h file needs to define all
 * atomic operations with arch_ prefix (e.g. arch_atomic_read()) and include
 * this file at the end. This file provides atomic_read() that forwards to
 * arch_atomic_read() for actual atomic operation.
 * Note: if an arch atomic operation is implemented by means of other atomic
 * operations (e.g. atomic_read()/atomic_cmpxchg() loop), then it needs to use
 * arch_ variants (i.e. arch_atomic_read()/arch_atomic_cmpxchg()) to avoid
 * double instrumentation.
 */





# 1 "./include/linux/instrumented.h" 1
/* SPDX-License-Identifier: GPL-2.0 */

/*
 * This header provides generic wrappers for memory access instrumentation that
 * the compiler cannot emit for: KASAN, KCSAN, KMSAN.
 */






# 1 "./include/linux/kmsan-checks.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * KMSAN checks to be used for one-off annotations in subsystems.
 *
 * Copyright (C) 2017-2022 Google LLC
 * Author: Alexander Potapenko <glider@google.com>
 *
 */
# 66 "./include/linux/kmsan-checks.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_poison_memory(const void *address, size_t size,
           gfp_t flags)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_unpoison_memory(const void *address, size_t size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_check_memory(const void *address, size_t size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_copy_to_user(void /* nothing */ *to, const void *from,
          size_t to_copy, size_t left)
{
}
# 14 "./include/linux/instrumented.h" 2


/**
 * instrument_read - instrument regular read access
 *
 * Instrument a regular read access. The instrumentation should be inserted
 * before the actual read happens.
 *
 * @ptr address of access
 * @size size of access
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void instrument_read(const volatile void *v, size_t size)
{
 kasan_check_read(v, size);
 kcsan_check_access(v, size, 0);
}

/**
 * instrument_write - instrument regular write access
 *
 * Instrument a regular write access. The instrumentation should be inserted
 * before the actual write happens.
 *
 * @ptr address of access
 * @size size of access
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void instrument_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 0) /* Access is a write. */);
}

/**
 * instrument_read_write - instrument regular read-write access
 *
 * Instrument a regular write access. The instrumentation should be inserted
 * before the actual write happens.
 *
 * @ptr address of access
 * @size size of access
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void instrument_read_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 1) /* Compounded read-write instrumentation. */ | (1 << 0) /* Access is a write. */);
}

/**
 * instrument_atomic_read - instrument atomic read access
 *
 * Instrument an atomic read access. The instrumentation should be inserted
 * before the actual read happens.
 *
 * @ptr address of access
 * @size size of access
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void instrument_atomic_read(const volatile void *v, size_t size)
{
 kasan_check_read(v, size);
 kcsan_check_access(v, size, (1 << 2) /* Access is atomic. */);
}

/**
 * instrument_atomic_write - instrument atomic write access
 *
 * Instrument an atomic write access. The instrumentation should be inserted
 * before the actual write happens.
 *
 * @ptr address of access
 * @size size of access
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void instrument_atomic_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 2) /* Access is atomic. */ | (1 << 0) /* Access is a write. */);
}

/**
 * instrument_atomic_read_write - instrument atomic read-write access
 *
 * Instrument an atomic read-write access. The instrumentation should be
 * inserted before the actual write happens.
 *
 * @ptr address of access
 * @size size of access
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void instrument_atomic_read_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 2) /* Access is atomic. */ | (1 << 0) /* Access is a write. */ | (1 << 1) /* Compounded read-write instrumentation. */);
}

/**
 * instrument_copy_to_user - instrument reads of copy_to_user
 *
 * Instrument reads from kernel memory, that are due to copy_to_user (and
 * variants). The instrumentation must be inserted before the accesses.
 *
 * @to destination address
 * @from source address
 * @n number of bytes to copy
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
instrument_copy_to_user(void /* nothing */ *to, const void *from, unsigned long n)
{
 kasan_check_read(from, n);
 kcsan_check_access(from, n, 0);
 kmsan_copy_to_user(to, from, n, 0);
}

/**
 * instrument_copy_from_user_before - add instrumentation before copy_from_user
 *
 * Instrument writes to kernel memory, that are due to copy_from_user (and
 * variants). The instrumentation should be inserted before the accesses.
 *
 * @to destination address
 * @from source address
 * @n number of bytes to copy
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
instrument_copy_from_user_before(const void *to, const void /* nothing */ *from, unsigned long n)
{
 kasan_check_write(to, n);
 kcsan_check_access(to, n, (1 << 0) /* Access is a write. */);
}

/**
 * instrument_copy_from_user_after - add instrumentation after copy_from_user
 *
 * Instrument writes to kernel memory, that are due to copy_from_user (and
 * variants). The instrumentation should be inserted after the accesses.
 *
 * @to destination address
 * @from source address
 * @n number of bytes to copy
 * @left number of bytes not copied (as returned by copy_from_user)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
instrument_copy_from_user_after(const void *to, const void /* nothing */ *from,
    unsigned long n, unsigned long left)
{
 kmsan_unpoison_memory(to, n - left);
}

/**
 * instrument_get_user() - add instrumentation to get_user()-like macros
 *
 * get_user() and friends are fragile, so it may depend on the implementation
 * whether the instrumentation happens before or after the data is copied from
 * the userspace.
 *
 * @to destination variable, may not be address-taken
 */
# 176 "./include/linux/instrumented.h"
/**
 * instrument_put_user() - add instrumentation to put_user()-like macros
 *
 * put_user() and friends are fragile, so it may depend on the implementation
 * whether the instrumentation happens before or after the data is copied from
 * the userspace.
 *
 * @from source address
 * @ptr userspace pointer to copy to
 * @size number of bytes to copy
 */
# 23 "./include/linux/atomic/atomic-instrumented.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_read(const atomic_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_read_acquire(const atomic_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return arch_atomic_read_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_set(atomic_t *v, int i)
{
 instrument_atomic_write(v, sizeof(*v));
 do { *(volatile typeof(((v)->counter)) *)&(((v)->counter)) = ((i)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_set_release(atomic_t *v, int i)
{
 do { } while (0);
 instrument_atomic_write(v, sizeof(*v));
 arch_atomic_set_release(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_add(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_add_return(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_add_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_add_return_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_add_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_add_return_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_add_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_add_return_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_add_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_add(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_add_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_add_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_add_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_add_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_add_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_add_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_sub(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_sub_return(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_sub_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_sub_return_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_sub_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_sub_return_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_sub_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_sub_return_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_sub_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_sub(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_sub_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_sub_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_sub_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_sub_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_sub_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_sub_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_inc(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_inc_return(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_inc_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_inc_return_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_inc_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_inc_return_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_inc_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_inc_return_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_inc_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_inc(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_inc_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_inc_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_inc_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_inc_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_inc_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_inc_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_dec(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_dec_return(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_dec_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_dec_return_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_dec_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_dec_return_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_dec_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_dec_return_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_dec_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_dec(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_dec_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_dec_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_dec_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_dec_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_dec_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_dec_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_and(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_and(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_and_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_and_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_and_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_and_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_and_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_and_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_andnot(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_andnot(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_andnot_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_andnot_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_andnot_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_andnot_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_andnot_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_andnot_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_or(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_or(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_or_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_or_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_or_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_or_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_or_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_or_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_xor(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_xor(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_xor_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_xor_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_xor_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_xor_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_xor_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_xor_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_xchg(atomic_t *v, int i)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_mb((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_xchg_acquire(atomic_t *v, int i)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_acq((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_xchg_release(atomic_t *v, int i)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_rel((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_xchg_relaxed(atomic_t *v, int i)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_cmpxchg(atomic_t *v, int old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_mb((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_acq((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_cmpxchg_release(atomic_t *v, int old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_rel((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg(atomic_t *v, int *old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_try_cmpxchg(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_try_cmpxchg_acquire(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_try_cmpxchg_release(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_try_cmpxchg_relaxed(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_sub_and_test(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_sub_and_test(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_dec_and_test(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_dec_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_inc_and_test(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_inc_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_add_negative(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_add_negative(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_fetch_add_unless(atomic_t *v, int a, int u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_fetch_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_add_unless(atomic_t *v, int a, int u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_inc_not_zero(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_inc_not_zero(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_inc_unless_negative(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_inc_unless_negative(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_dec_unless_positive(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_dec_unless_positive(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
atomic_dec_if_positive(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_dec_if_positive(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_read(const atomic64_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_read_acquire(const atomic64_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return arch_atomic64_read_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_set(atomic64_t *v, s64 i)
{
 instrument_atomic_write(v, sizeof(*v));
 do { *(volatile typeof(((v)->counter)) *)&(((v)->counter)) = ((i)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_set_release(atomic64_t *v, s64 i)
{
 do { } while (0);
 instrument_atomic_write(v, sizeof(*v));
 arch_atomic64_set_release(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_add(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_add_return(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_add_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_add_return_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_add_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_add_return_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_add_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_add_return_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_add_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_add(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_add_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_add_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_add_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_sub(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_sub_return(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_sub_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_sub_return_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_sub_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_sub_return_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_sub_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_sub_return_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_sub_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_sub_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_sub_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_sub_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_inc(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_inc_return(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_inc_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_inc_return_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_inc_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_inc_return_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_inc_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_inc_return_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_inc_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_inc_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_inc_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_inc_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_dec(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_dec_return(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_dec_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_dec_return_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_dec_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_dec_return_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_dec_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_dec_return_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_dec_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_dec_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_dec_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_dec_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_and(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_and(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_and_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_and_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_and_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_and_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_and_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_and_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_andnot(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_andnot_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_andnot_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_andnot_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_or(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_or(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_or_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_or_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_or_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_or_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_or_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_or_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic64_xor(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic64_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_xor_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_xor_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_xor_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_xchg(atomic64_t *v, s64 i)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_mb((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_xchg_acquire(atomic64_t *v, s64 i)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_acq((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_xchg_release(atomic64_t *v, s64 i)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg_rel((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_xchg_relaxed(atomic64_t *v, s64 i)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __xchg((unsigned long)((i)), (&((v)->counter)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_mb((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_acquire(atomic64_t *v, s64 old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_acq((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_release(atomic64_t *v, s64 old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg_rel((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_relaxed(atomic64_t *v, s64 old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return ({ __typeof__(*(&((v)->counter))) __ret; __ret = (__typeof__(*(&((v)->counter)))) __cmpxchg((&((v)->counter)), (unsigned long)((old)), (unsigned long)((new)), sizeof(*(&((v)->counter)))); __ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic64_try_cmpxchg(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_acquire(atomic64_t *v, s64 *old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic64_try_cmpxchg_acquire(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_release(atomic64_t *v, s64 *old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic64_try_cmpxchg_release(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_relaxed(atomic64_t *v, s64 *old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic64_try_cmpxchg_relaxed(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_sub_and_test(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_sub_and_test(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_dec_and_test(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_dec_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_inc_and_test(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_inc_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_add_negative(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_add_negative(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_fetch_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_inc_not_zero(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_inc_not_zero(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_inc_unless_negative(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_inc_unless_negative(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic64_dec_unless_positive(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_dec_unless_positive(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) s64
atomic64_dec_if_positive(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic64_dec_if_positive(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_read(const atomic_long_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return arch_atomic_long_read(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_read_acquire(const atomic_long_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return arch_atomic_long_read_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_set(atomic_long_t *v, long i)
{
 instrument_atomic_write(v, sizeof(*v));
 arch_atomic_long_set(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_set_release(atomic_long_t *v, long i)
{
 do { } while (0);
 instrument_atomic_write(v, sizeof(*v));
 arch_atomic_long_set_release(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_add(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_add_return(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_add_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_add_return_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_add_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_add_return_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_add_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_add_return_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_add_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_add(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_add(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_add_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_add_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_add_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_sub(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_sub_return(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_sub_return(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_sub_return_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_sub_return_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_sub_return_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_sub_return_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_sub_return_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_sub_return_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_sub(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_sub_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_sub_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_sub_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_inc(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_inc_return(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_inc_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_inc_return_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_inc_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_inc_return_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_inc_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_inc_return_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_inc_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_inc(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_inc_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_inc_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_inc_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_dec(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_dec_return(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_dec_return(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_dec_return_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_dec_return_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_dec_return_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_dec_return_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_dec_return_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_dec_return_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_dec(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_dec_acquire(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_dec_release(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_dec_relaxed(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_and(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_and(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_and(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_and_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_and_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_and_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_and_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_and_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_and_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_andnot(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_andnot(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_andnot_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_andnot_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_andnot_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_or(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_or(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_or(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_or_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_or_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_or_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_or_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_or_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_or_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
atomic_long_xor(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 arch_atomic_long_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_xor(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_xor_acquire(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_xor_release(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_xor_relaxed(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_xchg(atomic_long_t *v, long i)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_xchg(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_xchg_acquire(atomic_long_t *v, long i)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_xchg_acquire(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_xchg_release(atomic_long_t *v, long i)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_xchg_release(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_xchg_relaxed(atomic_long_t *v, long i)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_xchg_relaxed(v, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg(atomic_long_t *v, long old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_cmpxchg(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_acquire(atomic_long_t *v, long old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_cmpxchg_acquire(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_release(atomic_long_t *v, long old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_cmpxchg_release(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_relaxed(atomic_long_t *v, long old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_cmpxchg_relaxed(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg(atomic_long_t *v, long *old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_long_try_cmpxchg(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_acquire(atomic_long_t *v, long *old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_long_try_cmpxchg_acquire(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_release(atomic_long_t *v, long *old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_long_try_cmpxchg_release(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_relaxed(atomic_long_t *v, long *old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return arch_atomic_long_try_cmpxchg_relaxed(v, old, new);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_sub_and_test(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_sub_and_test(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_dec_and_test(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_dec_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_inc_and_test(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_inc_and_test(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_add_negative(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_add_negative(i, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_unless(atomic_long_t *v, long a, long u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_fetch_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_add_unless(atomic_long_t *v, long a, long u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_add_unless(v, a, u);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_inc_not_zero(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_inc_not_zero(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_inc_unless_negative(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_inc_unless_negative(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool
atomic_long_dec_unless_positive(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_dec_unless_positive(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
atomic_long_dec_if_positive(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return arch_atomic_long_dec_if_positive(v);
}
# 2086 "./include/linux/atomic/atomic-instrumented.h"
// 764f741eb77a7ad565dc8d99ce2837d5542e8aee
# 83 "./include/linux/atomic.h" 2
# 6 "./include/asm-generic/bitops/atomic.h" 2



/*
 * Implementation of atomic bitops using atomic-fetch ops.
 * See Documentation/atomic_bitops.txt for details.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_set_bit(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 arch_atomic_long_or(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_clear_bit(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 arch_atomic_long_andnot(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_change_bit(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 arch_atomic_long_xor(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_test_and_set_bit(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = arch_atomic_long_fetch_or(mask, (atomic_long_t *)p);
 return !!(old & mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_test_and_clear_bit(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = arch_atomic_long_fetch_andnot(mask, (atomic_long_t *)p);
 return !!(old & mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_test_and_change_bit(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = arch_atomic_long_fetch_xor(mask, (atomic_long_t *)p);
 return !!(old & mask);
}

# 1 "./include/asm-generic/bitops/instrumented-atomic.h" 1
/* SPDX-License-Identifier: GPL-2.0 */

/*
 * This file provides wrappers with sanitizer instrumentation for atomic bit
 * operations.
 *
 * To use this functionality, an arch's bitops.h file needs to define each of
 * the below bit operations with an arch_ prefix (e.g. arch_set_bit(),
 * arch___set_bit(), etc.).
 */





/**
 * set_bit - Atomically set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * This is a relaxed atomic operation (no implied memory barriers).
 *
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void set_bit(long nr, volatile unsigned long *addr)
{
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_set_bit(nr, addr);
}

/**
 * clear_bit - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * This is a relaxed atomic operation (no implied memory barriers).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void clear_bit(long nr, volatile unsigned long *addr)
{
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_clear_bit(nr, addr);
}

/**
 * change_bit - Toggle a bit in memory
 * @nr: Bit to change
 * @addr: Address to start counting from
 *
 * This is a relaxed atomic operation (no implied memory barriers).
 *
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void change_bit(long nr, volatile unsigned long *addr)
{
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_change_bit(nr, addr);
}

/**
 * test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This is an atomic fully-ordered operation (implied full memory barrier).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool test_and_set_bit(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_set_bit(nr, addr);
}

/**
 * test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This is an atomic fully-ordered operation (implied full memory barrier).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool test_and_clear_bit(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_clear_bit(nr, addr);
}

/**
 * test_and_change_bit - Change a bit and return its old value
 * @nr: Bit to change
 * @addr: Address to count from
 *
 * This is an atomic fully-ordered operation (implied full memory barrier).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool test_and_change_bit(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_change_bit(nr, addr);
}
# 69 "./include/asm-generic/bitops/atomic.h" 2
# 26 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/lock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







/**
 * arch_test_and_set_bit_lock - Set a bit and return its old value, for lock
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is atomic and provides acquire barrier semantics if
 * the returned value is 0.
 * It can be used to implement bit locks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int
arch_test_and_set_bit_lock(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 if (({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_144(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*p) == sizeof(char) || sizeof(*p) == sizeof(short) || sizeof(*p) == sizeof(int) || sizeof(*p) == sizeof(long)) || sizeof(*p) == sizeof(long long))) __compiletime_assert_144(); } while (0); (*(const volatile typeof( _Generic((*p), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*p))) *)&(*p)); }) & mask)
# 26 "./include/asm-generic/bitops/lock.h"
  return 1;

 old = arch_atomic_long_fetch_or_acquire(mask, (atomic_long_t *)p);
 return !!(old & mask);
}


/**
 * arch_clear_bit_unlock - Clear a bit in memory, for unlock
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * This operation is atomic and provides release barrier semantics.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
arch_clear_bit_unlock(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 arch_atomic_long_fetch_andnot_release(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}

/**
 * arch___clear_bit_unlock - Clear a bit in memory, for unlock
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * A weaker form of clear_bit_unlock() as used by __bit_lock_unlock(). If all
 * the bits in the word are protected by this lock some archs can use weaker
 * ops to safely unlock.
 *
 * See for example x86's implementation.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
arch___clear_bit_unlock(unsigned int nr, volatile unsigned long *p)
{
 unsigned long old;

 p += ((nr) / 64);
 old = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_145(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*p) == sizeof(char) || sizeof(*p) == sizeof(short) || sizeof(*p) == sizeof(int) || sizeof(*p) == sizeof(long)) || sizeof(*p) == sizeof(long long))) __compiletime_assert_145(); } while (0); (*(const volatile typeof( _Generic((*p), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*p))) *)&(*p)); });
# 65 "./include/asm-generic/bitops/lock.h"
 old &= ~((((1UL))) << ((nr) % 64));
 arch_atomic_long_set_release((atomic_long_t *)p, old);
}

/**
 * arch_clear_bit_unlock_is_negative_byte - Clear a bit in memory and test if bottom
 *                                          byte is negative, for unlock.
 * @nr: the bit to clear
 * @addr: the address to start counting from
 *
 * This is a bit of a one-trick-pony for the filemap code, which clears
 * PG_locked and tests PG_waiters,
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_clear_bit_unlock_is_negative_byte(unsigned int nr,
         volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = arch_atomic_long_fetch_andnot_release(mask, (atomic_long_t *)p);
 return !!(old & ((((1UL))) << (7)));
}



# 1 "./include/asm-generic/bitops/instrumented-lock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */

/*
 * This file provides wrappers with sanitizer instrumentation for bit
 * locking operations.
 *
 * To use this functionality, an arch's bitops.h file needs to define each of
 * the below bit operations with an arch_ prefix (e.g. arch_set_bit(),
 * arch___set_bit(), etc.).
 */





/**
 * clear_bit_unlock - Clear a bit in memory, for unlock
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * This operation is atomic and provides release barrier semantics.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_bit_unlock(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_clear_bit_unlock(nr, addr);
}

/**
 * __clear_bit_unlock - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * This is a non-atomic operation but implies a release barrier before the
 * memory operation. It can be used for an unlock if no other CPUs can
 * concurrently modify other bits in the word.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __clear_bit_unlock(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_write(addr + ((nr) / 64), sizeof(long));
 arch___clear_bit_unlock(nr, addr);
}

/**
 * test_and_set_bit_lock - Set a bit and return its old value, for lock
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is atomic and provides acquire barrier semantics if
 * the returned value is 0.
 * It can be used to implement bit locks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool test_and_set_bit_lock(long nr, volatile unsigned long *addr)
{
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_set_bit_lock(nr, addr);
}


/**
 * clear_bit_unlock_is_negative_byte - Clear a bit in memory and test if bottom
 *                                     byte is negative, for unlock.
 * @nr: the bit to clear
 * @addr: the address to start counting from
 *
 * This operation is atomic and provides release barrier semantics.
 *
 * This is a bit of a one-trick-pony for the filemap code, which clears
 * PG_locked and tests PG_waiters,
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
clear_bit_unlock_is_negative_byte(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 return arch_clear_bit_unlock_is_negative_byte(nr, addr);
}
/* Let everybody know we have it. */
# 93 "./include/asm-generic/bitops/lock.h" 2
# 27 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/non-atomic.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 18 "./include/asm-generic/bitops/non-atomic.h"
# 1 "./include/asm-generic/bitops/non-instrumented-non-atomic.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 19 "./include/asm-generic/bitops/non-atomic.h" 2
# 28 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/le.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 6 "./include/asm-generic/bitops/le.h" 2
# 19 "./include/asm-generic/bitops/le.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_bit_le(int nr, const void *addr)
{
 return ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? const_test_bit(nr ^ 0, addr) : generic_test_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_bit_le(int nr, void *addr)
{
 set_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_bit_le(int nr, void *addr)
{
 clear_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __set_bit_le(int nr, void *addr)
{
 ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___set_bit(nr ^ 0, addr) : generic___set_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __clear_bit_le(int nr, void *addr)
{
 ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___clear_bit(nr ^ 0, addr) : generic___clear_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_and_set_bit_le(int nr, void *addr)
{
 return test_and_set_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_and_clear_bit_le(int nr, void *addr)
{
 return test_and_clear_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __test_and_set_bit_le(int nr, void *addr)
{
 return ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___test_and_set_bit(nr ^ 0, addr) : generic___test_and_set_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __test_and_clear_bit_le(int nr, void *addr)
{
 return ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___test_and_clear_bit(nr ^ 0, addr) : generic___test_and_clear_bit(nr ^ 0, addr));
}
# 29 "./arch/arm64/include/asm/bitops.h" 2
# 1 "./include/asm-generic/bitops/ext2-atomic-setbit.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Atomic bitops based version of ext2 atomic bitops
 */
# 30 "./arch/arm64/include/asm/bitops.h" 2
# 69 "./include/linux/bitops.h" 2

/* Check that the bitops prototypes are sane */





_Static_assert(__builtin_types_compatible_p(typeof(generic___set_bit), typeof(generic___set_bit)) && __builtin_types_compatible_p(typeof(generic___set_bit), typeof(generic___set_bit)) && __builtin_types_compatible_p(typeof(generic___set_bit), typeof(generic___set_bit)), "__same_type(arch___set_bit, generic___set_bit) && __same_type(const___set_bit, generic___set_bit) && __same_type(___set_bit, generic___set_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___clear_bit), typeof(generic___clear_bit)) && __builtin_types_compatible_p(typeof(generic___clear_bit), typeof(generic___clear_bit)) && __builtin_types_compatible_p(typeof(generic___clear_bit), typeof(generic___clear_bit)), "__same_type(arch___clear_bit, generic___clear_bit) && __same_type(const___clear_bit, generic___clear_bit) && __same_type(___clear_bit, generic___clear_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___change_bit), typeof(generic___change_bit)) && __builtin_types_compatible_p(typeof(generic___change_bit), typeof(generic___change_bit)) && __builtin_types_compatible_p(typeof(generic___change_bit), typeof(generic___change_bit)), "__same_type(arch___change_bit, generic___change_bit) && __same_type(const___change_bit, generic___change_bit) && __same_type(___change_bit, generic___change_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___test_and_set_bit), typeof(generic___test_and_set_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_set_bit), typeof(generic___test_and_set_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_set_bit), typeof(generic___test_and_set_bit)), "__same_type(arch___test_and_set_bit, generic___test_and_set_bit) && __same_type(const___test_and_set_bit, generic___test_and_set_bit) && __same_type(___test_and_set_bit, generic___test_and_set_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___test_and_clear_bit), typeof(generic___test_and_clear_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_clear_bit), typeof(generic___test_and_clear_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_clear_bit), typeof(generic___test_and_clear_bit)), "__same_type(arch___test_and_clear_bit, generic___test_and_clear_bit) && __same_type(const___test_and_clear_bit, generic___test_and_clear_bit) && __same_type(___test_and_clear_bit, generic___test_and_clear_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___test_and_change_bit), typeof(generic___test_and_change_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_change_bit), typeof(generic___test_and_change_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_change_bit), typeof(generic___test_and_change_bit)), "__same_type(arch___test_and_change_bit, generic___test_and_change_bit) && __same_type(const___test_and_change_bit, generic___test_and_change_bit) && __same_type(___test_and_change_bit, generic___test_and_change_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic_test_bit), typeof(generic_test_bit)) && __builtin_types_compatible_p(typeof(const_test_bit), typeof(generic_test_bit)) && __builtin_types_compatible_p(typeof(generic_test_bit), typeof(generic_test_bit)), "__same_type(arch_test_bit, generic_test_bit) && __same_type(const_test_bit, generic_test_bit) && __same_type(_test_bit, generic_test_bit)");



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_bitmask_order(unsigned int count)
{
 int order;

 order = fls(count);
 return order; /* We could be slightly more clever with -1 here... */
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long hweight_long(unsigned long w)
{
 return sizeof(w) == 4 ? (__builtin_constant_p(w) ? ((((unsigned int) ((!!((w) & (1ULL << 0))) + (!!((w) & (1ULL << 1))) + (!!((w) & (1ULL << 2))) + (!!((w) & (1ULL << 3))) + (!!((w) & (1ULL << 4))) + (!!((w) & (1ULL << 5))) + (!!((w) & (1ULL << 6))) + (!!((w) & (1ULL << 7))))) + ((unsigned int) ((!!(((w) >> 8) & (1ULL << 0))) + (!!(((w) >> 8) & (1ULL << 1))) + (!!(((w) >> 8) & (1ULL << 2))) + (!!(((w) >> 8) & (1ULL << 3))) + (!!(((w) >> 8) & (1ULL << 4))) + (!!(((w) >> 8) & (1ULL << 5))) + (!!(((w) >> 8) & (1ULL << 6))) + (!!(((w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((w) >> 16) & (1ULL << 0))) + (!!(((w) >> 16) & (1ULL << 1))) + (!!(((w) >> 16) & (1ULL << 2))) + (!!(((w) >> 16) & (1ULL << 3))) + (!!(((w) >> 16) & (1ULL << 4))) + (!!(((w) >> 16) & (1ULL << 5))) + (!!(((w) >> 16) & (1ULL << 6))) + (!!(((w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((w) >> 16) >> 8) & (1ULL << 0))) + (!!((((w) >> 16) >> 8) & (1ULL << 1))) + (!!((((w) >> 16) >> 8) & (1ULL << 2))) + (!!((((w) >> 16) >> 8) & (1ULL << 3))) + (!!((((w) >> 16) >> 8) & (1ULL << 4))) + (!!((((w) >> 16) >> 8) & (1ULL << 5))) + (!!((((w) >> 16) >> 8) & (1ULL << 6))) + (!!((((w) >> 16) >> 8) & (1ULL << 7))))))) : __arch_hweight32(w)) : (__builtin_constant_p((__u64)w) ? (((((unsigned int) ((!!(((__u64)w) & (1ULL << 0))) + (!!(((__u64)w) & (1ULL << 1))) + (!!(((__u64)w) & (1ULL << 2))) + (!!(((__u64)w) & (1ULL << 3))) + (!!(((__u64)w) & (1ULL << 4))) + (!!(((__u64)w) & (1ULL << 5))) + (!!(((__u64)w) & (1ULL << 6))) + (!!(((__u64)w) & (1ULL << 7))))) + ((unsigned int) ((!!((((__u64)w) >> 8) & (1ULL << 0))) + (!!((((__u64)w) >> 8) & (1ULL << 1))) + (!!((((__u64)w) >> 8) & (1ULL << 2))) + (!!((((__u64)w) >> 8) & (1ULL << 3))) + (!!((((__u64)w) >> 8) & (1ULL << 4))) + (!!((((__u64)w) >> 8) & (1ULL << 5))) + (!!((((__u64)w) >> 8) & (1ULL << 6))) + (!!((((__u64)w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!((((__u64)w) >> 16) & (1ULL << 0))) + (!!((((__u64)w) >> 16) & (1ULL << 1))) + (!!((((__u64)w) >> 16) & (1ULL << 2))) + (!!((((__u64)w) >> 16) & (1ULL << 3))) + (!!((((__u64)w) >> 16) & (1ULL << 4))) + (!!((((__u64)w) >> 16) & (1ULL << 5))) + (!!((((__u64)w) >> 16) & (1ULL << 6))) + (!!((((__u64)w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!(((((__u64)w) >> 16) >> 8) & (1ULL << 0))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 1))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 2))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 3))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 4))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 5))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 6))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 7))))))) + ((((unsigned int) ((!!((((__u64)w) >> 32) & (1ULL << 0))) + (!!((((__u64)w) >> 32) & (1ULL << 1))) + (!!((((__u64)w) >> 32) & (1ULL << 2))) + (!!((((__u64)w) >> 32) & (1ULL << 3))) + (!!((((__u64)w) >> 32) & (1ULL << 4))) + (!!((((__u64)w) >> 32) & (1ULL << 5))) + (!!((((__u64)w) >> 32) & (1ULL << 6))) + (!!((((__u64)w) >> 32) & (1ULL << 7))))) + ((unsigned int) ((!!(((((__u64)w) >> 32) >> 8) & (1ULL << 0))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 1))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 2))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 3))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 4))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 5))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 6))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((((__u64)w) >> 32) >> 16) & (1ULL << 0))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 1))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 2))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 3))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 4))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 5))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 6))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 0))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 1))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 2))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 3))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 4))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 5))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 6))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 7)))))))) : __arch_hweight64((__u64)w));
}

/**
 * rol64 - rotate a 64-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u64 rol64(__u64 word, unsigned int shift)
{
 return (word << (shift & 63)) | (word >> ((-shift) & 63));
}

/**
 * ror64 - rotate a 64-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u64 ror64(__u64 word, unsigned int shift)
{
 return (word >> (shift & 63)) | (word << ((-shift) & 63));
}

/**
 * rol32 - rotate a 32-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u32 rol32(__u32 word, unsigned int shift)
{
 return (word << (shift & 31)) | (word >> ((-shift) & 31));
}

/**
 * ror32 - rotate a 32-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u32 ror32(__u32 word, unsigned int shift)
{
 return (word >> (shift & 31)) | (word << ((-shift) & 31));
}

/**
 * rol16 - rotate a 16-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u16 rol16(__u16 word, unsigned int shift)
{
 return (word << (shift & 15)) | (word >> ((-shift) & 15));
}

/**
 * ror16 - rotate a 16-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u16 ror16(__u16 word, unsigned int shift)
{
 return (word >> (shift & 15)) | (word << ((-shift) & 15));
}

/**
 * rol8 - rotate an 8-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u8 rol8(__u8 word, unsigned int shift)
{
 return (word << (shift & 7)) | (word >> ((-shift) & 7));
}

/**
 * ror8 - rotate an 8-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __u8 ror8(__u8 word, unsigned int shift)
{
 return (word >> (shift & 7)) | (word << ((-shift) & 7));
}

/**
 * sign_extend32 - sign extend a 32-bit value using specified bit as sign-bit
 * @value: value to sign extend
 * @index: 0 based bit index (0<=index<32) to sign bit
 *
 * This is safe to use for 16- and 8-bit types as well.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __s32 sign_extend32(__u32 value, int index)
{
 __u8 shift = 31 - index;
 return (__s32)(value << shift) >> shift;
}

/**
 * sign_extend64 - sign extend a 64-bit value using specified bit as sign-bit
 * @value: value to sign extend
 * @index: 0 based bit index (0<=index<64) to sign bit
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __s64 sign_extend64(__u64 value, int index)
{
 __u8 shift = 63 - index;
 return (__s64)(value << shift) >> shift;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned fls_long(unsigned long l)
{
 if (sizeof(l) == 4)
  return fls(l);
 return fls64(l);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_count_order(unsigned int count)
{
 if (count == 0)
  return -1;

 return fls(--count);
}

/**
 * get_count_order_long - get order after rounding @l up to power of 2
 * @l: parameter
 *
 * it is same as get_count_order() but with long type parameter
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_count_order_long(unsigned long l)
{
 if (l == 0UL)
  return -1;
 return (int)fls_long(--l);
}

/**
 * __ffs64 - find first set bit in a 64 bit word
 * @word: The 64 bit word
 *
 * On 64 bit arches this is a synonym for __ffs
 * The result is not defined if no bits are set, so check that @word
 * is non-zero before calling this.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __ffs64(u64 word)
{






 return __ffs((unsigned long)word);
}

/**
 * fns - find N'th set bit in a word
 * @word: The word to search
 * @n: Bit to find
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long fns(unsigned long word, unsigned int n)
{
 unsigned int bit;

 while (word) {
  bit = __ffs(word);
  if (n-- == 0)
   return bit;
  ((__builtin_constant_p(bit) && __builtin_constant_p((uintptr_t)(&word) != (uintptr_t)((void *)0)) && (uintptr_t)(&word) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&word))) ? generic___clear_bit(bit, &word) : generic___clear_bit(bit, &word));
 }

 return 64;
}

/**
 * assign_bit - Assign value to a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 * @value: the value to assign
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void assign_bit(long nr, volatile unsigned long *addr,
           bool value)
{
 if (value)
  set_bit(nr, addr);
 else
  clear_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __assign_bit(long nr, volatile unsigned long *addr,
      bool value)
{
 if (value)
  ((__builtin_constant_p(nr) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___set_bit(nr, addr) : generic___set_bit(nr, addr));
 else
  ((__builtin_constant_p(nr) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___clear_bit(nr, addr) : generic___clear_bit(nr, addr));
}

/**
 * __ptr_set_bit - Set bit in a pointer's value
 * @nr: the bit to set
 * @addr: the address of the pointer variable
 *
 * Example:
 *	void *p = foo();
 *	__ptr_set_bit(bit, &p);
 */






/**
 * __ptr_clear_bit - Clear bit in a pointer's value
 * @nr: the bit to clear
 * @addr: the address of the pointer variable
 *
 * Example:
 *	void *p = foo();
 *	__ptr_clear_bit(bit, &p);
 */






/**
 * __ptr_test_bit - Test bit in a pointer's value
 * @nr: the bit to test
 * @addr: the address of the pointer variable
 *
 * Example:
 *	void *p = foo();
 *	if (__ptr_test_bit(bit, &p)) {
 *	        ...
 *	} else {
 *		...
 *	}
 */
# 28 "./include/linux/thread_info.h" 2

/*
 * For per-arch arch_within_stack_frames() implementations, defined in
 * asm/thread_info.h.
 */
enum {
 BAD_STACK = -1,
 NOT_STACK = 0,
 GOOD_FRAME,
 GOOD_STACK,
};
# 60 "./include/linux/thread_info.h"
# 1 "./arch/arm64/include/asm/thread_info.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/thread_info.h
 *
 * Copyright (C) 2002 Russell King.
 * Copyright (C) 2012 ARM Ltd.
 */







struct task_struct;

# 1 "./arch/arm64/include/asm/memory.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/memory.h
 *
 * Copyright (C) 2000-2002 Russell King
 * Copyright (C) 2012 ARM Ltd.
 *
 * Note: this file should not be included by non-asm/.h files
 */




# 1 "./include/linux/sizes.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * include/linux/sizes.h
 */
# 15 "./arch/arm64/include/asm/memory.h" 2
# 1 "./arch/arm64/include/asm/page-def.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/page.h
 *
 * Copyright (C) 1995-2003 Russell King
 * Copyright (C) 2017 ARM Ltd.
 */





/* PAGE_SHIFT determines the page size */
# 16 "./arch/arm64/include/asm/memory.h" 2

/*
 * Size of the PCI I/O space. This must remain a power of two so that
 * IO_SPACE_LIMIT acts as a mask for the low bits of I/O addresses.
 */


/*
 * VMEMMAP_SIZE - allows the whole linear region to be covered by
 *                a struct page array
 *
 * If we are configured with a 52-bit kernel VA then our VMEMMAP_SIZE
 * needs to cover the memory region from the beginning of the 52-bit
 * PAGE_OFFSET all the way to PAGE_END for 48-bit. This allows us to
 * keep a constant PAGE_OFFSET and "fallback" to using the higher end
 * of the VMEMMAP where 52-bit support is not available in hardware.
 */



/*
 * PAGE_OFFSET - the virtual address of the start of the linear map, at the
 *               start of the TTBR1 address space.
 * PAGE_END - the end of the linear map, where all other kernel mappings begin.
 * KIMAGE_VADDR - the virtual address of the start of the kernel image.
 * VA_BITS - the maximum number of bits for virtual addresses.
 */
# 67 "./arch/arm64/include/asm/memory.h"
/*
 * Generic and tag-based KASAN require 1/8th and 1/16th of the kernel virtual
 * address space for the shadow region respectively. They can bloat the stack
 * significantly, so double the (minimum) stack size when they are in use.
 */
# 85 "./arch/arm64/include/asm/memory.h"
/*
 * VMAP'd stacks are allocated at page granularity, so we must ensure that such
 * stacks are a multiple of page size.
 */
# 101 "./arch/arm64/include/asm/memory.h"
/*
 * By aligning VMAP'd stacks to 2 * THREAD_SIZE, we can detect overflow by
 * checking sp & (1 << THREAD_SHIFT), which we can do cheaply in the entry
 * assembly.
 */
# 116 "./arch/arm64/include/asm/memory.h"
/*
 * With the minimum frame size of [x29, x30], exactly half the combined
 * sizes of the hyp and overflow stacks is the maximum size needed to
 * save the unwinded stacktrace; plus an additional entry to delimit the
 * end.
 */


/*
 * Alignment of kernel segments (e.g. .text, .data).
 *
 *  4 KB granule:  16 level 3 entries, with contiguous bit
 * 16 KB granule:   4 level 3 entries, without contiguous bit
 * 64 KB granule:   1 level 3 entry
 */


/*
 * Memory types available.
 *
 * IMPORTANT: MT_NORMAL must be index 0 since vm_get_page_prot() may 'or' in
 *	      the MT_NORMAL_TAGGED memory type for PROT_MTE mappings. Note
 *	      that protection_map[] only contains MT_NORMAL attributes.
 */






/*
 * Memory types for Stage-2 translation
 */



/*
 * Memory types for Stage-2 translation when ID_AA64MMFR2_EL1.FWB is 0001
 * Stage-2 enforces Normal-WB and Device-nGnRE
 */
# 165 "./arch/arm64/include/asm/memory.h"
/*
 *  Open-coded (swapper_pg_dir - reserved_pg_dir) as this cannot be calculated
 *  until link time.
 */


/*
 *  Open-coded (swapper_pg_dir - tramp_pg_dir) as this cannot be calculated
 *  until link time.
 */






# 1 "./include/linux/mmdebug.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






struct page;
struct vm_area_struct;
struct mm_struct;

void dump_page(struct page *page, const char *reason);
void dump_vma(const struct vm_area_struct *vma);
void dump_mm(const struct mm_struct *mm);
# 182 "./arch/arm64/include/asm/memory.h" 2
# 191 "./arch/arm64/include/asm/memory.h"
extern s64 memstart_addr;
/* PHYS_OFFSET - the physical address of the start of memory. */


/* the virtual base of the kernel image */
extern u64 kimage_vaddr;

/* the offset between the kernel virtual and physical mappings */
extern u64 kimage_voffset;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long kaslr_offset(void)
{
 return kimage_vaddr - ((((-((((1UL))) << ((((48))) - 1)))) + (0x08000000)));
}

/*
 * Allow all memory at the discovery stage. We will clip it later.
 */



/*
 * PFNs are used to describe any physical page; this means
 * PFN 0 == physical address 0.
 *
 * This is the PFN of the first RAM page in the kernel
 * direct-mapped view.  We assume this is the first page
 * of RAM in the mem_map as well.
 */


/*
 * When dealing with data aborts, watchpoints, or instruction traps we may end
 * up with a tagged userland pointer. Clear the tag to get a sane pointer to
 * pass on to access_ok(), for instance.
 */
# 246 "./arch/arm64/include/asm/memory.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const void *__tag_set(const void *addr, u8 tag)
{
 u64 __addr = (u64)addr & ~0UL;
 return (const void *)(__addr | 0UL);
}
# 263 "./arch/arm64/include/asm/memory.h"
/*
 * Physical vs virtual RAM address space conversion.  These are
 * private definitions which should NOT be used outside memory.h
 * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
 */


/*
 * Check whether an arbitrary address is within the linear map, which
 * lives in the [PAGE_OFFSET, PAGE_END) interval at the bottom of the
 * kernel's TTBR1 address range.
 */
# 298 "./arch/arm64/include/asm/memory.h"
/*
 * Convert a page to/from a physical address
 */



/*
 * Note: Drivers should NOT use these.  They are the wrong
 * translation for translating DMA addresses.  Use the driver
 * DMA support - see dma-mapping.h.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) phys_addr_t virt_to_phys(const volatile void *x)
{
 return ({ phys_addr_t __x = (phys_addr_t)(((unsigned long)(x))); (((u64)(__x) - ((-((((1UL))) << ((48)))))) < (((-((((1UL))) << ((((48))) - 1)))) - ((-((((1UL))) << ((48))))))) ? (((__x) - ((-((((1UL))) << ((48)))))) + ({ ((void)(sizeof(( long)(memstart_addr & 1)))); memstart_addr; })) : ((__x) - kimage_voffset); });
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *phys_to_virt(phys_addr_t x)
{
 return (void *)(((unsigned long)((x) - ({ ((void)(sizeof(( long)(memstart_addr & 1)))); memstart_addr; })) | ((-((((1UL))) << ((48)))))));
}

/*
 * Drivers should NOT use these either.
 */
# 332 "./arch/arm64/include/asm/memory.h"
/*
 *  virt_to_page(x)	convert a _valid_ virtual address to struct page *
 *  virt_addr_valid(x)	indicates whether a virtual address is valid
 */
# 365 "./arch/arm64/include/asm/memory.h"
void dump_mem_limit(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool defer_reserve_crashkernel(void)
{
 return 1 || 1;
}


/*
 * Given that the GIC architecture permits ITS implementations that can only be
 * configured with a LPI table address once, GICv3 systems with many CPUs may
 * end up reserving a lot of different regions after a kexec for their LPI
 * tables (one per CPU), as we are forced to reuse the same memory after kexec
 * (and thus reserve it persistently with EFI beforehand)
 */




/*
 * memory regions which marked with flag MEMBLOCK_NOMAP(for example, the memory
 * of the EFI_UNUSABLE_MEMORY type) may divide a continuous memory block into
 * multiple parts. As a result, the number of memory regions is large.
 */




# 1 "./include/asm-generic/memory_model.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/pfn.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/*
 * pfn_t: encapsulates a page-frame number that is optionally backed
 * by memmap (struct page).  Whether a pfn_t has a 'struct page'
 * backing is indicated by flags in the high bits of the value.
 */
typedef struct {
 u64 val;
} pfn_t;
# 6 "./include/asm-generic/memory_model.h" 2



/*
 * supports 3 memory models.
 */
# 24 "./include/asm-generic/memory_model.h"
/* memmap is virtually contiguous.  */
# 46 "./include/asm-generic/memory_model.h"
/*
 * Convert a physical address to a Page Frame Number and back
 */
# 394 "./arch/arm64/include/asm/memory.h" 2
# 18 "./arch/arm64/include/asm/thread_info.h" 2
# 1 "./arch/arm64/include/asm/stack_pointer.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * how to get the current stack pointer from C
 */
register unsigned long current_stack_pointer asm ("sp");
# 19 "./arch/arm64/include/asm/thread_info.h" 2
# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 20 "./arch/arm64/include/asm/thread_info.h" 2

/*
 * low level task data that entry.S needs immediate access to.
 */
struct thread_info {
 unsigned long flags; /* low level flags */



 union {
  u64 preempt_count; /* 0 => preemptible, <0 => bug */
  struct {




   u32 count;
   u32 need_resched;

  } preempt;
 };




 u32 cpu;
};
# 55 "./arch/arm64/include/asm/thread_info.h"
void arch_setup_new_exec(void);


void arch_release_task_struct(struct task_struct *tsk);
int arch_dup_task_struct(struct task_struct *dst,
    struct task_struct *src);
# 61 "./include/linux/thread_info.h" 2







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long set_restart_fn(struct restart_block *restart,
     long (*fn)(struct restart_block *))
{
 restart->fn = fn;
 do { } while (0);
 return -516 /* restart by calling sys_restart_syscall */;
}







/*
 * flag set/clear/test wrappers
 * - pass TIF_xxxx constants to these functions
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_ti_thread_flag(struct thread_info *ti, int flag)
{
 set_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_ti_thread_flag(struct thread_info *ti, int flag)
{
 clear_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void update_ti_thread_flag(struct thread_info *ti, int flag,
      bool value)
{
 if (value)
  set_ti_thread_flag(ti, flag);
 else
  clear_ti_thread_flag(ti, flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_and_set_ti_thread_flag(struct thread_info *ti, int flag)
{
 return test_and_set_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_and_clear_ti_thread_flag(struct thread_info *ti, int flag)
{
 return test_and_clear_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_ti_thread_flag(struct thread_info *ti, int flag)
{
 return ((__builtin_constant_p(flag) && __builtin_constant_p((uintptr_t)((unsigned long *)&ti->flags) != (uintptr_t)((void *)0)) && (uintptr_t)((unsigned long *)&ti->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)((unsigned long *)&ti->flags))) ? const_test_bit(flag, (unsigned long *)&ti->flags) : generic_test_bit(flag, (unsigned long *)&ti->flags));
}

/*
 * This may be used in noinstr code, and needs to be __always_inline to prevent
 * inadvertent instrumentation.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long read_ti_thread_flags(struct thread_info *ti)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_146(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ti->flags) == sizeof(char) || sizeof(ti->flags) == sizeof(short) || sizeof(ti->flags) == sizeof(int) || sizeof(ti->flags) == sizeof(long)) || sizeof(ti->flags) == sizeof(long long))) __compiletime_assert_146(); } while (0); (*(const volatile typeof( _Generic((ti->flags), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (ti->flags))) *)&(ti->flags)); });
# 128 "./include/linux/thread_info.h"
}
# 183 "./include/linux/thread_info.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_within_stack_frames(const void * const stack,
        const void * const stackend,
        const void *obj, unsigned long len)
{
 return 0;
}
# 202 "./include/linux/thread_info.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void check_object_size(const void *ptr, unsigned long n,
         bool to_user)
{ }


extern void __attribute__((__error__("copy source size is too small")))
__bad_copy_from(void);
extern void __attribute__((__error__("copy destination size is too small")))
__bad_copy_to(void);

void __copy_overflow(int size, unsigned long count);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void copy_overflow(int size, unsigned long count)
{
 if (1)
  __copy_overflow(size, count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__)) bool
check_copy_size(const void *addr, size_t bytes, bool is_source)
{
 int sz = __builtin_object_size(addr, 0);
 if (__builtin_expect(!!(sz >= 0 && sz < bytes), 0)) {
  if (!__builtin_constant_p(bytes))
   copy_overflow(sz, bytes);
  else if (is_source)
   __bad_copy_from();
  else
   __bad_copy_to();
  return false;
 }
 if (({ int __ret_warn_on = !!(bytes > ((int)(~0U >> 1))); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/thread_info.h\"; .popsection; .long 14472b - .; .short 233; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); }))
  return false;
 check_object_size(addr, bytes, is_source);
 return true;
}
# 7 "./arch/arm64/include/asm/preempt.h" 2




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int preempt_count(void)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_147(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(char) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(short) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(int) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long)) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long long))) __compiletime_assert_147(); } while (0); (*(const volatile typeof( _Generic((((struct thread_info *)get_current())->preempt.count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (((struct thread_info *)get_current())->preempt.count))) *)&(((struct thread_info *)get_current())->preempt.count)); });
# 14 "./arch/arm64/include/asm/preempt.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void preempt_count_set(u64 pc)
{
 /* Preserve existing value of PREEMPT_NEED_RESCHED */
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_148(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(char) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(short) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(int) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long)) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long long))) __compiletime_assert_148(); } while (0); do { *(volatile typeof(((struct thread_info *)get_current())->preempt.count) *)&(((struct thread_info *)get_current())->preempt.count) = (pc); } while (0); } while (0);
# 20 "./arch/arm64/include/asm/preempt.h"
}
# 30 "./arch/arm64/include/asm/preempt.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_preempt_need_resched(void)
{
 ((struct thread_info *)get_current())->preempt.need_resched = 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_preempt_need_resched(void)
{
 ((struct thread_info *)get_current())->preempt.need_resched = 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool test_preempt_need_resched(void)
{
 return !((struct thread_info *)get_current())->preempt.need_resched;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __preempt_count_add(int val)
{
 u32 pc = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_149(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(char) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(short) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(int) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long)) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long long))) __compiletime_assert_149(); } while (0); (*(const volatile typeof( _Generic((((struct thread_info *)get_current())->preempt.count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (((struct thread_info *)get_current())->preempt.count))) *)&(((struct thread_info *)get_current())->preempt.count)); });
# 48 "./arch/arm64/include/asm/preempt.h"
 pc += val;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_150(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(char) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(short) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(int) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long)) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long long))) __compiletime_assert_150(); } while (0); do { *(volatile typeof(((struct thread_info *)get_current())->preempt.count) *)&(((struct thread_info *)get_current())->preempt.count) = (pc); } while (0); } while (0);
# 50 "./arch/arm64/include/asm/preempt.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __preempt_count_sub(int val)
{
 u32 pc = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_151(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(char) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(short) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(int) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long)) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long long))) __compiletime_assert_151(); } while (0); (*(const volatile typeof( _Generic((((struct thread_info *)get_current())->preempt.count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (((struct thread_info *)get_current())->preempt.count))) *)&(((struct thread_info *)get_current())->preempt.count)); });
# 55 "./arch/arm64/include/asm/preempt.h"
 pc -= val;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_152(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(char) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(short) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(int) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long)) || sizeof(((struct thread_info *)get_current())->preempt.count) == sizeof(long long))) __compiletime_assert_152(); } while (0); do { *(volatile typeof(((struct thread_info *)get_current())->preempt.count) *)&(((struct thread_info *)get_current())->preempt.count) = (pc); } while (0); } while (0);
# 57 "./arch/arm64/include/asm/preempt.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __preempt_count_dec_and_test(void)
{
 struct thread_info *ti = ((struct thread_info *)get_current());
 u64 pc = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_153(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ti->preempt_count) == sizeof(char) || sizeof(ti->preempt_count) == sizeof(short) || sizeof(ti->preempt_count) == sizeof(int) || sizeof(ti->preempt_count) == sizeof(long)) || sizeof(ti->preempt_count) == sizeof(long long))) __compiletime_assert_153(); } while (0); (*(const volatile typeof( _Generic((ti->preempt_count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (ti->preempt_count))) *)&(ti->preempt_count)); });
# 64 "./arch/arm64/include/asm/preempt.h"
 /* Update only the count field, leaving need_resched unchanged */
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_154(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ti->preempt.count) == sizeof(char) || sizeof(ti->preempt.count) == sizeof(short) || sizeof(ti->preempt.count) == sizeof(int) || sizeof(ti->preempt.count) == sizeof(long)) || sizeof(ti->preempt.count) == sizeof(long long))) __compiletime_assert_154(); } while (0); do { *(volatile typeof(ti->preempt.count) *)&(ti->preempt.count) = (--pc); } while (0); } while (0);
# 67 "./arch/arm64/include/asm/preempt.h"
 /*
	 * If we wrote back all zeroes, then we're preemptible and in
	 * need of a reschedule. Otherwise, we need to reload the
	 * preempt_count in case the need_resched flag was cleared by an
	 * interrupt occurring between the non-atomic READ_ONCE/WRITE_ONCE
	 * pair.
	 */
 return !pc || !({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_155(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ti->preempt_count) == sizeof(char) || sizeof(ti->preempt_count) == sizeof(short) || sizeof(ti->preempt_count) == sizeof(int) || sizeof(ti->preempt_count) == sizeof(long)) || sizeof(ti->preempt_count) == sizeof(long long))) __compiletime_assert_155(); } while (0); (*(const volatile typeof( _Generic((ti->preempt_count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (ti->preempt_count))) *)&(ti->preempt_count)); });
# 75 "./arch/arm64/include/asm/preempt.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool should_resched(int preempt_offset)
{
 u64 pc = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_156(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((struct thread_info *)get_current())->preempt_count) == sizeof(char) || sizeof(((struct thread_info *)get_current())->preempt_count) == sizeof(short) || sizeof(((struct thread_info *)get_current())->preempt_count) == sizeof(int) || sizeof(((struct thread_info *)get_current())->preempt_count) == sizeof(long)) || sizeof(((struct thread_info *)get_current())->preempt_count) == sizeof(long long))) __compiletime_assert_156(); } while (0); (*(const volatile typeof( _Generic((((struct thread_info *)get_current())->preempt_count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (((struct thread_info *)get_current())->preempt_count))) *)&(((struct thread_info *)get_current())->preempt_count)); });
# 80 "./arch/arm64/include/asm/preempt.h"
 return pc == preempt_offset;
}



void preempt_schedule(void);
void preempt_schedule_notrace(void);
# 79 "./include/linux/preempt.h" 2

/**
 * interrupt_context_level - return interrupt context level
 *
 * Returns the current interrupt context level.
 *  0 - normal context
 *  1 - softirq context
 *  2 - hardirq context
 *  3 - NMI context
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned char interrupt_context_level(void)
{
 unsigned long pc = preempt_count();
 unsigned char level = 0;

 level += !!(pc & ((((1UL << (4))-1) << (((0 + 8) + 8) + 4))));
 level += !!(pc & ((((1UL << (4))-1) << (((0 + 8) + 8) + 4)) | (((1UL << (4))-1) << ((0 + 8) + 8))));
 level += !!(pc & ((((1UL << (4))-1) << (((0 + 8) + 8) + 4)) | (((1UL << (4))-1) << ((0 + 8) + 8)) | (1UL << (0 + 8))));

 return level;
}
# 110 "./include/linux/preempt.h"
/*
 * Macros to retrieve the current execution context:
 *
 * in_nmi()		- We're in NMI context
 * in_hardirq()		- We're in hard IRQ context
 * in_serving_softirq()	- We're in softirq context
 * in_task()		- We're in task context
 */





/*
 * The following macros are deprecated and should not be used in new code:
 * in_irq()       - Obsolete version of in_hardirq()
 * in_softirq()   - We have BH disabled, or are processing softirqs
 * in_interrupt() - We're in NMI,IRQ,SoftIRQ context or have BH disabled
 */




/*
 * The preempt_count offset after preempt_disable();
 */






/*
 * The preempt_count offset after spin_lock()
 */







/*
 * The preempt_count offset needed for things like:
 *
 *  spin_lock_bh()
 *
 * Which need to disable both preemption (CONFIG_PREEMPT_COUNT) and
 * softirqs, such that unlock sequences of:
 *
 *  spin_unlock();
 *  local_bh_enable();
 *
 * Work as expected.
 */


/*
 * Are we running in atomic context?  WARNING: this macro cannot
 * always detect atomic context; in particular, it cannot know about
 * held spinlocks in non-preemptible kernels.  Thus it should not be
 * used in the general case to determine whether sleeping is possible.
 * Do not use in_atomic() in driver code.
 */


/*
 * Check whether we were atomic before we did preempt_disable():
 * (used by the scheduler)
 */
# 309 "./include/linux/preempt.h"
struct preempt_notifier;

/**
 * preempt_ops - notifiers called when a task is preempted and rescheduled
 * @sched_in: we're about to be rescheduled:
 *    notifier: struct preempt_notifier for the task being scheduled
 *    cpu:  cpu we're scheduled on
 * @sched_out: we've just been preempted
 *    notifier: struct preempt_notifier for the task being preempted
 *    next: the task that's kicking us out
 *
 * Please note that sched_in and out are called under different
 * contexts.  sched_out is called with rq lock held and irq disabled
 * while sched_in is called without rq lock and irq enabled.  This
 * difference is intentional and depended upon by its users.
 */
struct preempt_ops {
 void (*sched_in)(struct preempt_notifier *notifier, int cpu);
 void (*sched_out)(struct preempt_notifier *notifier,
     struct task_struct *next);
};

/**
 * preempt_notifier - key for installing preemption notifiers
 * @link: internal use
 * @ops: defines the notifier functions to be called
 *
 * Usually used in conjunction with container_of().
 */
struct preempt_notifier {
 struct hlist_node link;
 struct preempt_ops *ops;
};

void preempt_notifier_inc(void);
void preempt_notifier_dec(void);
void preempt_notifier_register(struct preempt_notifier *notifier);
void preempt_notifier_unregister(struct preempt_notifier *notifier);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void preempt_notifier_init(struct preempt_notifier *notifier,
         struct preempt_ops *ops)
{
 INIT_HLIST_NODE(&notifier->link);
 notifier->ops = ops;
}





/*
 * Migrate-Disable and why it is undesired.
 *
 * When a preempted task becomes elegible to run under the ideal model (IOW it
 * becomes one of the M highest priority tasks), it might still have to wait
 * for the preemptee's migrate_disable() section to complete. Thereby suffering
 * a reduction in bandwidth in the exact duration of the migrate_disable()
 * section.
 *
 * Per this argument, the change from preempt_disable() to migrate_disable()
 * gets us:
 *
 * - a higher priority tasks gains reduced wake-up latency; with preempt_disable()
 *   it would have had to wait for the lower priority task.
 *
 * - a lower priority tasks; which under preempt_disable() could've instantly
 *   migrated away when another CPU becomes available, is now constrained
 *   by the ability to push the higher priority task away, which might itself be
 *   in a migrate_disable() section, reducing it's available bandwidth.
 *
 * IOW it trades latency / moves the interference term, but it stays in the
 * system, and as long as it remains unbounded, the system is not fully
 * deterministic.
 *
 *
 * The reason we have it anyway.
 *
 * PREEMPT_RT breaks a number of assumptions traditionally held. By forcing a
 * number of primitives into becoming preemptible, they would also allow
 * migration. This turns out to break a bunch of per-cpu usage. To this end,
 * all these primitives employ migirate_disable() to restore this implicit
 * assumption.
 *
 * This is a 'temporary' work-around at best. The correct solution is getting
 * rid of the above assumptions and reworking the code to employ explicit
 * per-cpu locking or short preempt-disable regions.
 *
 * The end goal must be to get rid of migrate_disable(), alternatively we need
 * a schedulability theory that does not depend on abritrary migration.
 *
 *
 * Notes on the implementation.
 *
 * The implementation is particularly tricky since existing code patterns
 * dictate neither migrate_disable() nor migrate_enable() is allowed to block.
 * This means that it cannot use cpus_read_lock() to serialize against hotplug,
 * nor can it easily migrate itself into a pending affinity mask change on
 * migrate_enable().
 *
 *
 * Note: even non-work-conserving schedulers like semi-partitioned depends on
 *       migration, so migrate_disable() is not only a problem for
 *       work-conserving schedulers.
 *
 */
extern void migrate_disable(void);
extern void migrate_enable(void);
# 424 "./include/linux/preempt.h"
/**
 * preempt_disable_nested - Disable preemption inside a normally preempt disabled section
 *
 * Use for code which requires preemption protection inside a critical
 * section which has preemption disabled implicitly on non-PREEMPT_RT
 * enabled kernels, by e.g.:
 *  - holding a spinlock/rwlock
 *  - soft interrupt context
 *  - regular interrupt handlers
 *
 * On PREEMPT_RT enabled kernels spinlock/rwlock held sections, soft
 * interrupt context and regular interrupt handlers are preemptible and
 * only prevent migration. preempt_disable_nested() ensures that preemption
 * is disabled for cases which require CPU local serialization even on
 * PREEMPT_RT. For non-PREEMPT_RT kernels this is a NOP.
 *
 * The use cases are code sequences which are not serialized by a
 * particular lock instance, e.g.:
 *  - seqcount write side critical sections where the seqcount is not
 *    associated to a particular lock and therefore the automatic
 *    protection mechanism does not work. This prevents a live lock
 *    against a preempting high priority reader.
 *  - RMW per CPU variable updates like vmstat.
 */
/* Macro to avoid header recursion hell vs. lockdep */
# 457 "./include/linux/preempt.h"
/**
 * preempt_enable_nested - Undo the effect of preempt_disable_nested()
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void preempt_enable_nested(void)
{
 if (0)
  do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}
# 57 "./include/linux/spinlock.h" 2


# 1 "./include/linux/irqflags.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * include/linux/irqflags.h
 *
 * IRQ flags tracing: follow the state of the hardirq and softirq flags and
 * provide callbacks for transitions between ON and OFF states.
 *
 * This file gets included from lowlevel asm headers too, to provide
 * wrapped versions of the local_irq_*() APIs, based on the
 * raw_local_irq_*() macros from the lowlevel headers.
 */




# 1 "./arch/arm64/include/asm/irqflags.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */





# 1 "./arch/arm64/include/asm/ptrace.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/ptrace.h
 *
 * Copyright (C) 1996-2003 Russell King
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./arch/arm64/include/asm/cpufeature.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2014 Linaro Ltd. <ard.biesheuvel@linaro.org>
 */






# 1 "./arch/arm64/include/asm/cputype.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 94 "./arch/arm64/include/asm/cputype.h"
/* OcteonTx2 series */
# 182 "./arch/arm64/include/asm/cputype.h"
/* Fujitsu Erratum 010001 affects A64FX 1.0 and 1.1, (v0r0 and v1r0) */






# 1 "./arch/arm64/include/asm/sysreg.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Macros for accessing system registers with older binutils.
 *
 * Copyright (C) 2014 ARM Ltd.
 * Author: Catalin Marinas <catalin.marinas@arm.com>
 */






# 1 "./include/linux/kasan-tags.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 15 "./arch/arm64/include/asm/sysreg.h" 2

# 1 "./arch/arm64/include/asm/gpr-num.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
# 17 "./arch/arm64/include/asm/sysreg.h" 2

/*
 * ARMv8 ARM reserves the following encoding for system registers:
 * (Ref: ARMv8 ARM, Section: "System instruction class encoding overview",
 *  C5.2, version:ARM DDI 0487A.f)
 *	[20-19] : Op0
 *	[18-16] : Op1
 *	[15-12] : CRn
 *	[11-8]  : CRm
 *	[7-5]   : Op2
 */
# 81 "./arch/arm64/include/asm/sysreg.h"
/*
 * Instructions for modifying PSTATE fields.
 * As per Arm ARM for v8-A, Section "C.5.1.3 op0 == 0b00, architectural hints,
 * barriers and CLREX, and PSTATE access", ARM DDI 0487 C.a, system instructions
 * for accessing PSTATE fields have the following encoding:
 *	Op0 = 0, CRn = 4
 *	Op1, Op2 encodes the PSTATE field modified and defines the constraints.
 *	CRm = Imm4 for the instruction.
 *	Rt = 0x1f
 */
# 121 "./arch/arm64/include/asm/sysreg.h"
/*
 * Automatically generated definitions for system registers, the
 * manual encodings below are in the process of being converted to
 * come from here. The header relies on the definition of sys_reg()
 * earlier in this file.
 */
# 1 "./arch/arm64/include/generated/asm/sysreg-defs.h" 1



/* Generated file - do not edit */
# 2919 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For CPACR_EL1 fields see CPACR_ELx */
# 2953 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For ZCR_EL1 fields see ZCR_ELx */
# 2976 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For SMCR_EL1 fields see SMCR_ELx */
# 3026 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For CONTEXTIDR_EL1 fields see CONTEXTIDR_ELx */
# 3309 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For ZCR_EL2 fields see ZCR_ELx */
# 3481 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For SMCR_EL2 fields see SMCR_ELx */
# 3598 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For CONTEXTIDR_EL2 fields see CONTEXTIDR_ELx */
# 3608 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For CPACR_EL12 fields see CPACR_ELx */
# 3618 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For ZCR_EL12 fields see ZCR_ELx */
# 3628 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For SMCR_EL12 fields see SMCR_ELx */
# 3654 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For CONTEXTIDR_EL12 fields see CONTEXTIDR_ELx */
# 3682 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For TTBR0_EL1 fields see TTBRx_EL1 */
# 3692 "./arch/arm64/include/generated/asm/sysreg-defs.h"
/* For TTBR1_EL1 fields see TTBRx_EL1 */
# 128 "./arch/arm64/include/asm/sysreg.h" 2

/*
 * System registers, organised loosely by encoding but grouped together
 * where the architected name contains an index. e.g. ID_MMFR<n>_EL1.
 */
# 218 "./arch/arm64/include/asm/sysreg.h"
/*** Statistical Profiling Extension ***/
/* ID registers */
# 240 "./arch/arm64/include/asm/sysreg.h"
/* Sampling controls */
# 264 "./arch/arm64/include/asm/sysreg.h"
/* Filtering controls */
# 285 "./arch/arm64/include/asm/sysreg.h"
/* Buffer controls */
# 294 "./arch/arm64/include/asm/sysreg.h"
/* Buffer error reporting */
# 315 "./arch/arm64/include/asm/sysreg.h"
/*** End of Statistical Profiling Extension ***/

/*
 * TRBE Registers
 */
# 433 "./arch/arm64/include/asm/sysreg.h"
/* Definitions for system register interface to AMU for ARMv8.4 onwards */
# 444 "./arch/arm64/include/asm/sysreg.h"
/*
 * Group 0 of activity monitors (architected):
 *                op0  op1  CRn   CRm       op2
 * Counter:       11   011  1101  010:n<3>  n<2:0>
 * Type:          11   011  1101  011:n<3>  n<2:0>
 * n: 0-15
 *
 * Group 1 of activity monitors (auxiliary):
 *                op0  op1  CRn   CRm       op2
 * Counter:       11   011  1101  110:n<3>  n<2:0>
 * Type:          11   011  1101  111:n<3>  n<2:0>
 * n: 0-15
 */






/* AMU v1: Fixed (architecturally defined) activity monitors */
# 551 "./arch/arm64/include/asm/sysreg.h"
/* VHE encodings for architectural EL0/1 system registers */
# 573 "./arch/arm64/include/asm/sysreg.h"
/* Common SCTLR_ELx flags. */
# 599 "./arch/arm64/include/asm/sysreg.h"
/* SCTLR_EL2 specific flags. */
# 618 "./arch/arm64/include/asm/sysreg.h"
/* SCTLR_EL1 specific flags. */
# 638 "./arch/arm64/include/asm/sysreg.h"
/* MAIR_ELx memory attributes (used by Linux) */







/* Position the attr at the correct index */


/* id_aa64pfr0 */



/* id_aa64mmfr0 */
# 700 "./arch/arm64/include/asm/sysreg.h"
/* GCR_EL1 Definitions */
# 719 "./arch/arm64/include/asm/sysreg.h"
/* RGSR_EL1 Definitions */




/* TFSR{,E0}_EL1 bit definitions */





/* Safe value for MPIDR_EL1: Bit31:RES1, Bit30:U:0, Bit24:MT:0 */
# 742 "./arch/arm64/include/asm/sysreg.h"
/* GIC Hypervisor interface registers */
/* ICH_MISR_EL2 bit definitions */



/* ICH_LR*_EL2 bit definitions */
# 761 "./arch/arm64/include/asm/sysreg.h"
/* ICH_HCR_EL2 bit definitions */
# 772 "./arch/arm64/include/asm/sysreg.h"
/* ICH_VMCR_EL2 bit definitions */
# 792 "./arch/arm64/include/asm/sysreg.h"
/* ICH_VTR_EL2 bit definitions */
# 804 "./arch/arm64/include/asm/sysreg.h"
/* HFG[WR]TR_EL2 bit definitions */







/* Create a mask for the feature bits of the specified feature. */
# 827 "./arch/arm64/include/asm/sysreg.h"
# 1 "./include/linux/bitfield.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2014 Felix Fietkau <nbd@nbd.name>
 * Copyright (C) 2004 - 2009 Ivo van Doorn <IvDoorn@gmail.com>
 */







/*
 * Bitfield access macros
 *
 * FIELD_{GET,PREP} macros take as first parameter shifted mask
 * from which they extract the base mask and shift amount.
 * Mask must be a compilation time constant.
 *
 * Example:
 *
 *  #include <linux/bitfield.h>
 *  #include <linux/bits.h>
 *
 *  #define REG_FIELD_A  GENMASK(6, 0)
 *  #define REG_FIELD_B  BIT(7)
 *  #define REG_FIELD_C  GENMASK(15, 8)
 *  #define REG_FIELD_D  GENMASK(31, 16)
 *
 * Get:
 *  a = FIELD_GET(REG_FIELD_A, reg);
 *  b = FIELD_GET(REG_FIELD_B, reg);
 *
 * Set:
 *  reg = FIELD_PREP(REG_FIELD_A, 1) |
 *	  FIELD_PREP(REG_FIELD_B, 0) |
 *	  FIELD_PREP(REG_FIELD_C, c) |
 *	  FIELD_PREP(REG_FIELD_D, 0x40);
 *
 * Modify:
 *  reg &= ~REG_FIELD_C;
 *  reg |= FIELD_PREP(REG_FIELD_C, c);
 */
# 78 "./include/linux/bitfield.h"
/**
 * FIELD_MAX() - produce the maximum value representable by a field
 * @_mask: shifted mask defining the field's length and position
 *
 * FIELD_MAX() returns the maximum value that can be held in the field
 * specified by @_mask.
 */






/**
 * FIELD_FIT() - check if value fits in the field
 * @_mask: shifted mask defining the field's length and position
 * @_val:  value to test against the field
 *
 * Return: true if @_val can fit inside @_mask, false if @_val is too big.
 */






/**
 * FIELD_PREP() - prepare a bitfield element
 * @_mask: shifted mask defining the field's length and position
 * @_val:  value to put in the field
 *
 * FIELD_PREP() masks and shifts up the value.  The result should
 * be combined with other fields of the bitfield using logical OR.
 */






/**
 * FIELD_GET() - extract a bitfield element
 * @_mask: shifted mask defining the field's length and position
 * @_reg:  value of entire bitfield
 *
 * FIELD_GET() extracts the field specified by @_mask from the
 * bitfield passed in as @_reg by masking and shifting it down.
 */






extern void __attribute__((__error__("value doesn't fit into mask")))
__field_overflow(void);
extern void __attribute__((__error__("bad bitfield mask")))
__bad_mask(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 field_multiplier(u64 field)
{
 if ((field | (field - 1)) & ((field | (field - 1)) + 1))
  __bad_mask();
 return field & -field;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 field_mask(u64 field)
{
 return field / field_multiplier(field);
}
# 172 "./include/linux/bitfield.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u8 u8_encode_bits(u8 v, u8 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return ((v & field_mask(field)) * field_multiplier(field)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u8 u8_replace_bits(__u8 old, u8 val, u8 field) { return (old & ~(field)) | u8_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void u8p_replace_bits(__u8 *p, u8 val, u8 field) { *p = (*p & ~(field)) | u8_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u8 u8_get_bits(__u8 v, u8 field) { return ((v) & field)/field_multiplier(field); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le16 le16_encode_bits(u16 v, u16 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return (( __le16)(__u16)((v & field_mask(field)) * field_multiplier(field))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le16 le16_replace_bits(__le16 old, u16 val, u16 field) { return (old & ~(( __le16)(__u16)(field))) | le16_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void le16p_replace_bits(__le16 *p, u16 val, u16 field) { *p = (*p & ~(( __le16)(__u16)(field))) | le16_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 le16_get_bits(__le16 v, u16 field) { return ((( __u16)(__le16)(v)) & field)/field_multiplier(field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be16 be16_encode_bits(u16 v, u16 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return (( __be16)(__u16)(__builtin_constant_p(((v & field_mask(field)) * field_multiplier(field))) ? ((__u16)( (((__u16)(((v & field_mask(field)) * field_multiplier(field))) & (__u16)0x00ffU) << 8) | (((__u16)(((v & field_mask(field)) * field_multiplier(field))) & (__u16)0xff00U) >> 8))) : __fswab16(((v & field_mask(field)) * field_multiplier(field))))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be16 be16_replace_bits(__be16 old, u16 val, u16 field) { return (old & ~(( __be16)(__u16)(__builtin_constant_p((field)) ? ((__u16)( (((__u16)((field)) & (__u16)0x00ffU) << 8) | (((__u16)((field)) & (__u16)0xff00U) >> 8))) : __fswab16((field))))) | be16_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void be16p_replace_bits(__be16 *p, u16 val, u16 field) { *p = (*p & ~(( __be16)(__u16)(__builtin_constant_p((field)) ? ((__u16)( (((__u16)((field)) & (__u16)0x00ffU) << 8) | (((__u16)((field)) & (__u16)0xff00U) >> 8))) : __fswab16((field))))) | be16_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 be16_get_bits(__be16 v, u16 field) { return ((__u16)(__builtin_constant_p(( __u16)(__be16)(v)) ? ((__u16)( (((__u16)(( __u16)(__be16)(v)) & (__u16)0x00ffU) << 8) | (((__u16)(( __u16)(__be16)(v)) & (__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(v))) & field)/field_multiplier(field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u16 u16_encode_bits(u16 v, u16 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return ((v & field_mask(field)) * field_multiplier(field)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u16 u16_replace_bits(__u16 old, u16 val, u16 field) { return (old & ~(field)) | u16_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void u16p_replace_bits(__u16 *p, u16 val, u16 field) { *p = (*p & ~(field)) | u16_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 u16_get_bits(__u16 v, u16 field) { return ((v) & field)/field_multiplier(field); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le32 le32_encode_bits(u32 v, u32 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return (( __le32)(__u32)((v & field_mask(field)) * field_multiplier(field))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le32 le32_replace_bits(__le32 old, u32 val, u32 field) { return (old & ~(( __le32)(__u32)(field))) | le32_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void le32p_replace_bits(__le32 *p, u32 val, u32 field) { *p = (*p & ~(( __le32)(__u32)(field))) | le32_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 le32_get_bits(__le32 v, u32 field) { return ((( __u32)(__le32)(v)) & field)/field_multiplier(field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be32 be32_encode_bits(u32 v, u32 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return (( __be32)(__u32)(__builtin_constant_p(((v & field_mask(field)) * field_multiplier(field))) ? ((__u32)( (((__u32)(((v & field_mask(field)) * field_multiplier(field))) & (__u32)0x000000ffUL) << 24) | (((__u32)(((v & field_mask(field)) * field_multiplier(field))) & (__u32)0x0000ff00UL) << 8) | (((__u32)(((v & field_mask(field)) * field_multiplier(field))) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(((v & field_mask(field)) * field_multiplier(field))) & (__u32)0xff000000UL) >> 24))) : __fswab32(((v & field_mask(field)) * field_multiplier(field))))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be32 be32_replace_bits(__be32 old, u32 val, u32 field) { return (old & ~(( __be32)(__u32)(__builtin_constant_p((field)) ? ((__u32)( (((__u32)((field)) & (__u32)0x000000ffUL) << 24) | (((__u32)((field)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((field)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((field)) & (__u32)0xff000000UL) >> 24))) : __fswab32((field))))) | be32_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void be32p_replace_bits(__be32 *p, u32 val, u32 field) { *p = (*p & ~(( __be32)(__u32)(__builtin_constant_p((field)) ? ((__u32)( (((__u32)((field)) & (__u32)0x000000ffUL) << 24) | (((__u32)((field)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((field)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((field)) & (__u32)0xff000000UL) >> 24))) : __fswab32((field))))) | be32_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 be32_get_bits(__be32 v, u32 field) { return ((__u32)(__builtin_constant_p(( __u32)(__be32)(v)) ? ((__u32)( (((__u32)(( __u32)(__be32)(v)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(__be32)(v)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(__be32)(v)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(__be32)(v)) & (__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(v))) & field)/field_multiplier(field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u32 u32_encode_bits(u32 v, u32 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return ((v & field_mask(field)) * field_multiplier(field)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u32 u32_replace_bits(__u32 old, u32 val, u32 field) { return (old & ~(field)) | u32_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void u32p_replace_bits(__u32 *p, u32 val, u32 field) { *p = (*p & ~(field)) | u32_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 u32_get_bits(__u32 v, u32 field) { return ((v) & field)/field_multiplier(field); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le64 le64_encode_bits(u64 v, u64 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return (( __le64)(__u64)((v & field_mask(field)) * field_multiplier(field))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __le64 le64_replace_bits(__le64 old, u64 val, u64 field) { return (old & ~(( __le64)(__u64)(field))) | le64_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void le64p_replace_bits(__le64 *p, u64 val, u64 field) { *p = (*p & ~(( __le64)(__u64)(field))) | le64_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 le64_get_bits(__le64 v, u64 field) { return ((( __u64)(__le64)(v)) & field)/field_multiplier(field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be64 be64_encode_bits(u64 v, u64 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return (( __be64)(__u64)(__builtin_constant_p(((v & field_mask(field)) * field_multiplier(field))) ? ((__u64)( (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(((v & field_mask(field)) * field_multiplier(field))) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(((v & field_mask(field)) * field_multiplier(field))))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __be64 be64_replace_bits(__be64 old, u64 val, u64 field) { return (old & ~(( __be64)(__u64)(__builtin_constant_p((field)) ? ((__u64)( (((__u64)((field)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)((field)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)((field)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)((field)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)((field)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)((field)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)((field)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)((field)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64((field))))) | be64_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void be64p_replace_bits(__be64 *p, u64 val, u64 field) { *p = (*p & ~(( __be64)(__u64)(__builtin_constant_p((field)) ? ((__u64)( (((__u64)((field)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)((field)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)((field)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)((field)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)((field)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)((field)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)((field)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)((field)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64((field))))) | be64_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 be64_get_bits(__be64 v, u64 field) { return ((__u64)(__builtin_constant_p(( __u64)(__be64)(v)) ? ((__u64)( (((__u64)(( __u64)(__be64)(v)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(__be64)(v)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(__be64)(v)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(__be64)(v)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(__be64)(v)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(__be64)(v)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(__be64)(v)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(__be64)(v)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(v))) & field)/field_multiplier(field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u64 u64_encode_bits(u64 v, u64 field) { if (__builtin_constant_p(v) && (v & ~field_mask(field))) __field_overflow(); return ((v & field_mask(field)) * field_multiplier(field)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __u64 u64_replace_bits(__u64 old, u64 val, u64 field) { return (old & ~(field)) | u64_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void u64p_replace_bits(__u64 *p, u64 val, u64 field) { *p = (*p & ~(field)) | u64_encode_bits(val, field); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 u64_get_bits(__u64 v, u64 field) { return ((v) & field)/field_multiplier(field); }
# 828 "./arch/arm64/include/asm/sysreg.h" 2
# 860 "./arch/arm64/include/asm/sysreg.h"
/*
 * Unlike read_cpuid, calls to read_sysreg are never expected to be
 * optimized away or replaced with synthetic values.
 */






/*
 * The "Z" constraint normally means a zero immediate, but when combined with
 * the "%x0" template means XZR.
 */






/*
 * For registers without architectural names, or simply unsupported by
 * GAS.
 */
# 895 "./arch/arm64/include/asm/sysreg.h"
/*
 * Modify bits in a sysreg. Bits in the clear mask are zeroed, then bits in the
 * set mask are set. Other bits are left as-is.
 */
# 190 "./arch/arm64/include/asm/cputype.h" 2



/*
 * Represent a range of MIDR values for a given CPU model and a
 * range of variant/revision values.
 *
 * @model	- CPU model as defined by MIDR_CPU_MODEL
 * @rv_min	- Minimum value for the revision/variant as defined by
 *		  MIDR_CPU_VAR_REV
 * @rv_max	- Maximum value for the variant/revision for the range.
 */
struct midr_range {
 u32 model;
 u32 rv_min;
 u32 rv_max;
};
# 219 "./arch/arm64/include/asm/cputype.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool midr_is_cpu_model_range(u32 midr, u32 model, u32 rv_min,
        u32 rv_max)
{
 u32 _model = midr & ((0xffU << 24) | (0xfff << 4) | (0xf << 16));
 u32 rv = midr & (0xf | (0xf << 20));

 return _model == model && rv >= rv_min && rv <= rv_max;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_midr_in_range(u32 midr, struct midr_range const *range)
{
 return midr_is_cpu_model_range(midr, range->model,
           range->rv_min, range->rv_max);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
is_midr_in_range_list(u32 midr, struct midr_range const *ranges)
{
 while (ranges->model)
  if (is_midr_in_range(midr, ranges++))
   return true;
 return false;
}

/*
 * The CPU ID never changes at run time, so we might as well tell the
 * compiler that it's constant.  Use this function to read the CPU ID
 * rather than directly reading processor_id or read_cpuid() directly.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __attribute__((__const__)) read_cpuid_id(void)
{
 return ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((0) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __attribute__((__const__)) read_cpuid_mpidr(void)
{
 return ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((0) << 8) | ((5) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __attribute__((__const__)) read_cpuid_implementor(void)
{
 return (((read_cpuid_id()) & (0xffU << 24)) >> 24);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __attribute__((__const__)) read_cpuid_part_number(void)
{
 return (((read_cpuid_id()) & (0xfff << 4)) >> 4);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __attribute__((__const__)) read_cpuid_cachetype(void)
{
 return ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((0) << 12) | ((0) << 8) | ((1) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
}
# 12 "./arch/arm64/include/asm/cpufeature.h" 2
# 1 "./arch/arm64/include/asm/hwcap.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./arch/arm64/include/uapi/asm/hwcap.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */



/*
 * HWCAP flags - for AT_HWCAP
 *
 * Bits 62 and 63 are reserved for use by libc.
 * Bits 32-61 are unallocated for potential use by libc.
 */
# 59 "./arch/arm64/include/uapi/asm/hwcap.h"
/*
 * HWCAP2 flags - for AT_HWCAP2
 */
# 9 "./arch/arm64/include/asm/hwcap.h" 2
# 1 "./arch/arm64/include/asm/cpufeature.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2014 Linaro Ltd. <ard.biesheuvel@linaro.org>
 */
# 10 "./arch/arm64/include/asm/hwcap.h" 2
# 42 "./arch/arm64/include/asm/hwcap.h"
# 1 "./include/linux/log2.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/* Integer base 2 logarithm calculation
 *
 * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 */







/*
 * non-constant log of base 2 calculators
 * - the arch may override these in asm/bitops.h if they can be implemented
 *   more efficiently than using fls() and fls64()
 * - the arch is not required to handle n==0 if implementing the fallback
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((const))
int __ilog2_u32(u32 n)
{
 return fls(n) - 1;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((const))
int __ilog2_u64(u64 n)
{
 return fls64(n) - 1;
}


/**
 * is_power_of_2() - check if a value is a power of two
 * @n: the value to check
 *
 * Determine whether some value is a power of two, where zero is
 * *not* considered a power of two.
 * Return: true if @n is a power of 2, otherwise false.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((const))
bool is_power_of_2(unsigned long n)
{
 return (n != 0 && ((n & (n - 1)) == 0));
}

/**
 * __roundup_pow_of_two() - round up to nearest power of two
 * @n: value to round up
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((const))
unsigned long __roundup_pow_of_two(unsigned long n)
{
 return 1UL << fls_long(n - 1);
}

/**
 * __rounddown_pow_of_two() - round down to nearest power of two
 * @n: value to round down
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((const))
unsigned long __rounddown_pow_of_two(unsigned long n)
{
 return 1UL << (fls_long(n) - 1);
}

/**
 * const_ilog2 - log base 2 of 32-bit or a 64-bit constant unsigned value
 * @n: parameter
 *
 * Use this where sparse expects a true constant expression, e.g. for array
 * indices.
 */
# 146 "./include/linux/log2.h"
/**
 * ilog2 - log base 2 of 32-bit or a 64-bit unsigned value
 * @n: parameter
 *
 * constant-capable log of base 2 calculation
 * - this can be used to initialise global variables from constant data, hence
 * the massive ternary operator construction
 *
 * selects the appropriately-sized optimised version depending on sizeof(n)
 */
# 166 "./include/linux/log2.h"
/**
 * roundup_pow_of_two - round the given value up to nearest power of two
 * @n: parameter
 *
 * round the given value up to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */
# 183 "./include/linux/log2.h"
/**
 * rounddown_pow_of_two - round the given value down to nearest power of two
 * @n: parameter
 *
 * round the given value down to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__const__))
int __order_base_2(unsigned long n)
{
 return n > 1 ? ( __builtin_constant_p(n - 1) ? ((n - 1) < 2 ? 0 : 63 - __builtin_clzll(n - 1)) : (sizeof(n - 1) <= 4) ? __ilog2_u32(n - 1) : __ilog2_u64(n - 1) ) + 1 : 0;
}

/**
 * order_base_2 - calculate the (rounded up) base 2 order of the argument
 * @n: parameter
 *
 * The first few values calculated by this routine:
 *  ob2(0) = 0
 *  ob2(1) = 0
 *  ob2(2) = 1
 *  ob2(3) = 2
 *  ob2(4) = 2
 *  ob2(5) = 3
 *  ... and so on.
 */
# 225 "./include/linux/log2.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((const))
int __bits_per(unsigned long n)
{
 if (n < 2)
  return 1;
 if (is_power_of_2(n))
  return ( __builtin_constant_p(n) ? ( ((n) == 0 || (n) == 1) ? 0 : ( __builtin_constant_p((n) - 1) ? (((n) - 1) < 2 ? 0 : 63 - __builtin_clzll((n) - 1)) : (sizeof((n) - 1) <= 4) ? __ilog2_u32((n) - 1) : __ilog2_u64((n) - 1) ) + 1) : __order_base_2(n) ) + 1;
 return ( __builtin_constant_p(n) ? ( ((n) == 0 || (n) == 1) ? 0 : ( __builtin_constant_p((n) - 1) ? (((n) - 1) < 2 ? 0 : 63 - __builtin_clzll((n) - 1)) : (sizeof((n) - 1) <= 4) ? __ilog2_u32((n) - 1) : __ilog2_u64((n) - 1) ) + 1) : __order_base_2(n) );
}

/**
 * bits_per - calculate the number of bits required for the argument
 * @n: parameter
 *
 * This is constant-capable and can be used for compile time
 * initializations, e.g bitfields.
 *
 * The first few values calculated by this routine:
 * bf(0) = 1
 * bf(1) = 1
 * bf(2) = 2
 * bf(3) = 2
 * bf(4) = 3
 * ... and so on.
 */
# 43 "./arch/arm64/include/asm/hwcap.h" 2

/*
 * For userspace we represent hwcaps as a collection of HWCAP{,2}_x bitfields
 * as described in uapi/asm/hwcap.h. For the kernel we represent hwcaps as
 * natural numbers (in a single range of size MAX_CPU_FEATURES) defined here
 * with prefix KERNEL_HWCAP_ mapped to their HWCAP{,2}_x counterpart.
 *
 * Hwcaps should be set and tested within the kernel via the
 * cpu_{set,have}_named_feature(feature) where feature is the unique suffix
 * of KERNEL_HWCAP_{feature}.
 */
# 127 "./arch/arm64/include/asm/hwcap.h"
/*
 * This yields a mask that user programs can use to figure out what
 * instruction set this cpu supports.
 */






extern unsigned int compat_elf_hwcap, compat_elf_hwcap2;


enum {
 CAP_HWCAP = 1,

 CAP_COMPAT_HWCAP,
 CAP_COMPAT_HWCAP2,

};
# 13 "./arch/arm64/include/asm/cpufeature.h" 2
# 22 "./arch/arm64/include/asm/cpufeature.h"
# 1 "./include/linux/kernel.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * NOTE:
 *
 * This header has combined a lot of unrelated to each other stuff.
 * The process of splitting its content is in progress while keeping
 * backward compatibility. That's why it's highly recommended NOT to
 * include this header inside another header file, especially under
 * generic or architectural include/ directory.
 */




# 1 "./include/linux/align.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/* @a is a power of 2 value */
# 16 "./include/linux/kernel.h" 2







# 1 "./include/linux/kstrtox.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/* Internal, do not use. */
int __attribute__((__warn_unused_result__)) _kstrtoul(const char *s, unsigned int base, unsigned long *res);
int __attribute__((__warn_unused_result__)) _kstrtol(const char *s, unsigned int base, long *res);

int __attribute__((__warn_unused_result__)) kstrtoull(const char *s, unsigned int base, unsigned long long *res);
int __attribute__((__warn_unused_result__)) kstrtoll(const char *s, unsigned int base, long long *res);

/**
 * kstrtoul - convert a string to an unsigned long
 * @s: The start of the string. The string must be null-terminated, and may also
 *  include a single newline before its terminating null. The first character
 *  may also be a plus sign, but not a minus sign.
 * @base: The number base to use. The maximum supported base is 16. If base is
 *  given as 0, then the base of the string is automatically detected with the
 *  conventional semantics - If it begins with 0x the number will be parsed as a
 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be
 *  parsed as an octal number. Otherwise it will be parsed as a decimal.
 * @res: Where to write the result of the conversion on success.
 *
 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
 * Preferred over simple_strtoul(). Return code must be checked.
*/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtoul(const char *s, unsigned int base, unsigned long *res)
{
 /*
	 * We want to shortcut function call, but
	 * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.
	 */
 if (sizeof(unsigned long) == sizeof(unsigned long long) &&
     __alignof__(unsigned long) == __alignof__(unsigned long long))
  return kstrtoull(s, base, (unsigned long long *)res);
 else
  return _kstrtoul(s, base, res);
}

/**
 * kstrtol - convert a string to a long
 * @s: The start of the string. The string must be null-terminated, and may also
 *  include a single newline before its terminating null. The first character
 *  may also be a plus sign or a minus sign.
 * @base: The number base to use. The maximum supported base is 16. If base is
 *  given as 0, then the base of the string is automatically detected with the
 *  conventional semantics - If it begins with 0x the number will be parsed as a
 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be
 *  parsed as an octal number. Otherwise it will be parsed as a decimal.
 * @res: Where to write the result of the conversion on success.
 *
 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
 * Preferred over simple_strtol(). Return code must be checked.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtol(const char *s, unsigned int base, long *res)
{
 /*
	 * We want to shortcut function call, but
	 * __builtin_types_compatible_p(long, long long) = 0.
	 */
 if (sizeof(long) == sizeof(long long) &&
     __alignof__(long) == __alignof__(long long))
  return kstrtoll(s, base, (long long *)res);
 else
  return _kstrtol(s, base, res);
}

int __attribute__((__warn_unused_result__)) kstrtouint(const char *s, unsigned int base, unsigned int *res);
int __attribute__((__warn_unused_result__)) kstrtoint(const char *s, unsigned int base, int *res);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtou64(const char *s, unsigned int base, u64 *res)
{
 return kstrtoull(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtos64(const char *s, unsigned int base, s64 *res)
{
 return kstrtoll(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtou32(const char *s, unsigned int base, u32 *res)
{
 return kstrtouint(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtos32(const char *s, unsigned int base, s32 *res)
{
 return kstrtoint(s, base, res);
}

int __attribute__((__warn_unused_result__)) kstrtou16(const char *s, unsigned int base, u16 *res);
int __attribute__((__warn_unused_result__)) kstrtos16(const char *s, unsigned int base, s16 *res);
int __attribute__((__warn_unused_result__)) kstrtou8(const char *s, unsigned int base, u8 *res);
int __attribute__((__warn_unused_result__)) kstrtos8(const char *s, unsigned int base, s8 *res);
int __attribute__((__warn_unused_result__)) kstrtobool(const char *s, bool *res);

int __attribute__((__warn_unused_result__)) kstrtoull_from_user(const char /* nothing */ *s, size_t count, unsigned int base, unsigned long long *res);
int __attribute__((__warn_unused_result__)) kstrtoll_from_user(const char /* nothing */ *s, size_t count, unsigned int base, long long *res);
int __attribute__((__warn_unused_result__)) kstrtoul_from_user(const char /* nothing */ *s, size_t count, unsigned int base, unsigned long *res);
int __attribute__((__warn_unused_result__)) kstrtol_from_user(const char /* nothing */ *s, size_t count, unsigned int base, long *res);
int __attribute__((__warn_unused_result__)) kstrtouint_from_user(const char /* nothing */ *s, size_t count, unsigned int base, unsigned int *res);
int __attribute__((__warn_unused_result__)) kstrtoint_from_user(const char /* nothing */ *s, size_t count, unsigned int base, int *res);
int __attribute__((__warn_unused_result__)) kstrtou16_from_user(const char /* nothing */ *s, size_t count, unsigned int base, u16 *res);
int __attribute__((__warn_unused_result__)) kstrtos16_from_user(const char /* nothing */ *s, size_t count, unsigned int base, s16 *res);
int __attribute__((__warn_unused_result__)) kstrtou8_from_user(const char /* nothing */ *s, size_t count, unsigned int base, u8 *res);
int __attribute__((__warn_unused_result__)) kstrtos8_from_user(const char /* nothing */ *s, size_t count, unsigned int base, s8 *res);
int __attribute__((__warn_unused_result__)) kstrtobool_from_user(const char /* nothing */ *s, size_t count, bool *res);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtou64_from_user(const char /* nothing */ *s, size_t count, unsigned int base, u64 *res)
{
 return kstrtoull_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtos64_from_user(const char /* nothing */ *s, size_t count, unsigned int base, s64 *res)
{
 return kstrtoll_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtou32_from_user(const char /* nothing */ *s, size_t count, unsigned int base, u32 *res)
{
 return kstrtouint_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kstrtos32_from_user(const char /* nothing */ *s, size_t count, unsigned int base, s32 *res)
{
 return kstrtoint_from_user(s, count, base, res);
}

/*
 * Use kstrto<foo> instead.
 *
 * NOTE: simple_strto<foo> does not check for the range overflow and,
 *	 depending on the input, may give interesting results.
 *
 * Use these functions if and only if you cannot use kstrto<foo>, because
 * the conversion ends on the first non-digit character, which may be far
 * beyond the supported range. It might be useful to parse the strings like
 * 10x50 or 12:21 without altering original string or temporary buffer in use.
 * Keep in mind above caveat.
 */

extern unsigned long simple_strtoul(const char *,char **,unsigned int);
extern long simple_strtol(const char *,char **,unsigned int);
extern unsigned long long simple_strtoull(const char *,char **,unsigned int);
extern long long simple_strtoll(const char *,char **,unsigned int);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int strtobool(const char *s, bool *res)
{
 return kstrtobool(s, res);
}
# 24 "./include/linux/kernel.h" 2


# 1 "./include/linux/minmax.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * min()/max()/clamp() macros must accomplish three things:
 *
 * - avoid multiple evaluations of the arguments (so side-effects like
 *   "x++" happen only once) when non-constant.
 * - perform strict type-checking (to generate warnings instead of
 *   nasty runtime surprises). See the "unnecessary" pointer comparison
 *   in __typecheck().
 * - retain result as a constant expressions when called with only
 *   constant expressions (to avoid tripping VLA warnings in stack
 *   allocation usage).
 */
# 62 "./include/linux/minmax.h"
/**
 * min - return minimum of two values of the same or compatible types
 * @x: first value
 * @y: second value
 */


/**
 * max - return maximum of two values of the same or compatible types
 * @x: first value
 * @y: second value
 */


/**
 * min3 - return minimum of three values
 * @x: first value
 * @y: second value
 * @z: third value
 */


/**
 * max3 - return maximum of three values
 * @x: first value
 * @y: second value
 * @z: third value
 */


/**
 * min_not_zero - return the minimum that is _not_ zero, unless both are zero
 * @x: value1
 * @y: value2
 */





/**
 * clamp - return a value clamped to a given range with strict typechecking
 * @val: current value
 * @lo: lowest allowable value
 * @hi: highest allowable value
 *
 * This macro does strict typechecking of @lo/@hi to make sure they are of the
 * same type as @val.  See the unnecessary pointer comparisons.
 */


/*
 * ..and if you can't take the strict
 * types, you can specify one yourself.
 *
 * Or not use min/max/clamp at all, of course.
 */

/**
 * min_t - return minimum of two values, using the specified type
 * @type: data type to use
 * @x: first value
 * @y: second value
 */


/**
 * max_t - return maximum of two values, using the specified type
 * @type: data type to use
 * @x: first value
 * @y: second value
 */


/**
 * clamp_t - return a value clamped to a given range using a given type
 * @type: the type of variable to use
 * @val: current value
 * @lo: minimum allowable value
 * @hi: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of type
 * @type to make all the comparisons.
 */


/**
 * clamp_val - return a value clamped to a given range using val's type
 * @val: current value
 * @lo: minimum allowable value
 * @hi: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of whatever
 * type the input argument @val is.  This is useful when @val is an unsigned
 * type and @lo and @hi are literals that will otherwise be assigned a signed
 * integer type.
 */


/**
 * swap - swap values of @a and @b
 * @a: first value
 * @b: second value
 */
# 27 "./include/linux/kernel.h" 2




# 1 "./include/linux/static_call_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 21 "./include/linux/static_call_types.h"
/*
 * Flags in the low bits of static_call_site::key.
 */




/*
 * The static call site table needs to be created by external tooling (objtool
 * or a compiler plugin).
 */
struct static_call_site {
 s32 addr;
 s32 key;
};
# 94 "./include/linux/static_call_types.h"
struct static_call_key {
 void *func;
};
# 32 "./include/linux/kernel.h" 2
# 1 "./include/linux/instruction_pointer.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 33 "./include/linux/kernel.h" 2






/**
 * REPEAT_BYTE - repeat the value @x multiple times as an unsigned long value
 * @x: value to repeat
 *
 * NOTE: @x is not checked for > 0xff; larger values produce odd results.
 */


/* generic data direction definitions */



/**
 * ARRAY_SIZE - get the number of elements in array @arr
 * @arr: array to be sized
 */
# 66 "./include/linux/kernel.h"
/**
 * upper_32_bits - return bits 32-63 of a number
 * @n: the number we're accessing
 *
 * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress
 * the "right shift count >= width of type" warning when that quantity is
 * 32-bits.
 */


/**
 * lower_32_bits - return bits 0-31 of a number
 * @n: the number we're accessing
 */


/**
 * upper_16_bits - return bits 16-31 of a number
 * @n: the number we're accessing
 */


/**
 * lower_16_bits - return bits 0-15 of a number
 * @n: the number we're accessing
 */


struct completion;
struct user;
# 182 "./include/linux/kernel.h"
  static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __might_resched(const char *file, int line,
         unsigned int offsets) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __might_sleep(const char *file, int line) { }
# 200 "./include/linux/kernel.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void might_fault(void) { }


void do_exit(long error_code) __attribute__((__noreturn__));

extern int num_to_str(char *buf, int size,
        unsigned long long num, unsigned int width);

/* lib/printf utilities */

extern __attribute__((__format__(printf, 2, 3))) int sprintf(char *buf, const char * fmt, ...);
extern __attribute__((__format__(printf, 2, 0))) int vsprintf(char *buf, const char *, va_list);
extern __attribute__((__format__(printf, 3, 4)))
int snprintf(char *buf, size_t size, const char *fmt, ...);
extern __attribute__((__format__(printf, 3, 0)))
int vsnprintf(char *buf, size_t size, const char *fmt, va_list args);
extern __attribute__((__format__(printf, 3, 4)))
int scnprintf(char *buf, size_t size, const char *fmt, ...);
extern __attribute__((__format__(printf, 3, 0)))
int vscnprintf(char *buf, size_t size, const char *fmt, va_list args);
extern __attribute__((__format__(printf, 2, 3))) __attribute__((__malloc__))
char *kasprintf(gfp_t gfp, const char *fmt, ...);
extern __attribute__((__format__(printf, 2, 0))) __attribute__((__malloc__))
char *kvasprintf(gfp_t gfp, const char *fmt, va_list args);
extern __attribute__((__format__(printf, 2, 0)))
const char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);

extern __attribute__((__format__(scanf, 2, 3)))
int sscanf(const char *, const char *, ...);
extern __attribute__((__format__(scanf, 2, 0)))
int vsscanf(const char *, const char *, va_list);

extern int no_hash_pointers_enable(char *str);

extern int get_option(char **str, int *pint);
extern char *get_options(const char *str, int nints, int *ints);
extern unsigned long long memparse(const char *ptr, char **retptr);
extern bool parse_option_str(const char *str, const char *option);
extern char *next_arg(char *args, char **param, char **val);

extern int core_kernel_text(unsigned long addr);
extern int __kernel_text_address(unsigned long addr);
extern int kernel_text_address(unsigned long addr);
extern int func_ptr_is_kernel_text(void *ptr);

extern void bust_spinlocks(int yes);

extern int root_mountflags;

extern bool early_boot_irqs_disabled;

/*
 * Values used for system_state. Ordering of the states must not be changed
 * as code checks for <, <=, >, >= STATE.
 */
extern enum system_states {
 SYSTEM_BOOTING,
 SYSTEM_SCHEDULING,
 SYSTEM_FREEING_INITMEM,
 SYSTEM_RUNNING,
 SYSTEM_HALT,
 SYSTEM_POWER_OFF,
 SYSTEM_RESTART,
 SYSTEM_SUSPEND,
} system_state;

extern const char hex_asc[];



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) char *hex_byte_pack(char *buf, u8 byte)
{
 *buf++ = hex_asc[((byte) & 0xf0) >> 4];
 *buf++ = hex_asc[((byte) & 0x0f)];
 return buf;
}

extern const char hex_asc_upper[];



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) char *hex_byte_pack_upper(char *buf, u8 byte)
{
 *buf++ = hex_asc_upper[((byte) & 0xf0) >> 4];
 *buf++ = hex_asc_upper[((byte) & 0x0f)];
 return buf;
}

extern int hex_to_bin(unsigned char ch);
extern int __attribute__((__warn_unused_result__)) hex2bin(u8 *dst, const char *src, size_t count);
extern char *bin2hex(char *dst, const void *src, size_t count);

bool mac_pton(const char *s, u8 *mac);

/*
 * General tracing related utility functions - trace_printk(),
 * tracing_on/tracing_off and tracing_start()/tracing_stop
 *
 * Use tracing_on/tracing_off when you want to quickly turn on or off
 * tracing. It simply enables or disables the recording of the trace events.
 * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on
 * file, which gives a means for the kernel and userspace to interact.
 * Place a tracing_off() in the kernel where you want tracing to end.
 * From user space, examine the trace, and then echo 1 > tracing_on
 * to continue tracing.
 *
 * tracing_stop/tracing_start has slightly more overhead. It is used
 * by things like suspend to ram where disabling the recording of the
 * trace is not enough, but tracing must actually stop because things
 * like calling smp_processor_id() may crash the system.
 *
 * Most likely, you want to use tracing_on/tracing_off.
 */

enum ftrace_dump_mode {
 DUMP_NONE,
 DUMP_ALL,
 DUMP_ORIG,
};
# 464 "./include/linux/kernel.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tracing_start(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tracing_stop(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void trace_dump_stack(int skip) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tracing_on(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tracing_off(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int tracing_is_on(void) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tracing_snapshot(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tracing_snapshot_alloc(void) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__format__(printf, 1, 2)))
int trace_printk(const char *fmt, ...)
{
 return 0;
}
static __attribute__((__format__(printf, 1, 0))) inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
ftrace_vprintk(const char *fmt, va_list ap)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_dump(enum ftrace_dump_mode oops_dump_mode) { }


/* This counts to 12. Any more, it will return 13th argument. */






/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */




/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */
# 23 "./arch/arm64/include/asm/cpufeature.h" 2

/*
 * CPU feature register tracking
 *
 * The safe value of a CPUID feature field is dependent on the implications
 * of the values assigned to it by the architecture. Based on the relationship
 * between the values, the features are classified into 3 types - LOWER_SAFE,
 * HIGHER_SAFE and EXACT.
 *
 * The lowest value of all the CPUs is chosen for LOWER_SAFE and highest
 * for HIGHER_SAFE. It is expected that all CPUs have the same value for
 * a field when EXACT is specified, failing which, the safe value specified
 * in the table is chosen.
 */

enum ftr_type {
 FTR_EXACT, /* Use a predefined safe value */
 FTR_LOWER_SAFE, /* Smaller value is safe */
 FTR_HIGHER_SAFE, /* Bigger value is safe */
 FTR_HIGHER_OR_ZERO_SAFE, /* Bigger value is safe, but 0 is biggest */
};
# 57 "./arch/arm64/include/asm/cpufeature.h"
struct arm64_ftr_bits {
 bool sign; /* Value is signed ? */
 bool visible;
 bool strict; /* CPU Sanity check: strict matching required ? */
 enum ftr_type type;
 u8 shift;
 u8 width;
 s64 safe_val; /* safe value for FTR_EXACT features */
};

/*
 * Describe the early feature override to the core override code:
 *
 * @val			Values that are to be merged into the final
 *			sanitised value of the register. Only the bitfields
 *			set to 1 in @mask are valid
 * @mask		Mask of the features that are overridden by @val
 *
 * A @mask field set to full-1 indicates that the corresponding field
 * in @val is a valid override.
 *
 * A @mask field set to full-0 with the corresponding @val field set
 * to full-0 denotes that this field has no override
 *
 * A @mask field set to full-0 with the corresponding @val field set
 * to full-1 denotes thath this field has an invalid override.
 */
struct arm64_ftr_override {
 u64 val;
 u64 mask;
};

/*
 * @arm64_ftr_reg - Feature register
 * @strict_mask		Bits which should match across all CPUs for sanity.
 * @sys_val		Safe value across the CPUs (system view)
 */
struct arm64_ftr_reg {
 const char *name;
 u64 strict_mask;
 u64 user_mask;
 u64 sys_val;
 u64 user_val;
 struct arm64_ftr_override *override;
 const struct arm64_ftr_bits *ftr_bits;
};

extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;

/*
 * CPU capabilities:
 *
 * We use arm64_cpu_capabilities to represent system features, errata work
 * arounds (both used internally by kernel and tracked in cpu_hwcaps) and
 * ELF HWCAPs (which are exposed to user).
 *
 * To support systems with heterogeneous CPUs, we need to make sure that we
 * detect the capabilities correctly on the system and take appropriate
 * measures to ensure there are no incompatibilities.
 *
 * This comment tries to explain how we treat the capabilities.
 * Each capability has the following list of attributes :
 *
 * 1) Scope of Detection : The system detects a given capability by
 *    performing some checks at runtime. This could be, e.g, checking the
 *    value of a field in CPU ID feature register or checking the cpu
 *    model. The capability provides a call back ( @matches() ) to
 *    perform the check. Scope defines how the checks should be performed.
 *    There are three cases:
 *
 *     a) SCOPE_LOCAL_CPU: check all the CPUs and "detect" if at least one
 *        matches. This implies, we have to run the check on all the
 *        booting CPUs, until the system decides that state of the
 *        capability is finalised. (See section 2 below)
 *		Or
 *     b) SCOPE_SYSTEM: check all the CPUs and "detect" if all the CPUs
 *        matches. This implies, we run the check only once, when the
 *        system decides to finalise the state of the capability. If the
 *        capability relies on a field in one of the CPU ID feature
 *        registers, we use the sanitised value of the register from the
 *        CPU feature infrastructure to make the decision.
 *		Or
 *     c) SCOPE_BOOT_CPU: Check only on the primary boot CPU to detect the
 *        feature. This category is for features that are "finalised"
 *        (or used) by the kernel very early even before the SMP cpus
 *        are brought up.
 *
 *    The process of detection is usually denoted by "update" capability
 *    state in the code.
 *
 * 2) Finalise the state : The kernel should finalise the state of a
 *    capability at some point during its execution and take necessary
 *    actions if any. Usually, this is done, after all the boot-time
 *    enabled CPUs are brought up by the kernel, so that it can make
 *    better decision based on the available set of CPUs. However, there
 *    are some special cases, where the action is taken during the early
 *    boot by the primary boot CPU. (e.g, running the kernel at EL2 with
 *    Virtualisation Host Extensions). The kernel usually disallows any
 *    changes to the state of a capability once it finalises the capability
 *    and takes any action, as it may be impossible to execute the actions
 *    safely. A CPU brought up after a capability is "finalised" is
 *    referred to as "Late CPU" w.r.t the capability. e.g, all secondary
 *    CPUs are treated "late CPUs" for capabilities determined by the boot
 *    CPU.
 *
 *    At the moment there are two passes of finalising the capabilities.
 *      a) Boot CPU scope capabilities - Finalised by primary boot CPU via
 *         setup_boot_cpu_capabilities().
 *      b) Everything except (a) - Run via setup_system_capabilities().
 *
 * 3) Verification: When a CPU is brought online (e.g, by user or by the
 *    kernel), the kernel should make sure that it is safe to use the CPU,
 *    by verifying that the CPU is compliant with the state of the
 *    capabilities finalised already. This happens via :
 *
 *	secondary_start_kernel()-> check_local_cpu_capabilities()
 *
 *    As explained in (2) above, capabilities could be finalised at
 *    different points in the execution. Each newly booted CPU is verified
 *    against the capabilities that have been finalised by the time it
 *    boots.
 *
 *	a) SCOPE_BOOT_CPU : All CPUs are verified against the capability
 *	except for the primary boot CPU.
 *
 *	b) SCOPE_LOCAL_CPU, SCOPE_SYSTEM: All CPUs hotplugged on by the
 *	user after the kernel boot are verified against the capability.
 *
 *    If there is a conflict, the kernel takes an action, based on the
 *    severity (e.g, a CPU could be prevented from booting or cause a
 *    kernel panic). The CPU is allowed to "affect" the state of the
 *    capability, if it has not been finalised already. See section 5
 *    for more details on conflicts.
 *
 * 4) Action: As mentioned in (2), the kernel can take an action for each
 *    detected capability, on all CPUs on the system. Appropriate actions
 *    include, turning on an architectural feature, modifying the control
 *    registers (e.g, SCTLR, TCR etc.) or patching the kernel via
 *    alternatives. The kernel patching is batched and performed at later
 *    point. The actions are always initiated only after the capability
 *    is finalised. This is usally denoted by "enabling" the capability.
 *    The actions are initiated as follows :
 *	a) Action is triggered on all online CPUs, after the capability is
 *	finalised, invoked within the stop_machine() context from
 *	enable_cpu_capabilitie().
 *
 *	b) Any late CPU, brought up after (1), the action is triggered via:
 *
 *	  check_local_cpu_capabilities() -> verify_local_cpu_capabilities()
 *
 * 5) Conflicts: Based on the state of the capability on a late CPU vs.
 *    the system state, we could have the following combinations :
 *
 *		x-----------------------------x
 *		| Type  | System   | Late CPU |
 *		|-----------------------------|
 *		|  a    |   y      |    n     |
 *		|-----------------------------|
 *		|  b    |   n      |    y     |
 *		x-----------------------------x
 *
 *     Two separate flag bits are defined to indicate whether each kind of
 *     conflict can be allowed:
 *		ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU - Case(a) is allowed
 *		ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU - Case(b) is allowed
 *
 *     Case (a) is not permitted for a capability that the system requires
 *     all CPUs to have in order for the capability to be enabled. This is
 *     typical for capabilities that represent enhanced functionality.
 *
 *     Case (b) is not permitted for a capability that must be enabled
 *     during boot if any CPU in the system requires it in order to run
 *     safely. This is typical for erratum work arounds that cannot be
 *     enabled after the corresponding capability is finalised.
 *
 *     In some non-typical cases either both (a) and (b), or neither,
 *     should be permitted. This can be described by including neither
 *     or both flags in the capability's type field.
 *
 *     In case of a conflict, the CPU is prevented from booting. If the
 *     ARM64_CPUCAP_PANIC_ON_CONFLICT flag is specified for the capability,
 *     then a kernel panic is triggered.
 */


/*
 * Decide how the capability is detected.
 * On any local CPU vs System wide vs the primary boot CPU
 */


/*
 * The capabilitiy is detected on the Boot CPU and is used by kernel
 * during early boot. i.e, the capability should be "detected" and
 * "enabled" as early as possibly on all booting CPUs.
 */
# 264 "./arch/arm64/include/asm/cpufeature.h"
/*
 * Is it permitted for a late CPU to have this capability when system
 * hasn't already enabled it ?
 */

/* Is it safe for a late CPU to miss this capability when system has it */

/* Panic when a conflict is detected */


/*
 * CPU errata workarounds that need to be enabled at boot time if one or
 * more CPUs in the system requires it. When one of these capabilities
 * has been enabled, it is safe to allow any CPU to boot that doesn't
 * require the workaround. However, it is not safe if a "late" CPU
 * requires a workaround and the system hasn't enabled it already.
 */


/*
 * CPU feature detected at boot time based on system-wide value of a
 * feature. It is safe for a late CPU to have this feature even though
 * the system hasn't enabled it, although the feature will not be used
 * by Linux in this case. If the system has enabled this feature already,
 * then every late CPU must have it.
 */


/*
 * CPU feature detected at boot time based on feature of one or more CPUs.
 * All possible conflicts for a late CPU are ignored.
 * NOTE: this means that a late CPU with the feature will *not* cause the
 * capability to be advertised by cpus_have_*cap()!
 */





/*
 * CPU feature detected at boot time, on one or more CPUs. A late CPU
 * is not allowed to have the capability when the system doesn't have it.
 * It is Ok for a late CPU to miss the feature.
 */




/*
 * CPU feature used early in the boot based on the boot CPU. All secondary
 * CPUs must match the state of the capability as detected by the boot CPU. In
 * case of a conflict, a kernel panic is triggered.
 */



/*
 * CPU feature used early in the boot based on the boot CPU. It is safe for a
 * late CPU to have this feature even though the boot CPU hasn't enabled it,
 * although the feature will not be used by Linux in this case. If the boot CPU
 * has enabled this feature already, then every late CPU must have it.
 */



struct arm64_cpu_capabilities {
 const char *desc;
 u16 capability;
 u16 type;
 bool (*matches)(const struct arm64_cpu_capabilities *caps, int scope);
 /*
	 * Take the appropriate actions to configure this capability
	 * for this CPU. If the capability is detected by the kernel
	 * this will be called on all the CPUs in the system,
	 * including the hotplugged CPUs, regardless of whether the
	 * capability is available on that specific CPU. This is
	 * useful for some capabilities (e.g, working around CPU
	 * errata), where all the CPUs must take some action (e.g,
	 * changing system control/configuration). Thus, if an action
	 * is required only if the CPU has the capability, then the
	 * routine must check it before taking any action.
	 */
 void (*cpu_enable)(const struct arm64_cpu_capabilities *cap);
 union {
  struct { /* To be used for erratum handling only */
   struct midr_range midr_range;
   const struct arm64_midr_revidr {
    u32 midr_rv; /* revision/variant */
    u32 revidr_mask;
   } * const fixed_revs;
  };

  const struct midr_range *midr_range_list;
  struct { /* Feature register checking */
   u32 sys_reg;
   u8 field_pos;
   u8 field_width;
   u8 min_field_value;
   u8 hwcap_type;
   bool sign;
   unsigned long hwcap;
  };
 };

 /*
	 * An optional list of "matches/cpu_enable" pair for the same
	 * "capability" of the same "type" as described by the parent.
	 * Only matches(), cpu_enable() and fields relevant to these
	 * methods are significant in the list. The cpu_enable is
	 * invoked only if the corresponding entry "matches()".
	 * However, if a cpu_enable() method is associated
	 * with multiple matches(), care should be taken that either
	 * the match criteria are mutually exclusive, or that the
	 * method is robust against being called multiple times.
	 */
 const struct arm64_cpu_capabilities *match_list;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpucap_default_scope(const struct arm64_cpu_capabilities *cap)
{
 return cap->type & (((u16)((((1UL))) << (1))) | ((u16)((((1UL))) << (0))) | ((u16)((((1UL))) << (2))));
}

/*
 * Generic helper for handling capabilities with multiple (match,enable) pairs
 * of call backs, sharing the same capability bit.
 * Iterate over each entry to see if at least one matches.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
cpucap_multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry,
          int scope)
{
 const struct arm64_cpu_capabilities *caps;

 for (caps = entry->match_list; caps->matches; caps++)
  if (caps->matches(caps, scope))
   return true;

 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_vhe_hyp_code(void)
{
 /* Only defined for code run in VHE hyp context */
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_nvhe_hyp_code(void)
{
 /* Only defined for code run in NVHE hyp context */
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_hyp_code(void)
{
 return is_vhe_hyp_code() || is_nvhe_hyp_code();
}

extern unsigned long cpu_hwcaps[(((84) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];

extern unsigned long boot_capabilities[(((84) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];




bool this_cpu_has_cap(unsigned int cap);
void cpu_set_feature(unsigned int num);
bool cpu_have_feature(unsigned int num);
unsigned long cpu_get_elf_hwcap(void);
unsigned long cpu_get_elf_hwcap2(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_capabilities_finalized(void)
{
 return alternative_has_feature_likely(1);
}

/*
 * Test for a capability with a runtime check.
 *
 * Before the capability is detected, this returns false.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool cpus_have_cap(unsigned int num)
{
 if (num >= 84)
  return false;
 return generic_test_bit(num, cpu_hwcaps);
}

/*
 * Test for a capability without a runtime check.
 *
 * Before capabilities are finalized, this returns false.
 * After capabilities are finalized, this is patched to avoid a runtime check.
 *
 * @num must be a compile-time constant.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool __cpus_have_const_cap(int num)
{
 if (num >= 84)
  return false;
 return alternative_has_feature_unlikely(num);
}

/*
 * Test for a capability without a runtime check.
 *
 * Before capabilities are finalized, this will BUG().
 * After capabilities are finalized, this is patched to avoid a runtime check.
 *
 * @num must be a compile-time constant.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool cpus_have_final_cap(int num)
{
 if (system_capabilities_finalized())
  return __cpus_have_const_cap(num);
 else
  do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"arch/arm64/include/asm/cpufeature.h\"; .popsection; .long 14472b - .; .short 483; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0);
}

/*
 * Test for a capability, possibly with a runtime check for non-hyp code.
 *
 * For hyp code, this behaves the same as cpus_have_final_cap().
 *
 * For non-hyp code:
 * Before capabilities are finalized, this behaves as cpus_have_cap().
 * After capabilities are finalized, this is patched to avoid a runtime check.
 *
 * @num must be a compile-time constant.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool cpus_have_const_cap(int num)
{
 if (is_hyp_code())
  return cpus_have_final_cap(num);
 else if (system_capabilities_finalized())
  return __cpus_have_const_cap(num);
 else
  return cpus_have_cap(num);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpus_set_cap(unsigned int num)
{
 if (num >= 84) {
  ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "4" /* warning conditions */ "Attempt to set an illegal CPU capability (%d >= %d)\n", num, 84); });

 } else {
  ((__builtin_constant_p(num) && __builtin_constant_p((uintptr_t)(cpu_hwcaps) != (uintptr_t)((void *)0)) && (uintptr_t)(cpu_hwcaps) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(cpu_hwcaps))) ? generic___set_bit(num, cpu_hwcaps) : generic___set_bit(num, cpu_hwcaps));
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__const__))
cpuid_feature_extract_signed_field_width(u64 features, int field, int width)
{
 return (s64)(features << (64 - width - field)) >> (64 - width);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__const__))
cpuid_feature_extract_signed_field(u64 features, int field)
{
 return cpuid_feature_extract_signed_field_width(features, field, 4);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned int __attribute__((__const__))
cpuid_feature_extract_unsigned_field_width(u64 features, int field, int width)
{
 return (u64)(features << (64 - width - field)) >> (64 - width);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned int __attribute__((__const__))
cpuid_feature_extract_unsigned_field(u64 features, int field)
{
 return cpuid_feature_extract_unsigned_field_width(features, field, 4);
}

/*
 * Fields that identify the version of the Performance Monitors Extension do
 * not follow the standard ID scheme. See ARM DDI 0487E.a page D13-2825,
 * "Alternative ID scheme used for the Performance Monitors Extension version".
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __attribute__((__const__))
cpuid_feature_cap_perfmon_field(u64 features, int field, u64 cap)
{
 u64 val = cpuid_feature_extract_unsigned_field(features, field);
 u64 mask = ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((field) > (field + 3)) * 0l)) : (int *)8))), (field) > (field + 3), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (field)) + 1) & (~(((0ULL))) >> (64 - 1 - (field + 3)))));

 /* Treat IMPLEMENTATION DEFINED functionality as unimplemented */
 if (val == (((0b1111UL))))
  val = 0;

 if (val > cap) {
  features &= ~mask;
  features |= (cap << field) & mask;
 }

 return features;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 arm64_ftr_mask(const struct arm64_ftr_bits *ftrp)
{
 return (u64)((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((ftrp->shift) > (ftrp->shift + ftrp->width - 1)) * 0l)) : (int *)8))), (ftrp->shift) > (ftrp->shift + ftrp->width - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (ftrp->shift)) + 1) & (~(((0UL))) >> (64 - 1 - (ftrp->shift + ftrp->width - 1)))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 arm64_ftr_reg_user_value(const struct arm64_ftr_reg *reg)
{
 return (reg->user_val | (reg->sys_val & reg->user_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__const__))
cpuid_feature_extract_field_width(u64 features, int field, int width, bool sign)
{
 if (({ int __ret_warn_on = !!(!width); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"arch/arm64/include/asm/cpufeature.h\"; .popsection; .long 14472b - .; .short 577; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); }))
  width = 4;
 return (sign) ?
  cpuid_feature_extract_signed_field_width(features, field, width) :
  cpuid_feature_extract_unsigned_field_width(features, field, width);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__const__))
cpuid_feature_extract_field(u64 features, int field, bool sign)
{
 return cpuid_feature_extract_field_width(features, field, 4, sign);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 arm64_ftr_value(const struct arm64_ftr_bits *ftrp, u64 val)
{
 return (s64)cpuid_feature_extract_field_width(val, ftrp->shift, ftrp->width, ftrp->sign);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)
{
 return cpuid_feature_extract_unsigned_field(mmfr0, 8) == 0x1 ||
  cpuid_feature_extract_unsigned_field(mmfr0, 16) == 0x1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool id_aa64pfr0_32bit_el1(u64 pfr0)
{
 u32 val = cpuid_feature_extract_unsigned_field(pfr0, 4);

 return val == 0x2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool id_aa64pfr0_32bit_el0(u64 pfr0)
{
 u32 val = cpuid_feature_extract_unsigned_field(pfr0, 0);

 return val == 0x2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool id_aa64pfr0_sve(u64 pfr0)
{
 u32 val = cpuid_feature_extract_unsigned_field(pfr0, 32);

 return val > 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool id_aa64pfr1_sme(u64 pfr1)
{
 u32 val = cpuid_feature_extract_unsigned_field(pfr1, 24);

 return val > 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool id_aa64pfr1_mte(u64 pfr1)
{
 u32 val = cpuid_feature_extract_unsigned_field(pfr1, 8);

 return val >= (((0b0010UL)));
}

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) setup_cpu_features(void);
void check_local_cpu_capabilities(void);

u64 read_sanitised_ftr_reg(u32 id);
u64 __read_sysreg_by_encoding(u32 sys_id);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_supports_mixed_endian_el0(void)
{
 return id_aa64mmfr0_mixed_endian_el0(({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; }));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool supports_csv2p3(int scope)
{
 u64 pfr0;
 u8 csv2_val;

 if (scope == ((u16)((((1UL))) << (0))))
  pfr0 = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((4) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
 else
  pfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((4) << 8) | ((0) << 5)));

 csv2_val = cpuid_feature_extract_unsigned_field(pfr0,
       56);
 return csv2_val == 3;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool supports_clearbhb(int scope)
{
 u64 isar2;

 if (scope == ((u16)((((1UL))) << (0))))
  isar2 = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((6) << 8) | ((2) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
 else
  isar2 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((6) << 8) | ((2) << 5)));

 return cpuid_feature_extract_unsigned_field(isar2,
          20);
}

const struct cpumask *system_32bit_el0_cpumask(void);
extern struct static_key_false arm64_mismatched_32bit_el0;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_32bit_el0(void)
{
 u64 pfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((4) << 8) | ((0) << 5)));

 return ({ bool branch; if (__builtin_types_compatible_p(typeof(*&arm64_mismatched_32bit_el0), struct static_key_true)) branch = arch_static_branch_jump(&(&arm64_mismatched_32bit_el0)->key, false); else if (__builtin_types_compatible_p(typeof(*&arm64_mismatched_32bit_el0), struct static_key_false)) branch = arch_static_branch(&(&arm64_mismatched_32bit_el0)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); }) ||
        id_aa64pfr0_32bit_el0(pfr0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_4kb_granule(void)
{
 u64 mmfr0;
 u32 val;

 mmfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((0) << 5)));
 val = cpuid_feature_extract_unsigned_field(mmfr0,
      28);

 return (val >= 0x0) &&
        (val <= 0x7);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_64kb_granule(void)
{
 u64 mmfr0;
 u32 val;

 mmfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((0) << 5)));
 val = cpuid_feature_extract_unsigned_field(mmfr0,
      24);

 return (val >= 0x0) &&
        (val <= 0x7);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_16kb_granule(void)
{
 u64 mmfr0;
 u32 val;

 mmfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((0) << 5)));
 val = cpuid_feature_extract_unsigned_field(mmfr0,
      20);

 return (val >= 0x1) &&
        (val <= 0xf);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_mixed_endian_el0(void)
{
 return id_aa64mmfr0_mixed_endian_el0(read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((0) << 5))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_mixed_endian(void)
{
 u64 mmfr0;
 u32 val;

 mmfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((0) << 5)));
 val = cpuid_feature_extract_unsigned_field(mmfr0,
      8);

 return val == 0x1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_supports_fpsimd(void)
{
 return !cpus_have_const_cap(28);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_uses_hw_pan(void)
{
 return 1 &&
  cpus_have_const_cap(30);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_uses_ttbr0_pan(void)
{
 return 0 &&
  !system_uses_hw_pan();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_supports_sve(void)
{
 return 1 &&
  cpus_have_const_cap(52);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_supports_sme(void)
{
 return 1 &&
  cpus_have_const_cap(45);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_supports_fa64(void)
{
 return 1 &&
  cpus_have_const_cap(46);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_supports_tpidr2(void)
{
 return system_supports_sme();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_supports_cnp(void)
{
 return 1 &&
  cpus_have_const_cap(13);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_address_auth(void)
{
 return 1 &&
  cpus_have_const_cap(5);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_generic_auth(void)
{
 return 1 &&
  cpus_have_const_cap(21);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_has_full_ptr_auth(void)
{
 return system_supports_address_auth() && system_supports_generic_auth();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool system_uses_irq_prio_masking(void)
{
 return 0 &&
        cpus_have_const_cap(25);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_mte(void)
{
 return 1 &&
  cpus_have_const_cap(43);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_has_prio_mask_debugging(void)
{
 return 0 &&
        system_uses_irq_prio_masking();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_bti(void)
{
 return 1 && cpus_have_const_cap(2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_supports_tlb_range(void)
{
 return 1 &&
  cpus_have_const_cap(37);
}

int do_emulate_mrs(struct pt_regs *regs, u32 sys_reg, u32 rt);
bool try_emulate_mrs(struct pt_regs *regs, u32 isn);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 id_aa64mmfr0_parange_to_phys_shift(int parange)
{
 switch (parange) {
 case (((0b0000UL))): return 32;
 case (((0b0001UL))): return 36;
 case (((0b0010UL))): return 40;
 case (((0b0011UL))): return 42;
 case (((0b0100UL))): return 44;
 case (((0b0101UL))): return 48;
 case (((0b0110UL))): return 52;
 /*
	 * A future PE could use a value unknown to the kernel.
	 * However, by the "D10.1.4 Principles of the ID scheme
	 * for fields in ID registers", ARM DDI 0487C.a, any new
	 * value is guaranteed to be higher than what we know already.
	 * As a safe limit, we return the limit supported by the kernel.
	 */
 default: return 48;
 }
}

/* Check whether hardware update of the Access flag is supported */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_has_hw_af(void)
{
 u64 mmfr1;

 if (!1)
  return false;

 mmfr1 = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((1) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
 return cpuid_feature_extract_unsigned_field(mmfr1,
      0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_has_pan(void)
{
 u64 mmfr1 = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((7) << 8) | ((1) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
 return cpuid_feature_extract_unsigned_field(mmfr1,
          20);
}


/* Check whether the cpu supports the Activity Monitors Unit (AMU) */
extern bool cpu_has_amu_feat(int cpu);







/* Get a cpu that supports the Activity Monitors Unit (AMU) */
extern int get_cpu_with_amu_feat(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int get_vmid_bits(u64 mmfr1)
{
 int vmid_bits;

 vmid_bits = cpuid_feature_extract_unsigned_field(mmfr1,
      4);
 if (vmid_bits == (((0b0010UL))))
  return 16;

 /*
	 * Return the default here even if any reserved
	 * value is fetched from the system register.
	 */
 return 8;
}

struct arm64_ftr_reg *get_arm64_ftr_reg(u32 sys_id);

extern struct arm64_ftr_override id_aa64mmfr1_override;
extern struct arm64_ftr_override id_aa64pfr0_override;
extern struct arm64_ftr_override id_aa64pfr1_override;
extern struct arm64_ftr_override id_aa64zfr0_override;
extern struct arm64_ftr_override id_aa64smfr0_override;
extern struct arm64_ftr_override id_aa64isar1_override;
extern struct arm64_ftr_override id_aa64isar2_override;

u32 get_kvm_ipa_limit(void);
void dump_cpu_features(void);
# 12 "./arch/arm64/include/asm/ptrace.h" 2

# 1 "./arch/arm64/include/uapi/asm/ptrace.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Based on arch/arm/include/asm/ptrace.h
 *
 * Copyright (C) 1996-2003 Russell King
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */






# 1 "./arch/arm64/include/uapi/asm/sve_context.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/* Copyright (C) 2017-2018 ARM Limited */

/*
 * For use by other UAPI headers only.
 * Do not make direct use of header or its definitions.
 */
# 27 "./arch/arm64/include/uapi/asm/ptrace.h" 2


/*
 * PSR bits
 */
# 41 "./arch/arm64/include/uapi/asm/ptrace.h"
/* AArch32 CPSR bits */


/* AArch64 SPSR bits */
# 62 "./arch/arm64/include/uapi/asm/ptrace.h"
/*
 * Groups of PSR bits
 */





/* Convenience names for the values of PSTATE.BTYPE */





/* syscall emulation path in ptrace */


/* MTE allocation tag access */





/*
 * User structures for general purpose, floating point and debug registers.
 */
struct user_pt_regs {
 __u64 regs[31];
 __u64 sp;
 __u64 pc;
 __u64 pstate;
};

struct user_fpsimd_state {
 __uint128_t vregs[32];
 __u32 fpsr;
 __u32 fpcr;
 __u32 __reserved[2];
};

struct user_hwdebug_state {
 __u32 dbg_info;
 __u32 pad;
 struct {
  __u64 addr;
  __u32 ctrl;
  __u32 pad;
 } dbg_regs[16];
};

/* SVE/FP/SIMD state (NT_ARM_SVE & NT_ARM_SSVE) */

struct user_sve_header {
 __u32 size; /* total meaningful regset content in bytes */
 __u32 max_size; /* maxmium possible size for this thread */
 __u16 vl; /* current vector length */
 __u16 max_vl; /* maximum possible vector length */
 __u16 flags;
 __u16 __reserved;
};

/* Definitions for user_sve_header.flags: */





/*
 * Common SVE_PT_* flags:
 * These must be kept in sync with prctl interface in <linux/prctl.h>
 */




/*
 * The remainder of the SVE state follows struct user_sve_header.  The
 * total size of the SVE state (including header) depends on the
 * metadata in the header:  SVE_PT_SIZE(vq, flags) gives the total size
 * of the state in bytes, including the header.
 *
 * Refer to <asm/sigcontext.h> for details of how to pass the correct
 * "vq" argument to these macros.
 */

/* Offset from the start of struct user_sve_header to the register data */




/*
 * The register data content and layout depends on the value of the
 * flags field.
 */

/*
 * (flags & SVE_PT_REGS_MASK) == SVE_PT_REGS_FPSIMD case:
 *
 * The payload starts at offset SVE_PT_FPSIMD_OFFSET, and is of type
 * struct user_fpsimd_state.  Additional data might be appended in the
 * future: use SVE_PT_FPSIMD_SIZE(vq, flags) to compute the total size.
 * SVE_PT_FPSIMD_SIZE(vq, flags) will never be less than
 * sizeof(struct user_fpsimd_state).
 */





/*
 * (flags & SVE_PT_REGS_MASK) == SVE_PT_REGS_SVE case:
 *
 * The payload starts at offset SVE_PT_SVE_OFFSET, and is of size
 * SVE_PT_SVE_SIZE(vq, flags).
 *
 * Additional macros describe the contents and layout of the payload.
 * For each, SVE_PT_SVE_x_OFFSET(args) is the start offset relative to
 * the start of struct user_sve_header, and SVE_PT_SVE_x_SIZE(args) is
 * the size in bytes:
 *
 *	x	type				description
 *	-	----				-----------
 *	ZREGS		\
 *	ZREG		|
 *	PREGS		| refer to <asm/sigcontext.h>
 *	PREG		|
 *	FFR		/
 *
 *	FPSR	uint32_t			FPSR
 *	FPCR	uint32_t			FPCR
 *
 * Additional data might be appended in the future.
 *
 * The Z-, P- and FFR registers are represented in memory in an endianness-
 * invariant layout which differs from the layout used for the FPSIMD
 * V-registers on big-endian systems: see sigcontext.h for more explanation.
 */
# 223 "./arch/arm64/include/uapi/asm/ptrace.h"
/* For streaming mode SVE (SSVE) FFR must be read and written as zero */
# 234 "./arch/arm64/include/uapi/asm/ptrace.h"
/*
 * Any future extension appended after FPCR must be aligned to the next
 * 128-bit boundary.
 */
# 251 "./arch/arm64/include/uapi/asm/ptrace.h"
/* pointer authentication masks (NT_ARM_PAC_MASK) */

struct user_pac_mask {
 __u64 data_mask;
 __u64 insn_mask;
};

/* pointer authentication keys (NT_ARM_PACA_KEYS, NT_ARM_PACG_KEYS) */

struct user_pac_address_keys {
 __uint128_t apiakey;
 __uint128_t apibkey;
 __uint128_t apdakey;
 __uint128_t apdbkey;
};

struct user_pac_generic_keys {
 __uint128_t apgakey;
};

/* ZA state (NT_ARM_ZA) */

struct user_za_header {
 __u32 size; /* total meaningful regset content in bytes */
 __u32 max_size; /* maxmium possible size for this thread */
 __u16 vl; /* current vector length */
 __u16 max_vl; /* maximum possible vector length */
 __u16 flags;
 __u16 __reserved;
};

/*
 * Common ZA_PT_* flags:
 * These must be kept in sync with prctl interface in <linux/prctl.h>
 */




/*
 * The remainder of the ZA state follows struct user_za_header.  The
 * total size of the ZA state (including header) depends on the
 * metadata in the header:  ZA_PT_SIZE(vq, flags) gives the total size
 * of the state in bytes, including the header.
 *
 * Refer to <asm/sigcontext.h> for details of how to pass the correct
 * "vq" argument to these macros.
 */

/* Offset from the start of struct user_za_header to the register data */




/*
 * The payload starts at offset ZA_PT_ZA_OFFSET, and is of size
 * ZA_PT_ZA_SIZE(vq, flags).
 *
 * The ZA array is stored as a sequence of horizontal vectors ZAV of SVL/8
 * bytes each, starting from vector 0.
 *
 * Additional data might be appended in the future.
 *
 * The ZA matrix is represented in memory in an endianness-invariant layout
 * which differs from the layout used for the FPSIMD V-registers on big-endian
 * systems: see sigcontext.h for more explanation.
 */
# 14 "./arch/arm64/include/asm/ptrace.h" 2

/* Current Exception Level values, as contained in CurrentEL */
# 24 "./arch/arm64/include/asm/ptrace.h"
/*
 * PMR values used to mask/unmask interrupts.
 *
 * GIC priority masking works as follows: if an IRQ's priority is a higher value
 * than the value held in PMR, that IRQ is masked. Lowering the value of PMR
 * means masking more IRQs (or at least that the same IRQs remain masked).
 *
 * To mask interrupts, we clear the most significant bit of PMR.
 *
 * Some code sections either automatically switch back to PSR.I or explicitly
 * require to not use priority masking. If bit GIC_PRIO_PSR_I_SET is included
 * in the priority mask, it indicates that PSR.I should be set and
 * interrupt disabling temporarily does not rely on IRQ priorities.
 */
# 54 "./arch/arm64/include/asm/ptrace.h"
/* Additional SPSR bits not exposed in the UABI */



/* AArch32-specific ptrace requests */
# 68 "./arch/arm64/include/asm/ptrace.h"
/* SPSR_ELx bits for exceptions taken from AArch32 */
# 100 "./arch/arm64/include/asm/ptrace.h"
/* AArch32 CPSR bits, as seen in AArch32 */


/*
 * These are 'magic' values for PTRACE_PEEKUSR that return info about where a
 * process is located in memory.
 */




/*
 * If pt_regs.syscallno == NO_SYSCALL, then the thread is not executing
 * a syscall -- i.e., its most recent entry into the kernel from
 * userspace was not via SVC, or otherwise a tracer cancelled the syscall.
 *
 * This must have the value -1, for ABI compatibility with ptrace etc.
 */






/* sizeof(struct user) for AArch32 */


/* Architecturally defined mapping between AArch32 and AArch64 registers */
# 149 "./arch/arm64/include/asm/ptrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long compat_psr_to_pstate(const unsigned long psr)
{
 unsigned long pstate;

 pstate = psr & ~0x00200000;

 if (psr & 0x00200000)
  pstate |= 0x01000000;

 return pstate;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pstate_to_compat_psr(const unsigned long pstate)
{
 unsigned long psr;

 psr = pstate & ~0x01000000;

 if (pstate & 0x01000000)
  psr |= 0x00200000;

 return psr;
}

/*
 * This struct defines the way the registers are stored on the stack during an
 * exception. Note that sizeof(struct pt_regs) has to be a multiple of 16 (for
 * stack alignment). struct user_pt_regs must form a prefix of struct pt_regs.
 */
struct pt_regs {
 union {
  struct user_pt_regs user_regs;
  struct {
   u64 regs[31];
   u64 sp;
   u64 pc;
   u64 pstate;
  };
 };
 u64 orig_x0;




 s32 syscallno;
 u32 unused2;

 u64 sdei_ttbr1;
 /* Only valid when ARM64_HAS_IRQ_PRIO_MASKING is enabled. */
 u64 pmr_save;
 u64 stackframe[2];

 /* Only valid for some EL1 exceptions. */
 u64 lockdep_hardirqs;
 u64 exit_rcu;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool in_syscall(struct pt_regs const *regs)
{
 return regs->syscallno != (-1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void forget_syscall(struct pt_regs *regs)
{
 regs->syscallno = (-1);
}
# 248 "./arch/arm64/include/asm/ptrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long user_stack_pointer(struct pt_regs *regs)
{
 if ((((regs)->pstate & (0x00000010 | 0x0000000f)) == (0x00000010 | 0x00000000)))
  return regs->regs[13];
 return regs->sp;
}

extern int regs_query_register_offset(const char *name);
extern unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
            unsigned int n);

/**
 * regs_get_register() - get register value from its offset
 * @regs:	pt_regs from which register value is gotten
 * @offset:	offset of the register.
 *
 * regs_get_register returns the value of a register whose offset from @regs.
 * The @offset is the offset of the register in struct pt_regs.
 * If @offset is bigger than MAX_REG_OFFSET, this returns 0.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 regs_get_register(struct pt_regs *regs, unsigned int offset)
{
 u64 val = 0;

 ({ int __ret_warn_on = !!(offset & 7); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"arch/arm64/include/asm/ptrace.h\"; .popsection; .long 14472b - .; .short 272; .short (1 << 0)|(((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });

 offset >>= 3;
 switch (offset) {
 case 0 ... 30:
  val = regs->regs[offset];
  break;
 case __builtin_offsetof(struct pt_regs, sp) >> 3:
  val = regs->sp;
  break;
 case __builtin_offsetof(struct pt_regs, pc) >> 3:
  val = regs->pc;
  break;
 case __builtin_offsetof(struct pt_regs, pstate) >> 3:
  val = regs->pstate;
  break;
 default:
  val = 0;
 }

 return val;
}

/*
 * Read a register given an architectural register index r.
 * This handles the common case where 31 means XZR, not SP.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pt_regs_read_reg(const struct pt_regs *regs, int r)
{
 return (r == 31) ? 0 : regs->regs[r];
}

/*
 * Write a register given an architectural register index r.
 * This handles the common case where 31 means XZR, not SP.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pt_regs_write_reg(struct pt_regs *regs, int r,
         unsigned long val)
{
 if (r != 31)
  regs->regs[r] = val;
}

/* Valid only for Kernel mode traps. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long kernel_stack_pointer(struct pt_regs *regs)
{
 return regs->sp;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long regs_return_value(struct pt_regs *regs)
{
 unsigned long val = regs->regs[0];

 /*
	 * Audit currently uses regs_return_value() instead of
	 * syscall_get_return_value(). Apply the same sign-extension here until
	 * audit is updated to use syscall_get_return_value().
	 */
 if ((((regs)->pstate & (0x00000010 | 0x0000000f)) == (0x00000010 | 0x00000000)))
  val = sign_extend64(val, 31);

 return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void regs_set_return_value(struct pt_regs *regs, unsigned long rc)
{
 regs->regs[0] = rc;
}

/**
 * regs_get_kernel_argument() - get Nth function argument in kernel
 * @regs:	pt_regs of that context
 * @n:		function argument number (start from 0)
 *
 * regs_get_argument() returns @n th argument of the function call.
 *
 * Note that this chooses the most likely register mapping. In very rare
 * cases this may not return correct data, for example, if one of the
 * function parameters is 16 bytes or bigger. In such cases, we cannot
 * get access the parameter correctly and the register assignment of
 * subsequent parameters will be shifted.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long regs_get_kernel_argument(struct pt_regs *regs,
           unsigned int n)
{

 if (n < 8)
  return pt_regs_read_reg(regs, n);
 return 0;
}

/* We must avoid circular header include via sched.h */
struct task_struct;
int valid_user_regs(struct user_pt_regs *regs, struct task_struct *task);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long instruction_pointer(struct pt_regs *regs)
{
 return regs->pc;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void instruction_pointer_set(struct pt_regs *regs,
  unsigned long val)
{
 regs->pc = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long frame_pointer(struct pt_regs *regs)
{
 return regs->regs[29];
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void procedure_link_pointer_set(struct pt_regs *regs,
        unsigned long val)
{
 ((regs)->regs[30]) = val;
}

extern unsigned long profile_pc(struct pt_regs *regs);
# 11 "./arch/arm64/include/asm/irqflags.h" 2


/*
 * Aarch64 has flags for masking: Debug, Asynchronous (serror), Interrupts and
 * FIQ exceptions, in the 'daif' register. We mask and unmask them in 'daif'
 * order:
 * Masking debug exceptions causes all other exceptions to be masked too/
 * Masking SError masks IRQ/FIQ, but not debug exceptions. IRQ and FIQ are
 * always masked and unmasked together, and have no side effects for other
 * flags. Keeping to this order makes it easier for entry.S to know which
 * exceptions should be unmasked.
 */

/*
 * CPU interrupt mask handling.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_local_irq_enable(void)
{
 if (system_has_prio_mask_debugging()) {
  u32 pmr = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((4) << 12) | ((6) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });

  ({ int __ret_warn_on = !!(pmr != 0xe0 && pmr != ({ extern struct static_key_false gic_nonsecure_priorities; u8 __prio = (0xe0 & ~0x80); if (({ bool branch; if (__builtin_types_compatible_p(typeof(*&gic_nonsecure_priorities), struct static_key_true)) branch = arch_static_branch_jump(&(&gic_nonsecure_priorities)->key, false); else if (__builtin_types_compatible_p(typeof(*&gic_nonsecure_priorities), struct static_key_false)) branch = arch_static_branch(&(&gic_nonsecure_priorities)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); })) __prio = 0xa0; __prio; })); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"arch/arm64/include/asm/irqflags.h\"; .popsection; .long 14472b - .; .short 32; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 }

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "msr	daifclr, #3		// arch_local_irq_enable" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "25" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((4) << 12) | ((6) << 8) | ((0) << 5))" ", " "%0" "\n" "	.purgem	msr_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"



  :
  : "r" ((unsigned long) 0xe0)
  : "memory");

 do {} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_local_irq_disable(void)
{
 if (system_has_prio_mask_debugging()) {
  u32 pmr = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((4) << 12) | ((6) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });

  ({ int __ret_warn_on = !!(pmr != 0xe0 && pmr != ({ extern struct static_key_false gic_nonsecure_priorities; u8 __prio = (0xe0 & ~0x80); if (({ bool branch; if (__builtin_types_compatible_p(typeof(*&gic_nonsecure_priorities), struct static_key_true)) branch = arch_static_branch_jump(&(&gic_nonsecure_priorities)->key, false); else if (__builtin_types_compatible_p(typeof(*&gic_nonsecure_priorities), struct static_key_false)) branch = arch_static_branch(&(&gic_nonsecure_priorities)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); })) __prio = 0xa0; __prio; })); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"arch/arm64/include/asm/irqflags.h\"; .popsection; .long 14472b - .; .short 51; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 }

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "msr	daifset, #3		// arch_local_irq_disable" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "25" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((4) << 12) | ((6) << 8) | ((0) << 5))" ", " "%0" "\n" "	.purgem	msr_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"



  :
  : "r" ((unsigned long) ({ extern struct static_key_false gic_nonsecure_priorities; u8 __prio = (0xe0 & ~0x80); if (({ bool branch; if (__builtin_types_compatible_p(typeof(*&gic_nonsecure_priorities), struct static_key_true)) branch = arch_static_branch_jump(&(&gic_nonsecure_priorities)->key, false); else if (__builtin_types_compatible_p(typeof(*&gic_nonsecure_priorities), struct static_key_false)) branch = arch_static_branch(&(&gic_nonsecure_priorities)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); })) __prio = 0xa0; __prio; }))
  : "memory");
}

/*
 * Save the current interrupt enable state.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long arch_local_save_flags(void)
{
 unsigned long flags;

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "mrs	%0, daif" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "25" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((4) << 12) | ((6) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"



  : "=&r" (flags)
  :
  : "memory");

 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_irqs_disabled_flags(unsigned long flags)
{
 int res;

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "and	%w0, %w1, #" "0x00000080" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "25" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "eor	%w0, %w1, #" "0xe0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"



  : "=&r" (res)
  : "r" ((int) flags)
  : "memory");

 return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_irqs_disabled(void)
{
 return arch_irqs_disabled_flags(arch_local_save_flags());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long arch_local_irq_save(void)
{
 unsigned long flags;

 flags = arch_local_save_flags();

 /*
	 * There are too many states with IRQs disabled, just keep the current
	 * state if interrupts are already disabled/masked.
	 */
 if (!arch_irqs_disabled_flags(flags))
  arch_local_irq_disable();

 return flags;
}

/*
 * restore saved IRQ state
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_local_irq_restore(unsigned long flags)
{
 asm volatile(".if ""1"" == 1\n" "661:\n\t" "msr	daif, %0" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "25" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((4) << 12) | ((6) << 8) | ((0) << 5))" ", " "%0" "\n" "	.purgem	msr_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"



  :
  : "r" (flags)
  : "memory");

 do {} while (0);
}
# 17 "./include/linux/irqflags.h" 2
# 1 "./arch/arm64/include/asm/percpu.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2013 ARM Ltd.
 */
# 15 "./arch/arm64/include/asm/percpu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_my_cpu_offset(unsigned long off)
{
 asm volatile(".if ""1"" == 1\n" "661:\n\t" "msr tpidr_el1, %0" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "38" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "msr tpidr_el2, %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


   :: "r" (off) : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __hyp_my_cpu_offset(void)
{
 /*
	 * Non-VHE hyp code runs with preemption disabled. No need to hazard
	 * the register access against barrier() as in __kern_my_cpu_offset.
	 */
 return ({ u64 __val; asm volatile("mrs %0, " "tpidr_el2" : "=r" (__val)); __val; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __kern_my_cpu_offset(void)
{
 unsigned long off;

 /*
	 * We want to allow caching the value, so avoid using volatile and
	 * instead use a fake stack read to hazard against barrier().
	 */
 asm(".if ""1"" == 1\n" "661:\n\t" "mrs %0, tpidr_el1" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "38" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "mrs %0, tpidr_el2" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


  : "=r" (off) :
  "Q" (*(const unsigned long *)current_stack_pointer));

 return off;
}
# 123 "./arch/arm64/include/asm/percpu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __percpu_read_8(void *ptr) { return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_157(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u8 *)ptr) == sizeof(char) || sizeof(*(u8 *)ptr) == sizeof(short) || sizeof(*(u8 *)ptr) == sizeof(int) || sizeof(*(u8 *)ptr) == sizeof(long)) || sizeof(*(u8 *)ptr) == sizeof(long long))) __compiletime_assert_157(); } while (0); (*(const volatile typeof( _Generic((*(u8 *)ptr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*(u8 *)ptr))) *)&(*(u8 *)ptr)); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_write_8(void *ptr, unsigned long val) { do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_158(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u8 *)ptr) == sizeof(char) || sizeof(*(u8 *)ptr) == sizeof(short) || sizeof(*(u8 *)ptr) == sizeof(int) || sizeof(*(u8 *)ptr) == sizeof(long)) || sizeof(*(u8 *)ptr) == sizeof(long long))) __compiletime_assert_158(); } while (0); do { *(volatile typeof(*(u8 *)ptr) *)&(*(u8 *)ptr) = ((u8)val); } while (0); } while (0); }
# 124 "./arch/arm64/include/asm/percpu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __percpu_read_16(void *ptr) { return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_159(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u16 *)ptr) == sizeof(char) || sizeof(*(u16 *)ptr) == sizeof(short) || sizeof(*(u16 *)ptr) == sizeof(int) || sizeof(*(u16 *)ptr) == sizeof(long)) || sizeof(*(u16 *)ptr) == sizeof(long long))) __compiletime_assert_159(); } while (0); (*(const volatile typeof( _Generic((*(u16 *)ptr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*(u16 *)ptr))) *)&(*(u16 *)ptr)); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_write_16(void *ptr, unsigned long val) { do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_160(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u16 *)ptr) == sizeof(char) || sizeof(*(u16 *)ptr) == sizeof(short) || sizeof(*(u16 *)ptr) == sizeof(int) || sizeof(*(u16 *)ptr) == sizeof(long)) || sizeof(*(u16 *)ptr) == sizeof(long long))) __compiletime_assert_160(); } while (0); do { *(volatile typeof(*(u16 *)ptr) *)&(*(u16 *)ptr) = ((u16)val); } while (0); } while (0); }
# 125 "./arch/arm64/include/asm/percpu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __percpu_read_32(void *ptr) { return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_161(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u32 *)ptr) == sizeof(char) || sizeof(*(u32 *)ptr) == sizeof(short) || sizeof(*(u32 *)ptr) == sizeof(int) || sizeof(*(u32 *)ptr) == sizeof(long)) || sizeof(*(u32 *)ptr) == sizeof(long long))) __compiletime_assert_161(); } while (0); (*(const volatile typeof( _Generic((*(u32 *)ptr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*(u32 *)ptr))) *)&(*(u32 *)ptr)); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_write_32(void *ptr, unsigned long val) { do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_162(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u32 *)ptr) == sizeof(char) || sizeof(*(u32 *)ptr) == sizeof(short) || sizeof(*(u32 *)ptr) == sizeof(int) || sizeof(*(u32 *)ptr) == sizeof(long)) || sizeof(*(u32 *)ptr) == sizeof(long long))) __compiletime_assert_162(); } while (0); do { *(volatile typeof(*(u32 *)ptr) *)&(*(u32 *)ptr) = ((u32)val); } while (0); } while (0); }
# 126 "./arch/arm64/include/asm/percpu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __percpu_read_64(void *ptr) { return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_163(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u64 *)ptr) == sizeof(char) || sizeof(*(u64 *)ptr) == sizeof(short) || sizeof(*(u64 *)ptr) == sizeof(int) || sizeof(*(u64 *)ptr) == sizeof(long)) || sizeof(*(u64 *)ptr) == sizeof(long long))) __compiletime_assert_163(); } while (0); (*(const volatile typeof( _Generic((*(u64 *)ptr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*(u64 *)ptr))) *)&(*(u64 *)ptr)); }); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_write_64(void *ptr, unsigned long val) { do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_164(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(u64 *)ptr) == sizeof(char) || sizeof(*(u64 *)ptr) == sizeof(short) || sizeof(*(u64 *)ptr) == sizeof(int) || sizeof(*(u64 *)ptr) == sizeof(long)) || sizeof(*(u64 *)ptr) == sizeof(long long))) __compiletime_assert_164(); } while (0); do { *(volatile typeof(*(u64 *)ptr) *)&(*(u64 *)ptr) = ((u64)val); } while (0); } while (0); }
# 127 "./arch/arm64/include/asm/percpu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_add_case_8(void *ptr, unsigned long val) { unsigned int loop; u8 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "b" "\t%" "w" "[tmp], %[ptr]\n" "add" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "b" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stadd" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u8 *)ptr) : [val] "r" ((u8)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_add_case_16(void *ptr, unsigned long val) { unsigned int loop; u16 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "h" "\t%" "w" "[tmp], %[ptr]\n" "add" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "h" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stadd" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u16 *)ptr) : [val] "r" ((u16)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_add_case_32(void *ptr, unsigned long val) { unsigned int loop; u32 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "w" "[tmp], %[ptr]\n" "add" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stadd" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u32 *)ptr) : [val] "r" ((u32)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_add_case_64(void *ptr, unsigned long val) { unsigned int loop; u64 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "" "[tmp], %[ptr]\n" "add" "\t%" "" "[tmp], %" "" "[tmp], %" "" "[val]\n" "	stxr" "" "\t%w[loop], %" "" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stadd" "\t%" "" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u64 *)ptr) : [val] "r" ((u64)(val))); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_andnot_case_8(void *ptr, unsigned long val) { unsigned int loop; u8 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "b" "\t%" "w" "[tmp], %[ptr]\n" "bic" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "b" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stclr" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u8 *)ptr) : [val] "r" ((u8)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_andnot_case_16(void *ptr, unsigned long val) { unsigned int loop; u16 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "h" "\t%" "w" "[tmp], %[ptr]\n" "bic" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "h" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stclr" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u16 *)ptr) : [val] "r" ((u16)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_andnot_case_32(void *ptr, unsigned long val) { unsigned int loop; u32 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "w" "[tmp], %[ptr]\n" "bic" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stclr" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u32 *)ptr) : [val] "r" ((u32)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_andnot_case_64(void *ptr, unsigned long val) { unsigned int loop; u64 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "" "[tmp], %[ptr]\n" "bic" "\t%" "" "[tmp], %" "" "[tmp], %" "" "[val]\n" "	stxr" "" "\t%w[loop], %" "" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stclr" "\t%" "" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u64 *)ptr) : [val] "r" ((u64)(val))); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_or_case_8(void *ptr, unsigned long val) { unsigned int loop; u8 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "b" "\t%" "w" "[tmp], %[ptr]\n" "orr" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "b" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stset" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u8 *)ptr) : [val] "r" ((u8)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_or_case_16(void *ptr, unsigned long val) { unsigned int loop; u16 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "h" "\t%" "w" "[tmp], %[ptr]\n" "orr" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "h" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stset" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u16 *)ptr) : [val] "r" ((u16)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_or_case_32(void *ptr, unsigned long val) { unsigned int loop; u32 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "w" "[tmp], %[ptr]\n" "orr" "\t%" "w" "[tmp], %" "w" "[tmp], %" "w" "[val]\n" "	stxr" "" "\t%w[loop], %" "w" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stset" "\t%" "w" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u32 *)ptr) : [val] "r" ((u32)(val))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __percpu_or_case_64(void *ptr, unsigned long val) { unsigned int loop; u64 tmp; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "" "[tmp], %[ptr]\n" "orr" "\t%" "" "[tmp], %" "" "[tmp], %" "" "[val]\n" "	stxr" "" "\t%w[loop], %" "" "[tmp], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "stset" "\t%" "" "[val], %[ptr]\n" ".rept	" "3" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [tmp] "=&r" (tmp), [ptr] "+Q"(*(u64 *)ptr) : [val] "r" ((u64)(val))); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __percpu_add_return_case_8(void *ptr, unsigned long val) { unsigned int loop; u8 ret; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "b" "\t%" "w" "[ret], %[ptr]\n" "add" "\t%" "w" "[ret], %" "w" "[ret], %" "w" "[val]\n" "	stxr" "b" "\t%w[loop], %" "w" "[ret], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "ldadd" "\t%" "w" "[val], %" "w" "[ret], %[ptr]\n" "add" "\t%" "w" "[ret], %" "w" "[ret], %" "w" "[val]\n" ".rept	" "2" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [ret] "=&r" (ret), [ptr] "+Q"(*(u8 *)ptr) : [val] "r" ((u8)(val))); return ret; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __percpu_add_return_case_16(void *ptr, unsigned long val) { unsigned int loop; u16 ret; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "h" "\t%" "w" "[ret], %[ptr]\n" "add" "\t%" "w" "[ret], %" "w" "[ret], %" "w" "[val]\n" "	stxr" "h" "\t%w[loop], %" "w" "[ret], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "ldadd" "\t%" "w" "[val], %" "w" "[ret], %[ptr]\n" "add" "\t%" "w" "[ret], %" "w" "[ret], %" "w" "[val]\n" ".rept	" "2" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [ret] "=&r" (ret), [ptr] "+Q"(*(u16 *)ptr) : [val] "r" ((u16)(val))); return ret; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __percpu_add_return_case_32(void *ptr, unsigned long val) { unsigned int loop; u32 ret; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "w" "[ret], %[ptr]\n" "add" "\t%" "w" "[ret], %" "w" "[ret], %" "w" "[val]\n" "	stxr" "" "\t%w[loop], %" "w" "[ret], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "ldadd" "\t%" "w" "[val], %" "w" "[ret], %[ptr]\n" "add" "\t%" "w" "[ret], %" "w" "[ret], %" "w" "[val]\n" ".rept	" "2" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [ret] "=&r" (ret), [ptr] "+Q"(*(u32 *)ptr) : [val] "r" ((u32)(val))); return ret; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __percpu_add_return_case_64(void *ptr, unsigned long val) { unsigned int loop; u64 ret; asm volatile (".if ""1"" == 1\n" "661:\n\t" /* LL/SC */ "1:	ldxr" "" "\t%" "" "[ret], %[ptr]\n" "add" "\t%" "" "[ret], %" "" "[ret], %" "" "[val]\n" "	stxr" "" "\t%w[loop], %" "" "[ret], %[ptr]\n" "	cbnz	%w[loop], 1b" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" /* LSE atomics */ "ldadd" "\t%" "" "[val], %" "" "[ret], %[ptr]\n" "add" "\t%" "" "[ret], %" "" "[ret], %" "" "[val]\n" ".rept	" "2" "\nnop\n.endr\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : [loop] "=&r" (loop), [ret] "=&r" (ret), [ptr] "+Q"(*(u64 *)ptr) : [val] "r" ((u64)(val))); return ret; }







/*
 * It would be nice to avoid the conditional call into the scheduler when
 * re-enabling preemption for preemptible kernels, but doing that in a way
 * which builds inside a module would mean messing directly with the preempt
 * count. If you do this, peterz and tglx will hunt you down.
 */
# 249 "./arch/arm64/include/asm/percpu.h"
# 1 "./include/asm-generic/percpu.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/threads.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




/*
 * The default limit for the nr of threads is now in
 * /proc/sys/kernel/threads-max.
 */

/*
 * Maximum supported processors.  Setting this smaller saves quite a
 * bit of memory.  Use nr_cpu_ids instead of this except for static bitmaps.
 */





/* Places which use this should consider cpumask_var_t. */




/*
 * This controls the default maximum pid allocated to a process
 */


/*
 * A maximum of 4 million PIDs should be enough for a while.
 * [NOTE: PID/TIDs are limited to 2^30 ~= 1 billion, see FUTEX_TID_MASK.]
 */



/*
 * Define a minimum number of pids per cpu.  Heuristically based
 * on original pid max of 32k for 32 cpus.  Also, increase the
 * minimum settable value for pid_max on the running system based
 * on similar defaults.  See kernel/pid.c:pid_idr_init() for details.
 */
# 7 "./include/asm-generic/percpu.h" 2
# 1 "./include/linux/percpu-defs.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * linux/percpu-defs.h - basic definitions for percpu areas
 *
 * DO NOT INCLUDE DIRECTLY OUTSIDE PERCPU IMPLEMENTATION PROPER.
 *
 * This file is separate from linux/percpu.h to avoid cyclic inclusion
 * dependency from arch header files.  Only to be included from
 * asm/percpu.h.
 *
 * This file includes macros necessary to declare percpu sections and
 * variables, and definitions of percpu accessors and operations.  It
 * should provide enough percpu features to arch header files even when
 * they can only include asm/percpu.h to avoid cyclic inclusion dependency.
 */
# 39 "./include/linux/percpu-defs.h"
/*
 * Base implementations of per-CPU variable declarations and definitions, where
 * the section in which the variable is to be placed is provided by the
 * 'sec' argument.  This may be used to affect the parameters governing the
 * variable's storage.
 *
 * NOTE!  The sections for the DECLARE and for the DEFINE must match, lest
 * linkage errors occur due the compiler generating the wrong code to access
 * that section.
 */







/*
 * s390 and alpha modules require percpu variables to be defined as
 * weak to force the compiler to generate GOT based external
 * references for them.  This is necessary because percpu sections
 * will be located outside of the usually addressable area.
 *
 * This definition puts the following two extra restrictions when
 * defining percpu variables.
 *
 * 1. The symbol must be globally unique, even the static ones.
 * 2. Static percpu variables cannot be defined inside a function.
 *
 * Archs which need weak percpu definitions should define
 * ARCH_NEEDS_WEAK_PER_CPU in asm/percpu.h when necessary.
 *
 * To ensure that the generic code observes the above two
 * restrictions, if CONFIG_DEBUG_FORCE_WEAK_PER_CPU is set weak
 * definition is used for all cases.
 */
# 97 "./include/linux/percpu-defs.h"
/*
 * Normal declaration and definition macros.
 */







/*
 * Variant on the per-CPU variable declaration/definition theme used for
 * ordinary per-CPU variables.
 */






/*
 * Declaration/definition used for per-CPU variables that must come first in
 * the set of variables.
 */






/*
 * Declaration/definition used for per-CPU variables that must be cacheline
 * aligned under SMP conditions so that, whilst a particular instance of the
 * data corresponds to a particular CPU, inefficiencies due to direct access by
 * other CPUs are reduced by preventing the data from unnecessarily spanning
 * cachelines.
 *
 * An example of this would be statistical data, where each CPU's set of data
 * is updated by that CPU alone, but the data from across all CPUs is collated
 * by a CPU processing a read from a proc file.
 */
# 154 "./include/linux/percpu-defs.h"
/*
 * Declaration/definition used for per-CPU variables that must be page aligned.
 */
# 165 "./include/linux/percpu-defs.h"
/*
 * Declaration/definition used for per-CPU variables that must be read mostly.
 */






/*
 * Declaration/definition used for per-CPU variables that should be accessed
 * as decrypted when memory encryption is enabled in the guest.
 */
# 188 "./include/linux/percpu-defs.h"
/*
 * Intermodule exports for per-CPU variables.  sparse forgets about
 * address space across EXPORT_SYMBOL(), change EXPORT_SYMBOL() to
 * noop if __CHECKER__.
 */
# 201 "./include/linux/percpu-defs.h"
/*
 * Accessors and operations.
 */


/*
 * __verify_pcpu_ptr() verifies @ptr is a percpu pointer without evaluating
 * @ptr and is invoked once before a percpu area is accessed by all
 * accessors and operations.  This is performed in the generic part of
 * percpu and arch overrides don't need to worry about it; however, if an
 * arch wants to implement an arch-specific percpu accessor or operation,
 * it may use __verify_pcpu_ptr() to verify the parameters.
 *
 * + 0 is required in order to convert the pointer type from a
 * potential array type to a pointer to a single item of the array.
 */
# 225 "./include/linux/percpu-defs.h"
/*
 * Add an offset to a pointer but keep the pointer as-is.  Use RELOC_HIDE()
 * to prevent the compiler from making incorrect assumptions about the
 * pointer value.  The weird cast keeps both GCC and sparse happy.
 */
# 271 "./include/linux/percpu-defs.h"
/*
 * Must be an lvalue. Since @var must be a simple identifier,
 * we force a syntax error here if it isn't.
 */






/*
 * The weird & is necessary because sparse considers (void)(var) to be
 * a direct dereference of percpu variable (var).
 */
# 303 "./include/linux/percpu-defs.h"
/*
 * Branching function to split up a function into a set of functions that
 * are called for different scalar sizes of the objects handled.
 */

extern void __bad_size_call_parameter(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __this_cpu_preempt_check(const char *op) { }
# 346 "./include/linux/percpu-defs.h"
/*
 * Special handling for cmpxchg_double.  cmpxchg_double is passed two
 * percpu variables.  The first has to be aligned to a double word
 * boundary and the second has to follow directly thereafter.
 * We enforce this on all architectures even if they don't support
 * a double cmpxchg instruction, since it's a cheap requirement, and it
 * avoids breaking the requirement for architectures with the instruction.
 */
# 386 "./include/linux/percpu-defs.h"
/*
 * this_cpu operations (C) 2008-2013 Christoph Lameter <cl@linux.com>
 *
 * Optimized manipulation for memory allocated through the per cpu
 * allocator or for addresses of per cpu variables.
 *
 * These operation guarantee exclusivity of access for other operations
 * on the *same* processor. The assumption is that per cpu data is only
 * accessed by a single processor instance (the current one).
 *
 * The arch code can provide optimized implementation by defining macros
 * for certain scalar sizes. F.e. provide this_cpu_add_2() to provide per
 * cpu atomic operations for 2 byte sized RMW actions. If arch code does
 * not provide operations for a scalar size then the fallback in the
 * generic code will be used.
 *
 * cmpxchg_double replaces two adjacent scalars at once.  The first two
 * parameters are per cpu variables which have to be of the same size.  A
 * truth value is returned to indicate success or failure (since a double
 * register result is difficult to handle).  There is very limited hardware
 * support for these operations, so only certain sizes may work.
 */

/*
 * Operations for contexts where we do not want to do any checks for
 * preemptions.  Unless strictly necessary, always use [__]this_cpu_*()
 * instead.
 *
 * If there is no other protection through preempt disable and/or disabling
 * interrupts then one of these RMW operations can show unexpected behavior
 * because the execution thread was rescheduled on another processor or an
 * interrupt occurred and the same percpu variable was modified from the
 * interrupt context.
 */
# 439 "./include/linux/percpu-defs.h"
/*
 * Operations for contexts that are safe from preemption/interrupts.  These
 * operations verify that preemption is disabled.
 */
# 503 "./include/linux/percpu-defs.h"
/*
 * Operations with implied preemption/interrupt protection.  These
 * operations can be used without worrying about preemption or interrupt.
 */
# 8 "./include/asm-generic/percpu.h" 2



/*
 * per_cpu_offset() is the offset that has to be added to a
 * percpu variable to get to the instance for a certain processor.
 *
 * Most arches use the __per_cpu_offset array for those offsets but
 * some arches have their own ways of determining the offset (x86_64, s390).
 */

extern unsigned long __per_cpu_offset[256];




/*
 * Determine the offset for the currently active processor.
 * An arch may define __my_cpu_offset to provide a more effective
 * means of obtaining the offset to the per cpu variables of the
 * current processor.
 */
# 39 "./include/asm-generic/percpu.h"
/*
 * Arch may define arch_raw_cpu_ptr() to provide more efficient address
 * translations for raw_cpu_ptr().
 */





extern void setup_per_cpu_areas(void);
# 250 "./arch/arm64/include/asm/percpu.h" 2

/* Redefine macros for nVHE hyp under DEBUG_PREEMPT to avoid its dependencies. */
# 18 "./include/linux/irqflags.h" 2

/* Currently lockdep_softirqs_on/off is used only by lockdep */







  static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_softirqs_on(unsigned long ip) { }
  static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_softirqs_off(unsigned long ip) { }
  static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_hardirqs_on_prepare(void) { }
  static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_hardirqs_on(unsigned long ip) { }
  static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_hardirqs_off(unsigned long ip) { }
# 170 "./include/linux/irqflags.h"
/*
 * Wrap the arch provided IRQ routines to provide appropriate checks.
 */
# 199 "./include/linux/irqflags.h"
/*
 * The local_irq_*() APIs are equal to the raw_local_irq*()
 * if !TRACE_IRQFLAGS.
 */
# 252 "./include/linux/irqflags.h"
/*
 * Some architectures don't define arch_irqs_disabled(), so even if either
 * definition would be fine we need to use different ones for the time being
 * to avoid build issues.
 */
# 60 "./include/linux/spinlock.h" 2


# 1 "./include/linux/bottom_half.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./include/linux/bottom_half.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
{
 __preempt_count_add(cnt);
 __asm__ __volatile__("": : :"memory");
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void local_bh_disable(void)
{
 __local_bh_disable_ip(({ __label__ __here; __here: (unsigned long)&&__here; }), (2 * (1UL << (0 + 8))));
}

extern void _local_bh_enable(void);
extern void __local_bh_enable_ip(unsigned long ip, unsigned int cnt);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void local_bh_enable_ip(unsigned long ip)
{
 __local_bh_enable_ip(ip, (2 * (1UL << (0 + 8))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void local_bh_enable(void)
{
 __local_bh_enable_ip(({ __label__ __here; __here: (unsigned long)&&__here; }), (2 * (1UL << (0 + 8))));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool local_bh_blocked(void) { return false; }
# 63 "./include/linux/spinlock.h" 2
# 1 "./include/linux/lockdep.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
 *
 * see Documentation/locking/lockdep-design.rst for more details.
 */




# 1 "./include/linux/smp.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 *	Generic SMP support
 *		Alan Cox. <alan@redhat.com>
 */




# 1 "./include/linux/cpumask.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Cpumasks provide a bitmap suitable for representing the
 * set of CPU's in a system, one bit position per CPU number.  In general,
 * only nr_cpu_ids (<= NR_CPUS) bits are valid.
 */


# 1 "./include/linux/bitmap.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







# 1 "./include/linux/find.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./include/linux/find.h"
unsigned long _find_next_bit(const unsigned long *addr1, unsigned long nbits,
    unsigned long start);
unsigned long _find_next_and_bit(const unsigned long *addr1, const unsigned long *addr2,
     unsigned long nbits, unsigned long start);
unsigned long _find_next_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
     unsigned long nbits, unsigned long start);
unsigned long _find_next_zero_bit(const unsigned long *addr, unsigned long nbits,
      unsigned long start);
extern unsigned long _find_first_bit(const unsigned long *addr, unsigned long size);
unsigned long __find_nth_bit(const unsigned long *addr, unsigned long size, unsigned long n);
unsigned long __find_nth_and_bit(const unsigned long *addr1, const unsigned long *addr2,
    unsigned long size, unsigned long n);
unsigned long __find_nth_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
     unsigned long size, unsigned long n);
extern unsigned long _find_first_and_bit(const unsigned long *addr1,
      const unsigned long *addr2, unsigned long size);
extern unsigned long _find_first_zero_bit(const unsigned long *addr, unsigned long size);
extern unsigned long _find_last_bit(const unsigned long *addr, unsigned long size);
# 39 "./include/linux/find.h"
/**
 * find_next_bit - find the next set bit in a memory region
 * @addr: The address to base the search on
 * @size: The bitmap size in bits
 * @offset: The bitnumber to start searching at
 *
 * Returns the bit number for the next set bit
 * If no bits are set, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_next_bit(const unsigned long *addr, unsigned long size,
       unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_bit(addr, size, offset);
}



/**
 * find_next_and_bit - find the next set bit in both memory regions
 * @addr1: The first address to base the search on
 * @addr2: The second address to base the search on
 * @size: The bitmap size in bits
 * @offset: The bitnumber to start searching at
 *
 * Returns the bit number for the next set bit
 * If no bits are set, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_next_and_bit(const unsigned long *addr1,
  const unsigned long *addr2, unsigned long size,
  unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr1 & *addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_and_bit(addr1, addr2, size, offset);
}



/**
 * find_next_andnot_bit - find the next set bit in *addr1 excluding all the bits
 *                        in *addr2
 * @addr1: The first address to base the search on
 * @addr2: The second address to base the search on
 * @size: The bitmap size in bits
 * @offset: The bitnumber to start searching at
 *
 * Returns the bit number for the next set bit
 * If no bits are set, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_next_andnot_bit(const unsigned long *addr1,
  const unsigned long *addr2, unsigned long size,
  unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr1 & ~*addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_andnot_bit(addr1, addr2, size, offset);
}



/**
 * find_next_zero_bit - find the next cleared bit in a memory region
 * @addr: The address to base the search on
 * @size: The bitmap size in bits
 * @offset: The bitnumber to start searching at
 *
 * Returns the bit number of the next zero bit
 * If no bits are zero, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
     unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr | ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val == ~0UL ? size : __ffs(~(val));
 }

 return _find_next_zero_bit(addr, size, offset);
}



/**
 * find_first_bit - find the first set bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum number of bits to search
 *
 * Returns the bit number of the first set bit.
 * If no bits are set, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_first_bit(const unsigned long *addr, unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? __ffs(val) : size;
 }

 return _find_first_bit(addr, size);
}


/**
 * find_nth_bit - find N'th set bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum number of bits to search
 * @n: The number of set bit, which position is needed, counting from 0
 *
 * The following is semantically equivalent:
 *	 idx = find_nth_bit(addr, size, 0);
 *	 idx = find_first_bit(addr, size);
 *
 * Returns the bit number of the N'th set bit.
 * If no such, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_nth_bit(const unsigned long *addr, unsigned long size, unsigned long n)
{
 if (n >= size)
  return size;

 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? fns(val, n) : size;
 }

 return __find_nth_bit(addr, size, n);
}

/**
 * find_nth_and_bit - find N'th set bit in 2 memory regions
 * @addr1: The 1st address to start the search at
 * @addr2: The 2nd address to start the search at
 * @size: The maximum number of bits to search
 * @n: The number of set bit, which position is needed, counting from 0
 *
 * Returns the bit number of the N'th set bit.
 * If no such, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_nth_and_bit(const unsigned long *addr1, const unsigned long *addr2,
    unsigned long size, unsigned long n)
{
 if (n >= size)
  return size;

 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr1 & *addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? fns(val, n) : size;
 }

 return __find_nth_and_bit(addr1, addr2, size, n);
}

/**
 * find_nth_andnot_bit - find N'th set bit in 2 memory regions,
 *			 flipping bits in 2nd region
 * @addr1: The 1st address to start the search at
 * @addr2: The 2nd address to start the search at
 * @size: The maximum number of bits to search
 * @n: The number of set bit, which position is needed, counting from 0
 *
 * Returns the bit number of the N'th set bit.
 * If no such, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_nth_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
    unsigned long size, unsigned long n)
{
 if (n >= size)
  return size;

 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr1 & (~*addr2) & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? fns(val, n) : size;
 }

 return __find_nth_andnot_bit(addr1, addr2, size, n);
}


/**
 * find_first_and_bit - find the first set bit in both memory regions
 * @addr1: The first address to base the search on
 * @addr2: The second address to base the search on
 * @size: The bitmap size in bits
 *
 * Returns the bit number for the next set bit
 * If no bits are set, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_first_and_bit(const unsigned long *addr1,
     const unsigned long *addr2,
     unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr1 & *addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? __ffs(val) : size;
 }

 return _find_first_and_bit(addr1, addr2, size);
}



/**
 * find_first_zero_bit - find the first cleared bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum number of bits to search
 *
 * Returns the bit number of the first cleared bit.
 * If no bits are zero, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_first_zero_bit(const unsigned long *addr, unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr | ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val == ~0UL ? size : __ffs(~(val));
 }

 return _find_first_zero_bit(addr, size);
}



/**
 * find_last_bit - find the last set bit in a memory region
 * @addr: The address to start the search at
 * @size: The number of bits to search
 *
 * Returns the bit number of the last set bit, or size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_last_bit(const unsigned long *addr, unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? __fls(val) : size;
 }

 return _find_last_bit(addr, size);
}


/**
 * find_next_and_bit_wrap - find the next set bit in both memory regions
 * @addr1: The first address to base the search on
 * @addr2: The second address to base the search on
 * @size: The bitmap size in bits
 * @offset: The bitnumber to start searching at
 *
 * Returns the bit number for the next set bit, or first set bit up to @offset
 * If no bits are set, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_next_and_bit_wrap(const unsigned long *addr1,
     const unsigned long *addr2,
     unsigned long size, unsigned long offset)
{
 unsigned long bit = find_next_and_bit(addr1, addr2, size, offset);

 if (bit < size)
  return bit;

 bit = find_first_and_bit(addr1, addr2, offset);
 return bit < offset ? bit : size;
}

/**
 * find_next_bit_wrap - find the next set bit in both memory regions
 * @addr: The first address to base the search on
 * @size: The bitmap size in bits
 * @offset: The bitnumber to start searching at
 *
 * Returns the bit number for the next set bit, or first set bit up to @offset
 * If no bits are set, returns @size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long find_next_bit_wrap(const unsigned long *addr,
     unsigned long size, unsigned long offset)
{
 unsigned long bit = find_next_bit(addr, size, offset);

 if (bit < size)
  return bit;

 bit = find_first_bit(addr, offset);
 return bit < offset ? bit : size;
}

/*
 * Helper for for_each_set_bit_wrap(). Make sure you're doing right thing
 * before using it alone.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long __for_each_wrap(const unsigned long *bitmap, unsigned long size,
     unsigned long start, unsigned long n)
{
 unsigned long bit;

 /* If not wrapped around */
 if (n > start) {
  /* and have a bit, just return it. */
  bit = find_next_bit(bitmap, size, n);
  if (bit < size)
   return bit;

  /* Otherwise, wrap around and ... */
  n = 0;
 }

 /* Search the other part. */
 bit = find_next_bit(bitmap, start, n);
 return bit < start ? bit : size;
}

/**
 * find_next_clump8 - find next 8-bit clump with set bits in a memory region
 * @clump: location to store copy of found clump
 * @addr: address to base the search on
 * @size: bitmap size in number of bits
 * @offset: bit offset at which to start searching
 *
 * Returns the bit offset for the next set clump; the found clump value is
 * copied to the location pointed by @clump. If no bits are set, returns @size.
 */
extern unsigned long find_next_clump8(unsigned long *clump,
          const unsigned long *addr,
          unsigned long size, unsigned long offset);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long find_next_zero_bit_le(const void *addr,
  unsigned long size, unsigned long offset)
{
 return find_next_zero_bit(addr, size, offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long find_next_bit_le(const void *addr,
  unsigned long size, unsigned long offset)
{
 return find_next_bit(addr, size, offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long find_first_zero_bit_le(const void *addr,
  unsigned long size)
{
 return find_first_zero_bit(addr, size);
}
# 506 "./include/linux/find.h"
/* same as for_each_set_bit() but use bit as value to start with */
# 515 "./include/linux/find.h"
/* same as for_each_clear_bit() but use bit as value to start with */



/**
 * for_each_set_bitrange - iterate over all set bit ranges [b; e)
 * @b: bit offset of start of current bitrange (first set bit)
 * @e: bit offset of end of current bitrange (first unset bit)
 * @addr: bitmap address to base the search on
 * @size: bitmap size in number of bits
 */







/**
 * for_each_set_bitrange_from - iterate over all set bit ranges [b; e)
 * @b: bit offset of start of current bitrange (first set bit); must be initialized
 * @e: bit offset of end of current bitrange (first unset bit)
 * @addr: bitmap address to base the search on
 * @size: bitmap size in number of bits
 */







/**
 * for_each_clear_bitrange - iterate over all unset bit ranges [b; e)
 * @b: bit offset of start of current bitrange (first unset bit)
 * @e: bit offset of end of current bitrange (first set bit)
 * @addr: bitmap address to base the search on
 * @size: bitmap size in number of bits
 */







/**
 * for_each_clear_bitrange_from - iterate over all unset bit ranges [b; e)
 * @b: bit offset of start of current bitrange (first set bit); must be initialized
 * @e: bit offset of end of current bitrange (first unset bit)
 * @addr: bitmap address to base the search on
 * @size: bitmap size in number of bits
 */







/**
 * for_each_set_bit_wrap - iterate over all set bits starting from @start, and
 * wrapping around the end of bitmap.
 * @bit: offset for current iteration
 * @addr: bitmap address to base the search on
 * @size: bitmap size in number of bits
 * @start: Starting bit for bitmap traversing, wrapping around the bitmap end
 */





/**
 * for_each_set_clump8 - iterate over bitmap for each 8-bit clump with set bits
 * @start: bit offset to start search and to store the current iteration offset
 * @clump: location to store copy of current 8-bit clump
 * @bits: bitmap address to base the search on
 * @size: bitmap size in number of bits
 */
# 10 "./include/linux/bitmap.h" 2

# 1 "./include/linux/string.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/string.h"
# 1 "./include/uapi/linux/string.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/* We don't want strings.h stuff being used by user stuff by accident */
# 11 "./include/linux/string.h" 2

extern char *strndup_user(const char /* nothing */ *, long);
extern void *memdup_user(const void /* nothing */ *, size_t);
extern void *vmemdup_user(const void /* nothing */ *, size_t);
extern void *memdup_user_nul(const void /* nothing */ *, size_t);

/*
 * Include machine specific inline routines
 */
# 1 "./arch/arm64/include/asm/string.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2013 ARM Ltd.
 */





extern char *strrchr(const char *, int c);


extern char *strchr(const char *, int c);


extern int strcmp(const char *, const char *);


extern int strncmp(const char *, const char *, __kernel_size_t);


extern __kernel_size_t strlen(const char *);


extern __kernel_size_t strnlen(const char *, __kernel_size_t);


extern int memcmp(const void *, const void *, size_t);


extern void *memchr(const void *, int, __kernel_size_t);



extern void *memcpy(void *, const void *, __kernel_size_t);
extern void *__memcpy(void *, const void *, __kernel_size_t);


extern void *memmove(void *, const void *, __kernel_size_t);
extern void *__memmove(void *, const void *, __kernel_size_t);


extern void *memset(void *, int, __kernel_size_t);
extern void *__memset(void *, int, __kernel_size_t);
# 21 "./include/linux/string.h" 2


extern char * strcpy(char *,const char *);


extern char * strncpy(char *,const char *, __kernel_size_t);


size_t strlcpy(char *, const char *, size_t);


ssize_t strscpy(char *, const char *, size_t);


/* Wraps calls to strscpy()/memset(), no arch specific code required */
ssize_t strscpy_pad(char *dest, const char *src, size_t count);


extern char * strcat(char *, const char *);


extern char * strncat(char *, const char *, __kernel_size_t);


extern size_t strlcat(char *, const char *, __kernel_size_t);
# 54 "./include/linux/string.h"
extern int strcasecmp(const char *s1, const char *s2);


extern int strncasecmp(const char *s1, const char *s2, size_t n);





extern char * strchrnul(const char *,int);

extern char * strnchrnul(const char *, size_t, int);

extern char * strnchr(const char *, size_t, int);




extern char * __attribute__((__warn_unused_result__)) skip_spaces(const char *);

extern char *strim(char *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) char *strstrip(char *str)
{
 return strim(str);
}


extern char * strstr(const char *, const char *);


extern char * strnstr(const char *, const char *, size_t);
# 94 "./include/linux/string.h"
extern char * strpbrk(const char *,const char *);


extern char * strsep(char **,const char *);


extern __kernel_size_t strspn(const char *,const char *);


extern __kernel_size_t strcspn(const char *,const char *);







extern void *memset16(uint16_t *, uint16_t, __kernel_size_t);



extern void *memset32(uint32_t *, uint32_t, __kernel_size_t);



extern void *memset64(uint64_t *, uint64_t, __kernel_size_t);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *memset_l(unsigned long *p, unsigned long v,
  __kernel_size_t n)
{
 if (64 == 32)
  return memset32((uint32_t *)p, v, n);
 else
  return memset64((uint64_t *)p, v, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *memset_p(void **p, void *v, __kernel_size_t n)
{
 if (64 == 32)
  return memset32((uint32_t *)p, (uintptr_t)v, n);
 else
  return memset64((uint64_t *)p, (uintptr_t)v, n);
}

extern void **__memcat_p(void **a, void **b);
# 153 "./include/linux/string.h"
extern void * memscan(void *,int,__kernel_size_t);





extern int bcmp(const void *,const void *,__kernel_size_t);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memcpy_flushcache(void *dst, const void *src, size_t cnt)
{
 memcpy(dst, src, cnt);
}


void *memchr_inv(const void *s, int c, size_t n);
char *strreplace(char *s, char old, char new);

extern void kfree_const(const void *x);

extern char *kstrdup(const char *s, gfp_t gfp) __attribute__((__malloc__));
extern const char *kstrdup_const(const char *s, gfp_t gfp);
extern char *kstrndup(const char *s, size_t len, gfp_t gfp);
extern void *kmemdup(const void *src, size_t len, gfp_t gfp) __attribute__((__alloc_size__(2)));
extern char *kmemdup_nul(const char *s, size_t len, gfp_t gfp);

extern char **argv_split(gfp_t gfp, const char *str, int *argcp);
extern void argv_free(char **argv);

extern bool sysfs_streq(const char *s1, const char *s2);
int match_string(const char * const *array, size_t n, const char *string);
int __sysfs_match_string(const char * const *array, size_t n, const char *s);

/**
 * sysfs_match_string - matches given string in an array
 * @_a: array of strings
 * @_s: string to match with
 *
 * Helper for __sysfs_match_string(). Calculates the size of @a automatically.
 */



int vbin_printf(u32 *bin_buf, size_t size, const char *fmt, va_list args);
int bstr_printf(char *buf, size_t size, const char *fmt, const u32 *bin_buf);
int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...) __attribute__((__format__(printf, 3, 4)));


extern ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
           const void *from, size_t available);

int ptr_to_hashval(const void *ptr, unsigned long *hashval_out);

/**
 * strstarts - does @str start with @prefix?
 * @str: string to examine
 * @prefix: prefix to look for.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool strstarts(const char *str, const char *prefix)
{
 return strncmp(str, prefix, strlen(prefix)) == 0;
}

size_t memweight(const void *ptr, size_t bytes);

/**
 * memzero_explicit - Fill a region of memory (e.g. sensitive
 *		      keying data) with 0s.
 * @s: Pointer to the start of the area.
 * @count: The size of the area.
 *
 * Note: usually using memset() is just fine (!), but in cases
 * where clearing out _local_ data at the end of a scope is
 * necessary, memzero_explicit() should be used instead in
 * order to prevent the compiler from optimising away zeroing.
 *
 * memzero_explicit() doesn't need an arch-specific version as
 * it just invokes the one of memset() implicitly.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memzero_explicit(void *s, size_t count)
{
 memset(s, 0, count);
 __asm__ __volatile__("": :"r"(s) :"memory");
}

/**
 * kbasename - return the last part of a pathname.
 *
 * @path: path to extract the filename from.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *kbasename(const char *path)
{
 const char *tail = strrchr(path, '/');
 return tail ? tail + 1 : path;
}
# 260 "./include/linux/string.h"
void memcpy_and_pad(void *dest, size_t dest_len, const void *src, size_t count,
      int pad);

/**
 * strtomem_pad - Copy NUL-terminated string to non-NUL-terminated buffer
 *
 * @dest: Pointer of destination character array (marked as __nonstring)
 * @src: Pointer to NUL-terminated string
 * @pad: Padding character to fill any remaining bytes of @dest after copy
 *
 * This is a replacement for strncpy() uses where the destination is not
 * a NUL-terminated string, but with bounds checking on the source size, and
 * an explicit padding character. If padding is not required, use strtomem().
 *
 * Note that the size of @dest is not an argument, as the length of @dest
 * must be discoverable by the compiler.
 */
# 285 "./include/linux/string.h"
/**
 * strtomem - Copy NUL-terminated string to non-NUL-terminated buffer
 *
 * @dest: Pointer of destination character array (marked as __nonstring)
 * @src: Pointer to NUL-terminated string
 *
 * This is a replacement for strncpy() uses where the destination is not
 * a NUL-terminated string, but with bounds checking on the source size, and
 * without trailing padding. If padding is required, use strtomem_pad().
 *
 * Note that the size of @dest is not an argument, as the length of @dest
 * must be discoverable by the compiler.
 */
# 306 "./include/linux/string.h"
/**
 * memset_after - Set a value after a struct member to the end of a struct
 *
 * @obj: Address of target struct instance
 * @v: Byte value to repeatedly write
 * @member: after which struct member to start writing bytes
 *
 * This is good for clearing padding following the given member.
 */
# 323 "./include/linux/string.h"
/**
 * memset_startat - Set a value starting at a member to the end of a struct
 *
 * @obj: Address of target struct instance
 * @v: Byte value to repeatedly write
 * @member: struct member to start writing at
 *
 * Note that if there is padding between the prior member and the target
 * member, memset_after() should be used to clear the prior padding.
 */
# 341 "./include/linux/string.h"
/**
 * str_has_prefix - Test if a string has a given prefix
 * @str: The string to test
 * @prefix: The string to see if @str starts with
 *
 * A common way to test a prefix of a string is to do:
 *  strncmp(str, prefix, sizeof(prefix) - 1)
 *
 * But this can lead to bugs due to typos, or if prefix is a pointer
 * and not a constant. Instead use str_has_prefix().
 *
 * Returns:
 * * strlen(@prefix) if @str starts with @prefix
 * * 0 if @str does not start with @prefix
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) size_t str_has_prefix(const char *str, const char *prefix)
{
 size_t len = strlen(prefix);
 return strncmp(str, prefix, len) == 0 ? len : 0;
}
# 12 "./include/linux/bitmap.h" 2


struct device;

/*
 * bitmaps provide bit arrays that consume one or more unsigned
 * longs.  The bitmap interface and available operations are listed
 * here, in bitmap.h
 *
 * Function implementations generic to all architectures are in
 * lib/bitmap.c.  Functions implementations that are architecture
 * specific are in various include/asm-<arch>/bitops.h headers
 * and other arch/<arch> specific files.
 *
 * See lib/bitmap.c for more details.
 */

/**
 * DOC: bitmap overview
 *
 * The available bitmap operations and their rough meaning in the
 * case that the bitmap is a single unsigned long are thus:
 *
 * The generated code is more efficient when nbits is known at
 * compile-time and at most BITS_PER_LONG.
 *
 * ::
 *
 *  bitmap_zero(dst, nbits)                     *dst = 0UL
 *  bitmap_fill(dst, nbits)                     *dst = ~0UL
 *  bitmap_copy(dst, src, nbits)                *dst = *src
 *  bitmap_and(dst, src1, src2, nbits)          *dst = *src1 & *src2
 *  bitmap_or(dst, src1, src2, nbits)           *dst = *src1 | *src2
 *  bitmap_xor(dst, src1, src2, nbits)          *dst = *src1 ^ *src2
 *  bitmap_andnot(dst, src1, src2, nbits)       *dst = *src1 & ~(*src2)
 *  bitmap_complement(dst, src, nbits)          *dst = ~(*src)
 *  bitmap_equal(src1, src2, nbits)             Are *src1 and *src2 equal?
 *  bitmap_intersects(src1, src2, nbits)        Do *src1 and *src2 overlap?
 *  bitmap_subset(src1, src2, nbits)            Is *src1 a subset of *src2?
 *  bitmap_empty(src, nbits)                    Are all bits zero in *src?
 *  bitmap_full(src, nbits)                     Are all bits set in *src?
 *  bitmap_weight(src, nbits)                   Hamming Weight: number set bits
 *  bitmap_weight_and(src1, src2, nbits)        Hamming Weight of and'ed bitmap
 *  bitmap_set(dst, pos, nbits)                 Set specified bit area
 *  bitmap_clear(dst, pos, nbits)               Clear specified bit area
 *  bitmap_find_next_zero_area(buf, len, pos, n, mask)  Find bit free area
 *  bitmap_find_next_zero_area_off(buf, len, pos, n, mask, mask_off)  as above
 *  bitmap_shift_right(dst, src, n, nbits)      *dst = *src >> n
 *  bitmap_shift_left(dst, src, n, nbits)       *dst = *src << n
 *  bitmap_cut(dst, src, first, n, nbits)       Cut n bits from first, copy rest
 *  bitmap_replace(dst, old, new, mask, nbits)  *dst = (*old & ~(*mask)) | (*new & *mask)
 *  bitmap_remap(dst, src, old, new, nbits)     *dst = map(old, new)(src)
 *  bitmap_bitremap(oldbit, old, new, nbits)    newbit = map(old, new)(oldbit)
 *  bitmap_onto(dst, orig, relmap, nbits)       *dst = orig relative to relmap
 *  bitmap_fold(dst, orig, sz, nbits)           dst bits = orig bits mod sz
 *  bitmap_parse(buf, buflen, dst, nbits)       Parse bitmap dst from kernel buf
 *  bitmap_parse_user(ubuf, ulen, dst, nbits)   Parse bitmap dst from user buf
 *  bitmap_parselist(buf, dst, nbits)           Parse bitmap dst from kernel buf
 *  bitmap_parselist_user(buf, dst, nbits)      Parse bitmap dst from user buf
 *  bitmap_find_free_region(bitmap, bits, order)  Find and allocate bit region
 *  bitmap_release_region(bitmap, pos, order)   Free specified bit region
 *  bitmap_allocate_region(bitmap, pos, order)  Allocate specified bit region
 *  bitmap_from_arr32(dst, buf, nbits)          Copy nbits from u32[] buf to dst
 *  bitmap_from_arr64(dst, buf, nbits)          Copy nbits from u64[] buf to dst
 *  bitmap_to_arr32(buf, src, nbits)            Copy nbits from buf to u32[] dst
 *  bitmap_to_arr64(buf, src, nbits)            Copy nbits from buf to u64[] dst
 *  bitmap_get_value8(map, start)               Get 8bit value from map at start
 *  bitmap_set_value8(map, value, start)        Set 8bit value to map at start
 *
 * Note, bitmap_zero() and bitmap_fill() operate over the region of
 * unsigned longs, that is, bits behind bitmap till the unsigned long
 * boundary will be zeroed or filled as well. Consider to use
 * bitmap_clear() or bitmap_set() to make explicit zeroing or filling
 * respectively.
 */

/**
 * DOC: bitmap bitops
 *
 * Also the following operations in asm/bitops.h apply to bitmaps.::
 *
 *  set_bit(bit, addr)                  *addr |= bit
 *  clear_bit(bit, addr)                *addr &= ~bit
 *  change_bit(bit, addr)               *addr ^= bit
 *  test_bit(bit, addr)                 Is bit set in *addr?
 *  test_and_set_bit(bit, addr)         Set bit and return old value
 *  test_and_clear_bit(bit, addr)       Clear bit and return old value
 *  test_and_change_bit(bit, addr)      Change bit and return old value
 *  find_first_zero_bit(addr, nbits)    Position first zero bit in *addr
 *  find_first_bit(addr, nbits)         Position first set bit in *addr
 *  find_next_zero_bit(addr, nbits, bit)
 *                                      Position next zero bit in *addr >= bit
 *  find_next_bit(addr, nbits, bit)     Position next set bit in *addr >= bit
 *  find_next_and_bit(addr1, addr2, nbits, bit)
 *                                      Same as find_next_bit, but in
 *                                      (*addr1 & *addr2)
 *
 */

/**
 * DOC: declare bitmap
 * The DECLARE_BITMAP(name,bits) macro, in linux/types.h, can be used
 * to declare an array named 'name' of just enough unsigned longs to
 * contain all bit positions from 0 to 'bits' - 1.
 */

/*
 * Allocation and deallocation of bitmap.
 * Provided in lib/bitmap.c to avoid circular dependency.
 */
unsigned long *bitmap_alloc(unsigned int nbits, gfp_t flags);
unsigned long *bitmap_zalloc(unsigned int nbits, gfp_t flags);
unsigned long *bitmap_alloc_node(unsigned int nbits, gfp_t flags, int node);
unsigned long *bitmap_zalloc_node(unsigned int nbits, gfp_t flags, int node);
void bitmap_free(const unsigned long *bitmap);

/* Managed variants of the above. */
unsigned long *devm_bitmap_alloc(struct device *dev,
     unsigned int nbits, gfp_t flags);
unsigned long *devm_bitmap_zalloc(struct device *dev,
      unsigned int nbits, gfp_t flags);

/*
 * lib/bitmap.c provides these functions:
 */

bool __bitmap_equal(const unsigned long *bitmap1,
      const unsigned long *bitmap2, unsigned int nbits);
bool __attribute__((__pure__)) __bitmap_or_equal(const unsigned long *src1,
         const unsigned long *src2,
         const unsigned long *src3,
         unsigned int nbits);
void __bitmap_complement(unsigned long *dst, const unsigned long *src,
    unsigned int nbits);
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
     unsigned int shift, unsigned int nbits);
void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
    unsigned int shift, unsigned int nbits);
void bitmap_cut(unsigned long *dst, const unsigned long *src,
  unsigned int first, unsigned int cut, unsigned int nbits);
bool __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
   const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
   const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
    const unsigned long *bitmap2, unsigned int nbits);
bool __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
      const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_replace(unsigned long *dst,
        const unsigned long *old, const unsigned long *new,
        const unsigned long *mask, unsigned int nbits);
bool __bitmap_intersects(const unsigned long *bitmap1,
    const unsigned long *bitmap2, unsigned int nbits);
bool __bitmap_subset(const unsigned long *bitmap1,
       const unsigned long *bitmap2, unsigned int nbits);
unsigned int __bitmap_weight(const unsigned long *bitmap, unsigned int nbits);
unsigned int __bitmap_weight_and(const unsigned long *bitmap1,
     const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_set(unsigned long *map, unsigned int start, int len);
void __bitmap_clear(unsigned long *map, unsigned int start, int len);

unsigned long bitmap_find_next_zero_area_off(unsigned long *map,
          unsigned long size,
          unsigned long start,
          unsigned int nr,
          unsigned long align_mask,
          unsigned long align_offset);

/**
 * bitmap_find_next_zero_area - find a contiguous aligned zero area
 * @map: The address to base the search on
 * @size: The bitmap size in bits
 * @start: The bitnumber to start searching at
 * @nr: The number of zeroed bits we're looking for
 * @align_mask: Alignment mask for zero area
 *
 * The @align_mask should be one less than a power of 2; the effect is that
 * the bit offset of all zero areas this function finds is multiples of that
 * power of 2. A @align_mask of 0 means no alignment is required.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
bitmap_find_next_zero_area(unsigned long *map,
      unsigned long size,
      unsigned long start,
      unsigned int nr,
      unsigned long align_mask)
{
 return bitmap_find_next_zero_area_off(map, size, start, nr,
           align_mask, 0);
}

int bitmap_parse(const char *buf, unsigned int buflen,
   unsigned long *dst, int nbits);
int bitmap_parse_user(const char /* nothing */ *ubuf, unsigned int ulen,
   unsigned long *dst, int nbits);
int bitmap_parselist(const char *buf, unsigned long *maskp,
   int nmaskbits);
int bitmap_parselist_user(const char /* nothing */ *ubuf, unsigned int ulen,
   unsigned long *dst, int nbits);
void bitmap_remap(unsigned long *dst, const unsigned long *src,
  const unsigned long *old, const unsigned long *new, unsigned int nbits);
int bitmap_bitremap(int oldbit,
  const unsigned long *old, const unsigned long *new, int bits);
void bitmap_onto(unsigned long *dst, const unsigned long *orig,
  const unsigned long *relmap, unsigned int bits);
void bitmap_fold(unsigned long *dst, const unsigned long *orig,
  unsigned int sz, unsigned int nbits);
int bitmap_find_free_region(unsigned long *bitmap, unsigned int bits, int order);
void bitmap_release_region(unsigned long *bitmap, unsigned int pos, int order);
int bitmap_allocate_region(unsigned long *bitmap, unsigned int pos, int order);






int bitmap_print_to_pagebuf(bool list, char *buf,
       const unsigned long *maskp, int nmaskbits);

extern int bitmap_print_bitmask_to_buf(char *buf, const unsigned long *maskp,
          int nmaskbits, loff_t off, size_t count);

extern int bitmap_print_list_to_buf(char *buf, const unsigned long *maskp,
          int nmaskbits, loff_t off, size_t count);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_zero(unsigned long *dst, unsigned int nbits)
{
 unsigned int len = (((nbits) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(unsigned long);

 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = 0;
 else
  memset(dst, 0, len);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_fill(unsigned long *dst, unsigned int nbits)
{
 unsigned int len = (((nbits) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(unsigned long);

 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = ~0UL;
 else
  memset(dst, 0xff, len);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_copy(unsigned long *dst, const unsigned long *src,
   unsigned int nbits)
{
 unsigned int len = (((nbits) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(unsigned long);

 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = *src;
 else
  memcpy(dst, src, len);
}

/*
 * Copy bitmap and clear tail bits in last word.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_copy_clear_tail(unsigned long *dst,
  const unsigned long *src, unsigned int nbits)
{
 bitmap_copy(dst, src, nbits);
 if (nbits % 64)
  dst[nbits / 64] &= (~0UL >> (-(nbits) & (64 - 1)));
}

/*
 * On 32-bit systems bitmaps are represented as u32 arrays internally. On LE64
 * machines the order of hi and lo parts of numbers match the bitmap structure.
 * In both cases conversion is not needed when copying data from/to arrays of
 * u32. But in LE64 case, typecast in bitmap_copy_clear_tail() may lead
 * to out-of-bound access. To avoid that, both LE and BE variants of 64-bit
 * architectures are not using bitmap_copy_clear_tail().
 */

void bitmap_from_arr32(unsigned long *bitmap, const u32 *buf,
       unsigned int nbits);
void bitmap_to_arr32(u32 *buf, const unsigned long *bitmap,
       unsigned int nbits);
# 304 "./include/linux/bitmap.h"
/*
 * On 64-bit systems bitmaps are represented as u64 arrays internally. On LE32
 * machines the order of hi and lo parts of numbers match the bitmap structure.
 * In both cases conversion is not needed when copying data from/to arrays of
 * u64.
 */
# 320 "./include/linux/bitmap.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_and(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return (*dst = *src1 & *src2 & (~0UL >> (-(nbits) & (64 - 1)))) != 0;
 return __bitmap_and(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_or(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = *src1 | *src2;
 else
  __bitmap_or(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_xor(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = *src1 ^ *src2;
 else
  __bitmap_xor(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_andnot(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return (*dst = *src1 & ~(*src2) & (~0UL >> (-(nbits) & (64 - 1)))) != 0;
 return __bitmap_andnot(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_complement(unsigned long *dst, const unsigned long *src,
   unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = ~(*src);
 else
  __bitmap_complement(dst, src, nbits);
}
# 370 "./include/linux/bitmap.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_equal(const unsigned long *src1,
    const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return !((*src1 ^ *src2) & (~0UL >> (-(nbits) & (64 - 1))));
 if (__builtin_constant_p(nbits & (8 - 1)) &&
     (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
  return !memcmp(src1, src2, nbits / 8);
 return __bitmap_equal(src1, src2, nbits);
}

/**
 * bitmap_or_equal - Check whether the or of two bitmaps is equal to a third
 * @src1:	Pointer to bitmap 1
 * @src2:	Pointer to bitmap 2 will be or'ed with bitmap 1
 * @src3:	Pointer to bitmap 3. Compare to the result of *@src1 | *@src2
 * @nbits:	number of bits in each of these bitmaps
 *
 * Returns: True if (*@src1 | *@src2) == *@src3, false otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_or_equal(const unsigned long *src1,
       const unsigned long *src2,
       const unsigned long *src3,
       unsigned int nbits)
{
 if (!(__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return __bitmap_or_equal(src1, src2, src3, nbits);

 return !(((*src1 | *src2) ^ *src3) & (~0UL >> (-(nbits) & (64 - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_intersects(const unsigned long *src1,
         const unsigned long *src2,
         unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ((*src1 & *src2) & (~0UL >> (-(nbits) & (64 - 1)))) != 0;
 else
  return __bitmap_intersects(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_subset(const unsigned long *src1,
     const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ! ((*src1 & ~(*src2)) & (~0UL >> (-(nbits) & (64 - 1))));
 else
  return __bitmap_subset(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_empty(const unsigned long *src, unsigned nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ! (*src & (~0UL >> (-(nbits) & (64 - 1))));

 return find_first_bit(src, nbits) == nbits;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bitmap_full(const unsigned long *src, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ! (~(*src) & (~0UL >> (-(nbits) & (64 - 1))));

 return find_first_zero_bit(src, nbits) == nbits;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__))
unsigned int bitmap_weight(const unsigned long *src, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return hweight_long(*src & (~0UL >> (-(nbits) & (64 - 1))));
 return __bitmap_weight(src, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__))
unsigned long bitmap_weight_and(const unsigned long *src1,
    const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return hweight_long(*src1 & *src2 & (~0UL >> (-(nbits) & (64 - 1))));
 return __bitmap_weight_and(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void bitmap_set(unsigned long *map, unsigned int start,
  unsigned int nbits)
{
 if (__builtin_constant_p(nbits) && nbits == 1)
  ((__builtin_constant_p(start) && __builtin_constant_p((uintptr_t)(map) != (uintptr_t)((void *)0)) && (uintptr_t)(map) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(map))) ? generic___set_bit(start, map) : generic___set_bit(start, map));
 else if ((__builtin_constant_p(start + nbits) && (start + nbits) <= 64 && (start + nbits) > 0))
  *map |= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((start) > (start + nbits - 1)) * 0l)) : (int *)8))), (start) > (start + nbits - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (start)) + 1) & (~(((0UL))) >> (64 - 1 - (start + nbits - 1)))));
 else if (__builtin_constant_p(start & (8 - 1)) &&
   (((start) & ((typeof(start))(8) - 1)) == 0) &&
   __builtin_constant_p(nbits & (8 - 1)) &&
   (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
  memset((char *)map + start / 8, 0xff, nbits / 8);
 else
  __bitmap_set(map, start, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void bitmap_clear(unsigned long *map, unsigned int start,
  unsigned int nbits)
{
 if (__builtin_constant_p(nbits) && nbits == 1)
  ((__builtin_constant_p(start) && __builtin_constant_p((uintptr_t)(map) != (uintptr_t)((void *)0)) && (uintptr_t)(map) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(map))) ? generic___clear_bit(start, map) : generic___clear_bit(start, map));
 else if ((__builtin_constant_p(start + nbits) && (start + nbits) <= 64 && (start + nbits) > 0))
  *map &= ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((start) > (start + nbits - 1)) * 0l)) : (int *)8))), (start) > (start + nbits - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (start)) + 1) & (~(((0UL))) >> (64 - 1 - (start + nbits - 1)))));
 else if (__builtin_constant_p(start & (8 - 1)) &&
   (((start) & ((typeof(start))(8) - 1)) == 0) &&
   __builtin_constant_p(nbits & (8 - 1)) &&
   (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
  memset((char *)map + start / 8, 0, nbits / 8);
 else
  __bitmap_clear(map, start, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_shift_right(unsigned long *dst, const unsigned long *src,
    unsigned int shift, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = (*src & (~0UL >> (-(nbits) & (64 - 1)))) >> shift;
 else
  __bitmap_shift_right(dst, src, shift, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_shift_left(unsigned long *dst, const unsigned long *src,
    unsigned int shift, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = (*src << shift) & (~0UL >> (-(nbits) & (64 - 1)));
 else
  __bitmap_shift_left(dst, src, shift, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_replace(unsigned long *dst,
      const unsigned long *old,
      const unsigned long *new,
      const unsigned long *mask,
      unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = (*old & ~(*mask)) | (*new & *mask);
 else
  __bitmap_replace(dst, old, new, mask, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_next_set_region(unsigned long *bitmap,
       unsigned int *rs, unsigned int *re,
       unsigned int end)
{
 *rs = find_next_bit(bitmap, end, *rs);
 *re = find_next_zero_bit(bitmap, end, *rs + 1);
}

/**
 * BITMAP_FROM_U64() - Represent u64 value in the format suitable for bitmap.
 * @n: u64 value
 *
 * Linux bitmaps are internally arrays of unsigned longs, i.e. 32-bit
 * integers in 32-bit environment, and 64-bit integers in 64-bit one.
 *
 * There are four combinations of endianness and length of the word in linux
 * ABIs: LE64, BE64, LE32 and BE32.
 *
 * On 64-bit kernels 64-bit LE and BE numbers are naturally ordered in
 * bitmaps and therefore don't require any special handling.
 *
 * On 32-bit kernels 32-bit LE ABI orders lo word of 64-bit number in memory
 * prior to hi, and 32-bit BE orders hi word prior to lo. The bitmap on the
 * other hand is represented as an array of 32-bit words and the position of
 * bit N may therefore be calculated as: word #(N/32) and bit #(N%32) in that
 * word.  For example, bit #42 is located at 10th position of 2nd word.
 * It matches 32-bit LE ABI, and we can simply let the compiler store 64-bit
 * values in memory as it usually does. But for BE we need to swap hi and lo
 * words manually.
 *
 * With all that, the macro BITMAP_FROM_U64() does explicit reordering of hi and
 * lo parts of u64.  For LE32 it does nothing, and for BE environment it swaps
 * hi and lo words, as is expected by bitmap.
 */







/**
 * bitmap_from_u64 - Check and swap words within u64.
 *  @mask: source bitmap
 *  @dst:  destination bitmap
 *
 * In 32-bit Big Endian kernel, when using ``(u32 *)(&val)[*]``
 * to read u64 mask, we will get the wrong word.
 * That is ``(u32 *)(&val)[0]`` gets the upper 32 bits,
 * but we expect the lower 32-bits of u64.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_from_u64(unsigned long *dst, u64 mask)
{
 bitmap_copy_clear_tail((unsigned long *)(dst), (const unsigned long *)(&mask), (64));
}

/**
 * bitmap_get_value8 - get an 8-bit value within a memory region
 * @map: address to the bitmap memory region
 * @start: bit offset of the 8-bit value; must be a multiple of 8
 *
 * Returns the 8-bit value located at the @start bit offset within the @src
 * memory region.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long bitmap_get_value8(const unsigned long *map,
           unsigned long start)
{
 const size_t index = ((start) / 64);
 const unsigned long offset = start % 64;

 return (map[index] >> offset) & 0xFF;
}

/**
 * bitmap_set_value8 - set an 8-bit value within a memory region
 * @map: address to the bitmap memory region
 * @value: the 8-bit value; values wider than 8 bits may clobber bitmap
 * @start: bit offset of the 8-bit value; must be a multiple of 8
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bitmap_set_value8(unsigned long *map, unsigned long value,
         unsigned long start)
{
 const size_t index = ((start) / 64);
 const unsigned long offset = start % 64;

 map[index] &= ~(0xFFUL << offset);
 map[index] |= value << offset;
}
# 13 "./include/linux/cpumask.h" 2


# 1 "./include/linux/gfp_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/* The typedef is in types.h but we want the documentation here */
# 19 "./include/linux/gfp_types.h"
/*
 * In case of changes, please don't forget to update
 * include/trace/events/mmflags.h and tools/perf/builtin-kmem.c
 */

/* Plain integer GFP bitmasks. Do not use this directly. */
# 63 "./include/linux/gfp_types.h"
/* If the above are modified, __GFP_BITS_SHIFT may need updating */

/*
 * Physical address zone modifiers (see linux/mmzone.h - low four bits)
 *
 * Do not put any conditional on these. If necessary modify the definitions
 * without the underscores and use them consistently. The definitions here may
 * be used in bit comparisons.
 */






/**
 * DOC: Page mobility and placement hints
 *
 * Page mobility and placement hints
 * ---------------------------------
 *
 * These flags provide hints about how mobile the page is. Pages with similar
 * mobility are placed within the same pageblocks to minimise problems due
 * to external fragmentation.
 *
 * %__GFP_MOVABLE (also a zone modifier) indicates that the page can be
 * moved by page migration during memory compaction or can be reclaimed.
 *
 * %__GFP_RECLAIMABLE is used for slab allocations that specify
 * SLAB_RECLAIM_ACCOUNT and whose pages can be freed via shrinkers.
 *
 * %__GFP_WRITE indicates the caller intends to dirty the page. Where possible,
 * these pages will be spread between local zones to avoid all the dirty
 * pages being in one zone (fair zone allocation policy).
 *
 * %__GFP_HARDWALL enforces the cpuset memory allocation policy.
 *
 * %__GFP_THISNODE forces the allocation to be satisfied from the requested
 * node with no fallbacks or placement policy enforcements.
 *
 * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.
 */






/**
 * DOC: Watermark modifiers
 *
 * Watermark modifiers -- controls access to emergency reserves
 * ------------------------------------------------------------
 *
 * %__GFP_HIGH indicates that the caller is high-priority and that granting
 * the request is necessary before the system can make forward progress.
 * For example, creating an IO context to clean pages.
 *
 * %__GFP_ATOMIC indicates that the caller cannot reclaim or sleep and is
 * high priority. Users are typically interrupt handlers. This may be
 * used in conjunction with %__GFP_HIGH
 *
 * %__GFP_MEMALLOC allows access to all memory. This should only be used when
 * the caller guarantees the allocation will allow more memory to be freed
 * very shortly e.g. process exiting or swapping. Users either should
 * be the MM or co-ordinating closely with the VM (e.g. swap over NFS).
 * Users of this flag have to be extremely careful to not deplete the reserve
 * completely and implement a throttling mechanism which controls the
 * consumption of the reserve based on the amount of freed memory.
 * Usage of a pre-allocated pool (e.g. mempool) should be always considered
 * before using this flag.
 *
 * %__GFP_NOMEMALLOC is used to explicitly forbid access to emergency reserves.
 * This takes precedence over the %__GFP_MEMALLOC flag if both are set.
 */





/**
 * DOC: Reclaim modifiers
 *
 * Reclaim modifiers
 * -----------------
 * Please note that all the following flags are only applicable to sleepable
 * allocations (e.g. %GFP_NOWAIT and %GFP_ATOMIC will ignore them).
 *
 * %__GFP_IO can start physical IO.
 *
 * %__GFP_FS can call down to the low-level FS. Clearing the flag avoids the
 * allocator recursing into the filesystem which might already be holding
 * locks.
 *
 * %__GFP_DIRECT_RECLAIM indicates that the caller may enter direct reclaim.
 * This flag can be cleared to avoid unnecessary delays when a fallback
 * option is available.
 *
 * %__GFP_KSWAPD_RECLAIM indicates that the caller wants to wake kswapd when
 * the low watermark is reached and have it reclaim pages until the high
 * watermark is reached. A caller may wish to clear this flag when fallback
 * options are available and the reclaim is likely to disrupt the system. The
 * canonical example is THP allocation where a fallback is cheap but
 * reclaim/compaction may cause indirect stalls.
 *
 * %__GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.
 *
 * The default allocator behavior depends on the request size. We have a concept
 * of so called costly allocations (with order > %PAGE_ALLOC_COSTLY_ORDER).
 * !costly allocations are too essential to fail so they are implicitly
 * non-failing by default (with some exceptions like OOM victims might fail so
 * the caller still has to check for failures) while costly requests try to be
 * not disruptive and back off even without invoking the OOM killer.
 * The following three modifiers might be used to override some of these
 * implicit rules
 *
 * %__GFP_NORETRY: The VM implementation will try only very lightweight
 * memory direct reclaim to get some memory under memory pressure (thus
 * it can sleep). It will avoid disruptive actions like OOM killer. The
 * caller must handle the failure which is quite likely to happen under
 * heavy memory pressure. The flag is suitable when failure can easily be
 * handled at small cost, such as reduced throughput
 *
 * %__GFP_RETRY_MAYFAIL: The VM implementation will retry memory reclaim
 * procedures that have previously failed if there is some indication
 * that progress has been made else where.  It can wait for other
 * tasks to attempt high level approaches to freeing memory such as
 * compaction (which removes fragmentation) and page-out.
 * There is still a definite limit to the number of retries, but it is
 * a larger limit than with %__GFP_NORETRY.
 * Allocations with this flag may fail, but only when there is
 * genuinely little unused memory. While these allocations do not
 * directly trigger the OOM killer, their failure indicates that
 * the system is likely to need to use the OOM killer soon.  The
 * caller must handle failure, but can reasonably do so by failing
 * a higher-level request, or completing it only in a much less
 * efficient manner.
 * If the allocation does fail, and the caller is in a position to
 * free some non-essential memory, doing so could benefit the system
 * as a whole.
 *
 * %__GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
 * cannot handle allocation failures. The allocation could block
 * indefinitely but will never return with failure. Testing for
 * failure is pointless.
 * New users should be evaluated carefully (and the flag should be
 * used only when there is no reasonable failure policy) but it is
 * definitely preferable to use the flag rather than opencode endless
 * loop around allocator.
 * Using this flag for costly allocations is _highly_ discouraged.
 */
# 223 "./include/linux/gfp_types.h"
/**
 * DOC: Action modifiers
 *
 * Action modifiers
 * ----------------
 *
 * %__GFP_NOWARN suppresses allocation failure reports.
 *
 * %__GFP_COMP address compound page metadata.
 *
 * %__GFP_ZERO returns a zeroed page on success.
 *
 * %__GFP_ZEROTAGS zeroes memory tags at allocation time if the memory itself
 * is being zeroed (either via __GFP_ZERO or via init_on_alloc, provided that
 * __GFP_SKIP_ZERO is not set). This flag is intended for optimization: setting
 * memory tags at the same time as zeroing memory has minimal additional
 * performace impact.
 *
 * %__GFP_SKIP_KASAN_UNPOISON makes KASAN skip unpoisoning on page allocation.
 * Only effective in HW_TAGS mode.
 *
 * %__GFP_SKIP_KASAN_POISON makes KASAN skip poisoning on page deallocation.
 * Typically, used for userspace pages. Only effective in HW_TAGS mode.
 */
# 255 "./include/linux/gfp_types.h"
/* Disable lockdep for GFP context tracking */


/* Room for N __GFP_FOO bits */



/**
 * DOC: Useful GFP flag combinations
 *
 * Useful GFP flag combinations
 * ----------------------------
 *
 * Useful GFP flag combinations that are commonly used. It is recommended
 * that subsystems start with one of these combinations and then set/clear
 * %__GFP_FOO flags as necessary.
 *
 * %GFP_ATOMIC users can not sleep and need the allocation to succeed. A lower
 * watermark is applied to allow access to "atomic reserves".
 * The current implementation doesn't support NMI and few other strict
 * non-preemptive contexts (e.g. raw_spin_lock). The same applies to %GFP_NOWAIT.
 *
 * %GFP_KERNEL is typical for kernel-internal allocations. The caller requires
 * %ZONE_NORMAL or a lower zone for direct access but can direct reclaim.
 *
 * %GFP_KERNEL_ACCOUNT is the same as GFP_KERNEL, except the allocation is
 * accounted to kmemcg.
 *
 * %GFP_NOWAIT is for kernel allocations that should not stall for direct
 * reclaim, start physical IO or use any filesystem callback.
 *
 * %GFP_NOIO will use direct reclaim to discard clean pages or slab pages
 * that do not require the starting of any physical IO.
 * Please try to avoid using this flag directly and instead use
 * memalloc_noio_{save,restore} to mark the whole scope which cannot
 * perform any IO with a short explanation why. All allocation requests
 * will inherit GFP_NOIO implicitly.
 *
 * %GFP_NOFS will use direct reclaim but will not use any filesystem interfaces.
 * Please try to avoid using this flag directly and instead use
 * memalloc_nofs_{save,restore} to mark the whole scope which cannot/shouldn't
 * recurse into the FS layer with a short explanation why. All allocation
 * requests will inherit GFP_NOFS implicitly.
 *
 * %GFP_USER is for userspace allocations that also need to be directly
 * accessibly by the kernel or hardware. It is typically used by hardware
 * for buffers that are mapped to userspace (e.g. graphics) that hardware
 * still must DMA to. cpuset limits are enforced for these allocations.
 *
 * %GFP_DMA exists for historical reasons and should be avoided where possible.
 * The flags indicates that the caller requires that the lowest zone be
 * used (%ZONE_DMA or 16M on x86-64). Ideally, this would be removed but
 * it would require careful auditing as some users really require it and
 * others use the flag to avoid lowmem reserves in %ZONE_DMA and treat the
 * lowest zone as a type of emergency reserve.
 *
 * %GFP_DMA32 is similar to %GFP_DMA except that the caller requires a 32-bit
 * address. Note that kmalloc(..., GFP_DMA32) does not return DMA32 memory
 * because the DMA32 kmalloc cache array is not implemented.
 * (Reason: there is no such user in kernel).
 *
 * %GFP_HIGHUSER is for userspace allocations that may be mapped to userspace,
 * do not need to be directly accessible by the kernel but that cannot
 * move once in use. An example may be a hardware allocation that maps
 * data directly into userspace but has no addressing limitations.
 *
 * %GFP_HIGHUSER_MOVABLE is for userspace allocations that the kernel does not
 * need direct access to but can use kmap() when access is required. They
 * are expected to be movable via page reclaim or page migration. Typically,
 * pages on the LRU would also be allocated with %GFP_HIGHUSER_MOVABLE.
 *
 * %GFP_TRANSHUGE and %GFP_TRANSHUGE_LIGHT are used for THP allocations. They
 * are compound allocations that will generally fail quickly if memory is not
 * available and will not wake kswapd/kcompactd on failure. The _LIGHT
 * version does not attempt reclaim/compaction at all and is by default used
 * in page fault path, while the non-light is used by khugepaged.
 */
# 16 "./include/linux/cpumask.h" 2
# 1 "./include/linux/numa.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 16 "./include/linux/numa.h"
/* optionally keep NUMA memory info available post init */
# 25 "./include/linux/numa.h"
# 1 "./arch/arm64/include/asm/sparsemem.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */





/*
 * Section size must be at least 512MB for 64K base
 * page size config. Otherwise it will be less than
 * (MAX_ORDER - 1) and the build process will fail.
 */





/*
 * Section size must be at least 128MB for 4K base
 * page size config. Otherwise PMD based huge page
 * entries could not be created for vmemmap mappings.
 * 16K follows 4K for simplicity.
 */
# 26 "./include/linux/numa.h" 2

/* Generic implementation available */
int numa_map_to_online_node(int node);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int memory_add_physaddr_to_nid(u64 start)
{
 ({ bool __ret_do_once = !!(true); if (({ static bool __attribute__((__section__(".data.once"))) __already_done; bool __ret_cond = !!(__ret_do_once); bool __ret_once = false; if (__builtin_expect(!!(__ret_cond && !__already_done), 0)) { __already_done = true; __ret_once = true; } __builtin_expect(!!(__ret_once), 0); })) ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "6" /* informational */ "Unknown online node for memory at 0x%llx, assuming node 0\n", start); }); __builtin_expect(!!(__ret_do_once), 0); });

 return 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int phys_to_target_node(u64 start)
{
 ({ bool __ret_do_once = !!(true); if (({ static bool __attribute__((__section__(".data.once"))) __already_done; bool __ret_cond = !!(__ret_do_once); bool __ret_once = false; if (__builtin_expect(!!(__ret_cond && !__already_done), 0)) { __already_done = true; __ret_once = true; } __builtin_expect(!!(__ret_once), 0); })) ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "6" /* informational */ "Unknown target node for memory at 0x%llx, assuming node 0\n", start); }); __builtin_expect(!!(__ret_do_once), 0); });

 return 0;
}
# 17 "./include/linux/cpumask.h" 2

/* Don't assign or return these: may not be this big! */
typedef struct cpumask { unsigned long bits[(((256) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))]; } cpumask_t;

/**
 * cpumask_bits - get the bits in a cpumask
 * @maskp: the struct cpumask *
 *
 * You should only assume nr_cpu_ids bits of this mask are valid.  This is
 * a macro so it's const-correct.
 */


/**
 * cpumask_pr_args - printf args to output a cpumask
 * @maskp: cpumask to be printed
 *
 * Can be used to provide arguments for '%*pb[l]' when printing a cpumask.
 */





extern unsigned int nr_cpu_ids;


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_nr_cpu_ids(unsigned int nr)
{



 nr_cpu_ids = nr;

}

/* Deprecated. Always use nr_cpu_ids. */


/*
 * The following particular system cpumasks and operations manage
 * possible, present, active and online cpus.
 *
 *     cpu_possible_mask- has bit 'cpu' set iff cpu is populatable
 *     cpu_present_mask - has bit 'cpu' set iff cpu is populated
 *     cpu_online_mask  - has bit 'cpu' set iff cpu available to scheduler
 *     cpu_active_mask  - has bit 'cpu' set iff cpu available to migration
 *
 *  If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.
 *
 *  The cpu_possible_mask is fixed at boot time, as the set of CPU id's
 *  that it is possible might ever be plugged in at anytime during the
 *  life of that system boot.  The cpu_present_mask is dynamic(*),
 *  representing which CPUs are currently plugged in.  And
 *  cpu_online_mask is the dynamic subset of cpu_present_mask,
 *  indicating those CPUs available for scheduling.
 *
 *  If HOTPLUG is enabled, then cpu_present_mask varies dynamically,
 *  depending on what ACPI reports as currently plugged in, otherwise
 *  cpu_present_mask is just a copy of cpu_possible_mask.
 *
 *  (*) Well, cpu_present_mask is dynamic in the hotplug case.  If not
 *      hotplug, it's a copy of cpu_possible_mask, hence fixed at boot.
 *
 * Subtleties:
 * 1) UP arch's (NR_CPUS == 1, CONFIG_SMP not defined) hardcode
 *    assumption that their single CPU is online.  The UP
 *    cpu_{online,possible,present}_masks are placebos.  Changing them
 *    will have no useful affect on the following num_*_cpus()
 *    and cpu_*() macros in the UP case.  This ugliness is a UP
 *    optimization - don't waste any instructions or memory references
 *    asking if you're online or how many CPUs there are if there is
 *    only one CPU.
 */

extern struct cpumask __cpu_possible_mask;
extern struct cpumask __cpu_online_mask;
extern struct cpumask __cpu_present_mask;
extern struct cpumask __cpu_active_mask;
extern struct cpumask __cpu_dying_mask;






extern atomic_t __num_online_cpus;

extern cpumask_t cpus_booted_once_mask;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void cpu_max_bits_warn(unsigned int cpu, unsigned int bits)
{



}

/* verify cpu argument to cpumask_* operators */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned int cpumask_check(unsigned int cpu)
{
 cpu_max_bits_warn(cpu, nr_cpu_ids);
 return cpu;
}

/**
 * cpumask_first - get the first cpu in a cpumask
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_first(const struct cpumask *srcp)
{
 return find_first_bit(((srcp)->bits), nr_cpu_ids);
}

/**
 * cpumask_first_zero - get the first unset cpu in a cpumask
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if all cpus are set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_first_zero(const struct cpumask *srcp)
{
 return find_first_zero_bit(((srcp)->bits), nr_cpu_ids);
}

/**
 * cpumask_first_and - return the first cpu from *srcp1 & *srcp2
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned int cpumask_first_and(const struct cpumask *srcp1, const struct cpumask *srcp2)
{
 return find_first_and_bit(((srcp1)->bits), ((srcp2)->bits), nr_cpu_ids);
}

/**
 * cpumask_last - get the last CPU in a cpumask
 * @srcp:	- the cpumask pointer
 *
 * Returns	>= nr_cpumask_bits if no CPUs set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_last(const struct cpumask *srcp)
{
 return find_last_bit(((srcp)->bits), nr_cpu_ids);
}

/**
 * cpumask_next - get the next cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
 /* -1 is a legal arg here. */
 if (n != -1)
  cpumask_check(n);
 return find_next_bit(((srcp)->bits), nr_cpu_ids, n + 1);
}

/**
 * cpumask_next_zero - get the next unset cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus unset.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)
{
 /* -1 is a legal arg here. */
 if (n != -1)
  cpumask_check(n);
 return find_next_zero_bit(((srcp)->bits), nr_cpu_ids, n+1);
}
# 216 "./include/linux/cpumask.h"
unsigned int cpumask_local_spread(unsigned int i, int node);
unsigned int cpumask_any_and_distribute(const struct cpumask *src1p,
          const struct cpumask *src2p);
unsigned int cpumask_any_distribute(const struct cpumask *srcp);


/**
 * cpumask_next_and - get the next cpu in *src1p & *src2p
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @src1p: the first cpumask pointer
 * @src2p: the second cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set in both.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned int cpumask_next_and(int n, const struct cpumask *src1p,
       const struct cpumask *src2p)
{
 /* -1 is a legal arg here. */
 if (n != -1)
  cpumask_check(n);
 return find_next_and_bit(((src1p)->bits), ((src2p)->bits),
  nr_cpu_ids, n + 1);
}

/**
 * for_each_cpu - iterate over every cpu in a mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */



/**
 * for_each_cpu_not - iterate over every cpu in a complemented mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
# 279 "./include/linux/cpumask.h"
unsigned int __attribute__((__pure__)) cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap);


/**
 * for_each_cpu_wrap - iterate over every cpu in a mask, starting at a specified location
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 * @start: the start location
 *
 * The implementation does not assume any bit in @mask is set (including @start).
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */



/**
 * for_each_cpu_and - iterate over every cpu in both masks
 * @cpu: the (optionally unsigned) integer iterator
 * @mask1: the first cpumask pointer
 * @mask2: the second cpumask pointer
 *
 * This saves a temporary CPU mask in many places.  It is equivalent to:
 *	struct cpumask tmp;
 *	cpumask_and(&tmp, &mask1, &mask2);
 *	for_each_cpu(cpu, &tmp)
 *		...
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */



/**
 * for_each_cpu_andnot - iterate over every cpu present in one mask, excluding
 *			 those present in another.
 * @cpu: the (optionally unsigned) integer iterator
 * @mask1: the first cpumask pointer
 * @mask2: the second cpumask pointer
 *
 * This saves a temporary CPU mask in many places.  It is equivalent to:
 *	struct cpumask tmp;
 *	cpumask_andnot(&tmp, &mask1, &mask2);
 *	for_each_cpu(cpu, &tmp)
 *		...
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */



/**
 * cpumask_any_but - return a "random" in a cpumask, but not this one.
 * @mask: the cpumask to search
 * @cpu: the cpu to ignore.
 *
 * Often used to find any cpu but smp_processor_id() in a mask.
 * Returns >= nr_cpu_ids if no cpus set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned int cpumask_any_but(const struct cpumask *mask, unsigned int cpu)
{
 unsigned int i;

 cpumask_check(cpu);
 for ((i) = 0; (i) = find_next_bit((((mask)->bits)), (nr_cpu_ids), (i)), (i) < (nr_cpu_ids); (i)++)
  if (i != cpu)
   break;
 return i;
}

/**
 * cpumask_nth - get the first cpu in a cpumask
 * @srcp: the cpumask pointer
 * @cpu: the N'th cpu to find, starting from 0
 *
 * Returns >= nr_cpu_ids if such cpu doesn't exist.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_nth(unsigned int cpu, const struct cpumask *srcp)
{
 return find_nth_bit(((srcp)->bits), nr_cpu_ids, cpumask_check(cpu));
}

/**
 * cpumask_nth_and - get the first cpu in 2 cpumasks
 * @srcp1: the cpumask pointer
 * @srcp2: the cpumask pointer
 * @cpu: the N'th cpu to find, starting from 0
 *
 * Returns >= nr_cpu_ids if such cpu doesn't exist.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned int cpumask_nth_and(unsigned int cpu, const struct cpumask *srcp1,
       const struct cpumask *srcp2)
{
 return find_nth_and_bit(((srcp1)->bits), ((srcp2)->bits),
    nr_cpu_ids, cpumask_check(cpu));
}

/**
 * cpumask_nth_andnot - get the first cpu set in 1st cpumask, and clear in 2nd.
 * @srcp1: the cpumask pointer
 * @srcp2: the cpumask pointer
 * @cpu: the N'th cpu to find, starting from 0
 *
 * Returns >= nr_cpu_ids if such cpu doesn't exist.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned int cpumask_nth_andnot(unsigned int cpu, const struct cpumask *srcp1,
       const struct cpumask *srcp2)
{
 return find_nth_andnot_bit(((srcp1)->bits), ((srcp2)->bits),
    nr_cpu_ids, cpumask_check(cpu));
}
# 404 "./include/linux/cpumask.h"
/**
 * cpumask_set_cpu - set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
 set_bit(cpumask_check(cpu), ((dstp)->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
 ((__builtin_constant_p(cpumask_check(cpu)) && __builtin_constant_p((uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0)) && (uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(((dstp)->bits)))) ? generic___set_bit(cpumask_check(cpu), ((dstp)->bits)) : generic___set_bit(cpumask_check(cpu), ((dstp)->bits)));
}


/**
 * cpumask_clear_cpu - clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
 clear_bit(cpumask_check(cpu), ((dstp)->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
 ((__builtin_constant_p(cpumask_check(cpu)) && __builtin_constant_p((uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0)) && (uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(((dstp)->bits)))) ? generic___clear_bit(cpumask_check(cpu), ((dstp)->bits)) : generic___clear_bit(cpumask_check(cpu), ((dstp)->bits)));
}

/**
 * cpumask_test_cpu - test for a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns true if @cpu is set in @cpumask, else returns false
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool cpumask_test_cpu(int cpu, const struct cpumask *cpumask)
{
 return ((__builtin_constant_p(cpumask_check(cpu)) && __builtin_constant_p((uintptr_t)((((cpumask))->bits)) != (uintptr_t)((void *)0)) && (uintptr_t)((((cpumask))->bits)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)((((cpumask))->bits)))) ? const_test_bit(cpumask_check(cpu), (((cpumask))->bits)) : generic_test_bit(cpumask_check(cpu), (((cpumask))->bits)));
}

/**
 * cpumask_test_and_set_cpu - atomically test and set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns true if @cpu is set in old bitmap of @cpumask, else returns false
 *
 * test_and_set_bit wrapper for cpumasks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool cpumask_test_and_set_cpu(int cpu, struct cpumask *cpumask)
{
 return test_and_set_bit(cpumask_check(cpu), ((cpumask)->bits));
}

/**
 * cpumask_test_and_clear_cpu - atomically test and clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns true if @cpu is set in old bitmap of @cpumask, else returns false
 *
 * test_and_clear_bit wrapper for cpumasks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool cpumask_test_and_clear_cpu(int cpu, struct cpumask *cpumask)
{
 return test_and_clear_bit(cpumask_check(cpu), ((cpumask)->bits));
}

/**
 * cpumask_setall - set all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_setall(struct cpumask *dstp)
{
 bitmap_fill(((dstp)->bits), nr_cpu_ids);
}

/**
 * cpumask_clear - clear all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_clear(struct cpumask *dstp)
{
 bitmap_zero(((dstp)->bits), nr_cpu_ids);
}

/**
 * cpumask_and - *dstp = *src1p & *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 *
 * If *@dstp is empty, returns false, else returns true
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_and(struct cpumask *dstp,
          const struct cpumask *src1p,
          const struct cpumask *src2p)
{
 return bitmap_and(((dstp)->bits), ((src1p)->bits),
           ((src2p)->bits), nr_cpu_ids);
}

/**
 * cpumask_or - *dstp = *src1p | *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_or(struct cpumask *dstp, const struct cpumask *src1p,
         const struct cpumask *src2p)
{
 bitmap_or(((dstp)->bits), ((src1p)->bits),
          ((src2p)->bits), nr_cpu_ids);
}

/**
 * cpumask_xor - *dstp = *src1p ^ *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_xor(struct cpumask *dstp,
          const struct cpumask *src1p,
          const struct cpumask *src2p)
{
 bitmap_xor(((dstp)->bits), ((src1p)->bits),
           ((src2p)->bits), nr_cpu_ids);
}

/**
 * cpumask_andnot - *dstp = *src1p & ~*src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 *
 * If *@dstp is empty, returns false, else returns true
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_andnot(struct cpumask *dstp,
      const struct cpumask *src1p,
      const struct cpumask *src2p)
{
 return bitmap_andnot(((dstp)->bits), ((src1p)->bits),
       ((src2p)->bits), nr_cpu_ids);
}

/**
 * cpumask_complement - *dstp = ~*srcp
 * @dstp: the cpumask result
 * @srcp: the input to invert
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_complement(struct cpumask *dstp,
          const struct cpumask *srcp)
{
 bitmap_complement(((dstp)->bits), ((srcp)->bits),
           nr_cpu_ids);
}

/**
 * cpumask_equal - *src1p == *src2p
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_equal(const struct cpumask *src1p,
    const struct cpumask *src2p)
{
 return bitmap_equal(((src1p)->bits), ((src2p)->bits),
       nr_cpu_ids);
}

/**
 * cpumask_or_equal - *src1p | *src2p == *src3p
 * @src1p: the first input
 * @src2p: the second input
 * @src3p: the third input
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_or_equal(const struct cpumask *src1p,
        const struct cpumask *src2p,
        const struct cpumask *src3p)
{
 return bitmap_or_equal(((src1p)->bits), ((src2p)->bits),
          ((src3p)->bits), nr_cpu_ids);
}

/**
 * cpumask_intersects - (*src1p & *src2p) != 0
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_intersects(const struct cpumask *src1p,
         const struct cpumask *src2p)
{
 return bitmap_intersects(((src1p)->bits), ((src2p)->bits),
            nr_cpu_ids);
}

/**
 * cpumask_subset - (*src1p & ~*src2p) == 0
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns true if *@src1p is a subset of *@src2p, else returns false
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_subset(const struct cpumask *src1p,
     const struct cpumask *src2p)
{
 return bitmap_subset(((src1p)->bits), ((src2p)->bits),
        nr_cpu_ids);
}

/**
 * cpumask_empty - *srcp == 0
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are clear.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_empty(const struct cpumask *srcp)
{
 return bitmap_empty(((srcp)->bits), nr_cpu_ids);
}

/**
 * cpumask_full - *srcp == 0xFFFFFFFF...
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_full(const struct cpumask *srcp)
{
 return bitmap_full(((srcp)->bits), nr_cpu_ids);
}

/**
 * cpumask_weight - Count of bits in *srcp
 * @srcp: the cpumask to count bits (< nr_cpu_ids) in.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_weight(const struct cpumask *srcp)
{
 return bitmap_weight(((srcp)->bits), nr_cpu_ids);
}

/**
 * cpumask_weight_and - Count of bits in (*srcp1 & *srcp2)
 * @srcp1: the cpumask to count bits (< nr_cpu_ids) in.
 * @srcp2: the cpumask to count bits (< nr_cpu_ids) in.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_weight_and(const struct cpumask *srcp1,
      const struct cpumask *srcp2)
{
 return bitmap_weight_and(((srcp1)->bits), ((srcp2)->bits), nr_cpu_ids);
}

/**
 * cpumask_shift_right - *dstp = *srcp >> n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_shift_right(struct cpumask *dstp,
           const struct cpumask *srcp, int n)
{
 bitmap_shift_right(((dstp)->bits), ((srcp)->bits), n,
            nr_cpu_ids);
}

/**
 * cpumask_shift_left - *dstp = *srcp << n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_shift_left(struct cpumask *dstp,
          const struct cpumask *srcp, int n)
{
 bitmap_shift_left(((dstp)->bits), ((srcp)->bits), n,
           nr_cpu_ids);
}

/**
 * cpumask_copy - *dstp = *srcp
 * @dstp: the result
 * @srcp: the input cpumask
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpumask_copy(struct cpumask *dstp,
    const struct cpumask *srcp)
{
 bitmap_copy(((dstp)->bits), ((srcp)->bits), nr_cpu_ids);
}

/**
 * cpumask_any - pick a "random" cpu from *srcp
 * @srcp: the input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */


/**
 * cpumask_any_and - pick a "random" cpu from *mask1 & *mask2
 * @mask1: the first input cpumask
 * @mask2: the second input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */


/**
 * cpumask_of - the cpumask containing just a given cpu
 * @cpu: the cpu (<= nr_cpu_ids)
 */


/**
 * cpumask_parse_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpumask_parse_user(const char /* nothing */ *buf, int len,
         struct cpumask *dstp)
{
 return bitmap_parse_user(buf, len, ((dstp)->bits), nr_cpu_ids);
}

/**
 * cpumask_parselist_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpumask_parselist_user(const char /* nothing */ *buf, int len,
         struct cpumask *dstp)
{
 return bitmap_parselist_user(buf, len, ((dstp)->bits),
         nr_cpu_ids);
}

/**
 * cpumask_parse - extract a cpumask from a string
 * @buf: the buffer to extract from
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpumask_parse(const char *buf, struct cpumask *dstp)
{
 return bitmap_parse(buf, (~0U), ((dstp)->bits), nr_cpu_ids);
}

/**
 * cpulist_parse - extract a cpumask from a user string of ranges
 * @buf: the buffer to extract from
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpulist_parse(const char *buf, struct cpumask *dstp)
{
 return bitmap_parselist(buf, ((dstp)->bits), nr_cpu_ids);
}

/**
 * cpumask_size - size to allocate for a 'struct cpumask' in bytes
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int cpumask_size(void)
{
 return (((nr_cpu_ids) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(long);
}

/*
 * cpumask_var_t: struct cpumask for stack usage.
 *
 * Oh, the wicked games we play!  In order to make kernel coding a
 * little more difficult, we typedef cpumask_var_t to an array or a
 * pointer: doing &mask on an array is a noop, so it still works.
 *
 * ie.
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	  ... use 'tmpmask' like a normal struct cpumask * ...
 *
 *	free_cpumask_var(tmpmask);
 *
 *
 * However, one notable exception is there. alloc_cpumask_var() allocates
 * only nr_cpumask_bits bits (in the other hand, real cpumask_t always has
 * NR_CPUS bits). Therefore you don't have to dereference cpumask_var_t.
 *
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	var = *tmpmask;
 *
 * This code makes NR_CPUS length memcopy and brings to a memory corruption.
 * cpumask_copy() provide safe copy functionality.
 *
 * Note that there is another evil here: If you define a cpumask_var_t
 * as a percpu variable then the way to obtain the address of the cpumask
 * structure differently influences what this_cpu_* operation needs to be
 * used. Please use this_cpu_cpumask_var_t in those cases. The direct use
 * of this_cpu_ptr() or this_cpu_read() will lead to failures when the
 * other type of cpumask_var_t implementation is configured.
 *
 * Please also note that __cpumask_var_read_mostly can be used to declare
 * a cpumask_var_t variable itself (not its content) as read mostly.
 */
# 861 "./include/linux/cpumask.h"
typedef struct cpumask cpumask_var_t[1];




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,
       int node)
{
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
 cpumask_clear(*mask);
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,
       int node)
{
 cpumask_clear(*mask);
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void alloc_bootmem_cpumask_var(cpumask_var_t *mask)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void free_cpumask_var(cpumask_var_t mask)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void free_bootmem_cpumask_var(cpumask_var_t mask)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpumask_available(cpumask_var_t mask)
{
 return true;
}


/* It's common to want to use cpu_all_mask in struct member initializers,
 * so it has to refer to an address rather than a pointer. */
extern const unsigned long cpu_all_bits[(((256) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];


/* First bits of cpu_bit_bitmap are in fact unset. */
# 927 "./include/linux/cpumask.h"
/* Wrappers for arch boot code to manipulate normally-constant masks */
void init_cpu_present(const struct cpumask *src);
void init_cpu_possible(const struct cpumask *src);
void init_cpu_online(const struct cpumask *src);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void reset_cpu_possible_mask(void)
{
 bitmap_zero(((&__cpu_possible_mask)->bits), 256);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
set_cpu_possible(unsigned int cpu, bool possible)
{
 if (possible)
  cpumask_set_cpu(cpu, &__cpu_possible_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_possible_mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
set_cpu_present(unsigned int cpu, bool present)
{
 if (present)
  cpumask_set_cpu(cpu, &__cpu_present_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_present_mask);
}

void set_cpu_online(unsigned int cpu, bool online);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
set_cpu_active(unsigned int cpu, bool active)
{
 if (active)
  cpumask_set_cpu(cpu, &__cpu_active_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_active_mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
set_cpu_dying(unsigned int cpu, bool dying)
{
 if (dying)
  cpumask_set_cpu(cpu, &__cpu_dying_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_dying_mask);
}

/**
 * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *
 * @bitmap: the bitmap
 *
 * There are a few places where cpumask_var_t isn't appropriate and
 * static cpumasks must be used (eg. very early boot), yet we don't
 * expose the definition of 'struct cpumask'.
 *
 * This does the conversion, and can be used as a constant initializer.
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __check_is_bitmap(const unsigned long *bitmap)
{
 return 1;
}

/*
 * Special-case data structure for "single bit set only" constant CPU masks.
 *
 * We pre-generate all the 64 (or 32) possible bit positions, with enough
 * padding to the left and the right, and return the constant pointer
 * appropriately offset.
 */
extern const unsigned long
 cpu_bit_bitmap[64 +1][(((256) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct cpumask *get_cpu_mask(unsigned int cpu)
{
 const unsigned long *p = cpu_bit_bitmap[1 + cpu % 64];
 p -= cpu / 64;
 return ((struct cpumask *)(1 ? (p) : (void *)sizeof(__check_is_bitmap(p))));
}


/**
 * num_online_cpus() - Read the number of online CPUs
 *
 * Despite the fact that __num_online_cpus is of type atomic_t, this
 * interface gives only a momentary snapshot and is not protected against
 * concurrent CPU hotplug operations unless invoked from a cpuhp_lock held
 * region.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int num_online_cpus(void)
{
 return atomic_read(&__num_online_cpus);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_online(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_online_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_possible(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_possible_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_present(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_present_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_active(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_active_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_dying(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_dying_mask));
}
# 1104 "./include/linux/cpumask.h"
/**
 * cpumap_print_to_pagebuf  - copies the cpumask into the buffer either
 *	as comma-separated list of cpus or hex values of cpumask
 * @list: indicates whether the cpumap must be list
 * @mask: the cpumask to copy
 * @buf: the buffer to copy into
 *
 * Returns the length of the (null-terminated) @buf string, zero if
 * nothing is copied.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t
cpumap_print_to_pagebuf(bool list, char *buf, const struct cpumask *mask)
{
 return bitmap_print_to_pagebuf(list, buf, ((mask)->bits),
          nr_cpu_ids);
}

/**
 * cpumap_print_bitmask_to_buf  - copies the cpumask into the buffer as
 *	hex values of cpumask
 *
 * @buf: the buffer to copy into
 * @mask: the cpumask to copy
 * @off: in the string from which we are copying, we copy to @buf
 * @count: the maximum number of bytes to print
 *
 * The function prints the cpumask into the buffer as hex values of
 * cpumask; Typically used by bin_attribute to export cpumask bitmask
 * ABI.
 *
 * Returns the length of how many bytes have been copied, excluding
 * terminating '\0'.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t
cpumap_print_bitmask_to_buf(char *buf, const struct cpumask *mask,
  loff_t off, size_t count)
{
 return bitmap_print_bitmask_to_buf(buf, ((mask)->bits),
       nr_cpu_ids, off, count) - 1;
}

/**
 * cpumap_print_list_to_buf  - copies the cpumask into the buffer as
 *	comma-separated list of cpus
 *
 * Everything is same with the above cpumap_print_bitmask_to_buf()
 * except the print format.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t
cpumap_print_list_to_buf(char *buf, const struct cpumask *mask,
  loff_t off, size_t count)
{
 return bitmap_print_list_to_buf(buf, ((mask)->bits),
       nr_cpu_ids, off, count) - 1;
}
# 1183 "./include/linux/cpumask.h"
/*
 * Provide a valid theoretical max size for cpumap and cpulist sysfs files
 * to avoid breaking userspace which may allocate a buffer based on the size
 * reported by e.g. fstat.
 *
 * for cpumap NR_CPUS * 9/32 - 1 should be an exact length.
 *
 * For cpulist 7 is (ceil(log10(NR_CPUS)) + 1) allowing for NR_CPUS to be up
 * to 2 orders of magnitude larger than 8192. And then we divide by 2 to
 * cover a worst-case of every other cpu being on one of two nodes for a
 * very large NR_CPUS.
 *
 *  Use PAGE_SIZE as a minimum for smaller configurations while avoiding
 *  unsigned comparison to -1.
 */
# 14 "./include/linux/smp.h" 2

# 1 "./include/linux/smp_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/llist.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */


/*
 * Lock-less NULL terminated single linked list
 *
 * Cases where locking is not needed:
 * If there are multiple producers and multiple consumers, llist_add can be
 * used in producers and llist_del_all can be used in consumers simultaneously
 * without locking. Also a single consumer can use llist_del_first while
 * multiple producers simultaneously use llist_add, without any locking.
 *
 * Cases where locking is needed:
 * If we have multiple consumers with llist_del_first used in one consumer, and
 * llist_del_first or llist_del_all used in other consumers, then a lock is
 * needed.  This is because llist_del_first depends on list->first->next not
 * changing, but without lock protection, there's no way to be sure about that
 * if a preemption happens in the middle of the delete operation and on being
 * preempted back, the list->first is the same as before causing the cmpxchg in
 * llist_del_first to succeed. For example, while a llist_del_first operation
 * is in progress in one consumer, then a llist_del_first, llist_add,
 * llist_add (or llist_del_all, llist_add, llist_add) sequence in another
 * consumer may cause violations.
 *
 * This can be summarized as follows:
 *
 *           |   add    | del_first |  del_all
 * add       |    -     |     -     |     -
 * del_first |          |     L     |     L
 * del_all   |          |           |     -
 *
 * Where, a particular row's operation can happen concurrently with a column's
 * operation, with "-" being no lock needed, while "L" being lock is needed.
 *
 * The list entries deleted via llist_del_all can be traversed with
 * traversing function such as llist_for_each etc.  But the list
 * entries can not be traversed safely before deleted from the list.
 * The order of deleted entries is from the newest to the oldest added
 * one.  If you want to traverse from the oldest to the newest, you
 * must reverse the order by yourself before traversing.
 *
 * The basic atomic operation of this list is cmpxchg on long.  On
 * architectures that don't have NMI-safe cmpxchg implementation, the
 * list can NOT be used in NMI handlers.  So code that uses the list in
 * an NMI handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
 *
 * Copyright 2010,2011 Intel Corp.
 *   Author: Huang Ying <ying.huang@intel.com>
 */






struct llist_head {
 struct llist_node *first;
};

struct llist_node {
 struct llist_node *next;
};




/**
 * init_llist_head - initialize lock-less list head
 * @head:	the head for your lock-less list
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_llist_head(struct llist_head *list)
{
 list->first = ((void *)0);
}

/**
 * llist_entry - get the struct of this entry
 * @ptr:	the &struct llist_node pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the llist_node within the struct.
 */



/**
 * member_address_is_nonnull - check whether the member address is not NULL
 * @ptr:	the object pointer (struct type * that contains the llist_node)
 * @member:	the name of the llist_node within the struct.
 *
 * This macro is conceptually the same as
 *	&ptr->member != NULL
 * but it works around the fact that compilers can decide that taking a member
 * address is never a NULL pointer.
 *
 * Real objects that start at a high address and have a member at NULL are
 * unlikely to exist, but such pointers may be returned e.g. by the
 * container_of() macro.
 */



/**
 * llist_for_each - iterate over some deleted entries of a lock-less list
 * @pos:	the &struct llist_node to use as a loop cursor
 * @node:	the first entry of deleted list entries
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being deleted from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */



/**
 * llist_for_each_safe - iterate over some deleted entries of a lock-less list
 *			 safe against removal of list entry
 * @pos:	the &struct llist_node to use as a loop cursor
 * @n:		another &struct llist_node to use as temporary storage
 * @node:	the first entry of deleted list entries
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being deleted from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */



/**
 * llist_for_each_entry - iterate over some deleted entries of lock-less list of given type
 * @pos:	the type * to use as a loop cursor.
 * @node:	the fist entry of deleted list entries.
 * @member:	the name of the llist_node with the struct.
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being removed from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */





/**
 * llist_for_each_entry_safe - iterate over some deleted entries of lock-less list of given type
 *			       safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @node:	the first entry of deleted list entries.
 * @member:	the name of the llist_node with the struct.
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being removed from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */






/**
 * llist_empty - tests whether a lock-less list is empty
 * @head:	the list to test
 *
 * Not guaranteed to be accurate or up to date.  Just a quick way to
 * test whether the list is empty without deleting something from the
 * list.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool llist_empty(const struct llist_head *head)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_165(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head->first) == sizeof(char) || sizeof(head->first) == sizeof(short) || sizeof(head->first) == sizeof(int) || sizeof(head->first) == sizeof(long)) || sizeof(head->first) == sizeof(long long))) __compiletime_assert_165(); } while (0); (*(const volatile typeof( _Generic((head->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head->first))) *)&(head->first)); }) == ((void *)0);
# 192 "./include/linux/llist.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct llist_node *llist_next(struct llist_node *node)
{
 return node->next;
}

extern bool llist_add_batch(struct llist_node *new_first,
       struct llist_node *new_last,
       struct llist_head *head);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __llist_add_batch(struct llist_node *new_first,
         struct llist_node *new_last,
         struct llist_head *head)
{
 new_last->next = head->first;
 head->first = new_first;
 return new_last->next == ((void *)0);
}

/**
 * llist_add - add a new entry
 * @new:	new entry to be added
 * @head:	the head for your lock-less list
 *
 * Returns true if the list was empty prior to adding this entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool llist_add(struct llist_node *new, struct llist_head *head)
{
 return llist_add_batch(new, new, head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __llist_add(struct llist_node *new, struct llist_head *head)
{
 return __llist_add_batch(new, new, head);
}

/**
 * llist_del_all - delete all entries from lock-less list
 * @head:	the head of lock-less list to delete all entries
 *
 * If list is empty, return NULL, otherwise, delete all entries and
 * return the pointer to the first entry.  The order of entries
 * deleted is from the newest to the oldest added one.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct llist_node *llist_del_all(struct llist_head *head)
{
 return ({ typeof(&head->first) __ai_ptr = (&head->first); do { } while (0); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __ret; __ret = (__typeof__(*(__ai_ptr))) __xchg_mb((unsigned long)(((void *)0)), (__ai_ptr), sizeof(*(__ai_ptr))); __ret; }); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct llist_node *__llist_del_all(struct llist_head *head)
{
 struct llist_node *first = head->first;

 head->first = ((void *)0);
 return first;
}

extern struct llist_node *llist_del_first(struct llist_head *head);

struct llist_node *llist_reverse_order(struct llist_node *head);
# 6 "./include/linux/smp_types.h" 2

enum {
 CSD_FLAG_LOCK = 0x01,

 IRQ_WORK_PENDING = 0x01,
 IRQ_WORK_BUSY = 0x02,
 IRQ_WORK_LAZY = 0x04, /* No IPI, wait for tick */
 IRQ_WORK_HARD_IRQ = 0x08, /* IRQ context on PREEMPT_RT */

 IRQ_WORK_CLAIMED = (IRQ_WORK_PENDING | IRQ_WORK_BUSY),

 CSD_TYPE_ASYNC = 0x00,
 CSD_TYPE_SYNC = 0x10,
 CSD_TYPE_IRQ_WORK = 0x20,
 CSD_TYPE_TTWU = 0x30,

 CSD_FLAG_TYPE_MASK = 0xF0,
};

/*
 * struct __call_single_node is the primary type on
 * smp.c:call_single_queue.
 *
 * flush_smp_call_function_queue() only reads the type from
 * __call_single_node::u_flags as a regular load, the above
 * (anonymous) enum defines all the bits of this word.
 *
 * Other bits are not modified until the type is known.
 *
 * CSD_TYPE_SYNC/ASYNC:
 *	struct {
 *		struct llist_node node;
 *		unsigned int flags;
 *		smp_call_func_t func;
 *		void *info;
 *	};
 *
 * CSD_TYPE_IRQ_WORK:
 *	struct {
 *		struct llist_node node;
 *		atomic_t flags;
 *		void (*func)(struct irq_work *);
 *	};
 *
 * CSD_TYPE_TTWU:
 *	struct {
 *		struct llist_node node;
 *		unsigned int flags;
 *	};
 *
 */

struct __call_single_node {
 struct llist_node llist;
 union {
  unsigned int u_flags;
  atomic_t a_flags;
 };

 u16 src, dst;

};
# 16 "./include/linux/smp.h" 2

typedef void (*smp_call_func_t)(void *info);
typedef bool (*smp_cond_func_t)(int cpu, void *info);

/*
 * structure shares (partial) layout with struct irq_work
 */
struct __call_single_data {
 struct __call_single_node node;
 smp_call_func_t func;
 void *info;
};




/* Use __aligned() to avoid to use 2 cache lines for 1 csd */
typedef struct __call_single_data call_single_data_t
 __attribute__((__aligned__(sizeof(struct __call_single_data))));






/*
 * Enqueue a llist_node on the call_single_queue; be very careful, read
 * flush_smp_call_function_queue() in detail.
 */
extern void __smp_call_single_queue(int cpu, struct llist_node *node);

/* total number of cpus in this system (may exceed NR_CPUS) */
extern unsigned int total_cpus;

int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
        int wait);

void on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,
      void *info, bool wait, const struct cpumask *mask);

int smp_call_function_single_async(int cpu, struct __call_single_data *csd);

/*
 * Cpus stopping functions in panic. All have default weak definitions.
 * Architecture-dependent code may override them.
 */
void panic_smp_self_stop(void);
void nmi_panic_self_stop(struct pt_regs *regs);
void crash_smp_send_stop(void);

/*
 * Call a function on all processors
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void on_each_cpu(smp_call_func_t func, void *info, int wait)
{
 on_each_cpu_cond_mask(((void *)0), func, info, wait, ((const struct cpumask *)&__cpu_online_mask));
}

/**
 * on_each_cpu_mask(): Run a function on processors specified by
 * cpumask, which may include the local processor.
 * @mask: The set of cpus to run on (only runs on online subset).
 * @func: The function to run. This must be fast and non-blocking.
 * @info: An arbitrary pointer to pass to the function.
 * @wait: If true, wait (atomically) until function has completed
 *        on other CPUs.
 *
 * If @wait is true, then returns once @func has returned.
 *
 * You must not call this function with disabled interrupts or from a
 * hardware interrupt handler or from a bottom half handler.  The
 * exception is that it may be used during early boot while
 * early_boot_irqs_disabled is set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void on_each_cpu_mask(const struct cpumask *mask,
        smp_call_func_t func, void *info, bool wait)
{
 on_each_cpu_cond_mask(((void *)0), func, info, wait, mask);
}

/*
 * Call a function on each processor for which the supplied function
 * cond_func returns a positive value. This may include the local
 * processor.  May be used during early boot while early_boot_irqs_disabled is
 * set. Use local_irq_save/restore() instead of local_irq_disable/enable().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void on_each_cpu_cond(smp_cond_func_t cond_func,
        smp_call_func_t func, void *info, bool wait)
{
 on_each_cpu_cond_mask(cond_func, func, info, wait, ((const struct cpumask *)&__cpu_online_mask));
}






# 1 "./arch/arm64/include/asm/smp.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */





/* Values for secondary_data.status */





/* The cpu invoked ops->cpu_die, synchronise it with cpu_kill */

/* The cpu couldn't die gracefully and is looping in the kernel */

/* Fatal system error detected by secondary CPU, crash the system */
# 34 "./arch/arm64/include/asm/smp.h"
extern /* nothing */ __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(int) cpu_number;

/*
 * We don't use this_cpu_read(cpu_number) as that has implicit writes to
 * preempt_count, and associated (compiler) barriers, that we'd like to avoid
 * the expense of. If we're preemptible, the value can be stale at use anyway.
 * And we can't use this_cpu_ptr() either, as that winds up recursing back
 * here under CONFIG_DEBUG_PREEMPT=y.
 */


/*
 * Logical CPU mapping.
 */
extern u64 __cpu_logical_map[256];
extern u64 cpu_logical_map(unsigned int cpu);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_cpu_logical_map(unsigned int cpu, u64 hwid)
{
 __cpu_logical_map[cpu] = hwid;
}

struct seq_file;

/*
 * Discover the set of possible CPUs and determine their
 * SMP operations.
 */
extern void smp_init_cpus(void);

/*
 * Register IPI interrupts with the arch SMP code
 */
extern void set_smp_ipi_range(int ipi_base, int nr_ipi);

/*
 * Called from the secondary holding pen, this is the secondary CPU entry point.
 */
           void secondary_start_kernel(void);

/*
 * Initial data for bringing up a secondary CPU.
 * @status - Result passed back from the secondary CPU to
 *           indicate failure.
 */
struct secondary_data {
 struct task_struct *task;
 long status;
};

extern struct secondary_data secondary_data;
extern long __early_cpu_boot_status;
extern void secondary_entry(void);

extern void arch_send_call_function_single_ipi(int cpu);
extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_send_wakeup_ipi_mask(const struct cpumask *mask)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_166(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_166(); } while (0);
# 97 "./arch/arm64/include/asm/smp.h"
}


extern int __cpu_disable(void);

extern void __cpu_die(unsigned int cpu);
extern void cpu_die(void);
extern void cpu_die_early(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpu_park_loop(void)
{
 for (;;) {
  asm volatile("wfe" : : : "memory");
  asm volatile("wfi" : : : "memory");
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void update_cpu_boot_status(int val)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_167(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(secondary_data.status) == sizeof(char) || sizeof(secondary_data.status) == sizeof(short) || sizeof(secondary_data.status) == sizeof(int) || sizeof(secondary_data.status) == sizeof(long)) || sizeof(secondary_data.status) == sizeof(long long))) __compiletime_assert_167(); } while (0); do { *(volatile typeof(secondary_data.status) *)&(secondary_data.status) = (val); } while (0); } while (0);
# 117 "./arch/arm64/include/asm/smp.h"
 /* Ensure the visibility of the status update */
 asm volatile("dsb " "ishst" : : : "memory");
}

/*
 * The calling secondary CPU has detected serious configuration mismatch,
 * which calls for a kernel panic. Update the boot status and park the calling
 * CPU.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpu_panic_kernel(void)
{
 update_cpu_boot_status((3));
 cpu_park_loop();
}

/*
 * If a secondary CPU enters the kernel but fails to come online,
 * (e.g. due to mismatched features), and cannot exit the kernel,
 * we increment cpus_stuck_in_kernel and leave the CPU in a
 * quiesecent loop within the kernel text. The memory containing
 * this loop must not be re-used for anything else as the 'stuck'
 * core is executing it.
 *
 * This function is used to inhibit features like kexec and hibernate.
 */
bool cpus_are_stuck_in_kernel(void);

extern void crash_smp_send_stop(void);
extern bool smp_crash_stop_failed(void);
extern void panic_smp_self_stop(void);
# 114 "./include/linux/smp.h" 2

/*
 * main cross-CPU interfaces, handles INIT, TLB flush, STOP, etc.
 * (defined in asm header):
 */

/*
 * stops all CPUs but the current one:
 */
extern void smp_send_stop(void);

/*
 * sends a 'reschedule' event to another CPU:
 */
extern void smp_send_reschedule(int cpu);


/*
 * Prepare machine for booting other CPUs.
 */
extern void smp_prepare_cpus(unsigned int max_cpus);

/*
 * Bring a CPU up
 */
extern int __cpu_up(unsigned int cpunum, struct task_struct *tidle);

/*
 * Final polishing of CPUs
 */
extern void smp_cpus_done(unsigned int max_cpus);

/*
 * Call a function on all other processors
 */
void smp_call_function(smp_call_func_t func, void *info, int wait);
void smp_call_function_many(const struct cpumask *mask,
       smp_call_func_t func, void *info, bool wait);

int smp_call_function_any(const struct cpumask *mask,
     smp_call_func_t func, void *info, int wait);

void kick_all_cpus_sync(void);
void wake_up_all_idle_cpus(void);

/*
 * Generic and arch helpers
 */
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) call_function_init(void);
void generic_smp_call_function_single_interrupt(void);



/*
 * Mark the boot cpu "online" so that it can call console drivers in
 * printk() and can access its per-cpu storage.
 */
void smp_prepare_boot_cpu(void);

extern unsigned int setup_max_cpus;
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) setup_nr_cpu_ids(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) smp_init(void);

extern int __boot_cpu_id;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_boot_cpu_id(void)
{
 return __boot_cpu_id;
}
# 228 "./include/linux/smp.h"
/**
 * raw_processor_id() - get the current (unstable) CPU id
 *
 * For then you know what you are doing and need an unstable
 * CPU id.
 */

/**
 * smp_processor_id() - get the current (stable) CPU id
 *
 * This is the normal accessor to the CPU id and should be used
 * whenever possible.
 *
 * The CPU id is stable when:
 *
 *  - IRQs are disabled;
 *  - preemption is disabled;
 *  - the task is CPU affine.
 *
 * When CONFIG_DEBUG_PREEMPT; we verify these assumption and WARN
 * when smp_processor_id() is used when the CPU id is not stable.
 */

/*
 * Allow the architecture to differentiate between a stable and unstable read.
 * For example, x86 uses an IRQ-safe asm-volatile read for the unstable but a
 * regular asm read for the stable.
 */
# 270 "./include/linux/smp.h"
/*
 * Callback to arch code if there's nosmp or maxcpus=0 on the
 * boot command line:
 */
extern void arch_disable_smp_support(void);

extern void arch_thaw_secondary_cpus_begin(void);
extern void arch_thaw_secondary_cpus_end(void);

void smp_setup_processor_id(void);

int smp_call_on_cpu(unsigned int cpu, int (*func)(void *), void *par,
      bool phys);

/* SMP core functions */
int smpcfd_prepare_cpu(unsigned int cpu);
int smpcfd_dead_cpu(unsigned int cpu);
int smpcfd_dying_cpu(unsigned int cpu);
# 15 "./include/linux/lockdep.h" 2


struct task_struct;
# 344 "./include/linux/lockdep.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_init_task(struct task_struct *task)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_off(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_on(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_set_selftest_task(struct task_struct *task)
{
}
# 384 "./include/linux/lockdep.h"
/*
 * We don't define lockdep_match_class() and lockdep_match_key() for !LOCKDEP
 * case since the result is not well defined and the caller should rather
 * #ifdef the call himself.
 */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_register_key(struct lock_class_key *key)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_unregister_key(struct lock_class_key *key)
{
}



/*
 * Dummy forward declarations, allow users to write less ifdef-y code
 * and depend on dead code elimination.
 */
extern int lock_is_held(const void *);
extern int lockdep_is_held(const void *);
# 432 "./include/linux/lockdep.h"
enum xhlock_context_t {
 XHLOCK_HARD,
 XHLOCK_SOFT,
 XHLOCK_CTX_NR,
};


/*
 * To initialize a lockdep_map statically use this macro.
 * Note that _name must not be NULL.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_invariant_state(bool force) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_free_task(struct task_struct *task) {}
# 491 "./include/linux/lockdep.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void print_irqtrace_events(struct task_struct *curr)
{
}


/* Variable used to make lockdep treat read_lock() as recursive in selftests */
# 506 "./include/linux/lockdep.h"
/* If !LOCKDEP, the value is meaningless */



/*
 * For trivial one-depth nesting of a lock-class, the following
 * global define can be used. (Subsystems with multiple levels
 * of nesting should define their own lock-nesting subclasses.)
 */


/*
 * Map the dependency ops to NOP or to real lockdep ops, depending
 * on the per lock-class debug mode:
 */
# 659 "./include/linux/lockdep.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
lockdep_rcu_suspicious(const char *file, const int line, const char *s)
{
}
# 64 "./include/linux/spinlock.h" 2

# 1 "./arch/arm64/include/generated/asm/mmiowb.h" 1
# 1 "./include/asm-generic/mmiowb.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Generic implementation of mmiowb() tracking for spinlocks.
 *
 * If your architecture doesn't ensure that writes to an I/O peripheral
 * within two spinlocked sections on two different CPUs are seen by the
 * peripheral in the order corresponding to the lock handover, then you
 * need to follow these FIVE easy steps:
 *
 * 	1. Implement mmiowb() (and arch_mmiowb_state() if you're fancy)
 *	   in asm/mmiowb.h, then #include this file
 *	2. Ensure your I/O write accessors call mmiowb_set_pending()
 *	3. Select ARCH_HAS_MMIOWB
 *	4. Untangle the resulting mess of header files
 *	5. Complain to your architects
 */
# 2 "./arch/arm64/include/generated/asm/mmiowb.h" 2
# 66 "./include/linux/spinlock.h" 2


/*
 * Must define these before including other files, inline functions need them
 */
# 85 "./include/linux/spinlock.h"
/*
 * Pull the arch_spinlock_t and arch_rwlock_t definitions:
 */
# 1 "./include/linux/spinlock_types.h" 1



/*
 * include/linux/spinlock_types.h - generic spinlock type definitions
 *                                  and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */





/* Non PREEMPT_RT kernels map spinlock to raw_spinlock */
typedef struct spinlock {
 union {
  struct raw_spinlock rlock;
# 28 "./include/linux/spinlock_types.h"
 };
} spinlock_t;
# 74 "./include/linux/spinlock_types.h"
# 1 "./include/linux/rwlock_types.h" 1
# 19 "./include/linux/rwlock_types.h"
/*
 * generic rwlock type definitions and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
typedef struct {
 arch_rwlock_t raw_lock;







} rwlock_t;
# 75 "./include/linux/spinlock_types.h" 2
# 89 "./include/linux/spinlock.h" 2

/*
 * Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):
 */

# 1 "./arch/arm64/include/asm/spinlock.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./arch/arm64/include/generated/asm/qspinlock.h" 1
# 1 "./include/asm-generic/qspinlock.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * Queued spinlock
 *
 * A 'generic' spinlock implementation that is based on MCS locks. For an
 * architecture that's looking for a 'generic' spinlock, please first consider
 * ticket-lock.h and only come looking here when you've considered all the
 * constraints below and can show your hardware does actually perform better
 * with qspinlock.
 *
 * qspinlock relies on atomic_*_release()/atomic_*_acquire() to be RCsc (or no
 * weaker than RCtso if you're power), where regular code only expects atomic_t
 * to be RCpc.
 *
 * qspinlock relies on a far greater (compared to asm-generic/spinlock.h) set
 * of atomic operations to behave well together, please audit them carefully to
 * ensure they all have forward progress. Many atomic operations may default to
 * cmpxchg() loops which will not have good forward progress properties on
 * LL/SC architectures.
 *
 * One notable example is atomic_fetch_or_acquire(), which x86 cannot (cheaply)
 * do. Carefully read the patches that introduced
 * queued_fetch_set_pending_acquire().
 *
 * qspinlock also heavily relies on mixed size atomic operations, in specific
 * it requires architectures to have xchg16; something which many LL/SC
 * architectures need to implement as a 32bit and+or in order to satisfy the
 * forward progress guarantees mentioned above.
 *
 * Further reading on mixed size atomics that might be relevant:
 *
 *   http://www.cl.cam.ac.uk/~pes20/popl17/mixed-size.pdf
 *
 * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.
 * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP
 *
 * Authors: Waiman Long <waiman.long@hpe.com>
 */







/**
 * queued_spin_is_locked - is the spinlock locked?
 * @lock: Pointer to queued spinlock structure
 * Return: 1 if it is locked, 0 otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int queued_spin_is_locked(struct qspinlock *lock)
{
 /*
	 * Any !0 state indicates it is locked, even if _Q_LOCKED_VAL
	 * isn't immediately observable.
	 */
 return atomic_read(&lock->val);
}


/**
 * queued_spin_value_unlocked - is the spinlock structure unlocked?
 * @lock: queued spinlock structure
 * Return: 1 if it is unlocked, 0 otherwise
 *
 * N.B. Whenever there are tasks waiting for the lock, it is considered
 *      locked wrt the lockref code to avoid lock stealing by the lockref
 *      code and change things underneath the lock. This also allows some
 *      optimizations to be applied without conflict with lockref.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int queued_spin_value_unlocked(struct qspinlock lock)
{
 return !atomic_read(&lock.val);
}

/**
 * queued_spin_is_contended - check if the lock is contended
 * @lock : Pointer to queued spinlock structure
 * Return: 1 if lock contended, 0 otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int queued_spin_is_contended(struct qspinlock *lock)
{
 return atomic_read(&lock->val) & ~(((1U << 8) - 1) << 0);
}
/**
 * queued_spin_trylock - try to acquire the queued spinlock
 * @lock : Pointer to queued spinlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int queued_spin_trylock(struct qspinlock *lock)
{
 int val = atomic_read(&lock->val);

 if (__builtin_expect(!!(val), 0))
  return 0;

 return __builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->val, &val, (1U << 0))), 1);
}

extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);


/**
 * queued_spin_lock - acquire a queued spinlock
 * @lock: Pointer to queued spinlock structure
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void queued_spin_lock(struct qspinlock *lock)
{
 int val = 0;

 if (__builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->val, &val, (1U << 0))), 1))
  return;

 queued_spin_lock_slowpath(lock, val);
}



/**
 * queued_spin_unlock - release a queued spinlock
 * @lock : Pointer to queued spinlock structure
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void queued_spin_unlock(struct qspinlock *lock)
{
 /*
	 * unlock() needs release semantics:
	 */
 do { do { } while (0); do { typeof(&lock->locked) __p = (&lock->locked); union { typeof( _Generic((*&lock->locked), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&lock->locked))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&lock->locked), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&lock->locked)))) (0) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_168(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&lock->locked) == sizeof(char) || sizeof(*&lock->locked) == sizeof(short) || sizeof(*&lock->locked) == sizeof(int) || sizeof(*&lock->locked) == sizeof(long)))) __compiletime_assert_168(); } while (0); kasan_check_write(__p, sizeof(*&lock->locked)); switch (sizeof(*&lock->locked)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0);
# 129 "./include/asm-generic/qspinlock.h"
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool virt_spin_lock(struct qspinlock *lock)
{
 return false;
}


/*
 * Remapping spinlock architecture specific functions to the corresponding
 * queued spinlock functions.
 */
# 2 "./arch/arm64/include/generated/asm/qspinlock.h" 2
# 9 "./arch/arm64/include/asm/spinlock.h" 2
# 1 "./arch/arm64/include/generated/asm/qrwlock.h" 1
# 1 "./include/asm-generic/qrwlock.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * Queue read/write lock
 *
 * These use generic atomic and locking routines, but depend on a fair spinlock
 * implementation in order to be fair themselves.  The implementation in
 * asm-generic/spinlock.h meets these requirements.
 *
 * (C) Copyright 2013-2014 Hewlett-Packard Development Company, L.P.
 *
 * Authors: Waiman Long <waiman.long@hp.com>
 */





# 1 "./arch/arm64/include/asm/processor.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/processor.h
 *
 * Copyright (C) 1995-1999 Russell King
 * Copyright (C) 2012 ARM Ltd.
 */



/*
 * On arm64 systems, unaligned accesses by the CPU are cheap, and so there is
 * no point in shifting all network buffers by 2 bytes just to make some IP
 * header fields appear aligned in memory, potentially sacrificing some DMA
 * performance on some platforms.
 */
# 29 "./arch/arm64/include/asm/processor.h"
# 1 "./include/linux/cache.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/asm/cache.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 19 "./arch/arm64/include/asm/cache.h"
/*
 * Memory returned by kmalloc() may be used for DMA, so we must make
 * sure that all such allocations are cache aligned. Otherwise,
 * unrelated code may cause parts of the buffer to be read into the
 * cache before the transfer is done, causing old data to be seen by
 * the CPU.
 */





# 1 "./include/linux/kasan-enabled.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/static_key.h" 1
# 6 "./include/linux/kasan-enabled.h" 2
# 23 "./include/linux/kasan-enabled.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kasan_enabled(void)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kasan_hw_tags_enabled(void)
{
 return false;
}
# 32 "./arch/arm64/include/asm/cache.h" 2


# 1 "./arch/arm64/include/asm/mte-def.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright (C) 2020 ARM Ltd.
 */
# 35 "./arch/arm64/include/asm/cache.h" 2
# 52 "./arch/arm64/include/asm/cache.h"
extern unsigned long __icache_flags;

/*
 * Whilst the D-side always behaves as PIPT on AArch64, aliasing is
 * permitted in the I-cache.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int icache_is_aliasing(void)
{
 return ((__builtin_constant_p(0) && __builtin_constant_p((uintptr_t)(&__icache_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&__icache_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&__icache_flags))) ? const_test_bit(0, &__icache_flags) : generic_test_bit(0, &__icache_flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int icache_is_vpipt(void)
{
 return ((__builtin_constant_p(1) && __builtin_constant_p((uintptr_t)(&__icache_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&__icache_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&__icache_flags))) ? const_test_bit(1, &__icache_flags) : generic_test_bit(1, &__icache_flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 cache_type_cwg(void)
{
 return ({ ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_169(void) __attribute__((__error__("FIELD_GET: " "mask is not constant"))); if (!(!(!__builtin_constant_p(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27))))))))) __compiletime_assert_169(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_170(void) __attribute__((__error__("FIELD_GET: " "mask is zero"))); if (!(!((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) == 0))) __compiletime_assert_170(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_171(void) __attribute__((__error__("FIELD_GET: " "value too large for the field"))); if (!(!(__builtin_constant_p(0U) ? ~((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) >> (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) - 1)) & (0U) : 0))) __compiletime_assert_171(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_172(void) __attribute__((__error__("FIELD_GET: " "type of reg too small for mask"))); if (!(!(((typeof( _Generic((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))))))(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27))))))) > ((typeof( _Generic((read_cpuid_cachetype()), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (read_cpuid_cachetype()))))(~0ull))))) __compiletime_assert_172(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_173(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) - 1))) - 1)) != 0"))); if (!(!((((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) - 1))) - 1)) != 0))) __compiletime_assert_173(); } while (0); }); (typeof(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))))(((read_cpuid_cachetype()) & (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27))))))) >> (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((24) > (27)) * 0l)) : (int *)8))), (24) > (27), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (24)) + 1) & (~(((0UL))) >> (64 - 1 - (27)))))) - 1)); });
# 71 "./arch/arm64/include/asm/cache.h"
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cache_line_size_of_cpu(void)
{
 u32 cwg = cache_type_cwg();

 return cwg ? 4 << cwg : (128);
}

int cache_line_size(void);

/*
 * Read the effective value of CTR_EL0.
 *
 * According to ARM ARM for ARMv8-A (ARM DDI 0487C.a),
 * section D10.2.33 "CTR_EL0, Cache Type Register" :
 *
 * CTR_EL0.IDC reports the data cache clean requirements for
 * instruction to data coherence.
 *
 *  0 - dcache clean to PoU is required unless :
 *     (CLIDR_EL1.LoC == 0) || (CLIDR_EL1.LoUIS == 0 && CLIDR_EL1.LoUU == 0)
 *  1 - dcache clean to PoU is not required for i-to-d coherence.
 *
 * This routine provides the CTR_EL0 with the IDC field updated to the
 * effective state.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __attribute__((__const__)) read_cpuid_effective_cachetype(void)
{
 u32 ctr = read_cpuid_cachetype();

 if (!(ctr & ((((1UL))) << (28)))) {
  u64 clidr = ({ u64 __val; asm volatile("mrs %0, " "clidr_el1" : "=r" (__val)); __val; });

  if ((((clidr) >> 24) & 0x7) == 0 ||
      ((((clidr) >> 21) & 0x7) == 0 && (((clidr) >> 27) & 0x7) == 0))
   ctr |= ((((1UL))) << (28));
 }

 return ctr;
}
# 7 "./include/linux/cache.h" 2
# 16 "./include/linux/cache.h"
/*
 * __read_mostly is used to keep rarely changing variables out of frequently
 * updated cachelines. Its use should be reserved for data that is used
 * frequently in hot paths. Performance traces can help decide when to use
 * this. You want __read_mostly data to be tightly packed, so that in the
 * best case multiple frequently read variables for a hot path will be next
 * to each other in order to reduce the number of cachelines needed to
 * execute a critical path. We should be mindful and selective of its use.
 * ie: if you're going to use it please supply a *good* justification in your
 * commit log
 */




/*
 * __ro_after_init is used to mark things that are read-only after init (i.e.
 * after mark_rodata_ro() has been called). These are effectively read-only,
 * but may get written to during init, so can't live in .rodata (via "const").
 */
# 66 "./include/linux/cache.h"
/*
 * The maximum alignment needed for some critical structures
 * These could be inter-node cacheline sizes/L3 cacheline
 * size etc.  Define this in asm/cache.h for your arch
 */
# 88 "./include/linux/cache.h"
/*
 * Helper to add padding within a struct to ensure data fall into separate
 * cachelines.
 */

struct cacheline_padding {
 char x[0];
} __attribute__((__aligned__(1 << ((6)))));
# 30 "./arch/arm64/include/asm/processor.h" 2





# 1 "./include/vdso/processor.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2020 ARM Ltd.
 */





# 1 "./arch/arm64/include/asm/vdso/processor.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2020 ARM Ltd.
 */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpu_relax(void)
{
 asm volatile("yield" ::: "memory");
}
# 11 "./include/vdso/processor.h" 2
# 36 "./arch/arm64/include/asm/processor.h" 2



# 1 "./arch/arm64/include/asm/hw_breakpoint.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */






# 1 "./arch/arm64/include/asm/virt.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 * Author: Marc Zyngier <marc.zyngier@arm.com>
 */




/*
 * The arm64 hcall implementation uses x0 to specify the hcall
 * number. A value less than HVC_STUB_HCALL_NR indicates a special
 * hcall, such as set vector. Any other value is handled in a
 * hypervisor specific way.
 *
 * The hypercall is allowed to clobber any of the caller-saved
 * registers (x0-x18), so it is advisable to use it through the
 * indirection of a function call (as implemented in hyp-stub.S).
 */

/*
 * HVC_SET_VECTORS - Set the value of the vbar_el2 register.
 *
 * @x1: Physical address of the new vector table.
 */


/*
 * HVC_SOFT_RESTART - CPU soft reset, used by the cpu_soft_restart routine.
 */


/*
 * HVC_RESET_VECTORS - Restore the vectors to the original HYP stubs
 */


/*
 * HVC_FINALISE_EL2 - Upgrade the CPU from EL1 to EL2, if possible
 */


/* Max number of HYP stub hypercalls */


/* Error returned when an invalid stub number is passed into x0 */





/*
 * Flags returned together with the boot mode, but not preserved in
 * __boot_cpu_mode. Used by the idreg override code to work out the
 * boot state.
 */





# 1 "./arch/arm64/include/asm/sections.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2016 ARM Limited
 */



# 1 "./include/asm-generic/sections.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/* References to section boundaries */




/*
 * Usage guidelines:
 * _text, _data: architecture specific, don't use them in arch-independent code
 * [_stext, _etext]: contains .text.* sections, may also contain .rodata.*
 *                   and/or .init.* sections
 * [_sdata, _edata]: contains .data.* sections, may also contain .rodata.*
 *                   and/or .init.* sections.
 * [__start_rodata, __end_rodata]: contains .rodata.* sections
 * [__start_ro_after_init, __end_ro_after_init]:
 *		     contains .data..ro_after_init section
 * [__init_begin, __init_end]: contains .init.* sections, but .init.text.*
 *                   may be out of this range on some architectures.
 * [_sinittext, _einittext]: contains .init.text.* sections
 * [__bss_start, __bss_stop]: contains BSS sections
 *
 * Following global variables are optional and may be unavailable on some
 * architectures and/or kernel configurations.
 *	_text, _data
 *	__kprobes_text_start, __kprobes_text_end
 *	__entry_text_start, __entry_text_end
 *	__ctors_start, __ctors_end
 *	__irqentry_text_start, __irqentry_text_end
 *	__softirqentry_text_start, __softirqentry_text_end
 *	__start_opd, __end_opd
 */
extern char _text[], _stext[], _etext[];
extern char _data[], _sdata[], _edata[];
extern char __bss_start[], __bss_stop[];
extern char __init_begin[], __init_end[];
extern char _sinittext[], _einittext[];
extern char __start_ro_after_init[], __end_ro_after_init[];
extern char _end[];
extern char __per_cpu_load[], __per_cpu_start[], __per_cpu_end[];
extern char __kprobes_text_start[], __kprobes_text_end[];
extern char __entry_text_start[], __entry_text_end[];
extern char __start_rodata[], __end_rodata[];
extern char __irqentry_text_start[], __irqentry_text_end[];
extern char __softirqentry_text_start[], __softirqentry_text_end[];
extern char __start_once[], __end_once[];

/* Start and end of .ctors section - used for constructor calls. */
extern char __ctors_start[], __ctors_end[];

/* Start and end of .opd section - used for function descriptors. */
extern char __start_opd[], __end_opd[];

/* Start and end of instrumentation protected text section */
extern char __noinstr_text_start[], __noinstr_text_end[];

extern const void __nosave_begin, __nosave_end;

/* Function descriptor handling (if any).  Override in asm/sections.h */







/* An address is simply the address of the function. */
typedef struct {
 unsigned long addr;
} func_desc_t;


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool have_function_descriptors(void)
{
 return 0;
}

/**
 * memory_contains - checks if an object is contained within a memory region
 * @begin: virtual address of the beginning of the memory region
 * @end: virtual address of the end of the memory region
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if the object specified by @virt and @size is entirely
 * contained within the memory region defined by @begin and @end, false
 * otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool memory_contains(void *begin, void *end, void *virt,
       size_t size)
{
 return virt >= begin && virt + size <= end;
}

/**
 * memory_intersects - checks if the region occupied by an object intersects
 *                     with another memory region
 * @begin: virtual address of the beginning of the memory region
 * @end: virtual address of the end of the memory region
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if an object's memory region, specified by @virt and @size,
 * intersects with the region specified by @begin and @end, false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool memory_intersects(void *begin, void *end, void *virt,
         size_t size)
{
 void *vend = virt + size;

 if (virt < end && vend > begin)
  return true;

 return false;
}

/**
 * init_section_contains - checks if an object is contained within the init
 *                         section
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if the object specified by @virt and @size is entirely
 * contained within the init section, false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool init_section_contains(void *virt, size_t size)
{
 return memory_contains(__init_begin, __init_end, virt, size);
}

/**
 * init_section_intersects - checks if the region occupied by an object
 *                           intersects with the init section
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if an object's memory region, specified by @virt and @size,
 * intersects with the init section, false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool init_section_intersects(void *virt, size_t size)
{
 return memory_intersects(__init_begin, __init_end, virt, size);
}

/**
 * is_kernel_core_data - checks if the pointer address is located in the
 *			 .data or .bss section
 *
 * @addr: address to check
 *
 * Returns: true if the address is located in .data or .bss, false otherwise.
 * Note: On some archs it may return true for core RODATA, and false
 *       for others. But will always be true for core RW data.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kernel_core_data(unsigned long addr)
{
 if (addr >= (unsigned long)_sdata && addr < (unsigned long)_edata)
  return true;

 if (addr >= (unsigned long)__bss_start &&
     addr < (unsigned long)__bss_stop)
  return true;

 return false;
}

/**
 * is_kernel_rodata - checks if the pointer address is located in the
 *                    .rodata section
 *
 * @addr: address to check
 *
 * Returns: true if the address is located in .rodata, false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kernel_rodata(unsigned long addr)
{
 return addr >= (unsigned long)__start_rodata &&
        addr < (unsigned long)__end_rodata;
}

/**
 * is_kernel_inittext - checks if the pointer address is located in the
 *                      .init.text section
 *
 * @addr: address to check
 *
 * Returns: true if the address is located in .init.text, false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kernel_inittext(unsigned long addr)
{
 return addr >= (unsigned long)_sinittext &&
        addr < (unsigned long)_einittext;
}

/**
 * __is_kernel_text - checks if the pointer address is located in the
 *                    .text section
 *
 * @addr: address to check
 *
 * Returns: true if the address is located in .text, false otherwise.
 * Note: an internal helper, only check the range of _stext to _etext.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __is_kernel_text(unsigned long addr)
{
 return addr >= (unsigned long)_stext &&
        addr < (unsigned long)_etext;
}

/**
 * __is_kernel - checks if the pointer address is located in the kernel range
 *
 * @addr: address to check
 *
 * Returns: true if the address is located in the kernel range, false otherwise.
 * Note: an internal helper, check the range of _stext to _end,
 *       and range from __init_begin to __init_end, which can be outside
 *       of the _stext to _end range.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __is_kernel(unsigned long addr)
{
 return ((addr >= (unsigned long)_stext &&
          addr < (unsigned long)_end) ||
  (addr >= (unsigned long)__init_begin &&
   addr < (unsigned long)__init_end));
}
# 9 "./arch/arm64/include/asm/sections.h" 2

extern char __alt_instructions[], __alt_instructions_end[];
extern char __hibernate_exit_text_start[], __hibernate_exit_text_end[];
extern char __hyp_idmap_text_start[], __hyp_idmap_text_end[];
extern char __hyp_text_start[], __hyp_text_end[];
extern char __hyp_rodata_start[], __hyp_rodata_end[];
extern char __hyp_reloc_begin[], __hyp_reloc_end[];
extern char __hyp_bss_start[], __hyp_bss_end[];
extern char __idmap_text_start[], __idmap_text_end[];
extern char __initdata_begin[], __initdata_end[];
extern char __inittext_begin[], __inittext_end[];
extern char __exittext_begin[], __exittext_end[];
extern char __irqentry_text_start[], __irqentry_text_end[];
extern char __mmuoff_data_start[], __mmuoff_data_end[];
extern char __entry_tramp_text_start[], __entry_tramp_text_end[];
extern char __relocate_new_kernel_start[], __relocate_new_kernel_end[];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t entry_tramp_text_size(void)
{
 return __entry_tramp_text_end - __entry_tramp_text_start;
}
# 63 "./arch/arm64/include/asm/virt.h" 2



/*
 * __boot_cpu_mode records what mode CPUs were booted in.
 * A correctly-implemented bootloader must start all CPUs in the same mode:
 * In this case, both 32bit halves of __boot_cpu_mode will contain the
 * same value (either 0 if booted in EL1, BOOT_CPU_MODE_EL2 if booted in EL2).
 *
 * Should the bootloader fail to do this, the two values will be different.
 * This allows the kernel to flag an error when the secondaries have come up.
 */
extern u32 __boot_cpu_mode[2];



void __hyp_set_vectors(phys_addr_t phys_vector_base);
void __hyp_reset_vectors(void);

extern struct static_key_false kvm_protected_mode_initialized;

/* Reports the availability of HYP mode */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_hyp_mode_available(void)
{
 /*
	 * If KVM protected mode is initialized, all CPUs must have been booted
	 * in EL2. Avoid checking __boot_cpu_mode as CPUs now come up in EL1.
	 */
 if (1 &&
     ({ bool branch; if (__builtin_types_compatible_p(typeof(*&kvm_protected_mode_initialized), struct static_key_true)) branch = !arch_static_branch(&(&kvm_protected_mode_initialized)->key, true); else if (__builtin_types_compatible_p(typeof(*&kvm_protected_mode_initialized), struct static_key_false)) branch = !arch_static_branch_jump(&(&kvm_protected_mode_initialized)->key, true); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 1); }))
  return true;

 return (__boot_cpu_mode[0] == (0xe12) &&
  __boot_cpu_mode[1] == (0xe12));
}

/* Check if the bootloader has booted CPUs in different modes */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_hyp_mode_mismatched(void)
{
 /*
	 * If KVM protected mode is initialized, all CPUs must have been booted
	 * in EL2. Avoid checking __boot_cpu_mode as CPUs now come up in EL1.
	 */
 if (1 &&
     ({ bool branch; if (__builtin_types_compatible_p(typeof(*&kvm_protected_mode_initialized), struct static_key_true)) branch = !arch_static_branch(&(&kvm_protected_mode_initialized)->key, true); else if (__builtin_types_compatible_p(typeof(*&kvm_protected_mode_initialized), struct static_key_false)) branch = !arch_static_branch_jump(&(&kvm_protected_mode_initialized)->key, true); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 1); }))
  return false;

 return __boot_cpu_mode[0] != __boot_cpu_mode[1];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kernel_in_hyp_mode(void)
{
 return ({ u64 __val; asm volatile("mrs %0, " "CurrentEL" : "=r" (__val)); __val; }) == (2 << 2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool has_vhe(void)
{
 /*
	 * Code only run in VHE/NVHE hyp context can assume VHE is present or
	 * absent. Otherwise fall back to caps.
	 * This allows the compiler to discard VHE-specific code from the
	 * nVHE object, reducing the number of external symbol references
	 * needed to link.
	 */
 if (is_vhe_hyp_code())
  return true;
 else if (is_nvhe_hyp_code())
  return false;
 else
  return cpus_have_final_cap(38);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_protected_kvm_enabled(void)
{
 if (is_vhe_hyp_code())
  return false;
 else
  return cpus_have_final_cap(41);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_hyp_nvhe(void)
{
 return is_hyp_mode_available() && !is_kernel_in_hyp_mode();
}
# 12 "./arch/arm64/include/asm/hw_breakpoint.h" 2

struct arch_hw_breakpoint_ctrl {
 u32 __reserved : 19,
 len : 8,
 type : 2,
 privilege : 2,
 enabled : 1;
};

struct arch_hw_breakpoint {
 u64 address;
 u64 trigger;
 struct arch_hw_breakpoint_ctrl ctrl;
};

/* Privilege Levels */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 encode_ctrl_reg(struct arch_hw_breakpoint_ctrl ctrl)
{
 u32 val = (ctrl.len << 5) | (ctrl.type << 3) | (ctrl.privilege << 1) |
  ctrl.enabled;

 if (is_kernel_in_hyp_mode() && ctrl.privilege == 1)
  val |= (1 << 13);

 return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void decode_ctrl_reg(u32 reg,
       struct arch_hw_breakpoint_ctrl *ctrl)
{
 ctrl->enabled = reg & 0x1;
 reg >>= 1;
 ctrl->privilege = reg & 0x3;
 reg >>= 2;
 ctrl->type = reg & 0x3;
 reg >>= 2;
 ctrl->len = reg & 0xff;
}

/* Breakpoint */


/* Watchpoints */




/* Lengths */
# 74 "./arch/arm64/include/asm/hw_breakpoint.h"
/* Kernel stepping */




/*
 * Limits.
 * Changing these will require modifications to the register accessors.
 */



/* Virtual debug register bases. */





/* Debug register names. */





/* Accessor macros for the debug registers. */
# 107 "./arch/arm64/include/asm/hw_breakpoint.h"
struct task_struct;
struct notifier_block;
struct perf_event_attr;
struct perf_event;
struct pmu;

extern int arch_bp_generic_fields(struct arch_hw_breakpoint_ctrl ctrl,
      int *gen_len, int *gen_type, int *offset);
extern int arch_check_bp_in_kernelspace(struct arch_hw_breakpoint *hw);
extern int hw_breakpoint_arch_parse(struct perf_event *bp,
        const struct perf_event_attr *attr,
        struct arch_hw_breakpoint *hw);
extern int hw_breakpoint_exceptions_notify(struct notifier_block *unused,
        unsigned long val, void *data);

extern int arch_install_hw_breakpoint(struct perf_event *bp);
extern void arch_uninstall_hw_breakpoint(struct perf_event *bp);
extern void hw_breakpoint_pmu_read(struct perf_event *bp);
extern int hw_breakpoint_slots(int type);


extern void hw_breakpoint_thread_switch(struct task_struct *next);
extern void ptrace_hw_copy_thread(struct task_struct *task);
# 139 "./arch/arm64/include/asm/hw_breakpoint.h"
/* Determine number of BRP registers available. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_num_brps(void)
{
 u64 dfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((5) << 8) | ((0) << 5)));
 return 1 +
  cpuid_feature_extract_unsigned_field(dfr0,
      12);
}

/* Determine number of WRP registers available. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_num_wrps(void)
{
 u64 dfr0 = read_sanitised_ftr_reg((((3) << 19) | ((0) << 16) | ((0) << 12) | ((5) << 8) | ((0) << 5)));
 return 1 +
  cpuid_feature_extract_unsigned_field(dfr0,
      20);
}
# 40 "./arch/arm64/include/asm/processor.h" 2
# 1 "./arch/arm64/include/asm/kasan.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







# 1 "./arch/arm64/include/asm/mte-kasan.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright (C) 2020 ARM Ltd.
 */
# 18 "./arch/arm64/include/asm/mte-kasan.h"
/*
 * These functions are meant to be only used from KASAN runtime through
 * the arch_*() interface defined in asm/memory.h.
 * These functions don't include system_supports_mte() checks,
 * as KASAN only calls them when MTE is supported and enabled.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 mte_get_ptr_tag(void *ptr)
{
 /* Note: The format of KASAN tags is 0xF<x> */
 u8 tag = 0xF0 | (u8)(((u64)(ptr)) >> 56);

 return tag;
}

/* Get allocation tag for the address. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 mte_get_mem_tag(void *addr)
{
 asm(".arch " "armv8.5-a" "\n" ".arch_extension memtag\n" "ldg %0, [%0]"
  : "+r" (addr));

 return mte_get_ptr_tag(addr);
}

/* Generate a random tag. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 mte_get_random_tag(void)
{
 void *addr;

 asm(".arch " "armv8.5-a" "\n" ".arch_extension memtag\n" "irg %0, %0"
  : "=r" (addr));

 return mte_get_ptr_tag(addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __stg_post(u64 p)
{
 asm volatile(".arch " "armv8.5-a" "\n" ".arch_extension memtag\n" "stg %0, [%0], #16"
       : "+r"(p)
       :
       : "memory");
 return p;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __stzg_post(u64 p)
{
 asm volatile(".arch " "armv8.5-a" "\n" ".arch_extension memtag\n" "stzg %0, [%0], #16"
       : "+r"(p)
       :
       : "memory");
 return p;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __dc_gva(u64 p)
{
 asm volatile(".arch " "armv8.5-a" "\n" ".arch_extension memtag\n" "dc gva, %0" : : "r"(p) : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __dc_gzva(u64 p)
{
 asm volatile(".arch " "armv8.5-a" "\n" ".arch_extension memtag\n" "dc gzva, %0" : : "r"(p) : "memory");
}

/*
 * Assign allocation tags for a region of memory based on the pointer tag.
 * Note: The address must be non-NULL and MTE_GRANULE_SIZE aligned and
 * size must be MTE_GRANULE_SIZE aligned.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mte_set_mem_tag_range(void *addr, size_t size, u8 tag,
      bool init)
{
 u64 curr, mask, dczid, dczid_bs, dczid_dzp, end1, end2, end3;

 /* Read DC G(Z)VA block size from the system register. */
 dczid = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((0) << 12) | ((0) << 8) | ((7) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
 dczid_bs = 4ul << (dczid & 0xf);
 dczid_dzp = (dczid >> 4) & 1;

 curr = (u64)__tag_set(addr, tag);
 mask = dczid_bs - 1;
 /* STG/STZG up to the end of the first block. */
 end1 = curr | mask;
 end3 = curr + size;
 /* DC GVA / GZVA in [end1, end2) */
 end2 = end3 & ~mask;

 /*
	 * The following code uses STG on the first DC GVA block even if the
	 * start address is aligned - it appears to be faster than an alignment
	 * check + conditional branch. Also, if the range size is at least 2 DC
	 * GVA blocks, the first two loops can use post-condition to save one
	 * branch each.
	 */
# 128 "./arch/arm64/include/asm/mte-kasan.h"
 if (init)
  do { if (!dczid_dzp && size >= 2 * dczid_bs) { do { curr = __stzg_post(curr); } while (curr < end1); do { __dc_gzva(curr); curr += dczid_bs; } while (curr < end2); } while (curr < end3) curr = __stzg_post(curr); } while (0);
 else
  do { if (!dczid_dzp && size >= 2 * dczid_bs) { do { curr = __stg_post(curr); } while (curr < end1); do { __dc_gva(curr); curr += dczid_bs; } while (curr < end2); } while (curr < end3) curr = __stg_post(curr); } while (0);

}

void mte_enable_kernel_sync(void);
void mte_enable_kernel_async(void);
void mte_enable_kernel_asymm(void);
# 10 "./arch/arm64/include/asm/kasan.h" 2
# 1 "./arch/arm64/include/asm/pgtable-types.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Page table types definitions.
 *
 * Copyright (C) 2014 ARM Ltd.
 * Author: Catalin Marinas <catalin.marinas@arm.com>
 */




# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 13 "./arch/arm64/include/asm/pgtable-types.h" 2

typedef u64 pteval_t;
typedef u64 pmdval_t;
typedef u64 pudval_t;
typedef u64 p4dval_t;
typedef u64 pgdval_t;

/*
 * These are used to make use of C type-checking..
 */
typedef struct { pteval_t pte; } pte_t;




typedef struct { pmdval_t pmd; } pmd_t;





typedef struct { pudval_t pud; } pud_t;




typedef struct { pgdval_t pgd; } pgd_t;



typedef struct { pteval_t pgprot; } pgprot_t;
# 52 "./arch/arm64/include/asm/pgtable-types.h"
# 1 "./include/asm-generic/pgtable-nop4d.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







typedef struct { pgd_t pgd; } p4d_t;






/*
 * The "pgd_xxx()" functions here are trivial for a folded two-level
 * setup: the p4d is never bad, and a p4d always exists (as it's folded
 * into the pgd entry)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pgd_none(pgd_t pgd) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pgd_bad(pgd_t pgd) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pgd_present(pgd_t pgd) { return 1; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgd_clear(pgd_t *pgd) { }




/*
 * (p4ds are folded into pgds so this doesn't get actually called,
 * but the define is needed for a generic inline function.)
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)
{
 return (p4d_t *)pgd;
}







/*
 * allocating and freeing a p4d is trivial: the 1-entry p4d is
 * inside the pgd, so has no extra memory associated with it.
 */
# 53 "./arch/arm64/include/asm/pgtable-types.h" 2
# 11 "./arch/arm64/include/asm/kasan.h" 2
# 43 "./arch/arm64/include/asm/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_init(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_copy_shadow(pgd_t *pgdir) { }
# 41 "./arch/arm64/include/asm/processor.h" 2

# 1 "./arch/arm64/include/asm/pgtable-hwdef.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */





/*
 * Number of page-table levels required to address 'va_bits' wide
 * address, without section mapping. We resolve the top (va_bits - PAGE_SHIFT)
 * bits with (PAGE_SHIFT - 3) bits at each page table level. Hence:
 *
 *  levels = DIV_ROUND_UP((va_bits - PAGE_SHIFT), (PAGE_SHIFT - 3))
 *
 * where DIV_ROUND_UP(n, d) => (((n) + (d) - 1) / (d))
 *
 * We cannot include linux/kernel.h which defines DIV_ROUND_UP here
 * due to build issues. So we open code DIV_ROUND_UP here:
 *
 *	((((va_bits) - PAGE_SHIFT) + (PAGE_SHIFT - 3) - 1) / (PAGE_SHIFT - 3))
 *
 * which gets simplified as :
 */


/*
 * Size mapped by an entry at level n ( 0 <= n <= 3)
 * We map (PAGE_SHIFT - 3) at all translation levels and PAGE_SHIFT bits
 * in the final page. The maximum number of translation levels supported by
 * the architecture is 4. Hence, starting at level n, we have further
 * ((4 - n) - 1) levels of translation excluding the offset within the page.
 * So, the total number of bits mapped by an entry at level n is :
 *
 *  ((4 - n) - 1) * (PAGE_SHIFT - 3) + PAGE_SHIFT
 *
 * Rearranging it a bit we get :
 *   (4 - n) * (PAGE_SHIFT - 3) + 3
 */




/*
 * PMD_SHIFT determines the size a level 2 page table entry can map.
 */







/*
 * PUD_SHIFT determines the size a level 1 page table entry can map.
 */







/*
 * PGDIR_SHIFT determines the size a top-level page table entry can map
 * (depending on the configuration, this level can be 0, 1 or 2).
 */





/*
 * Contiguous page definitions.
 */
# 87 "./arch/arm64/include/asm/pgtable-hwdef.h"
/*
 * Hardware page table definitions.
 *
 * Level 0 descriptor (P4D).
 */
# 100 "./arch/arm64/include/asm/pgtable-hwdef.h"
/*
 * Level 1 descriptor (PUD).
 */
# 111 "./arch/arm64/include/asm/pgtable-hwdef.h"
/*
 * Level 2 descriptor (PMD).
 */





/*
 * Section
 */
# 134 "./arch/arm64/include/asm/pgtable-hwdef.h"
/*
 * AttrIndx[2:0] encoding (mapping attributes defined in the MAIR* registers).
 */



/*
 * Level 3 descriptor (PTE).
 */
# 167 "./arch/arm64/include/asm/pgtable-hwdef.h"
/*
 * AttrIndx[2:0] encoding (mapping attributes defined in the MAIR* registers).
 */



/*
 * Memory Attribute override for Stage-2 (MemAttr[3:0])
 */


/*
 * Highest possible physical address supported.
 */





/*
 * TCR flags.
 */
# 280 "./arch/arm64/include/asm/pgtable-hwdef.h"
/*
 * TTBR.
 */
# 43 "./arch/arm64/include/asm/processor.h" 2
# 1 "./arch/arm64/include/asm/pointer_auth.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/uapi/linux/prctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





/* Values to pass as first argument to prctl() */




/* Get/set current->mm->dumpable */



/* Get/set unaligned access control bits (if meaningful) */





/* Get/set whether or not to drop capabilities on setuid() away from
 * uid 0 (as per security/commoncap.c) */



/* Get/set floating-point emulation control bits (if meaningful) */





/* Get/set floating-point exception mode (if meaningful) */
# 47 "./include/uapi/linux/prctl.h"
/* Get/set whether we use statistical process timing or accurate timestamp
 * based process timing */
# 59 "./include/uapi/linux/prctl.h"
/* Get/set process endian */






/* Get/set process seccomp mode */



/* Get/set the capability bounding set (as per security/commoncap.c) */



/* Get/set the process' ability to use the timestamp counter instruction */





/* Get/set securebits (as per security/commoncap.c) */



/*
 * Get/set the timerslack as used by poll/select/nanosleep
 * A value of 0 means "use default"
 */






/*
 * Set early/late kill mode for hwpoison memory corruption.
 * This influences when the process gets killed on a memory corruption.
 */
# 108 "./include/uapi/linux/prctl.h"
/*
 * Tune up process memory map specifics.
 */
# 128 "./include/uapi/linux/prctl.h"
/*
 * This structure provides new memory descriptor
 * map which mostly modifies /proc/pid/stat[m]
 * output for a task. This mostly done in a
 * sake of checkpoint/restore functionality.
 */
struct prctl_mm_map {
 __u64 start_code; /* code section bounds */
 __u64 end_code;
 __u64 start_data; /* data section bounds */
 __u64 end_data;
 __u64 start_brk; /* heap for brk() syscall */
 __u64 brk;
 __u64 start_stack; /* stack starts at */
 __u64 arg_start; /* command line arguments bounds */
 __u64 arg_end;
 __u64 env_start; /* environment variables bounds */
 __u64 env_end;
 __u64 *auxv; /* auxiliary vector */
 __u32 auxv_size; /* vector size */
 __u32 exe_fd; /* /proc/$pid/exe link file */
};

/*
 * Set specific pid that is allowed to ptrace the current task.
 * A value of 0 mean "no process".
 */






/*
 * If no_new_privs is set, then operations that grant new privileges (i.e.
 * execve) will either fail or not grant them.  This affects suid/sgid,
 * file capabilities, and LSMs.
 *
 * Operations that merely manipulate or drop existing privileges (setresuid,
 * capset, etc.) will still work.  Drop those privileges if you want them gone.
 *
 * Changing LSM security domain is considered a new privilege.  So, for example,
 * asking selinux for a specific new context (e.g. with runcon) will result
 * in execve returning -EPERM.
 *
 * See Documentation/userspace-api/no_new_privs.rst for more details.
 */
# 183 "./include/uapi/linux/prctl.h"
/*
 * No longer implemented, but left here to ensure the numbers stay reserved:
 */
# 194 "./include/uapi/linux/prctl.h"
/* Control the ambient capability set */






/* arm64 Scalable Vector Extension controls */
/* Flag values must be kept in sync with ptrace NT_ARM_SVE interface */



/* Bits common to PR_SVE_SET_VL and PR_SVE_GET_VL */



/* Per task speculation control */


/* Speculation control variants */



/* Return and control values for PR_SET/GET_SPECULATION_CTRL */







/* Reset arm64 pointer authentication keys */







/* Tagged user address controls for arm64 */



/* MTE tag check fault modes */




/* MTE tag inclusion mask */


/* Unused; kept only for source compatibility */


/* Control reclaim behavior when allocating memory */



/* Dispatch syscalls to a userspace handler */



/* The control values for the user space selector when dispatch is enabled */



/* Set/get enabled arm64 pointer authentication keys */



/* Request the scheduler to share a core */
# 275 "./include/uapi/linux/prctl.h"
/* arm64 Scalable Matrix Extension controls */
/* Flag values must be in sync with SVE versions */



/* Bits common to PR_SME_SET_VL and PR_SME_GET_VL */
# 7 "./arch/arm64/include/asm/pointer_auth.h" 2
# 1 "./include/linux/random.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/random.h"
# 1 "./include/uapi/linux/random.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * include/linux/random.h
 *
 * Include file for the random number generator.
 */





# 1 "./include/uapi/linux/ioctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./arch/arm64/include/generated/uapi/asm/ioctl.h" 1
# 1 "./include/asm-generic/ioctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/asm-generic/ioctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/* ioctl command encoding: 32 bits total, command in lower 16 bits,
 * size of the parameter structure in the lower 14 bits of the
 * upper 16 bits.
 * Encoding the size of the parameter structure in the ioctl request
 * is useful for catching programs compiled with old versions
 * and to avoid overwriting user space outside the user buffer area.
 * The highest 2 bits are reserved for indicating the ``access mode''.
 * NOTE: This limits the max parameter size to 16kB -1 !
 */

/*
 * The following is for compatibility across the various Linux
 * platforms.  The generic ioctl numbering scheme doesn't really enforce
 * a type field.  De facto, however, the top 8 bits of the lower 16
 * bits are indeed used as a type field, so we might just as well make
 * this explicit here.  Please be sure to use the decoding macros
 * below from now on.
 */



/*
 * Let any architecture override either of the following before
 * including this file.
 */
# 49 "./include/uapi/asm-generic/ioctl.h"
/*
 * Direction bits, which any architecture can choose to override
 * before including this file.
 *
 * NOTE: _IOC_WRITE means userland is writing and kernel is
 * reading. _IOC_READ means userland is reading and kernel is writing.
 */
# 79 "./include/uapi/asm-generic/ioctl.h"
/*
 * Used to create numbers.
 *
 * NOTE: _IOW means userland is writing and kernel is reading. _IOR
 * means userland is reading and kernel is writing.
 */
# 93 "./include/uapi/asm-generic/ioctl.h"
/* used to decode ioctl numbers.. */





/* ...and for the drivers/sound files... */
# 6 "./include/asm-generic/ioctl.h" 2




/* provoke compile error for invalid uses of size argument */
extern unsigned int __invalid_size_argument_for_IOC;
# 2 "./arch/arm64/include/generated/uapi/asm/ioctl.h" 2
# 6 "./include/uapi/linux/ioctl.h" 2
# 13 "./include/uapi/linux/random.h" 2
# 1 "./include/linux/irqnr.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/irqnr.h" 1
/*
 * There isn't anything here anymore, but the file must not be empty or patch
 * will delete it.
 */
# 6 "./include/linux/irqnr.h" 2


extern int nr_irqs;
extern struct irq_desc *irq_to_desc(unsigned int irq);
unsigned int irq_get_next_irq(unsigned int offset);
# 14 "./include/uapi/linux/random.h" 2

/* ioctl()'s for the random number generator */

/* Get the entropy count. */


/* Add to (or subtract from) the entropy count.  (Superuser only.) */


/* Get the contents of the entropy pool.  (Superuser only.) */


/*
 * Write bytes into the entropy pool and add to the entropy count.
 * (Superuser only.)
 */


/* Clear entropy count to 0.  (Superuser only.) */


/* Clear the entropy pool and associated counters.  (Superuser only.) */


/* Reseed CRNG.  (Superuser only.) */


struct rand_pool_info {
 int entropy_count;
 int buf_size;
 __u32 buf[];
};

/*
 * Flags for getrandom(2)
 *
 * GRND_NONBLOCK	Don't block and return EAGAIN instead
 * GRND_RANDOM		No effect
 * GRND_INSECURE	Return non-cryptographic random bytes
 */
# 11 "./include/linux/random.h" 2

struct notifier_block;

void add_device_randomness(const void *buf, size_t len);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) add_bootloader_randomness(const void *buf, size_t len);
void add_input_randomness(unsigned int type, unsigned int code,
     unsigned int value) ;
void add_interrupt_randomness(int irq) ;
void add_hwgenerator_randomness(const void *buf, size_t len, size_t entropy, bool sleep_after);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void add_latent_entropy(void)
{



 add_device_randomness(((void *)0), 0);

}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_random_vmfork_notifier(struct notifier_block *nb) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int unregister_random_vmfork_notifier(struct notifier_block *nb) { return 0; }


void get_random_bytes(void *buf, size_t len);
u8 get_random_u8(void);
u16 get_random_u16(void);
u32 get_random_u32(void);
u64 get_random_u64(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_random_long(void)
{

 return get_random_u64();



}

u32 __get_random_u32_below(u32 ceil);

/*
 * Returns a random integer in the interval [0, ceil), with uniform
 * distribution, suitable for all uses. Fastest when ceil is a constant, but
 * still fast for variable ceil as well.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 get_random_u32_below(u32 ceil)
{
 if (!__builtin_constant_p(ceil))
  return __get_random_u32_below(ceil);

 /*
	 * For the fast path, below, all operations on ceil are precomputed by
	 * the compiler, so this incurs no overhead for checking pow2, doing
	 * divisions, or branching based on integer size. The resultant
	 * algorithm does traditional reciprocal multiplication (typically
	 * optimized by the compiler into shifts and adds), rejecting samples
	 * whose lower half would indicate a range indivisible by ceil.
	 */
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_174(void) __attribute__((__error__("get_random_u32_below() must take ceil > 0"))); if (!(!(!ceil))) __compiletime_assert_174(); } while (0);
# 74 "./include/linux/random.h"
 if (ceil <= 1)
  return 0;
 for (;;) {
  if (ceil <= 1U << 8) {
   u32 mult = ceil * get_random_u8();
   if (__builtin_expect(!!(is_power_of_2(ceil) || (u8)mult >= (1U << 8) % ceil), 1))
    return mult >> 8;
  } else if (ceil <= 1U << 16) {
   u32 mult = ceil * get_random_u16();
   if (__builtin_expect(!!(is_power_of_2(ceil) || (u16)mult >= (1U << 16) % ceil), 1))
    return mult >> 16;
  } else {
   u64 mult = (u64)ceil * get_random_u32();
   if (__builtin_expect(!!(is_power_of_2(ceil) || (u32)mult >= -ceil % ceil), 1))
    return mult >> 32;
  }
 }
}

/*
 * Returns a random integer in the interval (floor, U32_MAX], with uniform
 * distribution, suitable for all uses. Fastest when floor is a constant, but
 * still fast for variable floor as well.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 get_random_u32_above(u32 floor)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_175(void) __attribute__((__error__("get_random_u32_above() must take floor < U32_MAX"))); if (!(!(__builtin_constant_p(floor) && floor == ((u32)~0U)))) __compiletime_assert_175(); } while (0);
# 102 "./include/linux/random.h"
 return floor + 1 + get_random_u32_below(((u32)~0U) - floor);
}

/*
 * Returns a random integer in the interval [floor, ceil], with uniform
 * distribution, suitable for all uses. Fastest when floor and ceil are
 * constant, but still fast for variable floor and ceil as well.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 get_random_u32_inclusive(u32 floor, u32 ceil)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_176(void) __attribute__((__error__("get_random_u32_inclusive() must take floor <= ceil"))); if (!(!(__builtin_constant_p(floor) && __builtin_constant_p(ceil) &&
# 112 "./include/linux/random.h"
 (floor > ceil || ceil - floor == ((u32)~0U))))) __compiletime_assert_176(); } while (0);


 return floor + get_random_u32_below(ceil - floor + 1);
}

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) random_init_early(const char *command_line);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) random_init(void);
bool rng_is_initialized(void);
int wait_for_random_bytes(void);
int execute_with_initialized_rng(struct notifier_block *nb);

/* Calls wait_for_random_bytes() and then calls get_random_bytes(buf, nbytes).
 * Returns the result of the call to wait_for_random_bytes. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_random_bytes_wait(void *buf, size_t nbytes)
{
 int ret = wait_for_random_bytes();
 get_random_bytes(buf, nbytes);
 return ret;
}
# 141 "./include/linux/random.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_random_u8_wait(u8 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u8(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_random_u16_wait(u16 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u16(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_random_u32_wait(u32 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u32(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_random_u64_wait(u32 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u64(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_random_long_wait(unsigned long *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_long(); return 0; }


/*
 * This is designed to be standalone for just prandom
 * users, but for now we include it from <linux/random.h>
 * for legacy reasons.
 */
# 1 "./include/linux/prandom.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * include/linux/prandom.h
 *
 * Include file for the fast pseudo-random 32-bit
 * generation.
 */




# 1 "./include/linux/once.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/* Helpers used from arbitrary contexts.
 * Hard irqs are blocked, be cautious.
 */
bool __do_once_start(bool *done, unsigned long *flags);
void __do_once_done(bool *done, struct static_key_true *once_key,
      unsigned long *flags, struct module *mod);

/* Variant for process contexts only. */
bool __do_once_sleepable_start(bool *done);
void __do_once_sleepable_done(bool *done, struct static_key_true *once_key,
         struct module *mod);

/* Call a function exactly once. The idea of DO_ONCE() is to perform
 * a function call such as initialization of random seeds, etc, only
 * once, where DO_ONCE() can live in the fast-path. After @func has
 * been called with the passed arguments, the static key will patch
 * out the condition into a nop. DO_ONCE() guarantees type safety of
 * arguments!
 *
 * Note that the following is not equivalent ...
 *
 *   DO_ONCE(func, arg);
 *   DO_ONCE(func, arg);
 *
 * ... to this version:
 *
 *   void foo(void)
 *   {
 *     DO_ONCE(func, arg);
 *   }
 *
 *   foo();
 *   foo();
 *
 * In case the one-time invocation could be triggered from multiple
 * places, then a common helper function must be defined, so that only
 * a single static key will be placed there!
 */
# 63 "./include/linux/once.h"
/* Variant of DO_ONCE() for process/sleepable contexts. */
# 13 "./include/linux/prandom.h" 2
# 1 "./include/linux/percpu.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 14 "./include/linux/percpu.h"
/* enough to cover all DEFINE_PER_CPUs in modules */






/* minimum unit size, also is the maximum supported allocation size */


/* minimum allocation size and shift in bytes */



/*
 * The PCPU_BITMAP_BLOCK_SIZE must be the same size as PAGE_SIZE as the
 * updating of hints is used to manage the nr_empty_pop_pages in both
 * the chunk and globally.
 */




/*
 * Percpu allocator can serve percpu allocations before slab is
 * initialized which allows slab to depend on the percpu allocator.
 * The following parameter decide how much resource to preallocate
 * for this.  Keep PERCPU_DYNAMIC_RESERVE equal to or larger than
 * PERCPU_DYNAMIC_EARLY_SIZE.
 */


/*
 * PERCPU_DYNAMIC_RESERVE indicates the amount of free area to piggy
 * back on the first chunk for dynamic percpu allocation if arch is
 * manually allocating and mapping it for faster access (as a part of
 * large page mapping for example).
 *
 * The following values give between one and two pages of free space
 * after typical minimal boot (2-way SMP, single disk and NIC) with
 * both defconfig and a distro config on x86_64 and 32.  More
 * intelligent way to determine this would be nice.
 */






extern void *pcpu_base_addr;
extern const unsigned long *pcpu_unit_offsets;

struct pcpu_group_info {
 int nr_units; /* aligned # of units */
 unsigned long base_offset; /* base address offset */
 unsigned int *cpu_map; /* unit->cpu map, empty
						 * entries contain NR_CPUS */
};

struct pcpu_alloc_info {
 size_t static_size;
 size_t reserved_size;
 size_t dyn_size;
 size_t unit_size;
 size_t atom_size;
 size_t alloc_size;
 size_t __ai_size; /* internal, don't use */
 int nr_groups; /* 0 if grouping unnecessary */
 struct pcpu_group_info groups[];
};

enum pcpu_fc {
 PCPU_FC_AUTO,
 PCPU_FC_EMBED,
 PCPU_FC_PAGE,

 PCPU_FC_NR,
};
extern const char * const pcpu_fc_names[PCPU_FC_NR];

extern enum pcpu_fc pcpu_chosen_fc;

typedef int (pcpu_fc_cpu_to_node_fn_t)(int cpu);
typedef int (pcpu_fc_cpu_distance_fn_t)(unsigned int from, unsigned int to);

extern struct pcpu_alloc_info * __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_alloc_alloc_info(int nr_groups,
            int nr_units);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_free_alloc_info(struct pcpu_alloc_info *ai);

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai,
      void *base_addr);


extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,
    size_t atom_size,
    pcpu_fc_cpu_distance_fn_t cpu_distance_fn,
    pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn);



void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_populate_pte(unsigned long addr);
extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_page_first_chunk(size_t reserved_size,
    pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn);


extern void /* nothing */ *__alloc_reserved_percpu(size_t size, size_t align) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr);
extern bool is_kernel_percpu_address(unsigned long addr);





extern void /* nothing */ *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void /* nothing */ *__alloc_percpu(size_t size, size_t align) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void free_percpu(void /* nothing */ *__pdata);
extern phys_addr_t per_cpu_ptr_to_phys(void *addr);
# 139 "./include/linux/percpu.h"
extern unsigned long pcpu_nr_pages(void);
# 14 "./include/linux/prandom.h" 2
# 1 "./include/linux/random.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 15 "./include/linux/prandom.h" 2

struct rnd_state {
 __u32 s1, s2, s3, s4;
};

u32 prandom_u32_state(struct rnd_state *state);
void prandom_bytes_state(struct rnd_state *state, void *buf, size_t nbytes);
void prandom_seed_full_state(struct rnd_state /* nothing */ *pcpu_state);




/* Deprecated: use get_random_u32_below() instead. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 prandom_u32_max(u32 ep_ro)
{
 return get_random_u32_below(ep_ro);
}

/*
 * Handle minimum values for seeds
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __seed(u32 x, u32 m)
{
 return (x < m) ? x + m : x;
}

/**
 * prandom_seed_state - set seed for prandom_u32_state().
 * @state: pointer to state structure to receive the seed.
 * @seed: arbitrary 64-bit value to use as a seed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void prandom_seed_state(struct rnd_state *state, u64 seed)
{
 u32 i = ((seed >> 32) ^ (seed << 10) ^ seed) & 0xffffffffUL;

 state->s1 = __seed(i, 2U);
 state->s2 = __seed(i, 8U);
 state->s3 = __seed(i, 16U);
 state->s4 = __seed(i, 128U);
}

/* Pseudo random number generator from numerical recipes. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 next_pseudo_random32(u32 seed)
{
 return seed * 1664525 + 1013904223;
}
# 154 "./include/linux/random.h" 2

# 1 "./arch/arm64/include/asm/archrandom.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/arm-smccc.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (c) 2015, Linaro Limited
 */






/*
 * This file provides common defines for ARM SMC Calling Convention as
 * specified in
 * https://developer.arm.com/docs/den0028/latest
 *
 * This code is up-to-date with version DEN 0028 C
 */
# 106 "./include/linux/arm-smccc.h"
/* KVM UID value: 28b46fb6-2ec5-11e9-a9ca-4b564d003a74 */





/* KVM "vendor specific" services */
# 126 "./include/linux/arm-smccc.h"
/*
 * ptp_kvm is a feature used for time sync between vm and host.
 * ptp_kvm module in guest kernel will get service from host using
 * this hypercall ID.
 */






/* ptp_kvm counter type ID */



/* Paravirtualised time calls (defined by ARM DEN0057A) */
# 154 "./include/linux/arm-smccc.h"
/* TRNG entropy source calls (defined by ARM DEN0098) */
# 185 "./include/linux/arm-smccc.h"
/*
 * Return codes defined in ARM DEN 0070A
 * ARM DEN 0070A is now merged/consolidated into ARM DEN 0028 C
 */
# 199 "./include/linux/arm-smccc.h"
enum arm_smccc_conduit {
 SMCCC_CONDUIT_NONE,
 SMCCC_CONDUIT_SMC,
 SMCCC_CONDUIT_HVC,
};

/**
 * arm_smccc_1_1_get_conduit()
 *
 * Returns the conduit to be used for SMCCCv1.1 or later.
 *
 * When SMCCCv1.1 is not present, returns SMCCC_CONDUIT_NONE.
 */
enum arm_smccc_conduit arm_smccc_1_1_get_conduit(void);

/**
 * arm_smccc_get_version()
 *
 * Returns the version to be used for SMCCCv1.1 or later.
 *
 * When SMCCCv1.1 or above is not present, returns SMCCCv1.0, but this
 * does not imply the presence of firmware or a valid conduit. Caller
 * handling SMCCCv1.0 must determine the conduit by other means.
 */
u32 arm_smccc_get_version(void);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) arm_smccc_version_init(u32 version, enum arm_smccc_conduit conduit);

extern u64 smccc_has_sve_hint;

/**
 * struct arm_smccc_res - Result from SMC/HVC call
 * @a0-a3 result values from registers 0 to 3
 */
struct arm_smccc_res {
 unsigned long a0;
 unsigned long a1;
 unsigned long a2;
 unsigned long a3;
};


/**
 * struct arm_smccc_1_2_regs - Arguments for or Results from SMC/HVC call
 * @a0-a17 argument values from registers 0 to 17
 */
struct arm_smccc_1_2_regs {
 unsigned long a0;
 unsigned long a1;
 unsigned long a2;
 unsigned long a3;
 unsigned long a4;
 unsigned long a5;
 unsigned long a6;
 unsigned long a7;
 unsigned long a8;
 unsigned long a9;
 unsigned long a10;
 unsigned long a11;
 unsigned long a12;
 unsigned long a13;
 unsigned long a14;
 unsigned long a15;
 unsigned long a16;
 unsigned long a17;
};

/**
 * arm_smccc_1_2_hvc() - make HVC calls
 * @args: arguments passed via struct arm_smccc_1_2_regs
 * @res: result values via struct arm_smccc_1_2_regs
 *
 * This function is used to make HVC calls following SMC Calling Convention
 * v1.2 or above. The content of the supplied param are copied from the
 * structure to registers prior to the HVC instruction. The return values
 * are updated with the content from registers on return from the HVC
 * instruction.
 */
           void arm_smccc_1_2_hvc(const struct arm_smccc_1_2_regs *args,
      struct arm_smccc_1_2_regs *res);

/**
 * arm_smccc_1_2_smc() - make SMC calls
 * @args: arguments passed via struct arm_smccc_1_2_regs
 * @res: result values via struct arm_smccc_1_2_regs
 *
 * This function is used to make SMC calls following SMC Calling Convention
 * v1.2 or above. The content of the supplied param are copied from the
 * structure to registers prior to the SMC instruction. The return values
 * are updated with the content from registers on return from the SMC
 * instruction.
 */
           void arm_smccc_1_2_smc(const struct arm_smccc_1_2_regs *args,
      struct arm_smccc_1_2_regs *res);


/**
 * struct arm_smccc_quirk - Contains quirk information
 * @id: quirk identification
 * @state: quirk specific information
 * @a6: Qualcomm quirk entry for returning post-smc call contents of a6
 */
struct arm_smccc_quirk {
 int id;
 union {
  unsigned long a6;
 } state;
};

/**
 * __arm_smccc_sve_check() - Set the SVE hint bit when doing SMC calls
 *
 * Sets the SMCCC hint bit to indicate if there is live state in the SVE
 * registers, this modifies x0 in place and should never be called from C
 * code.
 */
           unsigned long __arm_smccc_sve_check(unsigned long x0);

/**
 * __arm_smccc_smc() - make SMC calls
 * @a0-a7: arguments passed in registers 0 to 7
 * @res: result values from registers 0 to 3
 * @quirk: points to an arm_smccc_quirk, or NULL when no quirks are required.
 *
 * This function is used to make SMC calls following SMC Calling Convention.
 * The content of the supplied param are copied to registers 0 to 7 prior
 * to the SMC instruction. The return values are updated with the content
 * from register 0 to 3 on return from the SMC instruction.  An optional
 * quirk structure provides vendor specific behavior.
 */

           void __arm_smccc_smc(unsigned long a0, unsigned long a1,
   unsigned long a2, unsigned long a3, unsigned long a4,
   unsigned long a5, unsigned long a6, unsigned long a7,
   struct arm_smccc_res *res, struct arm_smccc_quirk *quirk);
# 344 "./include/linux/arm-smccc.h"
/**
 * __arm_smccc_hvc() - make HVC calls
 * @a0-a7: arguments passed in registers 0 to 7
 * @res: result values from registers 0 to 3
 * @quirk: points to an arm_smccc_quirk, or NULL when no quirks are required.
 *
 * This function is used to make HVC calls following SMC Calling
 * Convention.  The content of the supplied param are copied to registers 0
 * to 7 prior to the HVC instruction. The return values are updated with
 * the content from register 0 to 3 on return from the HVC instruction.  An
 * optional quirk structure provides vendor specific behavior.
 */
           void __arm_smccc_hvc(unsigned long a0, unsigned long a1,
   unsigned long a2, unsigned long a3, unsigned long a4,
   unsigned long a5, unsigned long a6, unsigned long a7,
   struct arm_smccc_res *res, struct arm_smccc_quirk *quirk);
# 369 "./include/linux/arm-smccc.h"
/* SMCCC v1.1 implementation madness follows */
# 384 "./include/linux/arm-smccc.h"
/* nVHE hypervisor doesn't have a current thread so needs separate checks */
# 468 "./include/linux/arm-smccc.h"
/*
 * We have an output list that is not necessarily used, and GCC feels
 * entitled to optimise the whole sequence away. "volatile" is what
 * makes it stick.
 */
# 488 "./include/linux/arm-smccc.h"
/*
 * arm_smccc_1_1_smc() - make an SMCCC v1.1 compliant SMC call
 *
 * This is a variadic macro taking one to eight source arguments, and
 * an optional return structure.
 *
 * @a0-a7: arguments passed in registers 0 to 7
 * @res: result values from registers 0 to 3
 *
 * This macro is used to make SMC calls following SMC Calling Convention v1.1.
 * The content of the supplied param are copied to registers 0 to 7 prior
 * to the SMC instruction. The return values are updated with the content
 * from register 0 to 3 on return from the SMC instruction if not NULL.
 */


/*
 * arm_smccc_1_1_hvc() - make an SMCCC v1.1 compliant HVC call
 *
 * This is a variadic macro taking one to eight source arguments, and
 * an optional return structure.
 *
 * @a0-a7: arguments passed in registers 0 to 7
 * @res: result values from registers 0 to 3
 *
 * This macro is used to make HVC calls following SMC Calling Convention v1.1.
 * The content of the supplied param are copied to registers 0 to 7 prior
 * to the HVC instruction. The return values are updated with the content
 * from register 0 to 3 on return from the HVC instruction if not NULL.
 */


/*
 * Like arm_smccc_1_1* but always returns SMCCC_RET_NOT_SUPPORTED.
 * Used when the SMCCC conduit is not defined. The empty asm statement
 * avoids compiler warnings about unused variables.
 */
# 533 "./include/linux/arm-smccc.h"
/*
 * arm_smccc_1_1_invoke() - make an SMCCC v1.1 compliant call
 *
 * This is a variadic macro taking one to eight source arguments, and
 * an optional return structure.
 *
 * @a0-a7: arguments passed in registers 0 to 7
 * @res: result values from registers 0 to 3
 *
 * This macro will make either an HVC call or an SMC call depending on the
 * current SMCCC conduit. If no valid conduit is available then -1
 * (SMCCC_RET_NOT_SUPPORTED) is returned in @res.a0 (if supplied).
 *
 * The return value also provides the conduit that was used.
 */
# 6 "./arch/arm64/include/asm/archrandom.h" 2







extern bool smccc_trng_available;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __attribute__((__section__(".init.text"))) __attribute__((__cold__)) smccc_probe_trng(void)
{
 struct arm_smccc_res res;

 ({ int method = arm_smccc_1_1_get_conduit(); switch (method) { case SMCCC_CONDUIT_HVC: do { register unsigned long r0 asm("r0"); register unsigned long r1 asm("r1"); register unsigned long r2 asm("r2"); register unsigned long r3 asm("r3"); struct arm_smccc_res *___res = &res; register unsigned long arg0 asm("r0") = (u32)((((1U)) << 31) | ((0) << 30) | (((4) & 0x3F) << 24) | ((0x50) & 0xFFFF)); asm volatile(".if ""1"" == 1\n" "661:\n\t" "nop \n" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "52" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "bl __arm_smccc_sve_check \n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" "hvc	#0" "\n" : "=r" (r0), "=r" (r1), "=r" (r2), "=r" (r3) : "r" (arg0) : "x16", "x30", "cc", "memory"); if (___res) *___res = (typeof(*___res)){r0, r1, r2, r3}; } while (0); break; case SMCCC_CONDUIT_SMC: do { register unsigned long r0 asm("r0"); register unsigned long r1 asm("r1"); register unsigned long r2 asm("r2"); register unsigned long r3 asm("r3"); struct arm_smccc_res *___res = &res; register unsigned long arg0 asm("r0") = (u32)((((1U)) << 31) | ((0) << 30) | (((4) & 0x3F) << 24) | ((0x50) & 0xFFFF)); asm volatile(".if ""1"" == 1\n" "661:\n\t" "nop \n" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "52" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "bl __arm_smccc_sve_check \n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" "smc	#0" "\n" : "=r" (r0), "=r" (r1), "=r" (r2), "=r" (r3) : "r" (arg0) : "x16", "x30", "cc", "memory"); if (___res) *___res = (typeof(*___res)){r0, r1, r2, r3}; } while (0); break; default: do { struct arm_smccc_res *___res = &res; register unsigned long arg0 asm("r0") = (u32)((((1U)) << 31) | ((0) << 30) | (((4) & 0x3F) << 24) | ((0x50) & 0xFFFF)); asm ("" : : "r" (arg0) : "x16", "x30", "cc", "memory"); if (___res) ___res->a0 = -1; } while (0); method = SMCCC_CONDUIT_NONE; break; } method; });
 if ((s32)res.a0 < 0)
  return false;

 return res.a0 >= 0x10000UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __arm64_rndr(unsigned long *v)
{
 bool ok;

 /*
	 * Reads of RNDR set PSTATE.NZCV to 0b0000 on success,
	 * and set PSTATE.NZCV to 0b0100 otherwise.
	 */
 asm volatile(
  "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((2) << 12) | ((4) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" "\n"
 "	cset %w1, ne\n"
 : "=r" (*v), "=r" (ok)
 :
 : "cc");

 return ok;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __arm64_rndrrs(unsigned long *v)
{
 bool ok;

 /*
	 * Reads of RNDRRS set PSTATE.NZCV to 0b0000 on success,
	 * and set PSTATE.NZCV to 0b0100 otherwise.
	 */
 asm volatile(
  "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((2) << 12) | ((4) << 8) | ((1) << 5))" "\n" "	.purgem	mrs_s\n" "\n"
 "	cset %w1, ne\n"
 : "=r" (*v), "=r" (ok)
 :
 : "cc");

 return ok;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool __cpu_has_rng(void)
{
 if (__builtin_expect(!!(!system_capabilities_finalized() && !(preempt_count() == 0 && !({ unsigned long _flags; do { ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _flags = arch_local_save_flags(); } while (0); ({ ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(_flags); }); }))), 0))
  return this_cpu_has_cap(32);
 return cpus_have_const_cap(32);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t __attribute__((__warn_unused_result__)) arch_get_random_longs(unsigned long *v, size_t max_longs)
{
 /*
	 * Only support the generic interface after we have detected
	 * the system wide capability, avoiding complexity with the
	 * cpufeature code and with potential scheduling between CPUs
	 * with and without the feature.
	 */
 if (max_longs && __cpu_has_rng() && __arm64_rndr(v))
  return 1;
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t __attribute__((__warn_unused_result__)) arch_get_random_seed_longs(unsigned long *v, size_t max_longs)
{
 if (!max_longs)
  return 0;

 /*
	 * We prefer the SMCCC call, since its semantics (return actual
	 * hardware backed entropy) is closer to the idea behind this
	 * function here than what even the RNDRSS register provides
	 * (the output of a pseudo RNG freshly seeded by a TRNG).
	 */
 if (smccc_trng_available) {
  struct arm_smccc_res res;

  max_longs = __builtin_choose_expr(((!!(sizeof((typeof((size_t)(3)) *)1 == (typeof((size_t)(max_longs)) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((size_t)(3)) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((size_t)(max_longs)) * 0l)) : (int *)8))))), (((size_t)(3)) < ((size_t)(max_longs)) ? ((size_t)(3)) : ((size_t)(max_longs))), ({ typeof((size_t)(3)) __UNIQUE_ID___x177 = ((size_t)(3)); typeof((size_t)(max_longs)) __UNIQUE_ID___y178 = ((size_t)(max_longs)); ((__UNIQUE_ID___x177) < (__UNIQUE_ID___y178) ? (__UNIQUE_ID___x177) : (__UNIQUE_ID___y178)); }));
  ({ int method = arm_smccc_1_1_get_conduit(); switch (method) { case SMCCC_CONDUIT_HVC: do { register unsigned long r0 asm("r0"); register unsigned long r1 asm("r1"); register unsigned long r2 asm("r2"); register unsigned long r3 asm("r3"); typeof(max_longs * 64) __a1 = max_longs * 64; struct arm_smccc_res *___res = &res; register unsigned long arg0 asm("r0") = (u32)((((1U)) << 31) | ((1) << 30) | (((4) & 0x3F) << 24) | ((0x53) & 0xFFFF)); register typeof(max_longs * 64) arg1 asm("r1") = __a1; asm volatile(".if ""1"" == 1\n" "661:\n\t" "nop \n" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "52" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "bl __arm_smccc_sve_check \n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" "hvc	#0" "\n" : "=r" (r0), "=r" (r1), "=r" (r2), "=r" (r3) : "r" (arg0), "r" (arg1) : "x16", "x30", "cc", "memory"); if (___res) *___res = (typeof(*___res)){r0, r1, r2, r3}; } while (0); break; case SMCCC_CONDUIT_SMC: do { register unsigned long r0 asm("r0"); register unsigned long r1 asm("r1"); register unsigned long r2 asm("r2"); register unsigned long r3 asm("r3"); typeof(max_longs * 64) __a1 = max_longs * 64; struct arm_smccc_res *___res = &res; register unsigned long arg0 asm("r0") = (u32)((((1U)) << 31) | ((1) << 30) | (((4) & 0x3F) << 24) | ((0x53) & 0xFFFF)); register typeof(max_longs * 64) arg1 asm("r1") = __a1; asm volatile(".if ""1"" == 1\n" "661:\n\t" "nop \n" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "52" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "bl __arm_smccc_sve_check \n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" "smc	#0" "\n" : "=r" (r0), "=r" (r1), "=r" (r2), "=r" (r3) : "r" (arg0), "r" (arg1) : "x16", "x30", "cc", "memory"); if (___res) *___res = (typeof(*___res)){r0, r1, r2, r3}; } while (0); break; default: do { typeof(max_longs * 64) __a1 = max_longs * 64; struct arm_smccc_res *___res = &res; register unsigned long arg0 asm("r0") = (u32)((((1U)) << 31) | ((1) << 30) | (((4) & 0x3F) << 24) | ((0x53) & 0xFFFF)); register typeof(max_longs * 64) arg1 asm("r1") = __a1; asm ("" : : "r" (arg0), "r" (arg1) : "x16", "x30", "cc", "memory"); if (___res) ___res->a0 = -1; } while (0); method = SMCCC_CONDUIT_NONE; break; } method; });
  if ((int)res.a0 >= 0) {
   switch (max_longs) {
   case 3:
    *v++ = res.a1;
    __attribute__((__fallthrough__));
   case 2:
    *v++ = res.a2;
    __attribute__((__fallthrough__));
   case 1:
    *v++ = res.a3;
    break;
   }
   return max_longs;
  }
 }

 /*
	 * RNDRRS is not backed by an entropy source but by a DRBG that is
	 * reseeded after each invocation. This is not a 100% fit but good
	 * enough to implement this API if no other entropy source exists.
	 */
 if (__cpu_has_rng() && __arm64_rndrrs(v))
  return 1;

 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __attribute__((__section__(".init.text"))) __attribute__((__cold__)) __early_cpu_has_rndr(void)
{
 /* Open code as we run prior to the first call to cpufeature. */
 unsigned long ftr = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((0) << 12) | ((6) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; });
 return (ftr >> 60) & 0xf;
}
# 156 "./include/linux/random.h" 2


int random_prepare_cpu(unsigned int cpu);
int random_online_cpu(unsigned int cpu);



extern const struct file_operations random_fops, urandom_fops;
# 8 "./arch/arm64/include/asm/pointer_auth.h" 2
# 17 "./arch/arm64/include/asm/pointer_auth.h"
/*
 * Each key is a 128-bit quantity which is split across a pair of 64-bit
 * registers (Lo and Hi).
 */
struct ptrauth_key {
 unsigned long lo, hi;
};

/*
 * We give each process its own keys, which are shared by all threads. The keys
 * are inherited upon fork(), and reinitialised upon exec*().
 */
struct ptrauth_keys_user {
 struct ptrauth_key apia;
 struct ptrauth_key apib;
 struct ptrauth_key apda;
 struct ptrauth_key apdb;
 struct ptrauth_key apga;
};
# 46 "./arch/arm64/include/asm/pointer_auth.h"
struct ptrauth_keys_kernel {
 struct ptrauth_key apia;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ptrauth_keys_init_kernel(struct ptrauth_keys_kernel *keys)
{
 if (system_supports_address_auth())
  get_random_bytes(&keys->apia, sizeof(keys->apia));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ptrauth_keys_switch_kernel(struct ptrauth_keys_kernel *keys)
{
 if (!system_supports_address_auth())
  return;

 do { struct ptrauth_key __pki_v = (keys->apia); do { u64 __val = (u64)(__pki_v.lo); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((1) << 8) | ((0) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); do { u64 __val = (u64)(__pki_v.hi); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((1) << 8) | ((1) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); } while (0);
 asm volatile("isb" : : : "memory");
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrauth_keys_install_user(struct ptrauth_keys_user *keys)
{
 if (system_supports_address_auth()) {
  do { struct ptrauth_key __pki_v = (keys->apib); do { u64 __val = (u64)(__pki_v.lo); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((1) << 8) | ((2) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); do { u64 __val = (u64)(__pki_v.hi); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((1) << 8) | ((3) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); } while (0);
  do { struct ptrauth_key __pki_v = (keys->apda); do { u64 __val = (u64)(__pki_v.lo); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((2) << 8) | ((0) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); do { u64 __val = (u64)(__pki_v.hi); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((2) << 8) | ((1) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); } while (0);
  do { struct ptrauth_key __pki_v = (keys->apdb); do { u64 __val = (u64)(__pki_v.lo); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((2) << 8) | ((2) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); do { u64 __val = (u64)(__pki_v.hi); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((2) << 8) | ((3) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); } while (0);
 }

 if (system_supports_generic_auth())
  do { struct ptrauth_key __pki_v = (keys->apga); do { u64 __val = (u64)(__pki_v.lo); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((3) << 8) | ((0) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); do { u64 __val = (u64)(__pki_v.hi); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((2) << 12) | ((3) << 8) | ((1) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrauth_keys_init_user(struct ptrauth_keys_user *keys)
{
 if (system_supports_address_auth()) {
  get_random_bytes(&keys->apia, sizeof(keys->apia));
  get_random_bytes(&keys->apib, sizeof(keys->apib));
  get_random_bytes(&keys->apda, sizeof(keys->apda));
  get_random_bytes(&keys->apdb, sizeof(keys->apdb));
 }

 if (system_supports_generic_auth())
  get_random_bytes(&keys->apga, sizeof(keys->apga));

 ptrauth_keys_install_user(keys);
}

extern int ptrauth_prctl_reset_keys(struct task_struct *tsk, unsigned long arg);

extern int ptrauth_set_enabled_keys(struct task_struct *tsk, unsigned long keys,
        unsigned long enabled);
extern int ptrauth_get_enabled_keys(struct task_struct *tsk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long ptrauth_strip_insn_pac(unsigned long ptr)
{
 return ((ptr & ((((1ULL))) << (55))) ? (ptr | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : (ptr & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ptrauth_enable(void)
{
 if (!system_supports_address_auth())
  return;
 do { u64 __scs_val = ({ u64 __val; asm volatile("mrs %0, " "sctlr_el1" : "=r" (__val)); __val; }); u64 __scs_new = (__scs_val & ~(u64)(0)) | (((((((1UL))) << (31))) | (((((1UL))) << (30))) | (((((1UL))) << (27))) | (((((1UL))) << (13))))); if (__scs_new != __scs_val) do { u64 __val = (u64)(__scs_new); asm volatile("msr " "sctlr_el1" ", %x0" : : "rZ" (__val)); } while (0); } while (0);

 asm volatile("isb" : : : "memory");
}
# 44 "./arch/arm64/include/asm/processor.h" 2

# 1 "./arch/arm64/include/asm/spectre.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Interface for managing mitigations for Spectre vulnerabilities.
 *
 * Copyright (C) 2020 Google LLC
 * Author: Will Deacon <will@kernel.org>
 */
# 22 "./arch/arm64/include/asm/spectre.h"
/* Watch out, ordering is important here. */
enum mitigation_state {
 SPECTRE_UNAFFECTED,
 SPECTRE_MITIGATED,
 SPECTRE_VULNERABLE,
};

struct pt_regs;
struct task_struct;

/*
 * Note: the order of this enum corresponds to __bp_harden_hyp_vecs and
 * we rely on having the direct vectors first.
 */
enum arm64_hyp_spectre_vector {
 /*
	 * Take exceptions directly to __kvm_hyp_vector. This must be
	 * 0 so that it used by default when mitigations are not needed.
	 */
 HYP_VECTOR_DIRECT,

 /*
	 * Bounce via a slot in the hypervisor text mapping of
	 * __bp_harden_hyp_vecs, which contains an SMC call.
	 */
 HYP_VECTOR_SPECTRE_DIRECT,

 /*
	 * Bounce via a slot in a special mapping of __bp_harden_hyp_vecs
	 * next to the idmap page.
	 */
 HYP_VECTOR_INDIRECT,

 /*
	 * Bounce via a slot in a special mapping of __bp_harden_hyp_vecs
	 * next to the idmap page, which contains an SMC call.
	 */
 HYP_VECTOR_SPECTRE_INDIRECT,
};

typedef void (*bp_hardening_cb_t)(void);

struct bp_hardening_data {
 enum arm64_hyp_spectre_vector slot;
 bp_hardening_cb_t fn;
};

extern /* nothing */ __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(struct bp_hardening_data) bp_hardening_data;

/* Called during entry so must be __always_inline */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void arm64_apply_bp_hardening(void)
{
 struct bp_hardening_data *d;

 if (!cpus_have_const_cap(47))
  return;

 d = ({ do { const void /* nothing */ *__vpp_verify = (typeof((&bp_hardening_data) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&bp_hardening_data)) *)(&bp_hardening_data)); (typeof((typeof(*(&bp_hardening_data)) *)(&bp_hardening_data))) (__ptr + ((__kern_my_cpu_offset()))); }); });
 if (d->fn)
  d->fn();
}

enum mitigation_state arm64_get_spectre_v2_state(void);
bool has_spectre_v2(const struct arm64_cpu_capabilities *cap, int scope);
void spectre_v2_enable_mitigation(const struct arm64_cpu_capabilities *__unused);

bool has_spectre_v3a(const struct arm64_cpu_capabilities *cap, int scope);
void spectre_v3a_enable_mitigation(const struct arm64_cpu_capabilities *__unused);

enum mitigation_state arm64_get_spectre_v4_state(void);
bool has_spectre_v4(const struct arm64_cpu_capabilities *cap, int scope);
void spectre_v4_enable_mitigation(const struct arm64_cpu_capabilities *__unused);
void spectre_v4_enable_task_mitigation(struct task_struct *tsk);

enum mitigation_state arm64_get_meltdown_state(void);

enum mitigation_state arm64_get_spectre_bhb_state(void);
bool is_spectre_bhb_affected(const struct arm64_cpu_capabilities *entry, int scope);
u8 spectre_bhb_loop_affected(int scope);
void spectre_bhb_enable_mitigation(const struct arm64_cpu_capabilities *__unused);
bool try_emulate_el1_ssbs(struct pt_regs *regs, u32 instr);
# 46 "./arch/arm64/include/asm/processor.h" 2
# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 47 "./arch/arm64/include/asm/processor.h" 2

/*
 * TASK_SIZE - the maximum size of a user space task.
 * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
 */
# 103 "./arch/arm64/include/asm/processor.h"
extern phys_addr_t arm64_dma_phys_limit;


struct debug_info {

 /* Have we suspended stepping by a debugger? */
 int suspended_step;
 /* Allow breakpoints and watchpoints to be disabled for this thread. */
 int bps_disabled;
 int wps_disabled;
 /* Hardware breakpoints pinned to this task. */
 struct perf_event *hbp_break[16];
 struct perf_event *hbp_watch[16];

};

enum vec_type {
 ARM64_VEC_SVE = 0,
 ARM64_VEC_SME,
 ARM64_VEC_MAX,
};

enum fp_type {
 FP_STATE_CURRENT, /* Save based on current task state. */
 FP_STATE_FPSIMD,
 FP_STATE_SVE,
};

struct cpu_context {
 unsigned long x19;
 unsigned long x20;
 unsigned long x21;
 unsigned long x22;
 unsigned long x23;
 unsigned long x24;
 unsigned long x25;
 unsigned long x26;
 unsigned long x27;
 unsigned long x28;
 unsigned long fp;
 unsigned long sp;
 unsigned long pc;
};

struct thread_struct {
 struct cpu_context cpu_context; /* cpu context */

 /*
	 * Whitelisted fields for hardened usercopy:
	 * Maintainers must ensure manually that this contains no
	 * implicit padding.
	 */
 struct {
  unsigned long tp_value; /* TLS register */
  unsigned long tp2_value;
  struct user_fpsimd_state fpsimd_state;
 } uw;

 enum fp_type fp_type; /* registers FPSIMD or SVE? */
 unsigned int fpsimd_cpu;
 void *sve_state; /* SVE registers, if any */
 void *za_state; /* ZA register, if any */
 unsigned int vl[ARM64_VEC_MAX]; /* vector length */
 unsigned int vl_onexec[ARM64_VEC_MAX]; /* vl after next exec */
 unsigned long fault_address; /* fault info */
 unsigned long fault_code; /* ESR_EL1 value */
 struct debug_info debug; /* debugging */

 struct ptrauth_keys_user keys_user;

 struct ptrauth_keys_kernel keys_kernel;



 u64 mte_ctrl;

 u64 sctlr_user;
 u64 svcr;
 u64 tpidr2_el0;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int thread_get_vl(struct thread_struct *thread,
      enum vec_type type)
{
 return thread->vl[type];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int thread_get_sve_vl(struct thread_struct *thread)
{
 return thread_get_vl(thread, ARM64_VEC_SVE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int thread_get_sme_vl(struct thread_struct *thread)
{
 return thread_get_vl(thread, ARM64_VEC_SME);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int thread_get_cur_vl(struct thread_struct *thread)
{
 if (system_supports_sme() && (thread->svcr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (0)) * 0l)) : (int *)8))), (0) > (0), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (0)))))))
  return thread_get_sme_vl(thread);
 else
  return thread_get_sve_vl(thread);
}

unsigned int task_get_vl(const struct task_struct *task, enum vec_type type);
void task_set_vl(struct task_struct *task, enum vec_type type,
   unsigned long vl);
void task_set_vl_onexec(struct task_struct *task, enum vec_type type,
   unsigned long vl);
unsigned int task_get_vl_onexec(const struct task_struct *task,
    enum vec_type type);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int task_get_sve_vl(const struct task_struct *task)
{
 return task_get_vl(task, ARM64_VEC_SVE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int task_get_sme_vl(const struct task_struct *task)
{
 return task_get_vl(task, ARM64_VEC_SME);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_sve_vl(struct task_struct *task, unsigned long vl)
{
 task_set_vl(task, ARM64_VEC_SVE, vl);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int task_get_sve_vl_onexec(const struct task_struct *task)
{
 return task_get_vl_onexec(task, ARM64_VEC_SVE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_sve_vl_onexec(struct task_struct *task,
       unsigned long vl)
{
 task_set_vl_onexec(task, ARM64_VEC_SVE, vl);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_thread_struct_whitelist(unsigned long *offset,
      unsigned long *size)
{
 /* Verify that there is no padding among the whitelisted fields: */
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_179(void) __attribute__((__error__("BUILD_BUG_ON failed: " "sizeof_field(struct thread_struct, uw) != sizeof_field(struct thread_struct, uw.tp_value) + sizeof_field(struct thread_struct, uw.tp2_value) + sizeof_field(struct thread_struct, uw.fpsimd_state)"))); if (!(!(sizeof((((struct thread_struct *)0)->uw)) !=
# 250 "./arch/arm64/include/asm/processor.h"
 sizeof((((struct thread_struct *)0)->uw.tp_value)) + sizeof((((struct thread_struct *)0)->uw.tp2_value)) + sizeof((((struct thread_struct *)0)->uw.fpsimd_state))))) __compiletime_assert_179(); } while (0);




 *offset = __builtin_offsetof(struct thread_struct, uw);
 *size = sizeof((((struct thread_struct *)0)->uw));
}
# 273 "./arch/arm64/include/asm/processor.h"
/* Sync TPIDR_EL0 back to thread_struct for current */
void tls_preserve_current_state(void);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void start_thread_common(struct pt_regs *regs, unsigned long pc)
{
 s32 previous_syscall = regs->syscallno;
 memset(regs, 0, sizeof(*regs));
 regs->syscallno = previous_syscall;
 regs->pc = pc;

 if (system_uses_irq_prio_masking())
  regs->pmr_save = 0xe0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void start_thread(struct pt_regs *regs, unsigned long pc,
    unsigned long sp)
{
 start_thread_common(regs, pc);
 regs->pstate = 0x00000000;
 spectre_v4_enable_task_mitigation(get_current());
 regs->sp = sp;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void compat_start_thread(struct pt_regs *regs, unsigned long pc,
           unsigned long sp)
{
 start_thread_common(regs, pc);
 regs->pstate = 0x00000010;
 if (pc & 1)
  regs->pstate |= 0x00000020;





 spectre_v4_enable_task_mitigation(get_current());
 regs->regs[13] = sp;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_ttbr0_addr(unsigned long addr)
{
 /* entry assembly clears tags for TTBR0 addrs */
 return addr < (test_ti_thread_flag(((struct thread_info *)get_current()), 22 /* 32bit process */) ? ((((0x100000000UL))) - ((1UL) << 12)) : ((((1UL))) << ((u64)(48))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_ttbr1_addr(unsigned long addr)
{
 /* TTBR1 addresses may have a tag if KASAN_SW_TAGS is in use */
 return (addr) >= ((-((((1UL))) << ((48)))));
}

/* Forward declaration, a strange C thing */
struct task_struct;

unsigned long __get_wchan(struct task_struct *p);

void update_sctlr_el1(u64 sctlr);

/* Thread switching */
extern struct task_struct *cpu_switch_to(struct task_struct *prev,
      struct task_struct *next);







/*
 * Prefetching support
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void prefetch(const void *ptr)
{
 asm volatile("prfm pldl1keep, %a0\n" : : "p" (ptr));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void prefetchw(const void *ptr)
{
 asm volatile("prfm pstl1keep, %a0\n" : : "p" (ptr));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void spin_lock_prefetch(const void *ptr)
{
 asm volatile(".if ""1"" == 1\n" "661:\n\t" "prfm pstl1strm, %a0" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "27" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".arch_extension lse\n" "nop" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "p" (ptr));


}

extern unsigned long __attribute__((__section__(".data..ro_after_init"))) signal_minsigstksz; /* sigframe size */
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) minsigstksz_setup(void);

/*
 * Not at the top of the file due to a direct #include cycle between
 * <asm/fpsimd.h> and <asm/processor.h>.  Deferring this #include
 * ensures that contents of processor.h are visible to fpsimd.h even if
 * processor.h is included first.
 *
 * These prctl helpers are the only things in this file that require
 * fpsimd.h.  The core code expects them to be in this header.
 */
# 1 "./arch/arm64/include/asm/fpsimd.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./arch/arm64/include/generated/uapi/asm/errno.h" 1
# 9 "./arch/arm64/include/asm/fpsimd.h" 2

# 1 "./arch/arm64/include/asm/processor.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/processor.h
 *
 * Copyright (C) 1995-1999 Russell King
 * Copyright (C) 2012 ARM Ltd.
 */
# 11 "./arch/arm64/include/asm/fpsimd.h" 2
# 1 "./arch/arm64/include/uapi/asm/sigcontext.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */







/*
 * Signal context structure - contains all info to do with the state
 * before the signal handler was invoked.
 */
struct sigcontext {
 __u64 fault_address;
 /* AArch64 registers */
 __u64 regs[31];
 __u64 sp;
 __u64 pc;
 __u64 pstate;
 /* 4K reserved for FP/SIMD state and future expansion */
 __u8 __reserved[4096] __attribute__((__aligned__(16)));
};

/*
 * Allocation of __reserved[]:
 * (Note: records do not necessarily occur in the order shown here.)
 *
 *	size		description
 *
 *	0x210		fpsimd_context
 *	 0x10		esr_context
 *	0x8a0		sve_context (vl <= 64) (optional)
 *	 0x20		extra_context (optional)
 *	 0x10		terminator (null _aarch64_ctx)
 *
 *	0x510		(reserved for future allocation)
 *
 * New records that can exceed this space need to be opt-in for userspace, so
 * that an expanded signal frame is not generated unexpectedly.  The mechanism
 * for opting in will depend on the extension that generates each new record.
 * The above table documents the maximum set and sizes of records than can be
 * generated when userspace does not opt in for any such extension.
 */

/*
 * Header to be used at the beginning of structures extending the user
 * context. Such structures must be placed after the rt_sigframe on the stack
 * and be 16-byte aligned. The last structure must be a dummy one with the
 * magic and size set to 0.
 *
 * Note that the values allocated for use as magic should be chosen to
 * be meaningful in ASCII to aid manual parsing, ZA doesn't follow this
 * convention due to oversight but it should be observed for future additions.
 */
struct _aarch64_ctx {
 __u32 magic;
 __u32 size;
};



struct fpsimd_context {
 struct _aarch64_ctx head;
 __u32 fpsr;
 __u32 fpcr;
 __uint128_t vregs[32];
};

/*
 * Note: similarly to all other integer fields, each V-register is stored in an
 * endianness-dependent format, with the byte at offset i from the start of the
 * in-memory representation of the register value containing
 *
 *    bits [(7 + 8 * i) : (8 * i)] of the register on little-endian hosts; or
 *    bits [(127 - 8 * i) : (120 - 8 * i)] on big-endian hosts.
 */

/* ESR_EL1 context */


struct esr_context {
 struct _aarch64_ctx head;
 __u64 esr;
};

/*
 * extra_context: describes extra space in the signal frame for
 * additional structures that don't fit in sigcontext.__reserved[].
 *
 * Note:
 *
 * 1) fpsimd_context, esr_context and extra_context must be placed in
 * sigcontext.__reserved[] if present.  They cannot be placed in the
 * extra space.  Any other record can be placed either in the extra
 * space or in sigcontext.__reserved[], unless otherwise specified in
 * this file.
 *
 * 2) There must not be more than one extra_context.
 *
 * 3) If extra_context is present, it must be followed immediately in
 * sigcontext.__reserved[] by the terminating null _aarch64_ctx.
 *
 * 4) The extra space to which datap points must start at the first
 * 16-byte aligned address immediately after the terminating null
 * _aarch64_ctx that follows the extra_context structure in
 * __reserved[].  The extra space may overrun the end of __reserved[],
 * as indicated by a sufficiently large value for the size field.
 *
 * 5) The extra space must itself be terminated with a null
 * _aarch64_ctx.
 */


struct extra_context {
 struct _aarch64_ctx head;
 __u64 datap; /* 16-byte aligned pointer to extra space cast to __u64 */
 __u32 size; /* size in bytes of the extra space */
 __u32 __reserved[3];
};



struct sve_context {
 struct _aarch64_ctx head;
 __u16 vl;
 __u16 flags;
 __u16 __reserved[2];
};





struct za_context {
 struct _aarch64_ctx head;
 __u16 vl;
 __u16 __reserved[3];
};





/*
 * The SVE architecture leaves space for future expansion of the
 * vector length beyond its initial architectural limit of 2048 bits
 * (16 quadwords).
 *
 * See linux/Documentation/arm64/sve.rst for a description of the VL/VQ
 * terminology.
 */
# 182 "./arch/arm64/include/uapi/asm/sigcontext.h"
/*
 * If the SVE registers are currently live for the thread at signal delivery,
 * sve_context.head.size >=
 *	SVE_SIG_CONTEXT_SIZE(sve_vq_from_vl(sve_context.vl))
 * and the register data may be accessed using the SVE_SIG_*() macros.
 *
 * If sve_context.head.size <
 *	SVE_SIG_CONTEXT_SIZE(sve_vq_from_vl(sve_context.vl)),
 * the SVE registers were not live for the thread and no register data
 * is included: in this case, the SVE_SIG_*() macros should not be
 * used except for this check.
 *
 * The same convention applies when returning from a signal: a caller
 * will need to remove or resize the sve_context block if it wants to
 * make the SVE registers live when they were previously non-live or
 * vice-versa.  This may require the caller to allocate fresh
 * memory and/or move other context blocks in the signal frame.
 *
 * Changing the vector length during signal return is not permitted:
 * sve_context.vl must equal the thread's current vector length when
 * doing a sigreturn.
 *
 * On systems with support for SME the SVE register state may reflect either
 * streaming or non-streaming mode.  In streaming mode the streaming mode
 * vector length will be used and the flag SVE_SIG_FLAG_SM will be set in
 * the flags field. It is permitted to enter or leave streaming mode in
 * a signal return, applications should take care to ensure that any difference
 * in vector length between the two modes is handled, including any resizing
 * and movement of context blocks.
 *
 * Note: for all these macros, the "vq" argument denotes the vector length
 * in quadwords (i.e., units of 128 bits).
 *
 * The correct way to obtain vq is to use sve_vq_from_vl(vl).  The
 * result is valid if and only if sve_vl_valid(vl) is true.  This is
 * guaranteed for a struct sve_context written by the kernel.
 *
 *
 * Additional macros describe the contents and layout of the payload.
 * For each, SVE_SIG_x_OFFSET(args) is the start offset relative to
 * the start of struct sve_context, and SVE_SIG_x_SIZE(args) is the
 * size in bytes:
 *
 *	x	type				description
 *	-	----				-----------
 *	REGS					the entire SVE context
 *
 *	ZREGS	__uint128_t[SVE_NUM_ZREGS][vq]	all Z-registers
 *	ZREG	__uint128_t[vq]			individual Z-register Zn
 *
 *	PREGS	uint16_t[SVE_NUM_PREGS][vq]	all P-registers
 *	PREG	uint16_t[vq]			individual P-register Pn
 *
 *	FFR	uint16_t[vq]			first-fault status register
 *
 * Additional data might be appended in the future.
 *
 * Unlike vregs[] in fpsimd_context, each SVE scalable register (Z-, P- or FFR)
 * is encoded in memory in an endianness-invariant format, with the byte at
 * offset i from the start of the in-memory representation containing bits
 * [(7 + 8 * i) : (8 * i)] of the register value.
 */
# 274 "./arch/arm64/include/uapi/asm/sigcontext.h"
/*
 * If the ZA register is enabled for the thread at signal delivery then,
 * za_context.head.size >= ZA_SIG_CONTEXT_SIZE(sve_vq_from_vl(za_context.vl))
 * and the register data may be accessed using the ZA_SIG_*() macros.
 *
 * If za_context.head.size < ZA_SIG_CONTEXT_SIZE(sve_vq_from_vl(za_context.vl))
 * then ZA was not enabled and no register data was included in which case
 * ZA register was not enabled for the thread and no register data
 * the ZA_SIG_*() macros should not be used except for this check.
 *
 * The same convention applies when returning from a signal: a caller
 * will need to remove or resize the za_context block if it wants to
 * enable the ZA register when it was previously non-live or vice-versa.
 * This may require the caller to allocate fresh memory and/or move other
 * context blocks in the signal frame.
 *
 * Changing the vector length during signal return is not permitted:
 * za_context.vl must equal the thread's current SME vector length when
 * doing a sigreturn.
 */
# 12 "./arch/arm64/include/asm/fpsimd.h" 2
# 25 "./arch/arm64/include/asm/fpsimd.h"
/* Masks for extracting the FPSR and FPCR from the FPSCR */


/*
 * The VFP state has 32x64-bit registers and a single 32-bit
 * control/status register.
 */



/*
 * When we defined the maximum SVE vector length we defined the ABI so
 * that the maximum vector length included all the reserved for future
 * expansion bits in ZCR rather than those just currently defined by
 * the architecture. While SME follows a similar pattern the fact that
 * it includes a square matrix means that any allocations that attempt
 * to cover the maximum potential vector length (such as happen with
 * the regset used for ptrace) end up being extremely large. Define
 * the much lower actual limit for use in such situations.
 */


struct task_struct;

extern void fpsimd_save_state(struct user_fpsimd_state *state);
extern void fpsimd_load_state(struct user_fpsimd_state *state);

extern void fpsimd_thread_switch(struct task_struct *next);
extern void fpsimd_flush_thread(void);

extern void fpsimd_signal_preserve_current_state(void);
extern void fpsimd_preserve_current_state(void);
extern void fpsimd_restore_current_state(void);
extern void fpsimd_update_current_state(struct user_fpsimd_state const *state);
extern void fpsimd_kvm_prepare(void);

struct cpu_fp_state {
 struct user_fpsimd_state *st;
 void *sve_state;
 void *za_state;
 u64 *svcr;
 unsigned int sve_vl;
 unsigned int sme_vl;
 enum fp_type *fp_type;
 enum fp_type to_save;
};

extern void fpsimd_bind_state_to_cpu(struct cpu_fp_state *fp_state);

extern void fpsimd_flush_task_state(struct task_struct *target);
extern void fpsimd_save_and_flush_cpu_state(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool thread_sm_enabled(struct thread_struct *thread)
{
 return system_supports_sme() && (thread->svcr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (0)) * 0l)) : (int *)8))), (0) > (0), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (0))))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool thread_za_enabled(struct thread_struct *thread)
{
 return system_supports_sme() && (thread->svcr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (1)) * 0l)) : (int *)8))), (1) > (1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (1)) + 1) & (~(((0UL))) >> (64 - 1 - (1))))));
}

/* Maximum VL that SVE/SME VL-agnostic software can transparently support */


/* Offset of FFR in the SVE register dump */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t sve_ffr_offset(int vl)
{
 return (((sizeof(struct sve_context) + (16 /* number of bytes per quadword */ - 1)) / 16 /* number of bytes per quadword */ * 16 /* number of bytes per quadword */) + ((0 + ((0 + ((__u32)(((vl) / 16 /* number of bytes per quadword */)) * 16 /* number of bytes per quadword */) * (32)) - 0)) + (((0 + ((0 + ((__u32)(((vl) / 16 /* number of bytes per quadword */)) * 16 /* number of bytes per quadword */) * (32)) - 0)) + ((__u32)(((vl) / 16 /* number of bytes per quadword */)) * (16 /* number of bytes per quadword */ / 8)) * (16)) - (0 + ((0 + ((__u32)(((vl) / 16 /* number of bytes per quadword */)) * 16 /* number of bytes per quadword */) * (32)) - 0))))) - ((sizeof(struct sve_context) + (16 /* number of bytes per quadword */ - 1)) / 16 /* number of bytes per quadword */ * 16 /* number of bytes per quadword */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *sve_pffr(struct thread_struct *thread)
{
 unsigned int vl;

 if (system_supports_sme() && thread_sm_enabled(thread))
  vl = thread_get_sme_vl(thread);
 else
  vl = thread_get_sve_vl(thread);

 return (char *)thread->sve_state + sve_ffr_offset(vl);
}

extern void sve_save_state(void *state, u32 *pfpsr, int save_ffr);
extern void sve_load_state(void const *state, u32 const *pfpsr,
      int restore_ffr);
extern void sve_flush_live(bool flush_ffr, unsigned long vq_minus_1);
extern unsigned int sve_get_vl(void);
extern void sve_set_vq(unsigned long vq_minus_1);
extern void sme_set_vq(unsigned long vq_minus_1);
extern void za_save_state(void *state);
extern void za_load_state(void const *state);

struct arm64_cpu_capabilities;
extern void sve_kernel_enable(const struct arm64_cpu_capabilities *__unused);
extern void sme_kernel_enable(const struct arm64_cpu_capabilities *__unused);
extern void fa64_kernel_enable(const struct arm64_cpu_capabilities *__unused);

extern u64 read_zcr_features(void);
extern u64 read_smcr_features(void);

/*
 * Helpers to translate bit indices in sve_vq_map to VQ values (and
 * vice versa).  This allows find_next_bit() to be used to find the
 * _maximum_ VQ not exceeding a certain value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __vq_to_bit(unsigned int vq)
{
 return 512 - vq;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __bit_to_vq(unsigned int bit)
{
 return 512 - bit;
}


struct vl_info {
 enum vec_type type;
 const char *name; /* For display purposes */

 /* Minimum supported vector length across all CPUs */
 int min_vl;

 /* Maximum supported vector length across all CPUs */
 int max_vl;
 int max_virtualisable_vl;

 /*
	 * Set of available vector lengths,
	 * where length vq encoded as bit __vq_to_bit(vq):
	 */
 unsigned long vq_map[(((512) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];

 /* Set of vector lengths present on at least one cpu: */
 unsigned long vq_partial_map[(((512) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];
};



extern void sve_alloc(struct task_struct *task, bool flush);
extern void fpsimd_release_task(struct task_struct *task);
extern void fpsimd_sync_to_sve(struct task_struct *task);
extern void fpsimd_force_sync_to_sve(struct task_struct *task);
extern void sve_sync_to_fpsimd(struct task_struct *task);
extern void sve_sync_from_fpsimd_zeropad(struct task_struct *task);

extern int vec_set_vector_length(struct task_struct *task, enum vec_type type,
     unsigned long vl, unsigned long flags);

extern int sve_set_current_vl(unsigned long arg);
extern int sve_get_current_vl(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sve_user_disable(void)
{
 do { u64 __scs_val = ({ u64 __val; asm volatile("mrs %0, " "cpacr_el1" : "=r" (__val)); __val; }); u64 __scs_new = (__scs_val & ~(u64)((((((1UL))) << (17))) /* enable EL0 access, if EL1EN set */)) | (0); if (__scs_new != __scs_val) do { u64 __val = (u64)(__scs_new); asm volatile("msr " "cpacr_el1" ", %x0" : : "rZ" (__val)); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sve_user_enable(void)
{
 do { u64 __scs_val = ({ u64 __val; asm volatile("mrs %0, " "cpacr_el1" : "=r" (__val)); __val; }); u64 __scs_new = (__scs_val & ~(u64)(0)) | ((((((1UL))) << (17))) /* enable EL0 access, if EL1EN set */); if (__scs_new != __scs_val) do { u64 __val = (u64)(__scs_new); asm volatile("msr " "cpacr_el1" ", %x0" : : "rZ" (__val)); } while (0); } while (0);
}
# 197 "./arch/arm64/include/asm/fpsimd.h"
/*
 * Probing and setup functions.
 * Calls to these functions must be serialised with one another.
 */
enum vec_type;

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) vec_init_vq_map(enum vec_type type);
extern void vec_update_vq_map(enum vec_type type);
extern int vec_verify_vq_map(enum vec_type type);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) sve_setup(void);

extern __attribute__((__section__(".data..ro_after_init"))) struct vl_info vl_info[ARM64_VEC_MAX];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void write_vl(enum vec_type type, u64 val)
{
 u64 tmp;

 switch (type) {

 case ARM64_VEC_SVE:
  tmp = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((1) << 12) | ((2) << 8) | ((0) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; }) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (3)) * 0l)) : (int *)8))), (0) > (3), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (3)))));
  do { u64 __val = (u64)(tmp | val); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((1) << 12) | ((2) << 8) | ((0) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0);
  break;


 case ARM64_VEC_SME:
  tmp = ({ u64 __val; asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((0) << 16) | ((1) << 12) | ((2) << 8) | ((6) << 5))" "\n" "	.purgem	mrs_s\n" : "=r" (__val)); __val; }) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (3)) * 0l)) : (int *)8))), (0) > (3), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (3)))));
  do { u64 __val = (u64)(tmp | val); asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((3) << 19) | ((0) << 16) | ((1) << 12) | ((2) << 8) | ((6) << 5))" ", " "%x0" "\n" "	.purgem	msr_s\n" : : "rZ" (__val)); } while (0);
  break;

 default:
  ({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"arch/arm64/include/asm/fpsimd.h\"; .popsection; .long 14472b - .; .short 228; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
  break;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vec_max_vl(enum vec_type type)
{
 return vl_info[type].max_vl;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vec_max_virtualisable_vl(enum vec_type type)
{
 return vl_info[type].max_virtualisable_vl;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sve_max_vl(void)
{
 return vec_max_vl(ARM64_VEC_SVE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sve_max_virtualisable_vl(void)
{
 return vec_max_virtualisable_vl(ARM64_VEC_SVE);
}

/* Ensure vq >= SVE_VQ_MIN && vq <= SVE_VQ_MAX before calling this function */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vq_available(enum vec_type type, unsigned int vq)
{
 return ((__builtin_constant_p(__vq_to_bit(vq)) && __builtin_constant_p((uintptr_t)(vl_info[type].vq_map) != (uintptr_t)((void *)0)) && (uintptr_t)(vl_info[type].vq_map) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(vl_info[type].vq_map))) ? const_test_bit(__vq_to_bit(vq), vl_info[type].vq_map) : generic_test_bit(__vq_to_bit(vq), vl_info[type].vq_map));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sve_vq_available(unsigned int vq)
{
 return vq_available(ARM64_VEC_SVE, vq);
}

size_t sve_state_size(struct task_struct const *task);
# 314 "./arch/arm64/include/asm/fpsimd.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sme_user_disable(void)
{
 do { u64 __scs_val = ({ u64 __val; asm volatile("mrs %0, " "cpacr_el1" : "=r" (__val)); __val; }); u64 __scs_new = (__scs_val & ~(u64)((((((1UL))) << (25))) /* enable EL0 access, if EL1EN set */)) | (0); if (__scs_new != __scs_val) do { u64 __val = (u64)(__scs_new); asm volatile("msr " "cpacr_el1" ", %x0" : : "rZ" (__val)); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sme_user_enable(void)
{
 do { u64 __scs_val = ({ u64 __val; asm volatile("mrs %0, " "cpacr_el1" : "=r" (__val)); __val; }); u64 __scs_new = (__scs_val & ~(u64)(0)) | ((((((1UL))) << (25))) /* enable EL0 access, if EL1EN set */); if (__scs_new != __scs_val) do { u64 __val = (u64)(__scs_new); asm volatile("msr " "cpacr_el1" ", %x0" : : "rZ" (__val)); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sme_smstart_sm(void)
{
 asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((0) << 19) | ((3) << 16) | ((4) << 12) | ((3) << 8) | ((3) << 5))" ", " "xzr" "\n" "	.purgem	msr_s\n");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sme_smstop_sm(void)
{
 asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((0) << 19) | ((3) << 16) | ((4) << 12) | ((2) << 8) | ((3) << 5))" ", " "xzr" "\n" "	.purgem	msr_s\n");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sme_smstop(void)
{
 asm volatile("	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	msr_s, sreg, rt\n" ".inst " "(0xd5000000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	msr_s " "(((0) << 19) | ((3) << 16) | ((4) << 12) | ((6) << 8) | ((3) << 5))" ", " "xzr" "\n" "	.purgem	msr_s\n");
}

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) sme_setup(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sme_max_vl(void)
{
 return vec_max_vl(ARM64_VEC_SME);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sme_max_virtualisable_vl(void)
{
 return vec_max_virtualisable_vl(ARM64_VEC_SME);
}

extern void sme_alloc(struct task_struct *task);
extern unsigned int sme_get_vl(void);
extern int sme_set_current_vl(unsigned long arg);
extern int sme_get_current_vl(void);

/*
 * Return how many bytes of memory are required to store the full SME
 * specific state (currently just ZA) for task, given task's currently
 * configured vector length.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t za_state_size(struct task_struct const *task)
{
 unsigned int vl = task_get_sme_vl(task);

 return ((((vl) / 16 /* number of bytes per quadword */) * 16 /* number of bytes per quadword */) * (((vl) / 16 /* number of bytes per quadword */) * 16 /* number of bytes per quadword */));
}
# 392 "./arch/arm64/include/asm/fpsimd.h"
/* For use by EFI runtime services calls only */
extern void __efi_fpsimd_begin(void);
extern void __efi_fpsimd_end(void);
# 383 "./arch/arm64/include/asm/processor.h" 2

/* Userspace interface for PR_S[MV]E_{SET,GET}_VL prctl()s: */





/* PR_PAC_RESET_KEYS prctl */


/* PR_PAC_{SET,GET}_ENABLED_KEYS prctl */





/* PR_{SET,GET}_TAGGED_ADDR_CTRL prctl */
long set_tagged_addr_ctrl(struct task_struct *task, unsigned long arg);
long get_tagged_addr_ctrl(struct task_struct *task);
# 19 "./include/asm-generic/qrwlock.h" 2



/* Must be included from asm/spinlock.h after defining arch_spin_is_locked.  */

/*
 * Writer states & reader shift and bias.
 */






/*
 * External function declarations
 */
extern void queued_read_lock_slowpath(struct qrwlock *lock);
extern void queued_write_lock_slowpath(struct qrwlock *lock);

/**
 * queued_read_trylock - try to acquire read lock of a queued rwlock
 * @lock : Pointer to queued rwlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int queued_read_trylock(struct qrwlock *lock)
{
 int cnts;

 cnts = atomic_read(&lock->cnts);
 if (__builtin_expect(!!(!(cnts & 0x1ff /* Writer mask		   */)), 1)) {
  cnts = (u32)atomic_add_return_acquire((1U << 9 /* Reader count shift	   */), &lock->cnts);
  if (__builtin_expect(!!(!(cnts & 0x1ff /* Writer mask		   */)), 1))
   return 1;
  atomic_sub((1U << 9 /* Reader count shift	   */), &lock->cnts);
 }
 return 0;
}

/**
 * queued_write_trylock - try to acquire write lock of a queued rwlock
 * @lock : Pointer to queued rwlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int queued_write_trylock(struct qrwlock *lock)
{
 int cnts;

 cnts = atomic_read(&lock->cnts);
 if (__builtin_expect(!!(cnts), 0))
  return 0;

 return __builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, 0x0ff /* A writer holds the lock */)), 1);

}
/**
 * queued_read_lock - acquire read lock of a queued rwlock
 * @lock: Pointer to queued rwlock structure
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void queued_read_lock(struct qrwlock *lock)
{
 int cnts;

 cnts = atomic_add_return_acquire((1U << 9 /* Reader count shift	   */), &lock->cnts);
 if (__builtin_expect(!!(!(cnts & 0x1ff /* Writer mask		   */)), 1))
  return;

 /* The slowpath will decrement the reader count, if necessary. */
 queued_read_lock_slowpath(lock);
}

/**
 * queued_write_lock - acquire write lock of a queued rwlock
 * @lock : Pointer to queued rwlock structure
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void queued_write_lock(struct qrwlock *lock)
{
 int cnts = 0;
 /* Optimize for the unfair lock case where the fair flag is 0. */
 if (__builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, 0x0ff /* A writer holds the lock */)), 1))
  return;

 queued_write_lock_slowpath(lock);
}

/**
 * queued_read_unlock - release read lock of a queued rwlock
 * @lock : Pointer to queued rwlock structure
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void queued_read_unlock(struct qrwlock *lock)
{
 /*
	 * Atomically decrement the reader count
	 */
 (void)atomic_sub_return_release((1U << 9 /* Reader count shift	   */), &lock->cnts);
}

/**
 * queued_write_unlock - release write lock of a queued rwlock
 * @lock : Pointer to queued rwlock structure
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void queued_write_unlock(struct qrwlock *lock)
{
 do { do { } while (0); do { typeof(&lock->wlocked) __p = (&lock->wlocked); union { typeof( _Generic((*&lock->wlocked), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&lock->wlocked))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&lock->wlocked), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&lock->wlocked)))) (0) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_180(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&lock->wlocked) == sizeof(char) || sizeof(*&lock->wlocked) == sizeof(short) || sizeof(*&lock->wlocked) == sizeof(int) || sizeof(*&lock->wlocked) == sizeof(long)))) __compiletime_assert_180(); } while (0); kasan_check_write(__p, sizeof(*&lock->wlocked)); switch (sizeof(*&lock->wlocked)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0);
# 123 "./include/asm-generic/qrwlock.h"
}

/**
 * queued_rwlock_is_contended - check if the lock is contended
 * @lock : Pointer to queued rwlock structure
 * Return: 1 if lock contended, 0 otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int queued_rwlock_is_contended(struct qrwlock *lock)
{
 return queued_spin_is_locked(&lock->wait_lock);
}

/*
 * Remapping rwlock architecture specific functions to the corresponding
 * queued rwlock functions.
 */
# 2 "./arch/arm64/include/generated/asm/qrwlock.h" 2
# 10 "./arch/arm64/include/asm/spinlock.h" 2

/* See include/linux/spinlock.h */


/*
 * Changing this will break osq_lock() thanks to the call inside
 * smp_cond_load_relaxed().
 *
 * See:
 * https://lore.kernel.org/lkml/20200110100612.GC2827@hirez.programming.kicks-ass.net
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vcpu_is_preempted(int cpu)
{
 return false;
}
# 95 "./include/linux/spinlock.h" 2
# 123 "./include/linux/spinlock.h"
/*
 * smp_mb__after_spinlock() provides the equivalent of a full memory barrier
 * between program-order earlier lock acquisitions and program-order later
 * memory accesses.
 *
 * This guarantees that the following two properties hold:
 *
 *   1) Given the snippet:
 *
 *	  { X = 0;  Y = 0; }
 *
 *	  CPU0				CPU1
 *
 *	  WRITE_ONCE(X, 1);		WRITE_ONCE(Y, 1);
 *	  spin_lock(S);			smp_mb();
 *	  smp_mb__after_spinlock();	r1 = READ_ONCE(X);
 *	  r0 = READ_ONCE(Y);
 *	  spin_unlock(S);
 *
 *      it is forbidden that CPU0 does not observe CPU1's store to Y (r0 = 0)
 *      and CPU1 does not observe CPU0's store to X (r1 = 0); see the comments
 *      preceding the call to smp_mb__after_spinlock() in __schedule() and in
 *      try_to_wake_up().
 *
 *   2) Given the snippet:
 *
 *  { X = 0;  Y = 0; }
 *
 *  CPU0		CPU1				CPU2
 *
 *  spin_lock(S);	spin_lock(S);			r1 = READ_ONCE(Y);
 *  WRITE_ONCE(X, 1);	smp_mb__after_spinlock();	smp_rmb();
 *  spin_unlock(S);	r0 = READ_ONCE(X);		r2 = READ_ONCE(X);
 *			WRITE_ONCE(Y, 1);
 *			spin_unlock(S);
 *
 *      it is forbidden that CPU0's critical section executes before CPU1's
 *      critical section (r0 = 1), CPU2 observes CPU1's store to Y (r1 = 1)
 *      and CPU2 does not observe CPU0's store to X (r2 = 0); see the comments
 *      preceding the calls to smp_rmb() in try_to_wake_up() for similar
 *      snippets but "projected" onto two CPUs.
 *
 * Property (2) upgrades the lock to an RCsc lock.
 *
 * Since most load-store architectures implement ACQUIRE with an smp_mb() after
 * the LL/SC loop, they need no further barriers. Similarly all our TSO
 * architectures imply an smp_mb() for each atomic instruction and equally don't
 * need more.
 *
 * Architectures that can implement ACQUIRE better need to take care.
 */
# 183 "./include/linux/spinlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_raw_spin_lock(raw_spinlock_t *lock)
{
 (void)0;
 queued_spin_lock(&lock->raw_lock);
 do { } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int do_raw_spin_trylock(raw_spinlock_t *lock)
{
 int ret = queued_spin_trylock(&(lock)->raw_lock);

 if (ret)
  do { } while (0);

 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_raw_spin_unlock(raw_spinlock_t *lock)
{
 do { } while (0);
 queued_spin_unlock(&lock->raw_lock);
 (void)0;
}


/*
 * Define the various spin_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPTION are set. The
 * various methods are defined as nops in the case they are not
 * required.
 */
# 228 "./include/linux/spinlock.h"
/*
 * Always evaluate the 'subclass' argument to avoid that the compiler
 * warns about set-but-not-used variables when building with
 * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.
 */
# 303 "./include/linux/spinlock.h"
/* Include rwlock functions for !RT */
# 1 "./include/linux/rwlock.h" 1







/*
 * rwlock related methods
 *
 * split out from spinlock.h
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
# 47 "./include/linux/rwlock.h"
/*
 * Define the various rw_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The various
 * methods are defined as nops in the case they are not required.
 */
# 305 "./include/linux/spinlock.h" 2


/*
 * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
 */

# 1 "./include/linux/spinlock_api_smp.h" 1







/*
 * include/linux/spinlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */

int in_lock_functions(unsigned long addr);



void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
                        ;
void __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_nest_lock(raw_spinlock_t *lock, struct lockdep_map *map)
                        ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_bh(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_irq(raw_spinlock_t *lock)
                        ;

unsigned long __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_irqsave(raw_spinlock_t *lock)
                        ;
unsigned long __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_irqsave_nested(raw_spinlock_t *lock, int subclass)
                        ;
int __attribute__((__section__(".spinlock.text"))) _raw_spin_trylock(raw_spinlock_t *lock);
int __attribute__((__section__(".spinlock.text"))) _raw_spin_trylock_bh(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text"))) _raw_spin_unlock(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_unlock_bh(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_unlock_irq(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text")))
_raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
                        ;
# 86 "./include/linux/spinlock_api_smp.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __raw_spin_trylock(raw_spinlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 if (do_raw_spin_trylock(lock)) {
  do { } while (0);
  return 1;
 }
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 return 0;
}

/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPTION, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { } while (0);
 do_raw_spin_lock(lock);
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_spin_lock_irq(raw_spinlock_t *lock)
{
 do { arch_local_irq_disable(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { } while (0);
 do_raw_spin_lock(lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_spin_lock_bh(raw_spinlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 do { } while (0);
 do_raw_spin_lock(lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_spin_lock(raw_spinlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { } while (0);
 do_raw_spin_lock(lock);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_spin_unlock(raw_spinlock_t *lock)
{
 do { } while (0);
 do_raw_spin_unlock(lock);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,
         unsigned long flags)
{
 do { } while (0);
 do_raw_spin_unlock(lock);
 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_spin_unlock_irq(raw_spinlock_t *lock)
{
 do { } while (0);
 do_raw_spin_unlock(lock);
 do { arch_local_irq_enable(); } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_spin_unlock_bh(raw_spinlock_t *lock)
{
 do { } while (0);
 do_raw_spin_unlock(lock);
 __local_bh_enable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __raw_spin_trylock_bh(raw_spinlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 if (do_raw_spin_trylock(lock)) {
  do { } while (0);
  return 1;
 }
 __local_bh_enable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 return 0;
}

/* PREEMPT_RT has its own rwlock implementation */

# 1 "./include/linux/rwlock_api_smp.h" 1







/*
 * include/linux/rwlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */

void __attribute__((__section__(".spinlock.text"))) _raw_read_lock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock_nested(rwlock_t *lock, int subclass) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_lock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_lock_irq(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock_irq(rwlock_t *lock) ;
unsigned long __attribute__((__section__(".spinlock.text"))) _raw_read_lock_irqsave(rwlock_t *lock)
                       ;
unsigned long __attribute__((__section__(".spinlock.text"))) _raw_write_lock_irqsave(rwlock_t *lock)
                       ;
int __attribute__((__section__(".spinlock.text"))) _raw_read_trylock(rwlock_t *lock);
int __attribute__((__section__(".spinlock.text"))) _raw_write_trylock(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text"))) _raw_read_unlock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_unlock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_unlock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_unlock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_unlock_irq(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_unlock_irq(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text")))
_raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
                       ;
void __attribute__((__section__(".spinlock.text")))
_raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
                       ;
# 118 "./include/linux/rwlock_api_smp.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __raw_read_trylock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 if (queued_read_trylock(&(lock)->raw_lock)) {
  do { if (0) do { } while (0); else do { } while (0); } while (0);
  return 1;
 }
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __raw_write_trylock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 if (queued_write_trylock(&(lock)->raw_lock)) {
  do { } while (0);
  return 1;
 }
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 return 0;
}

/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_read_lock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { if (0) do { } while (0); else do { } while (0); } while (0);
 do {(void)0; queued_read_lock(&(lock)->raw_lock); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __raw_read_lock_irqsave(rwlock_t *lock)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { if (0) do { } while (0); else do { } while (0); } while (0);
 do {(void)0; queued_read_lock(&(lock)->raw_lock); } while (0);
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_read_lock_irq(rwlock_t *lock)
{
 do { arch_local_irq_disable(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { if (0) do { } while (0); else do { } while (0); } while (0);
 do {(void)0; queued_read_lock(&(lock)->raw_lock); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_read_lock_bh(rwlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 do { if (0) do { } while (0); else do { } while (0); } while (0);
 do {(void)0; queued_read_lock(&(lock)->raw_lock); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __raw_write_lock_irqsave(rwlock_t *lock)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { } while (0);
 do {(void)0; queued_write_lock(&(lock)->raw_lock); } while (0);
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_lock_irq(rwlock_t *lock)
{
 do { arch_local_irq_disable(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { } while (0);
 do {(void)0; queued_write_lock(&(lock)->raw_lock); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_lock_bh(rwlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 do { } while (0);
 do {(void)0; queued_write_lock(&(lock)->raw_lock); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_lock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { } while (0);
 do {(void)0; queued_write_lock(&(lock)->raw_lock); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_lock_nested(rwlock_t *lock, int subclass)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { } while (0);
 do {(void)0; queued_write_lock(&(lock)->raw_lock); } while (0);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_unlock(rwlock_t *lock)
{
 do { } while (0);
 do {queued_write_unlock(&(lock)->raw_lock); (void)0; } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_read_unlock(rwlock_t *lock)
{
 do { } while (0);
 do {queued_read_unlock(&(lock)->raw_lock); (void)0; } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
__raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
{
 do { } while (0);
 do {queued_read_unlock(&(lock)->raw_lock); (void)0; } while (0);
 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_read_unlock_irq(rwlock_t *lock)
{
 do { } while (0);
 do {queued_read_unlock(&(lock)->raw_lock); (void)0; } while (0);
 do { arch_local_irq_enable(); } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_read_unlock_bh(rwlock_t *lock)
{
 do { } while (0);
 do {queued_read_unlock(&(lock)->raw_lock); (void)0; } while (0);
 __local_bh_enable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_unlock_irqrestore(rwlock_t *lock,
          unsigned long flags)
{
 do { } while (0);
 do {queued_write_unlock(&(lock)->raw_lock); (void)0; } while (0);
 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_unlock_irq(rwlock_t *lock)
{
 do { } while (0);
 do {queued_write_unlock(&(lock)->raw_lock); (void)0; } while (0);
 do { arch_local_irq_enable(); } while (0);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_write_unlock_bh(rwlock_t *lock)
{
 do { } while (0);
 do {queued_write_unlock(&(lock)->raw_lock); (void)0; } while (0);
 __local_bh_enable_ip((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}
# 184 "./include/linux/spinlock_api_smp.h" 2
# 312 "./include/linux/spinlock.h" 2




/* Non PREEMPT_RT kernel, map to raw spinlocks: */


/*
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
 return &lock->rlock;
}
# 348 "./include/linux/spinlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void spin_lock(spinlock_t *lock)
{
 _raw_spin_lock(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void spin_lock_bh(spinlock_t *lock)
{
 _raw_spin_lock_bh(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int spin_trylock(spinlock_t *lock)
{
 return (_raw_spin_trylock(&lock->rlock));
}
# 373 "./include/linux/spinlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void spin_lock_irq(spinlock_t *lock)
{
 _raw_spin_lock_irq(&lock->rlock);
}
# 388 "./include/linux/spinlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void spin_unlock(spinlock_t *lock)
{
 _raw_spin_unlock(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void spin_unlock_bh(spinlock_t *lock)
{
 _raw_spin_unlock_bh(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void spin_unlock_irq(spinlock_t *lock)
{
 _raw_spin_unlock_irq(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
 do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _raw_spin_unlock_irqrestore(&lock->rlock, flags); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int spin_trylock_bh(spinlock_t *lock)
{
 return (_raw_spin_trylock_bh(&lock->rlock));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int spin_trylock_irq(spinlock_t *lock)
{
 return ({ do { arch_local_irq_disable(); } while (0); (_raw_spin_trylock(&lock->rlock)) ? 1 : ({ do { arch_local_irq_enable(); } while (0); 0; }); });
}






/**
 * spin_is_locked() - Check whether a spinlock is locked.
 * @lock: Pointer to the spinlock.
 *
 * This function is NOT required to provide any memory ordering
 * guarantees; it could be used for debugging purposes or, when
 * additional synchronization is needed, accompanied with other
 * constructs (memory barriers) enforcing the synchronization.
 *
 * Returns: 1 if @lock is locked, 0 otherwise.
 *
 * Note that the function only tells you that the spinlock is
 * seen to be locked, not that it is locked on your CPU.
 *
 * Further, on CONFIG_SMP=n builds with CONFIG_DEBUG_SPINLOCK=n,
 * the return value is always 0 (see include/linux/spinlock_up.h).
 * Therefore you should not rely heavily on the return value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int spin_is_locked(spinlock_t *lock)
{
 return queued_spin_is_locked(&(&lock->rlock)->raw_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int spin_is_contended(spinlock_t *lock)
{
 return queued_spin_is_contended(&(&lock->rlock)->raw_lock);
}







/*
 * Pull the atomic_t declaration:
 * (asm-mips/atomic.h needs above definitions)
 */

/**
 * atomic_dec_and_lock - lock on reaching reference count zero
 * @atomic: the atomic counter
 * @lock: the spinlock in question
 *
 * Decrements @atomic by 1.  If the result is 0, returns true and locks
 * @lock.  Returns false for all other cases.
 */
extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);



extern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,
     unsigned long *flags);



int __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,
        size_t max_size, unsigned int cpu_mult,
        gfp_t gfp, const char *name,
        struct lock_class_key *key);
# 494 "./include/linux/spinlock.h"
void free_bucket_spinlocks(spinlock_t *locks);
# 10 "./include/linux/wait.h" 2


# 1 "./include/uapi/linux/wait.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 16 "./include/uapi/linux/wait.h"
/* First argument to waitid: */
# 13 "./include/linux/wait.h" 2

typedef struct wait_queue_entry wait_queue_entry_t;

typedef int (*wait_queue_func_t)(struct wait_queue_entry *wq_entry, unsigned mode, int flags, void *key);
int default_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int flags, void *key);

/* wait_queue_entry::flags */







/*
 * A single wait-queue entry structure:
 */
struct wait_queue_entry {
 unsigned int flags;
 void *private;
 wait_queue_func_t func;
 struct list_head entry;
};

struct wait_queue_head {
 spinlock_t lock;
 struct list_head head;
};
typedef struct wait_queue_head wait_queue_head_t;

struct task_struct;

/*
 * Macros for declaration and initialisaton of the datatypes
 */
# 64 "./include/linux/wait.h"
extern void __init_waitqueue_head(struct wait_queue_head *wq_head, const char *name, struct lock_class_key *);
# 82 "./include/linux/wait.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_waitqueue_entry(struct wait_queue_entry *wq_entry, struct task_struct *p)
{
 wq_entry->flags = 0;
 wq_entry->private = p;
 wq_entry->func = default_wake_function;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
init_waitqueue_func_entry(struct wait_queue_entry *wq_entry, wait_queue_func_t func)
{
 wq_entry->flags = 0;
 wq_entry->private = ((void *)0);
 wq_entry->func = func;
}

/**
 * waitqueue_active -- locklessly test for waiters on the queue
 * @wq_head: the waitqueue to test for waiters
 *
 * returns true if the wait list is not empty
 *
 * NOTE: this function is lockless and requires care, incorrect usage _will_
 * lead to sporadic and non-obvious failure.
 *
 * Use either while holding wait_queue_head::lock or when used for wakeups
 * with an extra smp_mb() like::
 *
 *      CPU0 - waker                    CPU1 - waiter
 *
 *                                      for (;;) {
 *      @cond = true;                     prepare_to_wait(&wq_head, &wait, state);
 *      smp_mb();                         // smp_mb() from set_current_state()
 *      if (waitqueue_active(wq_head))         if (@cond)
 *        wake_up(wq_head);                      break;
 *                                        schedule();
 *                                      }
 *                                      finish_wait(&wq_head, &wait);
 *
 * Because without the explicit smp_mb() it's possible for the
 * waitqueue_active() load to get hoisted over the @cond store such that we'll
 * observe an empty wait list while the waiter might not observe @cond.
 *
 * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
 * which (when the lock is uncontended) are of roughly equal cost.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int waitqueue_active(struct wait_queue_head *wq_head)
{
 return !list_empty(&wq_head->head);
}

/**
 * wq_has_single_sleeper - check if there is only one sleeper
 * @wq_head: wait queue head
 *
 * Returns true of wq_head has only one sleeper on the list.
 *
 * Please refer to the comment for waitqueue_active.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool wq_has_single_sleeper(struct wait_queue_head *wq_head)
{
 return list_is_singular(&wq_head->head);
}

/**
 * wq_has_sleeper - check if there are any waiting processes
 * @wq_head: wait queue head
 *
 * Returns true if wq_head has waiting processes
 *
 * Please refer to the comment for waitqueue_active.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool wq_has_sleeper(struct wait_queue_head *wq_head)
{
 /*
	 * We need to be sure we are in sync with the
	 * add_wait_queue modifications to the wait queue.
	 *
	 * This memory barrier should be paired with one on the
	 * waiting side.
	 */
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 return waitqueue_active(wq_head);
}

extern void add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
extern void add_wait_queue_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
extern void add_wait_queue_priority(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
extern void remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 struct list_head *head = &wq_head->head;
 struct wait_queue_entry *wq;

 for (wq = ({ void *__mptr = (void *)((&wq_head->head)->next); _Static_assert(__builtin_types_compatible_p(typeof(*((&wq_head->head)->next)), typeof(((typeof(*wq) *)0)->entry)) || __builtin_types_compatible_p(typeof(*((&wq_head->head)->next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*wq) *)(__mptr - __builtin_offsetof(typeof(*wq), entry))); }); !(&wq->entry == (&wq_head->head)); wq = ({ void *__mptr = (void *)((wq)->entry.next); _Static_assert(__builtin_types_compatible_p(typeof(*((wq)->entry.next)), typeof(((typeof(*(wq)) *)0)->entry)) || __builtin_types_compatible_p(typeof(*((wq)->entry.next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*(wq)) *)(__mptr - __builtin_offsetof(typeof(*(wq)), entry))); })) {
  if (!(wq->flags & 0x20))
   break;
  head = &wq->entry;
 }
 list_add(&wq_entry->entry, head);
}

/*
 * Used for wake-one threads:
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
__add_wait_queue_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 wq_entry->flags |= 0x01;
 __add_wait_queue(wq_head, wq_entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __add_wait_queue_entry_tail(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 list_add_tail(&wq_entry->entry, &wq_head->head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
__add_wait_queue_entry_tail_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 wq_entry->flags |= 0x01;
 __add_wait_queue_entry_tail(wq_head, wq_entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
__remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 list_del(&wq_entry->entry);
}

int __wake_up(struct wait_queue_head *wq_head, unsigned int mode, int nr, void *key);
void __wake_up_locked_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
void __wake_up_locked_key_bookmark(struct wait_queue_head *wq_head,
  unsigned int mode, void *key, wait_queue_entry_t *bookmark);
void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
void __wake_up_locked_sync_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
void __wake_up_locked(struct wait_queue_head *wq_head, unsigned int mode, int nr);
void __wake_up_sync(struct wait_queue_head *wq_head, unsigned int mode);
void __wake_up_pollfree(struct wait_queue_head *wq_head);
# 233 "./include/linux/wait.h"
/*
 * Wakeup macros to be used to report events to the targets.
 */
# 249 "./include/linux/wait.h"
/**
 * wake_up_pollfree - signal that a polled waitqueue is going away
 * @wq_head: the wait queue head
 *
 * In the very rare cases where a ->poll() implementation uses a waitqueue whose
 * lifetime is tied to a task rather than to the 'struct file' being polled,
 * this function must be called before the waitqueue is freed so that
 * non-blocking polls (e.g. epoll) are notified that the queue is going away.
 *
 * The caller must also RCU-delay the freeing of the wait_queue_head, e.g. via
 * an explicit synchronize_rcu() or call_rcu(), or via SLAB_TYPESAFE_BY_RCU.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void wake_up_pollfree(struct wait_queue_head *wq_head)
{
 /*
	 * For performance reasons, we don't always take the queue lock here.
	 * Therefore, we might race with someone removing the last entry from
	 * the queue, and proceed while they still hold the queue lock.
	 * However, rcu_read_lock() is required to be held in such cases, so we
	 * can safely proceed with an RCU-delayed free.
	 */
 if (waitqueue_active(wq_head))
  __wake_up_pollfree(wq_head);
}
# 286 "./include/linux/wait.h"
extern void init_wait_entry(struct wait_queue_entry *wq_entry, int flags);

/*
 * The below macro ___wait_event() has an explicit shadow of the __ret
 * variable when used from the wait_event_*() macros.
 *
 * This is so that both can use the ___wait_cond_timeout() construct
 * to wrap the condition.
 *
 * The type inconsistency of the wait_event_*() __ret variable is also
 * on purpose; we use long where we can return timeout values and int
 * otherwise.
 */
# 328 "./include/linux/wait.h"
/**
 * wait_event - sleep until a condition gets true
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
# 352 "./include/linux/wait.h"
/*
 * io_wait_event() -- like wait_event() but with io_schedule()
 */
# 367 "./include/linux/wait.h"
/**
 * wait_event_freezable - sleep (or freeze) until a condition gets true
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE -- so as not to contribute
 * to system load) until the @condition evaluates to true. The
 * @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
# 393 "./include/linux/wait.h"
/**
 * wait_event_timeout - sleep until a condition gets true or a timeout elapses
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * or the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed.
 */
# 426 "./include/linux/wait.h"
/*
 * like wait_event_timeout() -- except it uses TASK_INTERRUPTIBLE to avoid
 * increasing load and is freezable.
 */
# 442 "./include/linux/wait.h"
/*
 * Just like wait_event_cmd(), except it sets exclusive flag
 */
# 456 "./include/linux/wait.h"
/**
 * wait_event_cmd - sleep until a condition gets true
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @cmd1: the command will be executed before sleep
 * @cmd2: the command will be executed after sleep
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
# 481 "./include/linux/wait.h"
/**
 * wait_event_interruptible - sleep until a condition gets true
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
# 510 "./include/linux/wait.h"
/**
 * wait_event_interruptible_timeout - sleep until a condition gets true or a timeout elapses
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed, or -%ERESTARTSYS if it was
 * interrupted by a signal.
 */
# 565 "./include/linux/wait.h"
/**
 * wait_event_hrtimeout - sleep until a condition gets true or a timeout elapses
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, as a ktime_t
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if @condition became true, or -ETIME if the timeout
 * elapsed.
 */
# 591 "./include/linux/wait.h"
/**
 * wait_event_interruptible_hrtimeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, as a ktime_t
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if @condition became true, -ERESTARTSYS if it was
 * interrupted by a signal, or -ETIME if the timeout elapsed.
 */
# 657 "./include/linux/wait.h"
/**
 * wait_event_idle - wait for a condition without contributing to system load
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_IDLE) until the
 * @condition evaluates to true.
 * The @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 */







/**
 * wait_event_idle_exclusive - wait for a condition with contributing to system load
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_IDLE) until the
 * @condition evaluates to true.
 * The @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus if other processes wait on the same list, when this
 * process is woken further processes are not considered.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 */
# 706 "./include/linux/wait.h"
/**
 * wait_event_idle_timeout - sleep without load until a condition becomes true or a timeout elapses
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_IDLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * or the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed.
 */
# 739 "./include/linux/wait.h"
/**
 * wait_event_idle_exclusive_timeout - sleep without load until a condition becomes true or a timeout elapses
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_IDLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq_head is woken up.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus if other processes wait on the same list, when this
 * process is woken further processes are not considered.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * or the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed.
 */
# 771 "./include/linux/wait.h"
extern int do_wait_intr(wait_queue_head_t *, wait_queue_entry_t *);
extern int do_wait_intr_irq(wait_queue_head_t *, wait_queue_entry_t *);
# 791 "./include/linux/wait.h"
/**
 * wait_event_interruptible_locked - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock()/spin_unlock()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_locked_irq - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock_irq()/spin_unlock_irq()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_exclusive_locked - sleep exclusively until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock()/spin_unlock()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus when other process waits process on the list if this
 * process is awaken further processes are not considered.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_exclusive_locked_irq - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock_irq()/spin_unlock_irq()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus when other process waits process on the list if this
 * process is awaken further processes are not considered.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
# 911 "./include/linux/wait.h"
/**
 * wait_event_killable - sleep until a condition gets true
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_KILLABLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
# 938 "./include/linux/wait.h"
/**
 * wait_event_state - sleep until a condition gets true
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @state: state to sleep in
 *
 * The process is put to sleep (@state) until the @condition evaluates to true
 * or a signal is received (when allowed by @state).  The @condition is checked
 * each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a signal
 * (when allowed by @state) and 0 if @condition evaluated to true.
 */
# 968 "./include/linux/wait.h"
/**
 * wait_event_killable_timeout - sleep until a condition gets true or a timeout elapses
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_KILLABLE) until the
 * @condition evaluates to true or a kill signal is received.
 * The @condition is checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed, or -%ERESTARTSYS if it was
 * interrupted by a kill signal.
 *
 * Only kill signals interrupt this process.
 */
# 1008 "./include/linux/wait.h"
/**
 * wait_event_lock_irq_cmd - sleep until a condition gets true. The
 *			     condition is checked under the lock. This
 *			     is expected to be called with the lock
 *			     taken.
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before cmd
 *	  and schedule() and reacquired afterwards.
 * @cmd: a command which is invoked outside the critical section before
 *	 sleep
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before invoking the cmd and going to sleep and is reacquired
 * afterwards.
 */







/**
 * wait_event_lock_irq - sleep until a condition gets true. The
 *			 condition is checked under the lock. This
 *			 is expected to be called with the lock
 *			 taken.
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 */
# 1073 "./include/linux/wait.h"
/**
 * wait_event_interruptible_lock_irq_cmd - sleep until a condition gets true.
 *		The condition is checked under the lock. This is expected to
 *		be called with the lock taken.
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before cmd and
 *	  schedule() and reacquired afterwards.
 * @cmd: a command which is invoked outside the critical section before
 *	 sleep
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received. The @condition is
 * checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before invoking the cmd and going to sleep and is reacquired
 * afterwards.
 *
 * The macro will return -ERESTARTSYS if it was interrupted by a signal
 * and 0 if @condition evaluated to true.
 */
# 1107 "./include/linux/wait.h"
/**
 * wait_event_interruptible_lock_irq - sleep until a condition gets true.
 *		The condition is checked under the lock. This is expected
 *		to be called with the lock taken.
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or signal is received. The @condition is
 * checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 *
 * The macro will return -ERESTARTSYS if it was interrupted by a signal
 * and 0 if @condition evaluated to true.
 */
# 1145 "./include/linux/wait.h"
/**
 * wait_event_interruptible_lock_irq_timeout - sleep until a condition gets
 *		true or a timeout elapses. The condition is checked under
 *		the lock. This is expected to be called with the lock taken.
 * @wq_head: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or signal is received. The @condition is
 * checked each time the waitqueue @wq_head is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 *
 * The function returns 0 if the @timeout elapsed, -ERESTARTSYS if it
 * was interrupted by a signal, and the remaining jiffies otherwise
 * if the condition evaluated to true before the timeout elapsed.
 */
# 1190 "./include/linux/wait.h"
/*
 * Waitqueues which are removed from the waitqueue_head at wakeup time
 */
void prepare_to_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
bool prepare_to_wait_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
long prepare_to_wait_event(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
long wait_woken(struct wait_queue_entry *wq_entry, unsigned mode, long timeout);
int woken_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key);
int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key);
# 1218 "./include/linux/wait.h"
typedef int (*task_call_f)(struct task_struct *p, void *arg);
extern int task_call_func(struct task_struct *p, task_call_f func, void *arg);
# 9 "./include/linux/wait_bit.h" 2

struct wait_bit_key {
 void *flags;
 int bit_nr;
 unsigned long timeout;
};

struct wait_bit_queue_entry {
 struct wait_bit_key key;
 struct wait_queue_entry wq_entry;
};




typedef int wait_bit_action_f(struct wait_bit_key *key, int mode);

void __wake_up_bit(struct wait_queue_head *wq_head, void *word, int bit);
int __wait_on_bit(struct wait_queue_head *wq_head, struct wait_bit_queue_entry *wbq_entry, wait_bit_action_f *action, unsigned int mode);
int __wait_on_bit_lock(struct wait_queue_head *wq_head, struct wait_bit_queue_entry *wbq_entry, wait_bit_action_f *action, unsigned int mode);
void wake_up_bit(void *word, int bit);
int out_of_line_wait_on_bit(void *word, int, wait_bit_action_f *action, unsigned int mode);
int out_of_line_wait_on_bit_timeout(void *word, int, wait_bit_action_f *action, unsigned int mode, unsigned long timeout);
int out_of_line_wait_on_bit_lock(void *word, int, wait_bit_action_f *action, unsigned int mode);
struct wait_queue_head *bit_waitqueue(void *word, int bit);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) wait_bit_init(void);

int wake_bit_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key);
# 49 "./include/linux/wait_bit.h"
extern int bit_wait(struct wait_bit_key *key, int mode);
extern int bit_wait_io(struct wait_bit_key *key, int mode);
extern int bit_wait_timeout(struct wait_bit_key *key, int mode);
extern int bit_wait_io_timeout(struct wait_bit_key *key, int mode);

/**
 * wait_on_bit - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit.
 * For instance, if one were to have waiters on a bitflag, one would
 * call wait_on_bit() in threads waiting for the bit to clear.
 * One uses wait_on_bit() where one is waiting for the bit to clear,
 * but has no intention of setting it.
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
wait_on_bit(unsigned long *word, int bit, unsigned mode)
{
 do { do { } while (0); } while (0);
 if (!((__builtin_constant_p(bit) && __builtin_constant_p((uintptr_t)(word) != (uintptr_t)((void *)0)) && (uintptr_t)(word) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(word))) ? generic_test_bit_acquire(bit, word) : generic_test_bit_acquire(bit, word)))
  return 0;
 return out_of_line_wait_on_bit(word, bit,
           bit_wait,
           mode);
}

/**
 * wait_on_bit_io - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared.  This is similar to wait_on_bit(), but calls
 * io_schedule() instead of schedule() for the actual waiting.
 *
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
wait_on_bit_io(unsigned long *word, int bit, unsigned mode)
{
 do { do { } while (0); } while (0);
 if (!((__builtin_constant_p(bit) && __builtin_constant_p((uintptr_t)(word) != (uintptr_t)((void *)0)) && (uintptr_t)(word) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(word))) ? generic_test_bit_acquire(bit, word) : generic_test_bit_acquire(bit, word)))
  return 0;
 return out_of_line_wait_on_bit(word, bit,
           bit_wait_io,
           mode);
}

/**
 * wait_on_bit_timeout - wait for a bit to be cleared or a timeout elapses
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 * @timeout: timeout, in jiffies
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared. This is similar to wait_on_bit(), except also takes a
 * timeout parameter.
 *
 * Returned value will be zero if the bit was cleared before the
 * @timeout elapsed, or non-zero if the @timeout elapsed or process
 * received a signal and the mode permitted wakeup on that signal.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
wait_on_bit_timeout(unsigned long *word, int bit, unsigned mode,
      unsigned long timeout)
{
 do { do { } while (0); } while (0);
 if (!((__builtin_constant_p(bit) && __builtin_constant_p((uintptr_t)(word) != (uintptr_t)((void *)0)) && (uintptr_t)(word) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(word))) ? generic_test_bit_acquire(bit, word) : generic_test_bit_acquire(bit, word)))
  return 0;
 return out_of_line_wait_on_bit_timeout(word, bit,
            bit_wait_timeout,
            mode, timeout);
}

/**
 * wait_on_bit_action - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared, and allow the waiting action to be specified.
 * This is like wait_on_bit() but allows fine control of how the waiting
 * is done.
 *
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
wait_on_bit_action(unsigned long *word, int bit, wait_bit_action_f *action,
     unsigned mode)
{
 do { do { } while (0); } while (0);
 if (!((__builtin_constant_p(bit) && __builtin_constant_p((uintptr_t)(word) != (uintptr_t)((void *)0)) && (uintptr_t)(word) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(word))) ? generic_test_bit_acquire(bit, word) : generic_test_bit_acquire(bit, word)))
  return 0;
 return out_of_line_wait_on_bit(word, bit, action, mode);
}

/**
 * wait_on_bit_lock - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit
 * when one intends to set it, for instance, trying to lock bitflags.
 * For instance, if one were to have waiters trying to set bitflag
 * and waiting for it to clear before setting it, one would call
 * wait_on_bit() in threads waiting to be able to set the bit.
 * One uses wait_on_bit_lock() where one is waiting for the bit to
 * clear with the intention of setting it, and when done, clearing it.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
wait_on_bit_lock(unsigned long *word, int bit, unsigned mode)
{
 do { do { } while (0); } while (0);
 if (!test_and_set_bit(bit, word))
  return 0;
 return out_of_line_wait_on_bit_lock(word, bit, bit_wait, mode);
}

/**
 * wait_on_bit_lock_io - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared and then to atomically set it.  This is similar
 * to wait_on_bit(), but calls io_schedule() instead of schedule()
 * for the actual waiting.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
wait_on_bit_lock_io(unsigned long *word, int bit, unsigned mode)
{
 do { do { } while (0); } while (0);
 if (!test_and_set_bit(bit, word))
  return 0;
 return out_of_line_wait_on_bit_lock(word, bit, bit_wait_io, mode);
}

/**
 * wait_on_bit_lock_action - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared and then to set it, and allow the waiting action
 * to be specified.
 * This is like wait_on_bit() but allows fine control of how the waiting
 * is done.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
wait_on_bit_lock_action(unsigned long *word, int bit, wait_bit_action_f *action,
   unsigned mode)
{
 do { do { } while (0); } while (0);
 if (!test_and_set_bit(bit, word))
  return 0;
 return out_of_line_wait_on_bit_lock(word, bit, action, mode);
}

extern void init_wait_var_entry(struct wait_bit_queue_entry *wbq_entry, void *var, int flags);
extern void wake_up_var(void *var);
extern wait_queue_head_t *__var_waitqueue(void *p);
# 321 "./include/linux/wait_bit.h"
/**
 * clear_and_wake_up_bit - clear a bit and wake up anyone waiting on that bit
 *
 * @bit: the bit of the word being waited on
 * @word: the word being waited on, a kernel virtual address
 *
 * You can use this helper if bitflags are manipulated atomically rather than
 * non-atomically under a lock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_and_wake_up_bit(int bit, void *word)
{
 clear_bit_unlock(bit, word);
 /* See wake_up_bit() for which memory barrier you need to use. */
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 wake_up_bit(word, bit);
}
# 7 "./include/linux/fs.h" 2
# 1 "./include/linux/kdev_t.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/kdev_t.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 6 "./include/linux/kdev_t.h" 2
# 23 "./include/linux/kdev_t.h"
/* acceptable for old filesystems */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool old_valid_dev(dev_t dev)
{
 return ((unsigned int) ((dev) >> 20)) < 256 && ((unsigned int) ((dev) & ((1U << 20) - 1))) < 256;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u16 old_encode_dev(dev_t dev)
{
 return (((unsigned int) ((dev) >> 20)) << 8) | ((unsigned int) ((dev) & ((1U << 20) - 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) dev_t old_decode_dev(u16 val)
{
 return ((((val >> 8) & 255) << 20) | (val & 255));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 new_encode_dev(dev_t dev)
{
 unsigned major = ((unsigned int) ((dev) >> 20));
 unsigned minor = ((unsigned int) ((dev) & ((1U << 20) - 1)));
 return (minor & 0xff) | (major << 8) | ((minor & ~0xff) << 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) dev_t new_decode_dev(u32 dev)
{
 unsigned major = (dev & 0xfff00) >> 8;
 unsigned minor = (dev & 0xff) | ((dev >> 12) & 0xfff00);
 return (((major) << 20) | (minor));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 huge_encode_dev(dev_t dev)
{
 return new_encode_dev(dev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) dev_t huge_decode_dev(u64 dev)
{
 return new_decode_dev(dev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int sysv_valid_dev(dev_t dev)
{
 return ((unsigned int) ((dev) >> 20)) < (1<<14) && ((unsigned int) ((dev) & ((1U << 20) - 1))) < (1<<18);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 sysv_encode_dev(dev_t dev)
{
 return ((unsigned int) ((dev) & ((1U << 20) - 1))) | (((unsigned int) ((dev) >> 20)) << 18);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned sysv_major(u32 dev)
{
 return (dev >> 18) & 0x3fff;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned sysv_minor(u32 dev)
{
 return dev & 0x3ffff;
}
# 8 "./include/linux/fs.h" 2
# 1 "./include/linux/dcache.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






# 1 "./include/linux/rculist.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * RCU-protected list version
 */

# 1 "./include/linux/rcupdate.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/*
 * Read-Copy Update mechanism for mutual exclusion
 *
 * Copyright IBM Corporation, 2001
 *
 * Author: Dipankar Sarma <dipankar@in.ibm.com>
 *
 * Based on the original work by Paul McKenney <paulmck@vnet.ibm.com>
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
 * Papers:
 * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf
 * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 *		http://lse.sourceforge.net/locking/rcupdate.html
 *
 */
# 32 "./include/linux/rcupdate.h"
# 1 "./include/linux/context_tracking_irq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




void ct_irq_enter(void);
void ct_irq_exit(void);
void ct_irq_enter_irqson(void);
void ct_irq_exit_irqson(void);
void ct_nmi_enter(void);
void ct_nmi_exit(void);
# 33 "./include/linux/rcupdate.h" 2







/* Exported common interfaces */
void call_rcu(struct callback_head *head, rcu_callback_t func);
void rcu_barrier_tasks(void);
void rcu_barrier_tasks_rude(void);
void synchronize_rcu(void);

struct rcu_gp_oldstate;
unsigned long get_completed_synchronize_rcu(void);
void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);

// Maximum number of unsigned long values corresponding to
// not-yet-completed RCU grace periods.


/**
 * same_state_synchronize_rcu - Are two old-state values identical?
 * @oldstate1: First old-state value.
 * @oldstate2: Second old-state value.
 *
 * The two old-state values must have been obtained from either
 * get_state_synchronize_rcu(), start_poll_synchronize_rcu(), or
 * get_completed_synchronize_rcu().  Returns @true if the two values are
 * identical and @false otherwise.  This allows structures whose lifetimes
 * are tracked by old-state values to push these values to a list header,
 * allowing those structures to be slightly smaller.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool same_state_synchronize_rcu(unsigned long oldstate1, unsigned long oldstate2)
{
 return oldstate1 == oldstate2;
}



void __rcu_read_lock(void);
void __rcu_read_unlock(void);

/*
 * Defined as a macro as it is a very low level header included from
 * areas that don't even know about current.  This gives the rcu_read_lock()
 * nesting depth, but makes sense only if CONFIG_PREEMPT_RCU -- in other
 * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.
 */
# 114 "./include/linux/rcupdate.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void call_rcu_hurry(struct callback_head *head, rcu_callback_t func)
{
 call_rcu(head, func);
}


/* Internal to kernel */
void rcu_init(void);
extern int rcu_scheduler_active;
void rcu_sched_clock_irq(int user);
void rcu_report_dead(unsigned int cpu);
void rcutree_migrate_callbacks(int cpu);


void rcu_init_tasks_generic(void);





void rcu_sysrq_start(void);
void rcu_sysrq_end(void);
# 144 "./include/linux/rcupdate.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_irq_work_resched(void) { }
# 153 "./include/linux/rcupdate.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_init_nohz(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rcu_nocb_cpu_offload(int cpu) { return -22 /* Invalid argument */; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rcu_nocb_cpu_deoffload(int cpu) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_nocb_flush_deferred_wakeup(void) { }


/**
 * RCU_NONIDLE - Indicate idle-loop code that needs RCU readers
 * @a: Code that RCU needs to pay attention to.
 *
 * RCU read-side critical sections are forbidden in the inner idle loop,
 * that is, between the ct_idle_enter() and the ct_idle_exit() -- RCU
 * will happily ignore any such read-side critical sections.  However,
 * things like powertop need tracepoints in the inner idle loop.
 *
 * This macro provides the way out:  RCU_NONIDLE(do_something_with_RCU())
 * will tell RCU that it needs to pay attention, invoke its argument
 * (in this example, calling the do_something_with_RCU() function),
 * and then tell RCU to go back to ignoring this CPU.  It is permissible
 * to nest RCU_NONIDLE() wrappers, but not indefinitely (but the limit is
 * on the order of a million or so, even on 32-bit systems).  It is
 * not legal to block within RCU_NONIDLE(), nor is it permissible to
 * transfer control either into or out of RCU_NONIDLE()'s statement.
 */







/*
 * Note a quasi-voluntary context switch for RCU-tasks's benefit.
 * This is a macro rather than an inline function to avoid #include hell.
 */
# 196 "./include/linux/rcupdate.h"
void call_rcu_tasks(struct callback_head *head, rcu_callback_t func);
void synchronize_rcu_tasks(void);







// Bits for ->trc_reader_special.b.need_qs field.



u8 rcu_trc_cmpxchg_need_qs(struct task_struct *t, u8 old, u8 new);
void rcu_tasks_trace_qs_blkd(struct task_struct *t);
# 240 "./include/linux/rcupdate.h"
void exit_tasks_rcu_start(void);
void exit_tasks_rcu_finish(void);
# 252 "./include/linux/rcupdate.h"
/**
 * rcu_trace_implies_rcu_gp - does an RCU Tasks Trace grace period imply an RCU grace period?
 *
 * As an accident of implementation, an RCU Tasks Trace grace period also
 * acts as an RCU grace period.  However, this could change at any time.
 * Code relying on this accident must call this function to verify that
 * this accident is still happening.
 *
 * You have been warned!
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool rcu_trace_implies_rcu_gp(void) { return true; }

/**
 * cond_resched_tasks_rcu_qs - Report potential quiescent states to RCU
 *
 * This macro resembles cond_resched(), except that it is defined to
 * report potential quiescent states to RCU-tasks even if the cond_resched()
 * machinery were to be shut off, as some advocate for PREEMPTION kernels.
 */






/*
 * Infrastructure to implement the synchronize_() primitives in
 * TREE_RCU and rcu_barrier_() primitives in TINY_RCU.
 */


# 1 "./include/linux/rcutree.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/*
 * Read-Copy Update mechanism for mutual exclusion (tree-based version)
 *
 * Copyright IBM Corporation, 2008
 *
 * Author: Dipankar Sarma <dipankar@in.ibm.com>
 *	   Paul E. McKenney <paulmck@linux.ibm.com> Hierarchical algorithm
 *
 * Based on the original work by Paul McKenney <paulmck@linux.ibm.com>
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 *	Documentation/RCU
 */




void rcu_softirq_qs(void);
void rcu_note_context_switch(bool preempt);
int rcu_needs_cpu(void);
void rcu_cpu_stall_reset(void);

/*
 * Note a virtualization-based context switch.  This is simply a
 * wrapper around rcu_note_context_switch(), which allows TINY_RCU
 * to save a few bytes. The caller must have disabled interrupts.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_virt_note_context_switch(void)
{
 rcu_note_context_switch(false);
}

void synchronize_rcu_expedited(void);
void kvfree_call_rcu(struct callback_head *head, rcu_callback_t func);

void rcu_barrier(void);
bool rcu_eqs_special_set(int cpu);
void rcu_momentary_dyntick_idle(void);
void kfree_rcu_scheduler_running(void);
bool rcu_gp_might_be_stalled(void);

struct rcu_gp_oldstate {
 unsigned long rgos_norm;
 unsigned long rgos_exp;
};

// Maximum number of rcu_gp_oldstate values corresponding to
// not-yet-completed RCU grace periods.


/**
 * same_state_synchronize_rcu_full - Are two old-state values identical?
 * @rgosp1: First old-state value.
 * @rgosp2: Second old-state value.
 *
 * The two old-state values must have been obtained from either
 * get_state_synchronize_rcu_full(), start_poll_synchronize_rcu_full(),
 * or get_completed_synchronize_rcu_full().  Returns @true if the two
 * values are identical and @false otherwise.  This allows structures
 * whose lifetimes are tracked by old-state values to push these values
 * to a list header, allowing those structures to be slightly smaller.
 *
 * Note that equality is judged on a bitwise basis, so that an
 * @rcu_gp_oldstate structure with an already-completed state in one field
 * will compare not-equal to a structure with an already-completed state
 * in the other field.  After all, the @rcu_gp_oldstate structure is opaque
 * so how did such a situation come to pass in the first place?
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool same_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp1,
         struct rcu_gp_oldstate *rgosp2)
{
 return rgosp1->rgos_norm == rgosp2->rgos_norm && rgosp1->rgos_exp == rgosp2->rgos_exp;
}

unsigned long start_poll_synchronize_rcu_expedited(void);
void start_poll_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp);
void cond_synchronize_rcu_expedited(unsigned long oldstate);
void cond_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp);
unsigned long get_state_synchronize_rcu(void);
void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
unsigned long start_poll_synchronize_rcu(void);
void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
bool poll_state_synchronize_rcu(unsigned long oldstate);
bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
void cond_synchronize_rcu(unsigned long oldstate);
void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_irq_exit_check_preempt(void) { }


struct task_struct;
void rcu_preempt_deferred_qs(struct task_struct *t);

void exit_rcu(void);

void rcu_scheduler_starting(void);
extern int rcu_scheduler_active;
void rcu_end_inkernel_boot(void);
bool rcu_inkernel_boot_has_ended(void);
bool rcu_is_watching(void);




/* RCUtree hotplug events */
int rcutree_prepare_cpu(unsigned int cpu);
int rcutree_online_cpu(unsigned int cpu);
int rcutree_offline_cpu(unsigned int cpu);
int rcutree_dead_cpu(unsigned int cpu);
int rcutree_dying_cpu(unsigned int cpu);
void rcu_cpu_starting(unsigned int cpu);
# 284 "./include/linux/rcupdate.h" 2






/*
 * The init_rcu_head_on_stack() and destroy_rcu_head_on_stack() calls
 * are needed for dynamic initialization and destruction of rcu_head
 * on the stack, and init_rcu_head()/destroy_rcu_head() are needed for
 * dynamic initialization and destruction of statically allocated rcu_head
 * structures.  However, rcu_head structures allocated dynamically in the
 * heap don't need any initialization.
 */






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_rcu_head(struct callback_head *head) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void destroy_rcu_head(struct callback_head *head) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_rcu_head_on_stack(struct callback_head *head) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void destroy_rcu_head_on_stack(struct callback_head *head) { }





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool rcu_lockdep_current_cpu_online(void) { return true; }


extern struct lockdep_map rcu_lock_map;
extern struct lockdep_map rcu_bh_lock_map;
extern struct lockdep_map rcu_sched_lock_map;
extern struct lockdep_map rcu_callback_map;
# 344 "./include/linux/rcupdate.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rcu_read_lock_held(void)
{
 return 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rcu_read_lock_bh_held(void)
{
 return 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rcu_read_lock_sched_held(void)
{
 return !(preempt_count() == 0 && !({ unsigned long _flags; do { ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _flags = arch_local_save_flags(); } while (0); ({ ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(_flags); }); }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rcu_read_lock_any_held(void)
{
 return !(preempt_count() == 0 && !({ unsigned long _flags; do { ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _flags = arch_local_save_flags(); } while (0); ({ ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(_flags); }); }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int debug_lockdep_rcu_enabled(void)
{
 return 0;
}
# 414 "./include/linux/rcupdate.h"
/*
 * Helper functions for rcu_dereference_check(), rcu_dereference_protected()
 * and rcu_assign_pointer().  Some of these could be folded into their
 * callers, but they are left separate in order to ease introduction of
 * multiple pointers markings to match different RCU implementations
 * (e.g., __srcu), should this make sense in the future.
 */
# 435 "./include/linux/rcupdate.h"
/**
 * unrcu_pointer - mark a pointer as not being RCU protected
 * @p: pointer needing to lose its __rcu property
 *
 * Converts @p from an __rcu pointer to a __kernel pointer.
 * This allows an __rcu pointer to be used with xchg() and friends.
 */
# 472 "./include/linux/rcupdate.h"
/**
 * RCU_INITIALIZER() - statically initialize an RCU-protected global variable
 * @v: The value to statically initialize with.
 */


/**
 * rcu_assign_pointer() - assign to RCU-protected pointer
 * @p: pointer to assign to
 * @v: value to assign (publish)
 *
 * Assigns the specified value to the specified RCU-protected
 * pointer, ensuring that any concurrent RCU readers will see
 * any prior initialization.
 *
 * Inserts memory barriers on architectures that require them
 * (which is most of them), and also prevents the compiler from
 * reordering the code that initializes the structure after the pointer
 * assignment.  More importantly, this call documents which pointers
 * will be dereferenced by RCU read-side code.
 *
 * In some special cases, you may use RCU_INIT_POINTER() instead
 * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due
 * to the fact that it does not constrain either the CPU or the compiler.
 * That said, using RCU_INIT_POINTER() when you should have used
 * rcu_assign_pointer() is a very bad thing that results in
 * impossible-to-diagnose memory corruption.  So please be careful.
 * See the RCU_INIT_POINTER() comment header for details.
 *
 * Note that rcu_assign_pointer() evaluates each of its arguments only
 * once, appearances notwithstanding.  One of the "extra" evaluations
 * is in typeof() and the other visible only to sparse (__CHECKER__),
 * neither of which actually execute the argument.  As with most cpp
 * macros, this execute-arguments-only-once property is important, so
 * please be careful when making changes to rcu_assign_pointer() and the
 * other macros that it invokes.
 */
# 520 "./include/linux/rcupdate.h"
/**
 * rcu_replace_pointer() - replace an RCU pointer, returning its old value
 * @rcu_ptr: RCU pointer, whose old value is returned
 * @ptr: regular pointer
 * @c: the lockdep conditions under which the dereference will take place
 *
 * Perform a replacement, where @rcu_ptr is an RCU-annotated
 * pointer and @c is the lockdep argument that is passed to the
 * rcu_dereference_protected() call used to read that pointer.  The old
 * value of @rcu_ptr is returned, and @rcu_ptr is set to @ptr.
 */







/**
 * rcu_access_pointer() - fetch RCU pointer with no dereferencing
 * @p: The pointer to read
 *
 * Return the value of the specified RCU-protected pointer, but omit the
 * lockdep checks for being in an RCU read-side critical section.  This is
 * useful when the value of this pointer is accessed, but the pointer is
 * not dereferenced, for example, when testing an RCU-protected pointer
 * against NULL.  Although rcu_access_pointer() may also be used in cases
 * where update-side locks prevent the value of the pointer from changing,
 * you should instead use rcu_dereference_protected() for this use case.
 * Within an RCU read-side critical section, there is little reason to
 * use rcu_access_pointer().
 *
 * It is usually best to test the rcu_access_pointer() return value
 * directly in order to avoid accidental dereferences being introduced
 * by later inattentive changes.  In other words, assigning the
 * rcu_access_pointer() return value to a local variable results in an
 * accident waiting to happen.
 *
 * It is also permissible to use rcu_access_pointer() when read-side
 * access to the pointer was removed at least one grace period ago, as is
 * the case in the context of the RCU callback that is freeing up the data,
 * or after a synchronize_rcu() returns.  This can be useful when tearing
 * down multi-linked structures after a grace period has elapsed.  However,
 * rcu_dereference_protected() is normally preferred for this use case.
 */


/**
 * rcu_dereference_check() - rcu_dereference with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * Do an rcu_dereference(), but check that the conditions under which the
 * dereference will take place are correct.  Typically the conditions
 * indicate the various locking conditions that should be held at that
 * point.  The check should return true if the conditions are satisfied.
 * An implicit check for being in an RCU read-side critical section
 * (rcu_read_lock()) is included.
 *
 * For example:
 *
 *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock));
 *
 * could be used to indicate to lockdep that foo->bar may only be dereferenced
 * if either rcu_read_lock() is held, or that the lock required to replace
 * the bar struct at foo->bar is held.
 *
 * Note that the list of conditions may also include indications of when a lock
 * need not be held, for example during initialisation or destruction of the
 * target struct:
 *
 *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock) ||
 *					      atomic_read(&foo->usage) == 0);
 *
 * Inserts memory barriers on architectures that require them
 * (currently only the Alpha), prevents the compiler from refetching
 * (and from merging fetches), and, more importantly, documents exactly
 * which pointers are protected by RCU and checks that the pointer is
 * annotated as __rcu.
 */




/**
 * rcu_dereference_bh_check() - rcu_dereference_bh with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * This is the RCU-bh counterpart to rcu_dereference_check().  However,
 * please note that starting in v5.0 kernels, vanilla RCU grace periods
 * wait for local_bh_disable() regions of code in addition to regions of
 * code demarked by rcu_read_lock() and rcu_read_unlock().  This means
 * that synchronize_rcu(), call_rcu, and friends all take not only
 * rcu_read_lock() but also rcu_read_lock_bh() into account.
 */




/**
 * rcu_dereference_sched_check() - rcu_dereference_sched with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * This is the RCU-sched counterpart to rcu_dereference_check().
 * However, please note that starting in v5.0 kernels, vanilla RCU grace
 * periods wait for preempt_disable() regions of code in addition to
 * regions of code demarked by rcu_read_lock() and rcu_read_unlock().
 * This means that synchronize_rcu(), call_rcu, and friends all take not
 * only rcu_read_lock() but also rcu_read_lock_sched() into account.
 */





/*
 * The tracing infrastructure traces RCU (we want that), but unfortunately
 * some of the RCU checks causes tracing to lock up the system.
 *
 * The no-tracing version of rcu_dereference_raw() must not call
 * rcu_read_lock_held().
 */



/**
 * rcu_dereference_protected() - fetch RCU pointer when updates prevented
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * Return the value of the specified RCU-protected pointer, but omit
 * the READ_ONCE().  This is useful in cases where update-side locks
 * prevent the value of the pointer from changing.  Please note that this
 * primitive does *not* prevent the compiler from repeating this reference
 * or combining it with other references, so it should not be used without
 * protection of appropriate locks.
 *
 * This function is only for update-side use.  Using this function
 * when protected only by rcu_read_lock() will result in infrequent
 * but very ugly failures.
 */




/**
 * rcu_dereference() - fetch RCU-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * This is a simple wrapper around rcu_dereference_check().
 */


/**
 * rcu_dereference_bh() - fetch an RCU-bh-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * Makes rcu_dereference_check() do the dirty work.
 */


/**
 * rcu_dereference_sched() - fetch RCU-sched-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * Makes rcu_dereference_check() do the dirty work.
 */


/**
 * rcu_pointer_handoff() - Hand off a pointer from RCU to other mechanism
 * @p: The pointer to hand off
 *
 * This is simply an identity function, but it documents where a pointer
 * is handed off from RCU to some other synchronization mechanism, for
 * example, reference counting or locking.  In C11, it would map to
 * kill_dependency().  It could be used as follows::
 *
 *	rcu_read_lock();
 *	p = rcu_dereference(gp);
 *	long_lived = is_long_lived(p);
 *	if (long_lived) {
 *		if (!atomic_inc_not_zero(p->refcnt))
 *			long_lived = false;
 *		else
 *			p = rcu_pointer_handoff(p);
 *	}
 *	rcu_read_unlock();
 */


/**
 * rcu_read_lock() - mark the beginning of an RCU read-side critical section
 *
 * When synchronize_rcu() is invoked on one CPU while other CPUs
 * are within RCU read-side critical sections, then the
 * synchronize_rcu() is guaranteed to block until after all the other
 * CPUs exit their critical sections.  Similarly, if call_rcu() is invoked
 * on one CPU while other CPUs are within RCU read-side critical
 * sections, invocation of the corresponding RCU callback is deferred
 * until after the all the other CPUs exit their critical sections.
 *
 * In v5.0 and later kernels, synchronize_rcu() and call_rcu() also
 * wait for regions of code with preemption disabled, including regions of
 * code with interrupts or softirqs disabled.  In pre-v5.0 kernels, which
 * define synchronize_sched(), only code enclosed within rcu_read_lock()
 * and rcu_read_unlock() are guaranteed to be waited for.
 *
 * Note, however, that RCU callbacks are permitted to run concurrently
 * with new RCU read-side critical sections.  One way that this can happen
 * is via the following sequence of events: (1) CPU 0 enters an RCU
 * read-side critical section, (2) CPU 1 invokes call_rcu() to register
 * an RCU callback, (3) CPU 0 exits the RCU read-side critical section,
 * (4) CPU 2 enters a RCU read-side critical section, (5) the RCU
 * callback is invoked.  This is legal, because the RCU read-side critical
 * section that was running concurrently with the call_rcu() (and which
 * therefore might be referencing something that the corresponding RCU
 * callback would free up) has completed before the corresponding
 * RCU callback is invoked.
 *
 * RCU read-side critical sections may be nested.  Any deferred actions
 * will be deferred until the outermost RCU read-side critical section
 * completes.
 *
 * You can avoid reading and understanding the next paragraph by
 * following this rule: don't put anything in an rcu_read_lock() RCU
 * read-side critical section that would block in a !PREEMPTION kernel.
 * But if you want the full story, read on!
 *
 * In non-preemptible RCU implementations (pure TREE_RCU and TINY_RCU),
 * it is illegal to block while in an RCU read-side critical section.
 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPTION
 * kernel builds, RCU read-side critical sections may be preempted,
 * but explicit blocking is illegal.  Finally, in preemptible RCU
 * implementations in real-time (with -rt patchset) kernel builds, RCU
 * read-side critical sections may be preempted and they may also block, but
 * only when acquiring spinlocks that are subject to priority inheritance.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void rcu_read_lock(void)
{
 __rcu_read_lock();
 (void)0;
 do { } while (0);
 do { } while (0 && (!rcu_is_watching()));

}

/*
 * So where is rcu_write_lock()?  It does not exist, as there is no
 * way for writers to lock out RCU readers.  This is a feature, not
 * a bug -- this property is what provides RCU's performance benefits.
 * Of course, writers must coordinate with each other.  The normal
 * spinlock primitives work well for this, but any other technique may be
 * used as well.  RCU does not care how the writers keep out of each
 * others' way, as long as they do so.
 */

/**
 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
 *
 * In almost all situations, rcu_read_unlock() is immune from deadlock.
 * In recent kernels that have consolidated synchronize_sched() and
 * synchronize_rcu_bh() into synchronize_rcu(), this deadlock immunity
 * also extends to the scheduler's runqueue and priority-inheritance
 * spinlocks, courtesy of the quiescent-state deferral that is carried
 * out when rcu_read_unlock() is invoked with interrupts disabled.
 *
 * See rcu_read_lock() for more information.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_read_unlock(void)
{
 do { } while (0 && (!rcu_is_watching()));

 (void)0;
 __rcu_read_unlock();
 do { } while (0); /* Keep acq info for rls diags. */
}

/**
 * rcu_read_lock_bh() - mark the beginning of an RCU-bh critical section
 *
 * This is equivalent to rcu_read_lock(), but also disables softirqs.
 * Note that anything else that disables softirqs can also serve as an RCU
 * read-side critical section.  However, please note that this equivalence
 * applies only to v5.0 and later.  Before v5.0, rcu_read_lock() and
 * rcu_read_lock_bh() were unrelated.
 *
 * Note that rcu_read_lock_bh() and the matching rcu_read_unlock_bh()
 * must occur in the same context, for example, it is illegal to invoke
 * rcu_read_unlock_bh() from one task if the matching rcu_read_lock_bh()
 * was invoked from some other task.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_read_lock_bh(void)
{
 local_bh_disable();
 (void)0;
 do { } while (0);
 do { } while (0 && (!rcu_is_watching()));

}

/**
 * rcu_read_unlock_bh() - marks the end of a softirq-only RCU critical section
 *
 * See rcu_read_lock_bh() for more information.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_read_unlock_bh(void)
{
 do { } while (0 && (!rcu_is_watching()));

 do { } while (0);
 (void)0;
 local_bh_enable();
}

/**
 * rcu_read_lock_sched() - mark the beginning of a RCU-sched critical section
 *
 * This is equivalent to rcu_read_lock(), but also disables preemption.
 * Read-side critical sections can also be introduced by anything else that
 * disables preemption, including local_irq_disable() and friends.  However,
 * please note that the equivalence to rcu_read_lock() applies only to
 * v5.0 and later.  Before v5.0, rcu_read_lock() and rcu_read_lock_sched()
 * were unrelated.
 *
 * Note that rcu_read_lock_sched() and the matching rcu_read_unlock_sched()
 * must occur in the same context, for example, it is illegal to invoke
 * rcu_read_unlock_sched() from process context if the matching
 * rcu_read_lock_sched() was invoked from an NMI handler.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_read_lock_sched(void)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 (void)0;
 do { } while (0);
 do { } while (0 && (!rcu_is_watching()));

}

/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__no_instrument_function__)) void rcu_read_lock_sched_notrace(void)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 (void)0;
}

/**
 * rcu_read_unlock_sched() - marks the end of a RCU-classic critical section
 *
 * See rcu_read_lock_sched() for more information.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_read_unlock_sched(void)
{
 do { } while (0 && (!rcu_is_watching()));

 do { } while (0);
 (void)0;
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__no_instrument_function__)) void rcu_read_unlock_sched_notrace(void)
{
 (void)0;
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0);
}

/**
 * RCU_INIT_POINTER() - initialize an RCU protected pointer
 * @p: The pointer to be initialized.
 * @v: The value to initialized the pointer to.
 *
 * Initialize an RCU-protected pointer in special cases where readers
 * do not need ordering constraints on the CPU or the compiler.  These
 * special cases are:
 *
 * 1.	This use of RCU_INIT_POINTER() is NULLing out the pointer *or*
 * 2.	The caller has taken whatever steps are required to prevent
 *	RCU readers from concurrently accessing this pointer *or*
 * 3.	The referenced data structure has already been exposed to
 *	readers either at compile time or via rcu_assign_pointer() *and*
 *
 *	a.	You have not made *any* reader-visible changes to
 *		this structure since then *or*
 *	b.	It is OK for readers accessing this structure from its
 *		new location to see the old state of the structure.  (For
 *		example, the changes were to statistical counters or to
 *		other state where exact synchronization is not required.)
 *
 * Failure to follow these rules governing use of RCU_INIT_POINTER() will
 * result in impossible-to-diagnose memory corruption.  As in the structures
 * will look OK in crash dumps, but any concurrent RCU readers might
 * see pre-initialized values of the referenced data structure.  So
 * please be very careful how you use RCU_INIT_POINTER()!!!
 *
 * If you are creating an RCU-protected linked structure that is accessed
 * by a single external-to-structure RCU-protected pointer, then you may
 * use RCU_INIT_POINTER() to initialize the internal RCU-protected
 * pointers, but you must use rcu_assign_pointer() to initialize the
 * external-to-structure pointer *after* you have completely initialized
 * the reader-accessible portions of the linked structure.
 *
 * Note that unlike rcu_assign_pointer(), RCU_INIT_POINTER() provides no
 * ordering guarantees for either the CPU or the compiler.
 */






/**
 * RCU_POINTER_INITIALIZER() - statically initialize an RCU protected pointer
 * @p: The pointer to be initialized.
 * @v: The value to initialized the pointer to.
 *
 * GCC-style initialization for an RCU-protected pointer in a structure field.
 */



/*
 * Does the specified offset indicate that the corresponding rcu_head
 * structure can be handled by kvfree_rcu()?
 */


/**
 * kfree_rcu() - kfree an object after a grace period.
 * @ptr: pointer to kfree for both single- and double-argument invocations.
 * @rhf: the name of the struct rcu_head within the type of @ptr,
 *       but only for double-argument invocations.
 *
 * Many rcu callbacks functions just call kfree() on the base structure.
 * These functions are trivial, but their size adds up, and furthermore
 * when they are used in a kernel module, that module must invoke the
 * high-latency rcu_barrier() function at module-unload time.
 *
 * The kfree_rcu() function handles this issue.  Rather than encoding a
 * function address in the embedded rcu_head structure, kfree_rcu() instead
 * encodes the offset of the rcu_head structure within the base structure.
 * Because the functions are not allowed in the low-order 4096 bytes of
 * kernel virtual memory, offsets up to 4095 bytes can be accommodated.
 * If the offset is larger than 4095 bytes, a compile-time error will
 * be generated in kvfree_rcu_arg_2(). If this error is triggered, you can
 * either fall back to use of call_rcu() or rearrange the structure to
 * position the rcu_head structure into the first 4096 bytes.
 *
 * Note that the allowable offset might decrease in the future, for example,
 * to allow something like kmem_cache_free_rcu().
 *
 * The BUILD_BUG_ON check must not involve any function calls, hence the
 * checks are done in macros here.
 */


/**
 * kvfree_rcu() - kvfree an object after a grace period.
 *
 * This macro consists of one or two arguments and it is
 * based on whether an object is head-less or not. If it
 * has a head then a semantic stays the same as it used
 * to be before:
 *
 *     kvfree_rcu(ptr, rhf);
 *
 * where @ptr is a pointer to kvfree(), @rhf is the name
 * of the rcu_head structure within the type of @ptr.
 *
 * When it comes to head-less variant, only one argument
 * is passed and that is just a pointer which has to be
 * freed after a grace period. Therefore the semantic is
 *
 *     kvfree_rcu(ptr);
 *
 * where @ptr is the pointer to be freed by kvfree().
 *
 * Please note, head-less way of freeing is permitted to
 * use from a context that has to follow might_sleep()
 * annotation. Otherwise, please switch and embed the
 * rcu_head structure within the type of @ptr.
 */
# 1027 "./include/linux/rcupdate.h"
/*
 * Place this after a lock-acquisition primitive to guarantee that
 * an UNLOCK+LOCK pair acts as a full barrier.  This guarantee applies
 * if the UNLOCK and LOCK are executed by the same CPU or if the
 * UNLOCK and LOCK operate on the same lock variable.
 */







/* Has the specified rcu_head structure been handed to call_rcu()? */

/**
 * rcu_head_init - Initialize rcu_head for rcu_head_after_call_rcu()
 * @rhp: The rcu_head structure to initialize.
 *
 * If you intend to invoke rcu_head_after_call_rcu() to test whether a
 * given rcu_head structure has already been passed to call_rcu(), then
 * you must also invoke this rcu_head_init() function on it just after
 * allocating that structure.  Calls to this function must not race with
 * calls to call_rcu(), rcu_head_after_call_rcu(), or callback invocation.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcu_head_init(struct callback_head *rhp)
{
 rhp->func = (rcu_callback_t)~0L;
}

/**
 * rcu_head_after_call_rcu() - Has this rcu_head been passed to call_rcu()?
 * @rhp: The rcu_head structure to test.
 * @f: The function passed to call_rcu() along with @rhp.
 *
 * Returns @true if the @rhp has been passed to call_rcu() with @func,
 * and @false otherwise.  Emits a warning in any other case, including
 * the case where @rhp has already been invoked after a grace period.
 * Calls to this function must not race with callback invocation.  One way
 * to avoid such races is to enclose the call to rcu_head_after_call_rcu()
 * in an RCU read-side critical section that includes a read-side fetch
 * of the pointer to the structure containing @rhp.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
rcu_head_after_call_rcu(struct callback_head *rhp, rcu_callback_t f)
{
 rcu_callback_t func = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_181(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(rhp->func) == sizeof(char) || sizeof(rhp->func) == sizeof(short) || sizeof(rhp->func) == sizeof(int) || sizeof(rhp->func) == sizeof(long)) || sizeof(rhp->func) == sizeof(long long))) __compiletime_assert_181(); } while (0); (*(const volatile typeof( _Generic((rhp->func), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (rhp->func))) *)&(rhp->func)); });
# 1075 "./include/linux/rcupdate.h"
 if (func == f)
  return true;
 ({ int __ret_warn_on = !!(func != (rcu_callback_t)~0L); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/rcupdate.h\"; .popsection; .long 14472b - .; .short 1077; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 return false;
}

/* kernel/ksysfs.c definitions */
extern int rcu_expedited;
extern int rcu_normal;
# 12 "./include/linux/rculist.h" 2

/*
 * INIT_LIST_HEAD_RCU - Initialize a list_head visible to RCU readers
 * @list: list to be initialized
 *
 * You should instead use INIT_LIST_HEAD() for normal initialization and
 * cleanup tasks, when readers have no access to the list being initialized.
 * However, if the list being initialized is visible to readers, you
 * need to keep the compiler from being too mischievous.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void INIT_LIST_HEAD_RCU(struct list_head *list)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_182(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(list->next) == sizeof(char) || sizeof(list->next) == sizeof(short) || sizeof(list->next) == sizeof(int) || sizeof(list->next) == sizeof(long)) || sizeof(list->next) == sizeof(long long))) __compiletime_assert_182(); } while (0); do { *(volatile typeof(list->next) *)&(list->next) = (list); } while (0); } while (0);
# 25 "./include/linux/rculist.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_183(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(list->prev) == sizeof(char) || sizeof(list->prev) == sizeof(short) || sizeof(list->prev) == sizeof(int) || sizeof(list->prev) == sizeof(long)) || sizeof(list->prev) == sizeof(long long))) __compiletime_assert_183(); } while (0); do { *(volatile typeof(list->prev) *)&(list->prev) = (list); } while (0); } while (0);
# 26 "./include/linux/rculist.h"
}

/*
 * return the ->next pointer of a list_head in an rcu safe
 * way, we must not access it directly
 */


/**
 * list_tail_rcu - returns the prev pointer of the head of the list
 * @head: the head of the list
 *
 * Note: This should only be used with the list header, and even then
 * only if list_del() and similar primitives are not also used on the
 * list header.
 */


/*
 * Check during list traversal that we are within an RCU reader
 */
# 70 "./include/linux/rculist.h"
/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_add_rcu(struct list_head *new,
  struct list_head *prev, struct list_head *next)
{
 if (!__list_add_valid(new, prev, next))
  return;

 new->next = next;
 new->prev = prev;
 do { uintptr_t _r_a_p__v = (uintptr_t)(new); ; if (__builtin_constant_p(new) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_184(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(char) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(short) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(int) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(long)) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(long long))) __compiletime_assert_184(); } while (0); do { *(volatile typeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) *)&(((*((struct list_head /* nothing */ **)(&(prev)->next))))) = ((typeof((*((struct list_head /* nothing */ **)(&(prev)->next)))))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&(*((struct list_head /* nothing */ **)(&(prev)->next)))) __p = (&(*((struct list_head /* nothing */ **)(&(prev)->next)))); union { typeof( _Generic((*&(*((struct list_head /* nothing */ **)(&(prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct list_head /* nothing */ **)(&(prev)->next)))))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(*((struct list_head /* nothing */ **)(&(prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct list_head /* nothing */ **)(&(prev)->next))))))) ((typeof(*((typeof((*((struct list_head /* nothing */ **)(&(prev)->next)))))_r_a_p__v)) /* nothing */ *)((typeof((*((struct list_head /* nothing */ **)(&(prev)->next)))))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_185(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(char) || sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(short) || sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(int) || sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(long)))) __compiletime_assert_185(); } while (0); kasan_check_write(__p, sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next))))); switch (sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next))))) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 85 "./include/linux/rculist.h"
 next->prev = new;
}

/**
 * list_add_rcu - add a new entry to rcu-protected list
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_add_rcu()
 * or list_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_add_rcu(struct list_head *new, struct list_head *head)
{
 __list_add_rcu(new, head, head->next);
}

/**
 * list_add_tail_rcu - add a new entry to rcu-protected list
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_add_tail_rcu()
 * or list_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_add_tail_rcu(struct list_head *new,
     struct list_head *head)
{
 __list_add_rcu(new, head->prev, head);
}

/**
 * list_del_rcu - deletes entry from list without re-initialization
 * @entry: the element to delete from the list.
 *
 * Note: list_empty() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_del_rcu()
 * or list_add_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 *
 * Note that the caller is not permitted to immediately free
 * the newly deleted entry.  Instead, either synchronize_rcu()
 * or call_rcu() must be used to defer freeing until an RCU
 * grace period has elapsed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_del_rcu(struct list_head *entry)
{
 __list_del_entry(entry);
 entry->prev = ((void *) 0x122 + (0xdead000000000000UL));
}

/**
 * hlist_del_init_rcu - deletes entry from hash list with re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: list_unhashed() on the node return true after this. It is
 * useful for RCU based read lockfree traversal if the writer side
 * must know if the list entry is still hashed or already unhashed.
 *
 * In particular, it means that we can not poison the forward pointers
 * that may still be used for walking the hash list and we can only
 * zero the pprev pointer so list_unhashed() will return true after
 * this.
 *
 * The caller must take whatever precautions are necessary (such as
 * holding appropriate locks) to avoid racing with another
 * list-mutation primitive, such as hlist_add_head_rcu() or
 * hlist_del_rcu(), running on this same list.  However, it is
 * perfectly legal to run concurrently with the _rcu list-traversal
 * primitives, such as hlist_for_each_entry_rcu().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_del_init_rcu(struct hlist_node *n)
{
 if (!hlist_unhashed(n)) {
  __hlist_del(n);
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_186(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_186(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (((void *)0)); } while (0); } while (0);
# 186 "./include/linux/rculist.h"
 }
}

/**
 * list_replace_rcu - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * The @old entry will be replaced with the @new entry atomically.
 * Note: @old should not be empty.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_replace_rcu(struct list_head *old,
    struct list_head *new)
{
 new->next = old->next;
 new->prev = old->prev;
 do { uintptr_t _r_a_p__v = (uintptr_t)(new); ; if (__builtin_constant_p(new) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_187(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((*((struct list_head /* nothing */ **)(&(new->prev)->next))))) == sizeof(char) || sizeof(((*((struct list_head /* nothing */ **)(&(new->prev)->next))))) == sizeof(short) || sizeof(((*((struct list_head /* nothing */ **)(&(new->prev)->next))))) == sizeof(int) || sizeof(((*((struct list_head /* nothing */ **)(&(new->prev)->next))))) == sizeof(long)) || sizeof(((*((struct list_head /* nothing */ **)(&(new->prev)->next))))) == sizeof(long long))) __compiletime_assert_187(); } while (0); do { *(volatile typeof(((*((struct list_head /* nothing */ **)(&(new->prev)->next))))) *)&(((*((struct list_head /* nothing */ **)(&(new->prev)->next))))) = ((typeof((*((struct list_head /* nothing */ **)(&(new->prev)->next)))))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))) __p = (&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))); union { typeof( _Generic((*&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct list_head /* nothing */ **)(&(new->prev)->next))))))) ((typeof(*((typeof((*((struct list_head /* nothing */ **)(&(new->prev)->next)))))_r_a_p__v)) /* nothing */ *)((typeof((*((struct list_head /* nothing */ **)(&(new->prev)->next)))))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_188(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))) == sizeof(char) || sizeof(*&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))) == sizeof(short) || sizeof(*&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))) == sizeof(int) || sizeof(*&(*((struct list_head /* nothing */ **)(&(new->prev)->next)))) == sizeof(long)))) __compiletime_assert_188(); } while (0); kasan_check_write(__p, sizeof(*&(*((struct list_head /* nothing */ **)(&(new->prev)->next))))); switch (sizeof(*&(*((struct list_head /* nothing */ **)(&(new->prev)->next))))) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 203 "./include/linux/rculist.h"
 new->next->prev = new;
 old->prev = ((void *) 0x122 + (0xdead000000000000UL));
}

/**
 * __list_splice_init_rcu - join an RCU-protected list into an existing list.
 * @list:	the RCU-protected list to splice
 * @prev:	points to the last element of the existing list
 * @next:	points to the first element of the existing list
 * @sync:	synchronize_rcu, synchronize_rcu_expedited, ...
 *
 * The list pointed to by @prev and @next can be RCU-read traversed
 * concurrently with this function.
 *
 * Note that this function blocks.
 *
 * Important note: the caller must take whatever action is necessary to prevent
 * any other updates to the existing list.  In principle, it is possible to
 * modify the list as soon as sync() begins execution. If this sort of thing
 * becomes necessary, an alternative version based on call_rcu() could be
 * created.  But only if -really- needed -- there is no shortage of RCU API
 * members.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __list_splice_init_rcu(struct list_head *list,
       struct list_head *prev,
       struct list_head *next,
       void (*sync)(void))
{
 struct list_head *first = list->next;
 struct list_head *last = list->prev;

 /*
	 * "first" and "last" tracking list, so initialize it.  RCU readers
	 * have access to this list, so we must use INIT_LIST_HEAD_RCU()
	 * instead of INIT_LIST_HEAD().
	 */

 INIT_LIST_HEAD_RCU(list);

 /*
	 * At this point, the list body still points to the source list.
	 * Wait for any readers to finish using the list before splicing
	 * the list body into the new list.  Any new readers will see
	 * an empty list.
	 */

 sync();
 __kcsan_check_access(&(*first), sizeof(*first), (1 << 0) /* Access is a write. */ | (1 << 3) /* Access is an assertion. */);
 __kcsan_check_access(&(*last), sizeof(*last), (1 << 0) /* Access is a write. */ | (1 << 3) /* Access is an assertion. */);

 /*
	 * Readers are finished with the source list, so perform splice.
	 * The order is important if the new list is global and accessible
	 * to concurrent RCU readers.  Note that RCU readers are not
	 * permitted to traverse the prev pointers without excluding
	 * this function.
	 */

 last->next = next;
 do { uintptr_t _r_a_p__v = (uintptr_t)(first); ; if (__builtin_constant_p(first) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_189(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(char) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(short) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(int) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(long)) || sizeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) == sizeof(long long))) __compiletime_assert_189(); } while (0); do { *(volatile typeof(((*((struct list_head /* nothing */ **)(&(prev)->next))))) *)&(((*((struct list_head /* nothing */ **)(&(prev)->next))))) = ((typeof((*((struct list_head /* nothing */ **)(&(prev)->next)))))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&(*((struct list_head /* nothing */ **)(&(prev)->next)))) __p = (&(*((struct list_head /* nothing */ **)(&(prev)->next)))); union { typeof( _Generic((*&(*((struct list_head /* nothing */ **)(&(prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct list_head /* nothing */ **)(&(prev)->next)))))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(*((struct list_head /* nothing */ **)(&(prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct list_head /* nothing */ **)(&(prev)->next))))))) ((typeof(*((typeof((*((struct list_head /* nothing */ **)(&(prev)->next)))))_r_a_p__v)) /* nothing */ *)((typeof((*((struct list_head /* nothing */ **)(&(prev)->next)))))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_190(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(char) || sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(short) || sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(int) || sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next)))) == sizeof(long)))) __compiletime_assert_190(); } while (0); kasan_check_write(__p, sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next))))); switch (sizeof(*&(*((struct list_head /* nothing */ **)(&(prev)->next))))) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 263 "./include/linux/rculist.h"
 first->prev = prev;
 next->prev = last;
}

/**
 * list_splice_init_rcu - splice an RCU-protected list into an existing list,
 *                        designed for stacks.
 * @list:	the RCU-protected list to splice
 * @head:	the place in the existing list to splice the first list into
 * @sync:	synchronize_rcu, synchronize_rcu_expedited, ...
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_splice_init_rcu(struct list_head *list,
     struct list_head *head,
     void (*sync)(void))
{
 if (!list_empty(list))
  __list_splice_init_rcu(list, head, head->next, sync);
}

/**
 * list_splice_tail_init_rcu - splice an RCU-protected list into an existing
 *                             list, designed for queues.
 * @list:	the RCU-protected list to splice
 * @head:	the place in the existing list to splice the first list into
 * @sync:	synchronize_rcu, synchronize_rcu_expedited, ...
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void list_splice_tail_init_rcu(struct list_head *list,
          struct list_head *head,
          void (*sync)(void))
{
 if (!list_empty(list))
  __list_splice_init_rcu(list, head->prev, head, sync);
}

/**
 * list_entry_rcu - get the struct for this entry
 * @ptr:        the &struct list_head pointer.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */



/*
 * Where are list_empty_rcu() and list_first_entry_rcu()?
 *
 * They do not exist because they would lead to subtle race conditions:
 *
 * if (!list_empty_rcu(mylist)) {
 *	struct foo *bar = list_first_entry_rcu(mylist, struct foo, list_member);
 *	do_something(bar);
 * }
 *
 * The list might be non-empty when list_empty_rcu() checks it, but it
 * might have become empty by the time that list_first_entry_rcu() rereads
 * the ->next pointer, which would result in a SEGV.
 *
 * When not using RCU, it is OK for list_first_entry() to re-read that
 * pointer because both functions should be protected by some lock that
 * blocks writers.
 *
 * When using RCU, list_empty() uses READ_ONCE() to fetch the
 * RCU-protected ->next pointer and then compares it to the address of the
 * list head.  However, it neither dereferences this pointer nor provides
 * this pointer to its caller.  Thus, READ_ONCE() suffices (that is,
 * rcu_dereference() is not needed), which means that list_empty() can be
 * used anywhere you would want to use list_empty_rcu().  Just don't
 * expect anything useful to happen if you do a subsequent lockless
 * call to list_first_entry_rcu()!!!
 *
 * See list_first_or_null_rcu for an alternative.
 */

/**
 * list_first_or_null_rcu - get the first element from a list
 * @ptr:        the list head to take the element from.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * Note that if the list is empty, it returns NULL.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */







/**
 * list_next_or_null_rcu - get the first element from a list
 * @head:	the head for the list.
 * @ptr:        the list head to take the next element from.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * Note that if the ptr is at the end of the list, NULL is returned.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */
# 378 "./include/linux/rculist.h"
/**
 * list_for_each_entry_rcu	-	iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 * @cond:	optional lockdep expression if called from non-RCU protection.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as list_add_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */






/**
 * list_for_each_entry_srcu	-	iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 * @cond:	lockdep expression for the lock required to traverse the list.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as list_add_rcu()
 * as long as the traversal is guarded by srcu_read_lock().
 * The lockdep expression srcu_read_lock_held() can be passed as the
 * cond argument from read side.
 */






/**
 * list_entry_lockless - get the struct for this entry
 * @ptr:        the &struct list_head pointer.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * This primitive may safely run concurrently with the _rcu
 * list-mutation primitives such as list_add_rcu(), but requires some
 * implicit RCU read-side guarding.  One example is running within a special
 * exception-time environment where preemption is disabled and where lockdep
 * cannot be invoked.  Another example is when items are added to the list,
 * but never deleted.
 */



/**
 * list_for_each_entry_lockless - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * This primitive may safely run concurrently with the _rcu
 * list-mutation primitives such as list_add_rcu(), but requires some
 * implicit RCU read-side guarding.  One example is running within a special
 * exception-time environment where preemption is disabled and where lockdep
 * cannot be invoked.  Another example is when items are added to the list,
 * but never deleted.
 */





/**
 * list_for_each_entry_continue_rcu - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Continue to iterate over list of given type, continuing after
 * the current position which must have been in the list when the RCU read
 * lock was taken.
 * This would typically require either that you obtained the node from a
 * previous walk of the list in the same RCU read-side critical section, or
 * that you held some sort of non-RCU reference (such as a reference count)
 * to keep the node alive *and* in the list.
 *
 * This iterator is similar to list_for_each_entry_from_rcu() except
 * this starts after the given position and that one starts at the given
 * position.
 */





/**
 * list_for_each_entry_from_rcu - iterate over a list from current point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_node within the struct.
 *
 * Iterate over the tail of a list starting from a given position,
 * which must have been in the list when the RCU read lock was taken.
 * This would typically require either that you obtained the node from a
 * previous walk of the list in the same RCU read-side critical section, or
 * that you held some sort of non-RCU reference (such as a reference count)
 * to keep the node alive *and* in the list.
 *
 * This iterator is similar to list_for_each_entry_continue_rcu() except
 * this starts from the given position and that one starts from the position
 * after the given position.
 */




/**
 * hlist_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: list_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_del_rcu(struct hlist_node *n)
{
 __hlist_del(n);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_191(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_191(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (((void *) 0x122 + (0xdead000000000000UL))); } while (0); } while (0);
# 515 "./include/linux/rculist.h"
}

/**
 * hlist_replace_rcu - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * The @old entry will be replaced with the @new entry atomically.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_replace_rcu(struct hlist_node *old,
     struct hlist_node *new)
{
 struct hlist_node *next = old->next;

 new->next = next;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_192(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(new->pprev) == sizeof(char) || sizeof(new->pprev) == sizeof(short) || sizeof(new->pprev) == sizeof(int) || sizeof(new->pprev) == sizeof(long)) || sizeof(new->pprev) == sizeof(long long))) __compiletime_assert_192(); } while (0); do { *(volatile typeof(new->pprev) *)&(new->pprev) = (old->pprev); } while (0); } while (0);
# 531 "./include/linux/rculist.h"
 do { uintptr_t _r_a_p__v = (uintptr_t)(new); ; if (__builtin_constant_p(new) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_193(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((*(struct hlist_node /* nothing */ **)new->pprev)) == sizeof(char) || sizeof((*(struct hlist_node /* nothing */ **)new->pprev)) == sizeof(short) || sizeof((*(struct hlist_node /* nothing */ **)new->pprev)) == sizeof(int) || sizeof((*(struct hlist_node /* nothing */ **)new->pprev)) == sizeof(long)) || sizeof((*(struct hlist_node /* nothing */ **)new->pprev)) == sizeof(long long))) __compiletime_assert_193(); } while (0); do { *(volatile typeof((*(struct hlist_node /* nothing */ **)new->pprev)) *)&((*(struct hlist_node /* nothing */ **)new->pprev)) = ((typeof(*(struct hlist_node /* nothing */ **)new->pprev))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&*(struct hlist_node /* nothing */ **)new->pprev) __p = (&*(struct hlist_node /* nothing */ **)new->pprev); union { typeof( _Generic((*&*(struct hlist_node /* nothing */ **)new->pprev), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&*(struct hlist_node /* nothing */ **)new->pprev))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&*(struct hlist_node /* nothing */ **)new->pprev), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&*(struct hlist_node /* nothing */ **)new->pprev)))) ((typeof(*((typeof(*(struct hlist_node /* nothing */ **)new->pprev))_r_a_p__v)) /* nothing */ *)((typeof(*(struct hlist_node /* nothing */ **)new->pprev))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_194(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&*(struct hlist_node /* nothing */ **)new->pprev) == sizeof(char) || sizeof(*&*(struct hlist_node /* nothing */ **)new->pprev) == sizeof(short) || sizeof(*&*(struct hlist_node /* nothing */ **)new->pprev) == sizeof(int) || sizeof(*&*(struct hlist_node /* nothing */ **)new->pprev) == sizeof(long)))) __compiletime_assert_194(); } while (0); kasan_check_write(__p, sizeof(*&*(struct hlist_node /* nothing */ **)new->pprev)); switch (sizeof(*&*(struct hlist_node /* nothing */ **)new->pprev)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 532 "./include/linux/rculist.h"
 if (next)
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_195(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(new->next->pprev) == sizeof(char) || sizeof(new->next->pprev) == sizeof(short) || sizeof(new->next->pprev) == sizeof(int) || sizeof(new->next->pprev) == sizeof(long)) || sizeof(new->next->pprev) == sizeof(long long))) __compiletime_assert_195(); } while (0); do { *(volatile typeof(new->next->pprev) *)&(new->next->pprev) = (&new->next); } while (0); } while (0);
# 534 "./include/linux/rculist.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_196(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(old->pprev) == sizeof(char) || sizeof(old->pprev) == sizeof(short) || sizeof(old->pprev) == sizeof(int) || sizeof(old->pprev) == sizeof(long)) || sizeof(old->pprev) == sizeof(long long))) __compiletime_assert_196(); } while (0); do { *(volatile typeof(old->pprev) *)&(old->pprev) = (((void *) 0x122 + (0xdead000000000000UL))); } while (0); } while (0);
# 535 "./include/linux/rculist.h"
}

/**
 * hlists_swap_heads_rcu - swap the lists the hlist heads point to
 * @left:  The hlist head on the left
 * @right: The hlist head on the right
 *
 * The lists start out as [@left  ][node1 ... ] and
 *                        [@right ][node2 ... ]
 * The lists end up as    [@left  ][node2 ... ]
 *                        [@right ][node1 ... ]
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlists_swap_heads_rcu(struct hlist_head *left, struct hlist_head *right)
{
 struct hlist_node *node1 = left->first;
 struct hlist_node *node2 = right->first;

 do { uintptr_t _r_a_p__v = (uintptr_t)(node2); ; if (__builtin_constant_p(node2) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_197(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((left->first)) == sizeof(char) || sizeof((left->first)) == sizeof(short) || sizeof((left->first)) == sizeof(int) || sizeof((left->first)) == sizeof(long)) || sizeof((left->first)) == sizeof(long long))) __compiletime_assert_197(); } while (0); do { *(volatile typeof((left->first)) *)&((left->first)) = ((typeof(left->first))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&left->first) __p = (&left->first); union { typeof( _Generic((*&left->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&left->first))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&left->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&left->first)))) ((typeof(*((typeof(left->first))_r_a_p__v)) /* nothing */ *)((typeof(left->first))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_198(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&left->first) == sizeof(char) || sizeof(*&left->first) == sizeof(short) || sizeof(*&left->first) == sizeof(int) || sizeof(*&left->first) == sizeof(long)))) __compiletime_assert_198(); } while (0); kasan_check_write(__p, sizeof(*&left->first)); switch (sizeof(*&left->first)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 553 "./include/linux/rculist.h"
 do { uintptr_t _r_a_p__v = (uintptr_t)(node1); ; if (__builtin_constant_p(node1) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_199(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((right->first)) == sizeof(char) || sizeof((right->first)) == sizeof(short) || sizeof((right->first)) == sizeof(int) || sizeof((right->first)) == sizeof(long)) || sizeof((right->first)) == sizeof(long long))) __compiletime_assert_199(); } while (0); do { *(volatile typeof((right->first)) *)&((right->first)) = ((typeof(right->first))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&right->first) __p = (&right->first); union { typeof( _Generic((*&right->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&right->first))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&right->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&right->first)))) ((typeof(*((typeof(right->first))_r_a_p__v)) /* nothing */ *)((typeof(right->first))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_200(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&right->first) == sizeof(char) || sizeof(*&right->first) == sizeof(short) || sizeof(*&right->first) == sizeof(int) || sizeof(*&right->first) == sizeof(long)))) __compiletime_assert_200(); } while (0); kasan_check_write(__p, sizeof(*&right->first)); switch (sizeof(*&right->first)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 554 "./include/linux/rculist.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_201(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(node2->pprev) == sizeof(char) || sizeof(node2->pprev) == sizeof(short) || sizeof(node2->pprev) == sizeof(int) || sizeof(node2->pprev) == sizeof(long)) || sizeof(node2->pprev) == sizeof(long long))) __compiletime_assert_201(); } while (0); do { *(volatile typeof(node2->pprev) *)&(node2->pprev) = (&left->first); } while (0); } while (0);
# 555 "./include/linux/rculist.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_202(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(node1->pprev) == sizeof(char) || sizeof(node1->pprev) == sizeof(short) || sizeof(node1->pprev) == sizeof(int) || sizeof(node1->pprev) == sizeof(long)) || sizeof(node1->pprev) == sizeof(long long))) __compiletime_assert_202(); } while (0); do { *(volatile typeof(node1->pprev) *)&(node1->pprev) = (&right->first); } while (0); } while (0);
# 556 "./include/linux/rculist.h"
}

/*
 * return the first or the next element in an RCU protected hlist
 */




/**
 * hlist_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_head_rcu(struct hlist_node *n,
     struct hlist_head *h)
{
 struct hlist_node *first = h->first;

 n->next = first;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_203(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_203(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&h->first); } while (0); } while (0);
# 591 "./include/linux/rculist.h"
 do { uintptr_t _r_a_p__v = (uintptr_t)(n); ; if (__builtin_constant_p(n) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_204(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((*((struct hlist_node /* nothing */ **)(&(h)->first))))) == sizeof(char) || sizeof(((*((struct hlist_node /* nothing */ **)(&(h)->first))))) == sizeof(short) || sizeof(((*((struct hlist_node /* nothing */ **)(&(h)->first))))) == sizeof(int) || sizeof(((*((struct hlist_node /* nothing */ **)(&(h)->first))))) == sizeof(long)) || sizeof(((*((struct hlist_node /* nothing */ **)(&(h)->first))))) == sizeof(long long))) __compiletime_assert_204(); } while (0); do { *(volatile typeof(((*((struct hlist_node /* nothing */ **)(&(h)->first))))) *)&(((*((struct hlist_node /* nothing */ **)(&(h)->first))))) = ((typeof((*((struct hlist_node /* nothing */ **)(&(h)->first)))))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&(*((struct hlist_node /* nothing */ **)(&(h)->first)))) __p = (&(*((struct hlist_node /* nothing */ **)(&(h)->first)))); union { typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)(&(h)->first)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)(&(h)->first)))))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)(&(h)->first)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)(&(h)->first))))))) ((typeof(*((typeof((*((struct hlist_node /* nothing */ **)(&(h)->first)))))_r_a_p__v)) /* nothing */ *)((typeof((*((struct hlist_node /* nothing */ **)(&(h)->first)))))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_205(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(*((struct hlist_node /* nothing */ **)(&(h)->first)))) == sizeof(char) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(h)->first)))) == sizeof(short) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(h)->first)))) == sizeof(int) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(h)->first)))) == sizeof(long)))) __compiletime_assert_205(); } while (0); kasan_check_write(__p, sizeof(*&(*((struct hlist_node /* nothing */ **)(&(h)->first))))); switch (sizeof(*&(*((struct hlist_node /* nothing */ **)(&(h)->first))))) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 592 "./include/linux/rculist.h"
 if (first)
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_206(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(first->pprev) == sizeof(char) || sizeof(first->pprev) == sizeof(short) || sizeof(first->pprev) == sizeof(int) || sizeof(first->pprev) == sizeof(long)) || sizeof(first->pprev) == sizeof(long long))) __compiletime_assert_206(); } while (0); do { *(volatile typeof(first->pprev) *)&(first->pprev) = (&n->next); } while (0); } while (0);
# 594 "./include/linux/rculist.h"
}

/**
 * hlist_add_tail_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_tail_rcu(struct hlist_node *n,
          struct hlist_head *h)
{
 struct hlist_node *i, *last = ((void *)0);

 /* Note: write side code, so rcu accessors are not needed. */
 for (i = h->first; i; i = i->next)
  last = i;

 if (last) {
  n->next = last->next;
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_207(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_207(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&last->next); } while (0); } while (0);
# 627 "./include/linux/rculist.h"
  do { uintptr_t _r_a_p__v = (uintptr_t)(n); ; if (__builtin_constant_p(n) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_208(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((*((struct hlist_node /* nothing */ **)(&(last)->next))))) == sizeof(char) || sizeof(((*((struct hlist_node /* nothing */ **)(&(last)->next))))) == sizeof(short) || sizeof(((*((struct hlist_node /* nothing */ **)(&(last)->next))))) == sizeof(int) || sizeof(((*((struct hlist_node /* nothing */ **)(&(last)->next))))) == sizeof(long)) || sizeof(((*((struct hlist_node /* nothing */ **)(&(last)->next))))) == sizeof(long long))) __compiletime_assert_208(); } while (0); do { *(volatile typeof(((*((struct hlist_node /* nothing */ **)(&(last)->next))))) *)&(((*((struct hlist_node /* nothing */ **)(&(last)->next))))) = ((typeof((*((struct hlist_node /* nothing */ **)(&(last)->next)))))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&(*((struct hlist_node /* nothing */ **)(&(last)->next)))) __p = (&(*((struct hlist_node /* nothing */ **)(&(last)->next)))); union { typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)(&(last)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)(&(last)->next)))))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)(&(last)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)(&(last)->next))))))) ((typeof(*((typeof((*((struct hlist_node /* nothing */ **)(&(last)->next)))))_r_a_p__v)) /* nothing */ *)((typeof((*((struct hlist_node /* nothing */ **)(&(last)->next)))))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_209(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(*((struct hlist_node /* nothing */ **)(&(last)->next)))) == sizeof(char) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(last)->next)))) == sizeof(short) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(last)->next)))) == sizeof(int) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(last)->next)))) == sizeof(long)))) __compiletime_assert_209(); } while (0); kasan_check_write(__p, sizeof(*&(*((struct hlist_node /* nothing */ **)(&(last)->next))))); switch (sizeof(*&(*((struct hlist_node /* nothing */ **)(&(last)->next))))) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 628 "./include/linux/rculist.h"
 } else {
  hlist_add_head_rcu(n, h);
 }
}

/**
 * hlist_add_before_rcu
 * @n: the new element to add to the hash list.
 * @next: the existing element to add the new element before.
 *
 * Description:
 * Adds the specified element to the specified hlist
 * before the specified node while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_before_rcu(struct hlist_node *n,
     struct hlist_node *next)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_210(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_210(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (next->pprev); } while (0); } while (0);
# 655 "./include/linux/rculist.h"
 n->next = next;
 do { uintptr_t _r_a_p__v = (uintptr_t)(n); ; if (__builtin_constant_p(n) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_211(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((*((struct hlist_node /* nothing */ **)((n)->pprev))))) == sizeof(char) || sizeof(((*((struct hlist_node /* nothing */ **)((n)->pprev))))) == sizeof(short) || sizeof(((*((struct hlist_node /* nothing */ **)((n)->pprev))))) == sizeof(int) || sizeof(((*((struct hlist_node /* nothing */ **)((n)->pprev))))) == sizeof(long)) || sizeof(((*((struct hlist_node /* nothing */ **)((n)->pprev))))) == sizeof(long long))) __compiletime_assert_211(); } while (0); do { *(volatile typeof(((*((struct hlist_node /* nothing */ **)((n)->pprev))))) *)&(((*((struct hlist_node /* nothing */ **)((n)->pprev))))) = ((typeof((*((struct hlist_node /* nothing */ **)((n)->pprev)))))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&(*((struct hlist_node /* nothing */ **)((n)->pprev)))) __p = (&(*((struct hlist_node /* nothing */ **)((n)->pprev)))); union { typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)((n)->pprev)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)((n)->pprev)))))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)((n)->pprev)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)((n)->pprev))))))) ((typeof(*((typeof((*((struct hlist_node /* nothing */ **)((n)->pprev)))))_r_a_p__v)) /* nothing */ *)((typeof((*((struct hlist_node /* nothing */ **)((n)->pprev)))))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_212(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(*((struct hlist_node /* nothing */ **)((n)->pprev)))) == sizeof(char) || sizeof(*&(*((struct hlist_node /* nothing */ **)((n)->pprev)))) == sizeof(short) || sizeof(*&(*((struct hlist_node /* nothing */ **)((n)->pprev)))) == sizeof(int) || sizeof(*&(*((struct hlist_node /* nothing */ **)((n)->pprev)))) == sizeof(long)))) __compiletime_assert_212(); } while (0); kasan_check_write(__p, sizeof(*&(*((struct hlist_node /* nothing */ **)((n)->pprev))))); switch (sizeof(*&(*((struct hlist_node /* nothing */ **)((n)->pprev))))) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 657 "./include/linux/rculist.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_213(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(next->pprev) == sizeof(char) || sizeof(next->pprev) == sizeof(short) || sizeof(next->pprev) == sizeof(int) || sizeof(next->pprev) == sizeof(long)) || sizeof(next->pprev) == sizeof(long long))) __compiletime_assert_213(); } while (0); do { *(volatile typeof(next->pprev) *)&(next->pprev) = (&n->next); } while (0); } while (0);
# 658 "./include/linux/rculist.h"
}

/**
 * hlist_add_behind_rcu
 * @n: the new element to add to the hash list.
 * @prev: the existing element to add the new element after.
 *
 * Description:
 * Adds the specified element to the specified hlist
 * after the specified node while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_add_behind_rcu(struct hlist_node *n,
     struct hlist_node *prev)
{
 n->next = prev->next;
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_214(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_214(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&prev->next); } while (0); } while (0);
# 683 "./include/linux/rculist.h"
 do { uintptr_t _r_a_p__v = (uintptr_t)(n); ; if (__builtin_constant_p(n) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_215(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(((*((struct hlist_node /* nothing */ **)(&(prev)->next))))) == sizeof(char) || sizeof(((*((struct hlist_node /* nothing */ **)(&(prev)->next))))) == sizeof(short) || sizeof(((*((struct hlist_node /* nothing */ **)(&(prev)->next))))) == sizeof(int) || sizeof(((*((struct hlist_node /* nothing */ **)(&(prev)->next))))) == sizeof(long)) || sizeof(((*((struct hlist_node /* nothing */ **)(&(prev)->next))))) == sizeof(long long))) __compiletime_assert_215(); } while (0); do { *(volatile typeof(((*((struct hlist_node /* nothing */ **)(&(prev)->next))))) *)&(((*((struct hlist_node /* nothing */ **)(&(prev)->next))))) = ((typeof((*((struct hlist_node /* nothing */ **)(&(prev)->next)))))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))) __p = (&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))); union { typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&(*((struct hlist_node /* nothing */ **)(&(prev)->next))))))) ((typeof(*((typeof((*((struct hlist_node /* nothing */ **)(&(prev)->next)))))_r_a_p__v)) /* nothing */ *)((typeof((*((struct hlist_node /* nothing */ **)(&(prev)->next)))))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_216(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))) == sizeof(char) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))) == sizeof(short) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))) == sizeof(int) || sizeof(*&(*((struct hlist_node /* nothing */ **)(&(prev)->next)))) == sizeof(long)))) __compiletime_assert_216(); } while (0); kasan_check_write(__p, sizeof(*&(*((struct hlist_node /* nothing */ **)(&(prev)->next))))); switch (sizeof(*&(*((struct hlist_node /* nothing */ **)(&(prev)->next))))) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 684 "./include/linux/rculist.h"
 if (n->next)
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_217(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next->pprev) == sizeof(char) || sizeof(n->next->pprev) == sizeof(short) || sizeof(n->next->pprev) == sizeof(int) || sizeof(n->next->pprev) == sizeof(long)) || sizeof(n->next->pprev) == sizeof(long long))) __compiletime_assert_217(); } while (0); do { *(volatile typeof(n->next->pprev) *)&(n->next->pprev) = (&n->next); } while (0); } while (0);
# 686 "./include/linux/rculist.h"
}






/**
 * hlist_for_each_entry_rcu - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 * @cond:	optional lockdep expression if called from non-RCU protection.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */
# 712 "./include/linux/rculist.h"
/**
 * hlist_for_each_entry_srcu - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 * @cond:	lockdep expression for the lock required to traverse the list.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by srcu_read_lock().
 * The lockdep expression srcu_read_lock_held() can be passed as the
 * cond argument from read side.
 */
# 733 "./include/linux/rculist.h"
/**
 * hlist_for_each_entry_rcu_notrace - iterate over rcu list of given type (for tracing)
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 *
 * This is the same as hlist_for_each_entry_rcu() except that it does
 * not do any RCU debugging or tracing.
 */







/**
 * hlist_for_each_entry_rcu_bh - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */







/**
 * hlist_for_each_entry_continue_rcu - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */







/**
 * hlist_for_each_entry_continue_rcu_bh - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */







/**
 * hlist_for_each_entry_from_rcu - iterate over a hlist continuing from current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */
# 9 "./include/linux/dcache.h" 2
# 1 "./include/linux/rculist_bl.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * RCU-protected bl list version. See include/linux/list_bl.h.
 */
# 1 "./include/linux/list_bl.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/bit_spinlock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/bit_spinlock.h"
/*
 *  bit-based spin_lock()
 *
 * Don't use this unless you really need to: spin_lock() and spin_unlock()
 * are significantly faster.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bit_spin_lock(int bitnum, unsigned long *addr)
{
 /*
	 * Assuming the lock is uncontended, this never enters
	 * the body of the outer loop. If it is contended, then
	 * within the inner loop a non-atomic test is used to
	 * busywait with less bus contention for a good time to
	 * attempt to acquire the lock bit.
	 */
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);

 while (__builtin_expect(!!(test_and_set_bit_lock(bitnum, addr)), 0)) {
  do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
  do {
   cpu_relax();
  } while (((__builtin_constant_p(bitnum) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? const_test_bit(bitnum, addr) : generic_test_bit(bitnum, addr)));
  do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 }

 (void)0;
}

/*
 * Return true if it was acquired
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int bit_spin_trylock(int bitnum, unsigned long *addr)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);

 if (__builtin_expect(!!(test_and_set_bit_lock(bitnum, addr)), 0)) {
  do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
  return 0;
 }

 (void)0;
 return 1;
}

/*
 *  bit-based spin_unlock()
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bit_spin_unlock(int bitnum, unsigned long *addr)
{




 clear_bit_unlock(bitnum, addr);

 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 (void)0;
}

/*
 *  bit-based spin_unlock()
 *  non-atomic version, which can be used eg. if the bit lock itself is
 *  protecting the rest of the flags in the word.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __bit_spin_unlock(int bitnum, unsigned long *addr)
{




 __clear_bit_unlock(bitnum, addr);

 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 (void)0;
}

/*
 * Return true if the lock is held.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int bit_spin_is_locked(int bitnum, unsigned long *addr)
{

 return ((__builtin_constant_p(bitnum) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? const_test_bit(bitnum, addr) : generic_test_bit(bitnum, addr));





}
# 7 "./include/linux/list_bl.h" 2

/*
 * Special version of lists, where head of the list has a lock in the lowest
 * bit. This is useful for scalable hash tables without increasing memory
 * footprint overhead.
 *
 * For modification operations, the 0 bit of hlist_bl_head->first
 * pointer must be set.
 *
 * With some small modifications, this can easily be adapted to store several
 * arbitrary bits (not just a single lock bit), if the need arises to store
 * some fast and compact auxiliary data.
 */
# 34 "./include/linux/list_bl.h"
struct hlist_bl_head {
 struct hlist_bl_node *first;
};

struct hlist_bl_node {
 struct hlist_bl_node *next, **pprev;
};



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void INIT_HLIST_BL_NODE(struct hlist_bl_node *h)
{
 h->next = ((void *)0);
 h->pprev = ((void *)0);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool hlist_bl_unhashed(const struct hlist_bl_node *h)
{
 return !h->pprev;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct hlist_bl_node *hlist_bl_first(struct hlist_bl_head *h)
{
 return (struct hlist_bl_node *)
  ((unsigned long)h->first & ~1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_set_first(struct hlist_bl_head *h,
     struct hlist_bl_node *n)
{
                                                    ;

                        ;
 h->first = (struct hlist_bl_node *)((unsigned long)n | 1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool hlist_bl_empty(const struct hlist_bl_head *h)
{
 return !((unsigned long)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_218(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->first) == sizeof(char) || sizeof(h->first) == sizeof(short) || sizeof(h->first) == sizeof(int) || sizeof(h->first) == sizeof(long)) || sizeof(h->first) == sizeof(long long))) __compiletime_assert_218(); } while (0); (*(const volatile typeof( _Generic((h->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (h->first))) *)&(h->first)); }) & ~1UL);
# 75 "./include/linux/list_bl.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_add_head(struct hlist_bl_node *n,
     struct hlist_bl_head *h)
{
 struct hlist_bl_node *first = hlist_bl_first(h);

 n->next = first;
 if (first)
  first->pprev = &n->next;
 n->pprev = &h->first;
 hlist_bl_set_first(h, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_add_before(struct hlist_bl_node *n,
           struct hlist_bl_node *next)
{
 struct hlist_bl_node **pprev = next->pprev;

 n->pprev = pprev;
 n->next = next;
 next->pprev = &n->next;

 /* pprev may be `first`, so be careful not to lose the lock bit */
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_219(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pprev) == sizeof(char) || sizeof(*pprev) == sizeof(short) || sizeof(*pprev) == sizeof(int) || sizeof(*pprev) == sizeof(long)) || sizeof(*pprev) == sizeof(long long))) __compiletime_assert_219(); } while (0); do { *(volatile typeof(*pprev) *)&(*pprev) = ((struct hlist_bl_node *)
# 99 "./include/linux/list_bl.h"
 ((uintptr_t)n | ((uintptr_t)*pprev & 1UL))); } while (0); } while (0);


}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_add_behind(struct hlist_bl_node *n,
           struct hlist_bl_node *prev)
{
 n->next = prev->next;
 n->pprev = &prev->next;
 prev->next = n;

 if (n->next)
  n->next->pprev = &n->next;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __hlist_bl_del(struct hlist_bl_node *n)
{
 struct hlist_bl_node *next = n->next;
 struct hlist_bl_node **pprev = n->pprev;

                                                    ;

 /* pprev may be `first`, so be careful not to lose the lock bit */
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_220(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pprev) == sizeof(char) || sizeof(*pprev) == sizeof(short) || sizeof(*pprev) == sizeof(int) || sizeof(*pprev) == sizeof(long)) || sizeof(*pprev) == sizeof(long long))) __compiletime_assert_220(); } while (0); do { *(volatile typeof(*pprev) *)&(*pprev) = ((struct hlist_bl_node *)
# 123 "./include/linux/list_bl.h"
 ((unsigned long)next | ((unsigned long)*pprev & 1UL))); } while (0); } while (0);



 if (next)
  next->pprev = pprev;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_del(struct hlist_bl_node *n)
{
 __hlist_bl_del(n);
 n->next = ((void *) 0x100 + (0xdead000000000000UL));
 n->pprev = ((void *) 0x122 + (0xdead000000000000UL));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_del_init(struct hlist_bl_node *n)
{
 if (!hlist_bl_unhashed(n)) {
  __hlist_bl_del(n);
  INIT_HLIST_BL_NODE(n);
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_lock(struct hlist_bl_head *b)
{
 bit_spin_lock(0, (unsigned long *)b);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_unlock(struct hlist_bl_head *b)
{
 __bit_spin_unlock(0, (unsigned long *)b);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool hlist_bl_is_locked(struct hlist_bl_head *b)
{
 return bit_spin_is_locked(0, (unsigned long *)b);
}

/**
 * hlist_bl_for_each_entry	- iterate over list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 */






/**
 * hlist_bl_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @n:		another &struct hlist_node to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */
# 9 "./include/linux/rculist_bl.h" 2


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_set_first_rcu(struct hlist_bl_head *h,
     struct hlist_bl_node *n)
{
                                                    ;

                        ;
 do { uintptr_t _r_a_p__v = (uintptr_t)((struct hlist_bl_node *)((unsigned long)n | 1UL)); ; if (__builtin_constant_p((struct hlist_bl_node *)((unsigned long)n | 1UL)) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_221(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((h->first)) == sizeof(char) || sizeof((h->first)) == sizeof(short) || sizeof((h->first)) == sizeof(int) || sizeof((h->first)) == sizeof(long)) || sizeof((h->first)) == sizeof(long long))) __compiletime_assert_221(); } while (0); do { *(volatile typeof((h->first)) *)&((h->first)) = ((typeof(h->first))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&h->first) __p = (&h->first); union { typeof( _Generic((*&h->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&h->first))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&h->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&h->first)))) ((typeof(*((typeof(h->first))_r_a_p__v)) /* nothing */ *)((typeof(h->first))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_222(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&h->first) == sizeof(char) || sizeof(*&h->first) == sizeof(short) || sizeof(*&h->first) == sizeof(int) || sizeof(*&h->first) == sizeof(long)))) __compiletime_assert_222(); } while (0); kasan_check_write(__p, sizeof(*&h->first)); switch (sizeof(*&h->first)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 19 "./include/linux/rculist_bl.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct hlist_bl_node *hlist_bl_first_rcu(struct hlist_bl_head *h)
{
 return (struct hlist_bl_node *)
  ((unsigned long)({ /* Dependency order vs. p above. */ typeof(*(h->first)) *__UNIQUE_ID_rcu223 = (typeof(*(h->first)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_224(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((h->first)) == sizeof(char) || sizeof((h->first)) == sizeof(short) || sizeof((h->first)) == sizeof(int) || sizeof((h->first)) == sizeof(long)) || sizeof((h->first)) == sizeof(long long))) __compiletime_assert_224(); } while (0); (*(const volatile typeof( _Generic(((h->first)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((h->first)))) *)&((h->first))); }); do { } while (0 && (!((hlist_bl_is_locked(h)) || rcu_read_lock_held()))); ; ((typeof(*(h->first)) *)(__UNIQUE_ID_rcu223)); }) & ~1UL);
# 25 "./include/linux/rculist_bl.h"
}

/**
 * hlist_bl_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_bl_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_bl_add_head_rcu()
 * or hlist_bl_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_bl_for_each_entry().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_del_rcu(struct hlist_bl_node *n)
{
 __hlist_bl_del(n);
 n->pprev = ((void *) 0x122 + (0xdead000000000000UL));
}

/**
 * hlist_bl_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist_bl,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_bl_add_head_rcu()
 * or hlist_bl_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_bl_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hlist_bl_add_head_rcu(struct hlist_bl_node *n,
     struct hlist_bl_head *h)
{
 struct hlist_bl_node *first;

 /* don't need hlist_bl_first_rcu because we're under lock */
 first = hlist_bl_first(h);

 n->next = first;
 if (first)
  first->pprev = &n->next;
 n->pprev = &h->first;

 /* need _rcu because we can have concurrent lock free readers */
 hlist_bl_set_first_rcu(h, n);
}
/**
 * hlist_bl_for_each_entry_rcu - iterate over rcu list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_bl_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_bl_node within the struct.
 *
 */
# 10 "./include/linux/dcache.h" 2

# 1 "./include/linux/seqlock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * seqcount_t / seqlock_t - a reader-writer consistency mechanism with
 * lockless readers (read-only retry loops), and no writer starvation.
 *
 * See Documentation/locking/seqlock.rst
 *
 * Copyrights:
 * - Based on x86_64 vsyscall gettimeofday: Keith Owens, Andrea Arcangeli
 * - Sequence counters with associated locks, (C) 2020 Linutronix GmbH
 */




# 1 "./include/linux/mutex.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Mutexes: blocking mutual exclusion locks
 *
 * started by Ingo Molnar:
 *
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *
 * This file contains the main data structure and API definitions.
 */
# 20 "./include/linux/mutex.h"
# 1 "./include/linux/osq_lock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * An MCS like lock especially tailored for optimistic spinning for sleeping
 * lock implementations (mutex, rwsem, etc).
 */
struct optimistic_spin_node {
 struct optimistic_spin_node *next, *prev;
 int locked; /* 1 if lock acquired */
 int cpu; /* encoded CPU # + 1 value */
};

struct optimistic_spin_queue {
 /*
	 * Stores an encoded value of the CPU # of the tail node in the queue.
	 * If the queue is empty, then it's set to OSQ_UNLOCKED_VAL.
	 */
 atomic_t tail;
};



/* Init macro and function. */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void osq_lock_init(struct optimistic_spin_queue *lock)
{
 atomic_set(&lock->tail, (0));
}

extern bool osq_lock(struct optimistic_spin_queue *lock);
extern void osq_unlock(struct optimistic_spin_queue *lock);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool osq_is_locked(struct optimistic_spin_queue *lock)
{
 return atomic_read(&lock->tail) != (0);
}
# 21 "./include/linux/mutex.h" 2
# 1 "./include/linux/debug_locks.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






struct task_struct;

extern int debug_locks __attribute__((__section__(".data..read_mostly")));
extern int debug_locks_silent __attribute__((__section__(".data..read_mostly")));


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __debug_locks_off(void)
{
 return ({ typeof(&debug_locks) __ai_ptr = (&debug_locks); do { } while (0); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __ret; __ret = (__typeof__(*(__ai_ptr))) __xchg_mb((unsigned long)(0), (__ai_ptr), sizeof(*(__ai_ptr))); __ret; }); });
}

/*
 * Generic 'turn off all lock debugging' function:
 */
extern int debug_locks_off(void);
# 56 "./include/linux/debug_locks.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void debug_show_all_locks(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void debug_show_held_locks(struct task_struct *task)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_check_no_locks_freed(const void *from, unsigned long len)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_check_no_locks_held(void)
{
}
# 22 "./include/linux/mutex.h" 2
# 35 "./include/linux/mutex.h"
/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
struct mutex {
 atomic_long_t owner;
 raw_spinlock_t wait_lock;

 struct optimistic_spin_queue osq; /* Spinner MCS lock */

 struct list_head wait_list;






};
# 89 "./include/linux/mutex.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mutex_destroy(struct mutex *lock) {}



/**
 * mutex_init - initialize the mutex
 * @mutex: the mutex to be initialized
 *
 * Initialize the mutex to unlocked state.
 *
 * It is not allowed to initialize an already locked mutex.
 */
# 118 "./include/linux/mutex.h"
extern void __mutex_init(struct mutex *lock, const char *name,
    struct lock_class_key *key);

/**
 * mutex_is_locked - is the mutex locked
 * @lock: the mutex to be queried
 *
 * Returns true if the mutex is locked, false if unlocked.
 */
extern bool mutex_is_locked(struct mutex *lock);
# 173 "./include/linux/mutex.h"
/*
 * See kernel/locking/mutex.c for detailed documentation of these APIs.
 * Also see Documentation/locking/mutex-design.rst.
 */
# 199 "./include/linux/mutex.h"
extern void mutex_lock(struct mutex *lock);
extern int __attribute__((__warn_unused_result__)) mutex_lock_interruptible(struct mutex *lock);
extern int __attribute__((__warn_unused_result__)) mutex_lock_killable(struct mutex *lock);
extern void mutex_lock_io(struct mutex *lock);
# 211 "./include/linux/mutex.h"
/*
 * NOTE: mutex_trylock() follows the spin_trylock() convention,
 *       not the down_trylock() convention!
 *
 * Returns 1 if the mutex has been acquired successfully, and 0 on contention.
 */
extern int mutex_trylock(struct mutex *lock);
extern void mutex_unlock(struct mutex *lock);

extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
# 20 "./include/linux/seqlock.h" 2





/*
 * The seqlock seqcount_t interface does not prescribe a precise sequence of
 * read begin/retry/end. For readers, typically there is a call to
 * read_seqcount_begin() and read_seqcount_retry(), however, there are more
 * esoteric cases which do not follow this pattern.
 *
 * As a consequence, we take the following best-effort approach for raw usage
 * via seqcount_t under KCSAN: upon beginning a seq-reader critical section,
 * pessimistically mark the next KCSAN_SEQLOCK_REGION_MAX memory accesses as
 * atomics; if there is a matching read_seqcount_retry() call, no following
 * memory operations are considered atomic. Usage of the seqlock_t interface
 * is not affected.
 */


/*
 * Sequence counters (seqcount_t)
 *
 * This is the raw counting mechanism, without any writer protection.
 *
 * Write side critical sections must be serialized and non-preemptible.
 *
 * If readers can be invoked from hardirq or softirq contexts,
 * interrupts or bottom halves must also be respectively disabled before
 * entering the write section.
 *
 * This mechanism can't be used if the protected data contains pointers,
 * as the writer can invalidate a pointer that a reader is following.
 *
 * If the write serialization mechanism is one of the common kernel
 * locking primitives, use a sequence counter with associated lock
 * (seqcount_LOCKNAME_t) instead.
 *
 * If it's desired to automatically handle the sequence counter writer
 * serialization and non-preemptibility requirements, use a sequential
 * lock (seqlock_t) instead.
 *
 * See Documentation/locking/seqlock.rst
 */
typedef struct seqcount {
 unsigned sequence;



} seqcount_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __seqcount_init(seqcount_t *s, const char *name,
       struct lock_class_key *key)
{
 /*
	 * Make sure we are not reinitializing a held lock:
	 */
 do { (void)(name); (void)(key); } while (0);
 s->sequence = 0;
}
# 113 "./include/linux/seqlock.h"
/**
 * SEQCNT_ZERO() - static initializer for seqcount_t
 * @name: Name of the seqcount_t instance
 */


/*
 * Sequence counters with associated locks (seqcount_LOCKNAME_t)
 *
 * A sequence counter which associates the lock used for writer
 * serialization at initialization time. This enables lockdep to validate
 * that the write side critical section is properly serialized.
 *
 * For associated locks which do not implicitly disable preemption,
 * preemption protection is enforced in the write side function.
 *
 * Lockdep is never used in any for the raw write variants.
 *
 * See Documentation/locking/seqlock.rst
 */

/*
 * For PREEMPT_RT, seqcount_LOCKNAME_t write side critical sections cannot
 * disable preemption. It can lead to higher latencies, and the write side
 * sections will not be able to acquire locks which become sleeping locks
 * (e.g. spinlock_t).
 *
 * To remain preemptible while avoiding a possible livelock caused by the
 * reader preempting the writer, use a different technique: let the reader
 * detect if a seqcount_LOCKNAME_t writer is in progress. If that is the
 * case, acquire then release the associated LOCKNAME writer serialization
 * lock. This will allow any possibly-preempted writer to make progress
 * until the end of its writer serialization lock critical section.
 *
 * This lock-unlock technique must be implemented for all of PREEMPT_RT
 * sleeping locks.  See Documentation/locking/locktypes.rst
 */






/*
 * typedef seqcount_LOCKNAME_t - sequence counter with LOCKNAME associated
 * @seqcount:	The real sequence counter
 * @lock:	Pointer to the associated lock
 *
 * A plain sequence counter with external writer synchronization by
 * LOCKNAME @lock. The lock is associated to the sequence counter in the
 * static initializer or init function. This enables lockdep to validate
 * that the write side critical section is properly serialized.
 *
 * LOCKNAME:	raw_spinlock, spinlock, rwlock or mutex
 */

/*
 * seqcount_LOCKNAME_init() - runtime initializer for seqcount_LOCKNAME_t
 * @s:		Pointer to the seqcount_LOCKNAME_t instance
 * @lock:	Pointer to the associated lock
 */
# 187 "./include/linux/seqlock.h"
/*
 * SEQCOUNT_LOCKNAME()	- Instantiate seqcount_LOCKNAME_t and helpers
 * seqprop_LOCKNAME_*()	- Property accessors for seqcount_LOCKNAME_t
 *
 * @lockname:		"LOCKNAME" part of seqcount_LOCKNAME_t
 * @locktype:		LOCKNAME canonical C data type
 * @preemptible:	preemptibility of above locktype
 * @lockmember:		argument for lockdep_assert_held()
 * @lockbase:		associated lock release function (prefix only)
 * @lock_acquire:	associated lock acquisition function (full call)
 */
# 248 "./include/linux/seqlock.h"
/*
 * __seqprop() for seqcount_t
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) seqcount_t *__seqprop_ptr(seqcount_t *s)
{
 return s;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned __seqprop_sequence(const seqcount_t *s)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_225(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->sequence) == sizeof(char) || sizeof(s->sequence) == sizeof(short) || sizeof(s->sequence) == sizeof(int) || sizeof(s->sequence) == sizeof(long)) || sizeof(s->sequence) == sizeof(long long))) __compiletime_assert_225(); } while (0); (*(const volatile typeof( _Generic((s->sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->sequence))) *)&(s->sequence)); });
# 260 "./include/linux/seqlock.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __seqprop_preemptible(const seqcount_t *s)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __seqprop_assert(const seqcount_t *s)
{
 do { } while (0);
}



typedef struct seqcount_raw_spinlock { seqcount_t seqcount; ; } seqcount_raw_spinlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) seqcount_t * __seqprop_raw_spinlock_ptr(seqcount_raw_spinlock_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned __seqprop_raw_spinlock_sequence(const seqcount_raw_spinlock_t *s) { unsigned seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_226(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_226(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (false && __builtin_expect(!!(seq & 1), 0)) { ; ; /*							\
		 * Re-read the sequence counter since the (possibly	\
		 * preempted) writer made progress.			\
		 */ seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_227(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_227(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool __seqprop_raw_spinlock_preemptible(const seqcount_raw_spinlock_t *s) { if (!0) return false; /* PREEMPT_RT relies on the above LOCK+UNLOCK */ return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __seqprop_raw_spinlock_assert(const seqcount_raw_spinlock_t *s) { ; }
# 275 "./include/linux/seqlock.h"
typedef struct seqcount_spinlock { seqcount_t seqcount; ; } seqcount_spinlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) seqcount_t * __seqprop_spinlock_ptr(seqcount_spinlock_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned __seqprop_spinlock_sequence(const seqcount_spinlock_t *s) { unsigned seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_228(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_228(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (0 && __builtin_expect(!!(seq & 1), 0)) { ; ; /*							\
		 * Re-read the sequence counter since the (possibly	\
		 * preempted) writer made progress.			\
		 */ seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_229(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_229(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool __seqprop_spinlock_preemptible(const seqcount_spinlock_t *s) { if (!0) return 0; /* PREEMPT_RT relies on the above LOCK+UNLOCK */ return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __seqprop_spinlock_assert(const seqcount_spinlock_t *s) { ; }
# 276 "./include/linux/seqlock.h"
typedef struct seqcount_rwlock { seqcount_t seqcount; ; } seqcount_rwlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) seqcount_t * __seqprop_rwlock_ptr(seqcount_rwlock_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned __seqprop_rwlock_sequence(const seqcount_rwlock_t *s) { unsigned seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_230(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_230(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (0 && __builtin_expect(!!(seq & 1), 0)) { ; ; /*							\
		 * Re-read the sequence counter since the (possibly	\
		 * preempted) writer made progress.			\
		 */ seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_231(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_231(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool __seqprop_rwlock_preemptible(const seqcount_rwlock_t *s) { if (!0) return 0; /* PREEMPT_RT relies on the above LOCK+UNLOCK */ return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __seqprop_rwlock_assert(const seqcount_rwlock_t *s) { ; }
# 277 "./include/linux/seqlock.h"
typedef struct seqcount_mutex { seqcount_t seqcount; ; } seqcount_mutex_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) seqcount_t * __seqprop_mutex_ptr(seqcount_mutex_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned __seqprop_mutex_sequence(const seqcount_mutex_t *s) { unsigned seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_232(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_232(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (true && __builtin_expect(!!(seq & 1), 0)) { ; ; /*							\
		 * Re-read the sequence counter since the (possibly	\
		 * preempted) writer made progress.			\
		 */ seq = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_233(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_233(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool __seqprop_mutex_preemptible(const seqcount_mutex_t *s) { if (!0) return true; /* PREEMPT_RT relies on the above LOCK+UNLOCK */ return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __seqprop_mutex_assert(const seqcount_mutex_t *s) { ; }
# 279 "./include/linux/seqlock.h"
/*
 * SEQCNT_LOCKNAME_ZERO - static initializer for seqcount_LOCKNAME_t
 * @name:	Name of the seqcount_LOCKNAME_t instance
 * @lock:	Pointer to the associated LOCKNAME
 */
# 311 "./include/linux/seqlock.h"
/**
 * __read_seqcount_begin() - begin a seqcount_t read section w/o barrier
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * __read_seqcount_begin is like read_seqcount_begin, but has no smp_rmb()
 * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
 * provided before actually loading any of the variables that are to be
 * protected in this critical section.
 *
 * Use carefully, only in critical code, and comment how the barrier is
 * provided.
 *
 * Return: count to be passed to read_seqcount_retry()
 */
# 336 "./include/linux/seqlock.h"
/**
 * raw_read_seqcount_begin() - begin a seqcount_t read section w/o lockdep
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * Return: count to be passed to read_seqcount_retry()
 */
# 350 "./include/linux/seqlock.h"
/**
 * read_seqcount_begin() - begin a seqcount_t read critical section
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * Return: count to be passed to read_seqcount_retry()
 */






/**
 * raw_read_seqcount() - read the raw seqcount_t counter value
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * raw_read_seqcount opens a read critical section of the given
 * seqcount_t, without any lockdep checking, and without checking or
 * masking the sequence counter LSB. Calling code is responsible for
 * handling that.
 *
 * Return: count to be passed to read_seqcount_retry()
 */
# 382 "./include/linux/seqlock.h"
/**
 * raw_seqcount_begin() - begin a seqcount_t read critical section w/o
 *                        lockdep and w/o counter stabilization
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * raw_seqcount_begin opens a read critical section of the given
 * seqcount_t. Unlike read_seqcount_begin(), this function will not wait
 * for the count to stabilize. If a writer is active when it begins, it
 * will fail the read_seqcount_retry() at the end of the read critical
 * section instead of stabilizing at the beginning of it.
 *
 * Use this only in special kernel hot paths where the read section is
 * small and has a high probability of success through other external
 * means. It will save a single branching instruction.
 *
 * Return: count to be passed to read_seqcount_retry()
 */
# 408 "./include/linux/seqlock.h"
/**
 * __read_seqcount_retry() - end a seqcount_t read section w/o barrier
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 * @start: count, from read_seqcount_begin()
 *
 * __read_seqcount_retry is like read_seqcount_retry, but has no smp_rmb()
 * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
 * provided before actually loading any of the variables that are to be
 * protected in this critical section.
 *
 * Use carefully, only in critical code, and comment how the barrier is
 * provided.
 *
 * Return: true if a read section retry is required, else false
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int do___read_seqcount_retry(const seqcount_t *s, unsigned start)
{
 kcsan_atomic_next(0);
 return __builtin_expect(!!(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_234(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->sequence) == sizeof(char) || sizeof(s->sequence) == sizeof(short) || sizeof(s->sequence) == sizeof(int) || sizeof(s->sequence) == sizeof(long)) || sizeof(s->sequence) == sizeof(long long))) __compiletime_assert_234(); } while (0); (*(const volatile typeof( _Generic((s->sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->sequence))) *)&(s->sequence)); }) != start), 0);
# 430 "./include/linux/seqlock.h"
}

/**
 * read_seqcount_retry() - end a seqcount_t read critical section
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 * @start: count, from read_seqcount_begin()
 *
 * read_seqcount_retry closes the read critical section of given
 * seqcount_t.  If the critical section was invalid, it must be ignored
 * (and typically retried).
 *
 * Return: true if a read section retry is required, else false
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int do_read_seqcount_retry(const seqcount_t *s, unsigned start)
{
 do { do { } while (0); asm volatile("dmb " "ishld" : : : "memory"); } while (0);
 return do___read_seqcount_retry(s, start);
}

/**
 * raw_write_seqcount_begin() - start a seqcount_t write section w/o lockdep
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * Context: check write_seqcount_begin()
 */
# 466 "./include/linux/seqlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_raw_write_seqcount_begin(seqcount_t *s)
{
 kcsan_nestable_atomic_begin();
 s->sequence++;
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0);
}

/**
 * raw_write_seqcount_end() - end a seqcount_t write section w/o lockdep
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * Context: check write_seqcount_end()
 */
# 487 "./include/linux/seqlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_raw_write_seqcount_end(seqcount_t *s)
{
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0);
 s->sequence++;
 kcsan_nestable_atomic_end();
}

/**
 * write_seqcount_begin_nested() - start a seqcount_t write section with
 *                                 custom lockdep nesting level
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 * @subclass: lockdep nesting level
 *
 * See Documentation/locking/lockdep-design.rst
 * Context: check write_seqcount_begin()
 */
# 513 "./include/linux/seqlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_write_seqcount_begin_nested(seqcount_t *s, int subclass)
{
 do_raw_write_seqcount_begin(s);
 do { } while (0);
}

/**
 * write_seqcount_begin() - start a seqcount_t write side critical section
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * Context: sequence counter write side sections must be serialized and
 * non-preemptible. Preemption will be automatically disabled if and
 * only if the seqcount write serialization lock is associated, and
 * preemptible.  If readers can be invoked from hardirq or softirq
 * context, interrupts or bottom halves must be respectively disabled.
 */
# 539 "./include/linux/seqlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_write_seqcount_begin(seqcount_t *s)
{
 do_write_seqcount_begin_nested(s, 0);
}

/**
 * write_seqcount_end() - end a seqcount_t write side critical section
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * Context: Preemption will be automatically re-enabled if and only if
 * the seqcount write serialization lock is associated, and preemptible.
 */
# 559 "./include/linux/seqlock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_write_seqcount_end(seqcount_t *s)
{
 do { } while (0);
 do_raw_write_seqcount_end(s);
}

/**
 * raw_write_seqcount_barrier() - do a seqcount_t write barrier
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * This can be used to provide an ordering guarantee instead of the usual
 * consistency guarantee. It is one wmb cheaper, because it can collapse
 * the two back-to-back wmb()s.
 *
 * Note that writes surrounding the barrier should be declared atomic (e.g.
 * via WRITE_ONCE): a) to ensure the writes become visible to other threads
 * atomically, avoiding compiler optimizations; b) to document which writes are
 * meant to propagate to the reader critical section. This is necessary because
 * neither writes before and after the barrier are enclosed in a seq-writer
 * critical section that would ensure readers are aware of ongoing writes::
 *
 *	seqcount_t seq;
 *	bool X = true, Y = false;
 *
 *	void read(void)
 *	{
 *		bool x, y;
 *
 *		do {
 *			int s = read_seqcount_begin(&seq);
 *
 *			x = X; y = Y;
 *
 *		} while (read_seqcount_retry(&seq, s));
 *
 *		BUG_ON(!x && !y);
 *      }
 *
 *      void write(void)
 *      {
 *		WRITE_ONCE(Y, true);
 *
 *		raw_write_seqcount_barrier(seq);
 *
 *		WRITE_ONCE(X, false);
 *      }
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_raw_write_seqcount_barrier(seqcount_t *s)
{
 kcsan_nestable_atomic_begin();
 s->sequence++;
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0);
 s->sequence++;
 kcsan_nestable_atomic_end();
}

/**
 * write_seqcount_invalidate() - invalidate in-progress seqcount_t read
 *                               side operations
 * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants
 *
 * After write_seqcount_invalidate, no seqcount_t read side operations
 * will complete successfully and see data older than this.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_write_seqcount_invalidate(seqcount_t *s)
{
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0);
 kcsan_nestable_atomic_begin();
 s->sequence+=2;
 kcsan_nestable_atomic_end();
}

/*
 * Latch sequence counters (seqcount_latch_t)
 *
 * A sequence counter variant where the counter even/odd value is used to
 * switch between two copies of protected data. This allows the read path,
 * typically NMIs, to safely interrupt the write side critical section.
 *
 * As the write sections are fully preemptible, no special handling for
 * PREEMPT_RT is needed.
 */
typedef struct {
 seqcount_t seqcount;
} seqcount_latch_t;

/**
 * SEQCNT_LATCH_ZERO() - static initializer for seqcount_latch_t
 * @seq_name: Name of the seqcount_latch_t instance
 */




/**
 * seqcount_latch_init() - runtime initializer for seqcount_latch_t
 * @s: Pointer to the seqcount_latch_t instance
 */


/**
 * raw_read_seqcount_latch() - pick even/odd latch data copy
 * @s: Pointer to seqcount_latch_t
 *
 * See raw_write_seqcount_latch() for details and a full reader/writer
 * usage example.
 *
 * Return: sequence counter raw value. Use the lowest bit as an index for
 * picking which data copy to read. The full counter must then be checked
 * with read_seqcount_latch_retry().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned raw_read_seqcount_latch(const seqcount_latch_t *s)
{
 /*
	 * Pairs with the first smp_wmb() in raw_write_seqcount_latch().
	 * Due to the dependent load, a full smp_rmb() is not needed.
	 */
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_235(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_235(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); });
# 683 "./include/linux/seqlock.h"
}

/**
 * read_seqcount_latch_retry() - end a seqcount_latch_t read section
 * @s:		Pointer to seqcount_latch_t
 * @start:	count, from raw_read_seqcount_latch()
 *
 * Return: true if a read section retry is required, else false
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
read_seqcount_latch_retry(const seqcount_latch_t *s, unsigned start)
{
 return do_read_seqcount_retry(_Generic(*(&s->seqcount), seqcount_t: __seqprop_ptr((void *)(&s->seqcount)), seqcount_raw_spinlock_t: __seqprop_raw_spinlock_ptr((void *)((&s->seqcount))), seqcount_spinlock_t: __seqprop_spinlock_ptr((void *)((&s->seqcount))), seqcount_rwlock_t: __seqprop_rwlock_ptr((void *)((&s->seqcount))), seqcount_mutex_t: __seqprop_mutex_ptr((void *)((&s->seqcount)))), start);
}

/**
 * raw_write_seqcount_latch() - redirect latch readers to even/odd copy
 * @s: Pointer to seqcount_latch_t
 *
 * The latch technique is a multiversion concurrency control method that allows
 * queries during non-atomic modifications. If you can guarantee queries never
 * interrupt the modification -- e.g. the concurrency is strictly between CPUs
 * -- you most likely do not need this.
 *
 * Where the traditional RCU/lockless data structures rely on atomic
 * modifications to ensure queries observe either the old or the new state the
 * latch allows the same for non-atomic updates. The trade-off is doubling the
 * cost of storage; we have to maintain two copies of the entire data
 * structure.
 *
 * Very simply put: we first modify one copy and then the other. This ensures
 * there is always one copy in a stable state, ready to give us an answer.
 *
 * The basic form is a data structure like::
 *
 *	struct latch_struct {
 *		seqcount_latch_t	seq;
 *		struct data_struct	data[2];
 *	};
 *
 * Where a modification, which is assumed to be externally serialized, does the
 * following::
 *
 *	void latch_modify(struct latch_struct *latch, ...)
 *	{
 *		smp_wmb();	// Ensure that the last data[1] update is visible
 *		latch->seq.sequence++;
 *		smp_wmb();	// Ensure that the seqcount update is visible
 *
 *		modify(latch->data[0], ...);
 *
 *		smp_wmb();	// Ensure that the data[0] update is visible
 *		latch->seq.sequence++;
 *		smp_wmb();	// Ensure that the seqcount update is visible
 *
 *		modify(latch->data[1], ...);
 *	}
 *
 * The query will have a form like::
 *
 *	struct entry *latch_query(struct latch_struct *latch, ...)
 *	{
 *		struct entry *entry;
 *		unsigned seq, idx;
 *
 *		do {
 *			seq = raw_read_seqcount_latch(&latch->seq);
 *
 *			idx = seq & 0x01;
 *			entry = data_query(latch->data[idx], ...);
 *
 *		// This includes needed smp_rmb()
 *		} while (read_seqcount_latch_retry(&latch->seq, seq));
 *
 *		return entry;
 *	}
 *
 * So during the modification, queries are first redirected to data[1]. Then we
 * modify data[0]. When that is complete, we redirect queries back to data[0]
 * and we can modify data[1].
 *
 * NOTE:
 *
 *	The non-requirement for atomic modifications does _NOT_ include
 *	the publishing of new entries in the case where data is a dynamic
 *	data structure.
 *
 *	An iteration might start in data[0] and get suspended long enough
 *	to miss an entire modification sequence, once it resumes it might
 *	observe the new entry.
 *
 * NOTE2:
 *
 *	When data is a dynamic data structure; one should use regular RCU
 *	patterns to manage the lifetimes of the objects within.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void raw_write_seqcount_latch(seqcount_latch_t *s)
{
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0); /* prior stores before incrementing "sequence" */
 s->seqcount.sequence++;
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0); /* increment "sequence" before following stores */
}

/*
 * Sequential locks (seqlock_t)
 *
 * Sequence counters with an embedded spinlock for writer serialization
 * and non-preemptibility.
 *
 * For more info, see:
 *    - Comments on top of seqcount_t
 *    - Documentation/locking/seqlock.rst
 */
typedef struct {
 /*
	 * Make sure that readers don't starve writers on PREEMPT_RT: use
	 * seqcount_spinlock_t instead of seqcount_t. Check __SEQ_LOCK().
	 */
 seqcount_spinlock_t seqcount;
 spinlock_t lock;
} seqlock_t;







/**
 * seqlock_init() - dynamic initializer for seqlock_t
 * @sl: Pointer to the seqlock_t instance
 */






/**
 * DEFINE_SEQLOCK(sl) - Define a statically allocated seqlock_t
 * @sl: Name of the seqlock_t instance
 */



/**
 * read_seqbegin() - start a seqlock_t read side critical section
 * @sl: Pointer to seqlock_t
 *
 * Return: count, to be passed to read_seqretry()
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned read_seqbegin(const seqlock_t *sl)
{
 unsigned ret = ({ ; ({ unsigned _seq = ({ unsigned __seq; while ((__seq = _Generic(*(&sl->seqcount), seqcount_t: __seqprop_sequence((void *)(&sl->seqcount)), seqcount_raw_spinlock_t: __seqprop_raw_spinlock_sequence((void *)((&sl->seqcount))), seqcount_spinlock_t: __seqprop_spinlock_sequence((void *)((&sl->seqcount))), seqcount_rwlock_t: __seqprop_rwlock_sequence((void *)((&sl->seqcount))), seqcount_mutex_t: __seqprop_mutex_sequence((void *)((&sl->seqcount))))) & 1) cpu_relax(); kcsan_atomic_next(1000); __seq; }); do { do { } while (0); asm volatile("dmb " "ishld" : : : "memory"); } while (0); _seq; }); });

 kcsan_atomic_next(0); /* non-raw usage, assume closing read_seqretry() */
 kcsan_flat_atomic_begin();
 return ret;
}

/**
 * read_seqretry() - end a seqlock_t read side section
 * @sl: Pointer to seqlock_t
 * @start: count, from read_seqbegin()
 *
 * read_seqretry closes the read side critical section of given seqlock_t.
 * If the critical section was invalid, it must be ignored (and typically
 * retried).
 *
 * Return: true if a read section retry is required, else false
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned read_seqretry(const seqlock_t *sl, unsigned start)
{
 /*
	 * Assume not nested: read_seqretry() may be called multiple times when
	 * completing read critical section.
	 */
 kcsan_flat_atomic_end();

 return do_read_seqcount_retry(_Generic(*(&sl->seqcount), seqcount_t: __seqprop_ptr((void *)(&sl->seqcount)), seqcount_raw_spinlock_t: __seqprop_raw_spinlock_ptr((void *)((&sl->seqcount))), seqcount_spinlock_t: __seqprop_spinlock_ptr((void *)((&sl->seqcount))), seqcount_rwlock_t: __seqprop_rwlock_ptr((void *)((&sl->seqcount))), seqcount_mutex_t: __seqprop_mutex_ptr((void *)((&sl->seqcount)))), start);
}

/*
 * For all seqlock_t write side functions, use the the internal
 * do_write_seqcount_begin() instead of generic write_seqcount_begin().
 * This way, no redundant lockdep_assert_held() checks are added.
 */

/**
 * write_seqlock() - start a seqlock_t write side critical section
 * @sl: Pointer to seqlock_t
 *
 * write_seqlock opens a write side critical section for the given
 * seqlock_t.  It also implicitly acquires the spinlock_t embedded inside
 * that sequential lock. All seqlock_t write side sections are thus
 * automatically serialized and non-preemptible.
 *
 * Context: if the seqlock_t read section, or other write side critical
 * sections, can be invoked from hardirq or softirq contexts, use the
 * _irqsave or _bh variants of this function instead.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void write_seqlock(seqlock_t *sl)
{
 spin_lock(&sl->lock);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
}

/**
 * write_sequnlock() - end a seqlock_t write side critical section
 * @sl: Pointer to seqlock_t
 *
 * write_sequnlock closes the (serialized and non-preemptible) write side
 * critical section of given seqlock_t.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void write_sequnlock(seqlock_t *sl)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock(&sl->lock);
}

/**
 * write_seqlock_bh() - start a softirqs-disabled seqlock_t write section
 * @sl: Pointer to seqlock_t
 *
 * _bh variant of write_seqlock(). Use only if the read side section, or
 * other write side sections, can be invoked from softirq contexts.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void write_seqlock_bh(seqlock_t *sl)
{
 spin_lock_bh(&sl->lock);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
}

/**
 * write_sequnlock_bh() - end a softirqs-disabled seqlock_t write section
 * @sl: Pointer to seqlock_t
 *
 * write_sequnlock_bh closes the serialized, non-preemptible, and
 * softirqs-disabled, seqlock_t write side critical section opened with
 * write_seqlock_bh().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void write_sequnlock_bh(seqlock_t *sl)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock_bh(&sl->lock);
}

/**
 * write_seqlock_irq() - start a non-interruptible seqlock_t write section
 * @sl: Pointer to seqlock_t
 *
 * _irq variant of write_seqlock(). Use only if the read side section, or
 * other write sections, can be invoked from hardirq contexts.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void write_seqlock_irq(seqlock_t *sl)
{
 spin_lock_irq(&sl->lock);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
}

/**
 * write_sequnlock_irq() - end a non-interruptible seqlock_t write section
 * @sl: Pointer to seqlock_t
 *
 * write_sequnlock_irq closes the serialized and non-interruptible
 * seqlock_t write side section opened with write_seqlock_irq().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void write_sequnlock_irq(seqlock_t *sl)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock_irq(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __write_seqlock_irqsave(seqlock_t *sl)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = _raw_spin_lock_irqsave(spinlock_check(&sl->lock)); } while (0); } while (0);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
 return flags;
}

/**
 * write_seqlock_irqsave() - start a non-interruptible seqlock_t write
 *                           section
 * @lock:  Pointer to seqlock_t
 * @flags: Stack-allocated storage for saving caller's local interrupt
 *         state, to be passed to write_sequnlock_irqrestore().
 *
 * _irqsave variant of write_seqlock(). Use it only if the read side
 * section, or other write sections, can be invoked from hardirq context.
 */



/**
 * write_sequnlock_irqrestore() - end non-interruptible seqlock_t write
 *                                section
 * @sl:    Pointer to seqlock_t
 * @flags: Caller's saved interrupt state, from write_seqlock_irqsave()
 *
 * write_sequnlock_irqrestore closes the serialized and non-interruptible
 * seqlock_t write section previously opened with write_seqlock_irqsave().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock_irqrestore(&sl->lock, flags);
}

/**
 * read_seqlock_excl() - begin a seqlock_t locking reader section
 * @sl:	Pointer to seqlock_t
 *
 * read_seqlock_excl opens a seqlock_t locking reader critical section.  A
 * locking reader exclusively locks out *both* other writers *and* other
 * locking readers, but it does not update the embedded sequence number.
 *
 * Locking readers act like a normal spin_lock()/spin_unlock().
 *
 * Context: if the seqlock_t write section, *or other read sections*, can
 * be invoked from hardirq or softirq contexts, use the _irqsave or _bh
 * variant of this function instead.
 *
 * The opened read section must be closed with read_sequnlock_excl().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void read_seqlock_excl(seqlock_t *sl)
{
 spin_lock(&sl->lock);
}

/**
 * read_sequnlock_excl() - end a seqlock_t locking reader critical section
 * @sl: Pointer to seqlock_t
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void read_sequnlock_excl(seqlock_t *sl)
{
 spin_unlock(&sl->lock);
}

/**
 * read_seqlock_excl_bh() - start a seqlock_t locking reader section with
 *			    softirqs disabled
 * @sl: Pointer to seqlock_t
 *
 * _bh variant of read_seqlock_excl(). Use this variant only if the
 * seqlock_t write side section, *or other read sections*, can be invoked
 * from softirq contexts.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void read_seqlock_excl_bh(seqlock_t *sl)
{
 spin_lock_bh(&sl->lock);
}

/**
 * read_sequnlock_excl_bh() - stop a seqlock_t softirq-disabled locking
 *			      reader section
 * @sl: Pointer to seqlock_t
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void read_sequnlock_excl_bh(seqlock_t *sl)
{
 spin_unlock_bh(&sl->lock);
}

/**
 * read_seqlock_excl_irq() - start a non-interruptible seqlock_t locking
 *			     reader section
 * @sl: Pointer to seqlock_t
 *
 * _irq variant of read_seqlock_excl(). Use this only if the seqlock_t
 * write side section, *or other read sections*, can be invoked from a
 * hardirq context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void read_seqlock_excl_irq(seqlock_t *sl)
{
 spin_lock_irq(&sl->lock);
}

/**
 * read_sequnlock_excl_irq() - end an interrupts-disabled seqlock_t
 *                             locking reader section
 * @sl: Pointer to seqlock_t
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void read_sequnlock_excl_irq(seqlock_t *sl)
{
 spin_unlock_irq(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __read_seqlock_excl_irqsave(seqlock_t *sl)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = _raw_spin_lock_irqsave(spinlock_check(&sl->lock)); } while (0); } while (0);
 return flags;
}

/**
 * read_seqlock_excl_irqsave() - start a non-interruptible seqlock_t
 *				 locking reader section
 * @lock:  Pointer to seqlock_t
 * @flags: Stack-allocated storage for saving caller's local interrupt
 *         state, to be passed to read_sequnlock_excl_irqrestore().
 *
 * _irqsave variant of read_seqlock_excl(). Use this only if the seqlock_t
 * write side section, *or other read sections*, can be invoked from a
 * hardirq context.
 */



/**
 * read_sequnlock_excl_irqrestore() - end non-interruptible seqlock_t
 *				      locking reader section
 * @sl:    Pointer to seqlock_t
 * @flags: Caller saved interrupt state, from read_seqlock_excl_irqsave()
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
read_sequnlock_excl_irqrestore(seqlock_t *sl, unsigned long flags)
{
 spin_unlock_irqrestore(&sl->lock, flags);
}

/**
 * read_seqbegin_or_lock() - begin a seqlock_t lockless or locking reader
 * @lock: Pointer to seqlock_t
 * @seq : Marker and return parameter. If the passed value is even, the
 * reader will become a *lockless* seqlock_t reader as in read_seqbegin().
 * If the passed value is odd, the reader will become a *locking* reader
 * as in read_seqlock_excl().  In the first call to this function, the
 * caller *must* initialize and pass an even value to @seq; this way, a
 * lockless read can be optimistically tried first.
 *
 * read_seqbegin_or_lock is an API designed to optimistically try a normal
 * lockless seqlock_t read section first.  If an odd counter is found, the
 * lockless read trial has failed, and the next read iteration transforms
 * itself into a full seqlock_t locking reader.
 *
 * This is typically used to avoid seqlock_t lockless readers starvation
 * (too much retry loops) in the case of a sharp spike in write side
 * activity.
 *
 * Context: if the seqlock_t write section, *or other read sections*, can
 * be invoked from hardirq or softirq contexts, use the _irqsave or _bh
 * variant of this function instead.
 *
 * Check Documentation/locking/seqlock.rst for template example code.
 *
 * Return: the encountered sequence counter value, through the @seq
 * parameter, which is overloaded as a return parameter. This returned
 * value must be checked with need_seqretry(). If the read section need to
 * be retried, this returned value must also be passed as the @seq
 * parameter of the next read_seqbegin_or_lock() iteration.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void read_seqbegin_or_lock(seqlock_t *lock, int *seq)
{
 if (!(*seq & 1)) /* Even */
  *seq = read_seqbegin(lock);
 else /* Odd */
  read_seqlock_excl(lock);
}

/**
 * need_seqretry() - validate seqlock_t "locking or lockless" read section
 * @lock: Pointer to seqlock_t
 * @seq: sequence count, from read_seqbegin_or_lock()
 *
 * Return: true if a read section retry is required, false otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int need_seqretry(seqlock_t *lock, int seq)
{
 return !(seq & 1) && read_seqretry(lock, seq);
}

/**
 * done_seqretry() - end seqlock_t "locking or lockless" reader section
 * @lock: Pointer to seqlock_t
 * @seq: count, from read_seqbegin_or_lock()
 *
 * done_seqretry finishes the seqlock_t read side critical section started
 * with read_seqbegin_or_lock() and validated by need_seqretry().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void done_seqretry(seqlock_t *lock, int seq)
{
 if (seq & 1)
  read_sequnlock_excl(lock);
}

/**
 * read_seqbegin_or_lock_irqsave() - begin a seqlock_t lockless reader, or
 *                                   a non-interruptible locking reader
 * @lock: Pointer to seqlock_t
 * @seq:  Marker and return parameter. Check read_seqbegin_or_lock().
 *
 * This is the _irqsave variant of read_seqbegin_or_lock(). Use it only if
 * the seqlock_t write section, *or other read sections*, can be invoked
 * from hardirq context.
 *
 * Note: Interrupts will be disabled only for "locking reader" mode.
 *
 * Return:
 *
 *   1. The saved local interrupts state in case of a locking reader, to
 *      be passed to done_seqretry_irqrestore().
 *
 *   2. The encountered sequence counter value, returned through @seq
 *      overloaded as a return parameter. Check read_seqbegin_or_lock().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
read_seqbegin_or_lock_irqsave(seqlock_t *lock, int *seq)
{
 unsigned long flags = 0;

 if (!(*seq & 1)) /* Even */
  *seq = read_seqbegin(lock);
 else /* Odd */
  do { flags = __read_seqlock_excl_irqsave(lock); } while (0);

 return flags;
}

/**
 * done_seqretry_irqrestore() - end a seqlock_t lockless reader, or a
 *				non-interruptible locking reader section
 * @lock:  Pointer to seqlock_t
 * @seq:   Count, from read_seqbegin_or_lock_irqsave()
 * @flags: Caller's saved local interrupt state in case of a locking
 *	   reader, also from read_seqbegin_or_lock_irqsave()
 *
 * This is the _irqrestore variant of done_seqretry(). The read section
 * must've been opened with read_seqbegin_or_lock_irqsave(), and validated
 * by need_seqretry().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
done_seqretry_irqrestore(seqlock_t *lock, int seq, unsigned long flags)
{
 if (seq & 1)
  read_sequnlock_excl_irqrestore(lock, flags);
}
# 12 "./include/linux/dcache.h" 2


# 1 "./include/linux/lockref.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Locked reference counts.
 *
 * These are different from just plain atomic refcounts in that they
 * are atomic with respect to the spinlock that goes with them.  In
 * particular, there can be implementations that don't actually get
 * the spinlock for the common decrement/increment operations, but they
 * still have to check that the operation is done semantically as if
 * the spinlock had been taken (using a cmpxchg operation that covers
 * both the lock and the count word, or using memory transactions, for
 * example).
 */


# 1 "./include/generated/bounds.h" 1


/*
 * DO NOT MODIFY.
 *
 * This file was generated by Kbuild
 */
# 20 "./include/linux/lockref.h" 2





struct lockref {
 union {

  __u64 __attribute__((aligned(8))) lock_count;

  struct {
   spinlock_t lock;
   int count;
  };
 };
};

extern void lockref_get(struct lockref *);
extern int lockref_put_return(struct lockref *);
extern int lockref_get_not_zero(struct lockref *);
extern int lockref_put_not_zero(struct lockref *);
extern int lockref_put_or_lock(struct lockref *);

extern void lockref_mark_dead(struct lockref *);
extern int lockref_get_not_dead(struct lockref *);

/* Must be called under spinlock for reliable results */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __lockref_is_dead(const struct lockref *l)
{
 return ((int)l->count < 0);
}
# 15 "./include/linux/dcache.h" 2
# 1 "./include/linux/stringhash.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/hash.h" 1


/* Fast hashing routine for ints,  longs and pointers.
   (C) 2002 Nadia Yvette Chambers, IBM */

# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 7 "./include/linux/hash.h" 2


/*
 * The "GOLDEN_RATIO_PRIME" is used in ifs/btrfs/brtfs_inode.h and
 * fs/inode.c.  It's not actually prime any more (the previous primes
 * were actively bad for hashing), but the name remains.
 */
# 24 "./include/linux/hash.h"
/*
 * This hash multiplies the input by a large odd number and takes the
 * high bits.  Since multiplication propagates changes to the most
 * significant end only, it is essential that the high bits of the
 * product be used for the hash value.
 *
 * Chuck Lever verified the effectiveness of this technique:
 * http://www.citi.umich.edu/techreports/reports/citi-tr-00-1.pdf
 *
 * Although a random odd number will do, it turns out that the golden
 * ratio phi = (sqrt(5)-1)/2, or its negative, has particularly nice
 * properties.  (See Knuth vol 3, section 6.4, exercise 9.)
 *
 * These are the negative, (1 - phi) = phi**2 = (3 - sqrt(5))/2,
 * which is very slightly easier to multiply by and makes no
 * difference to the hash distribution.
 */
# 49 "./include/linux/hash.h"
/*
 * The _generic versions exist only so lib/test_hash.c can compare
 * the arch-optimized versions with the generic.
 *
 * Note that if you change these, any <asm/hash.h> that aren't updated
 * to match need to have their HAVE_ARCH_* define values updated so the
 * self-test will not false-positive.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 __hash_32_generic(u32 val)
{
 return val * 0x61C88647;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 hash_32(u32 val, unsigned int bits)
{
 /* High bits are more random, so use them. */
 return __hash_32_generic(val) >> (32 - bits);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 hash_64_generic(u64 val, unsigned int bits)
{

 /* 64x64-bit multiply is efficient on all 64-bit processors */
 return val * 0x61C8864680B583EBull >> (64 - bits);




}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 hash_ptr(const void *ptr, unsigned int bits)
{
 return hash_64_generic((unsigned long)ptr, bits);
}

/* This really should be called fold32_ptr; it does no hashing to speak of. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 hash32_ptr(const void *ptr)
{
 unsigned long val = (unsigned long)ptr;


 val ^= (val >> 32);

 return (u32)val;
}
# 8 "./include/linux/stringhash.h" 2

/*
 * Routines for hashing strings of bytes to a 32-bit hash value.
 *
 * These hash functions are NOT GUARANTEED STABLE between kernel
 * versions, architectures, or even repeated boots of the same kernel.
 * (E.g. they may depend on boot-time hardware detection or be
 * deliberately randomized.)
 *
 * They are also not intended to be secure against collisions caused by
 * malicious inputs; much slower hash functions are required for that.
 *
 * They are optimized for pathname components, meaning short strings.
 * Even if a majority of files have longer names, the dynamic profile of
 * pathname components skews short due to short directory names.
 * (E.g. /usr/lib/libsesquipedalianism.so.3.141.)
 */

/*
 * Version 1: one byte at a time.  Example of use:
 *
 * unsigned long hash = init_name_hash;
 * while (*p)
 *	hash = partial_name_hash(tolower(*p++), hash);
 * hash = end_name_hash(hash);
 *
 * Although this is designed for bytes, fs/hfsplus/unicode.c
 * abuses it to hash 16-bit values.
 */

/* Hash courtesy of the R5 hash in reiserfs modulo sign bits */


/* partial hash update function. Assume roughly 4 bits per character */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
partial_name_hash(unsigned long c, unsigned long prevhash)
{
 return (prevhash + (c << 4) + (c >> 4)) * 11;
}

/*
 * Finally: cut down the number of bits to a int value (and try to avoid
 * losing bits).  This also has the property (wanted by the dcache)
 * that the msbits make a good hash table index.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int end_name_hash(unsigned long hash)
{
 return hash_64_generic(hash, 32);
}

/*
 * Version 2: One word (32 or 64 bits) at a time.
 * If CONFIG_DCACHE_WORD_ACCESS is defined (meaning <asm/word-at-a-time.h>
 * exists, which describes major Linux platforms like x86 and ARM), then
 * this computes a different hash function much faster.
 *
 * If not set, this falls back to a wrapper around the preceding.
 */
extern unsigned int __attribute__((__pure__)) full_name_hash(const void *salt, const char *, unsigned int);

/*
 * A hash_len is a u64 with the hash of a string in the low
 * half and the length in the high half.
 */




/* Return the "hash_len" (hash and length) of a null-terminated string */
extern u64 __attribute__((__pure__)) hashlen_string(const void *salt, const char *name);
# 16 "./include/linux/dcache.h" 2


struct path;
struct file;
struct vfsmount;

/*
 * linux/include/linux/dcache.h
 *
 * Dirent cache data structures
 *
 * (C) Copyright 1997 Thomas Schoebel-Theuer,
 * with heavy changes by Linus Torvalds
 */



/* The hash is always the low bits of hash_len */
# 42 "./include/linux/dcache.h"
/*
 * "quick string" -- eases parameter passing, but more importantly
 * saves "metadata" about the string (ie length and the hash).
 *
 * hash comes first so it snuggles against d_parent in the
 * dentry.
 */
struct qstr {
 union {
  struct {
   u32 hash; u32 len;
  };
  u64 hash_len;
 };
 const unsigned char *name;
};



extern const struct qstr empty_name;
extern const struct qstr slash_name;
extern const struct qstr dotdot_name;

/*
 * Try to keep struct dentry aligned on 64 byte cachelines (this will
 * give reasonable cacheline footprint with larger lines without the
 * large memory footprint increase).
 */
# 82 "./include/linux/dcache.h"
struct dentry {
 /* RCU lookup touched fields */
 unsigned int d_flags; /* protected by d_lock */
 seqcount_spinlock_t d_seq; /* per dentry seqlock */
 struct hlist_bl_node d_hash; /* lookup hash list */
 struct dentry *d_parent; /* parent directory */
 struct qstr d_name;
 struct inode *d_inode; /* Where the name belongs to - NULL is
					 * negative */
 unsigned char d_iname[32 /* 192 bytes */]; /* small names */

 /* Ref lookup also touches following */
 struct lockref d_lockref; /* per-dentry lock and refcount */
 const struct dentry_operations *d_op;
 struct super_block *d_sb; /* The root of the dentry tree */
 unsigned long d_time; /* used by d_revalidate */
 void *d_fsdata; /* fs-specific data */

 union {
  struct list_head d_lru; /* LRU list */
  wait_queue_head_t *d_wait; /* in-lookup ones only */
 };
 struct list_head d_child; /* child of parent list */
 struct list_head d_subdirs; /* our children */
 /*
	 * d_alias and d_rcu can share memory
	 */
 union {
  struct hlist_node d_alias; /* inode alias list */
  struct hlist_bl_node d_in_lookup_hash; /* only for in-lookup ones */
   struct callback_head d_rcu;
 } d_u;
} ;

/*
 * dentry->d_lock spinlock nesting subclasses:
 *
 * 0: normal
 * 1: nested
 */
enum dentry_d_lock_class
{
 DENTRY_D_LOCK_NORMAL, /* implicitly used by plain spin_lock() APIs. */
 DENTRY_D_LOCK_NESTED
};

struct dentry_operations {
 int (*d_revalidate)(struct dentry *, unsigned int);
 int (*d_weak_revalidate)(struct dentry *, unsigned int);
 int (*d_hash)(const struct dentry *, struct qstr *);
 int (*d_compare)(const struct dentry *,
   unsigned int, const char *, const struct qstr *);
 int (*d_delete)(const struct dentry *);
 int (*d_init)(struct dentry *);
 void (*d_release)(struct dentry *);
 void (*d_prune)(struct dentry *);
 void (*d_iput)(struct dentry *, struct inode *);
 char *(*d_dname)(struct dentry *, char *, int);
 struct vfsmount *(*d_automount)(struct path *);
 int (*d_manage)(const struct path *, bool);
 struct dentry *(*d_real)(struct dentry *, const struct inode *);
} __attribute__((__aligned__((1 << (6)))));

/*
 * Locking rules for dentry_operations callbacks are to be found in
 * Documentation/filesystems/locking.rst. Keep it updated!
 *
 * FUrther descriptions are found in Documentation/filesystems/vfs.rst.
 * Keep it updated too!
 */

/* d_flags entries */







     /* This dentry is possibly not currently connected to the dcache tree, in
      * which case its parent will either be itself, or will have this flag as
      * well.  nfsd will not use a dentry with this bit set, but will first
      * endeavour to clear the bit either by discovering that it is connected,
      * or by performing lookup operations.   Any filesystem which supports
      * nfsd_operations MUST have a lookup function which, if it finds a
      * directory inode with a DCACHE_DISCONNECTED dentry, will d_move that
      * dentry into place and return that dentry rather than the passed one,
      * typically using d_splice_alias. */
# 182 "./include/linux/dcache.h"
     /* this dentry has been "silly renamed" and has to be deleted on the last
      * dput() */


     /* Parent inode is watched by some fsnotify listener */
# 216 "./include/linux/dcache.h"
extern seqlock_t rename_lock;

/*
 * These are the low-level FS interfaces to the dcache..
 */
extern void d_instantiate(struct dentry *, struct inode *);
extern void d_instantiate_new(struct dentry *, struct inode *);
extern struct dentry * d_instantiate_unique(struct dentry *, struct inode *);
extern struct dentry * d_instantiate_anon(struct dentry *, struct inode *);
extern void __d_drop(struct dentry *dentry);
extern void d_drop(struct dentry *dentry);
extern void d_delete(struct dentry *);
extern void d_set_d_op(struct dentry *dentry, const struct dentry_operations *op);

/* allocate/de-allocate */
extern struct dentry * d_alloc(struct dentry *, const struct qstr *);
extern struct dentry * d_alloc_anon(struct super_block *);
extern struct dentry * d_alloc_parallel(struct dentry *, const struct qstr *,
     wait_queue_head_t *);
extern struct dentry * d_splice_alias(struct inode *, struct dentry *);
extern struct dentry * d_add_ci(struct dentry *, struct inode *, struct qstr *);
extern bool d_same_name(const struct dentry *dentry, const struct dentry *parent,
   const struct qstr *name);
extern struct dentry * d_exact_alias(struct dentry *, struct inode *);
extern struct dentry *d_find_any_alias(struct inode *inode);
extern struct dentry * d_obtain_alias(struct inode *);
extern struct dentry * d_obtain_root(struct inode *);
extern void shrink_dcache_sb(struct super_block *);
extern void shrink_dcache_parent(struct dentry *);
extern void shrink_dcache_for_umount(struct super_block *);
extern void d_invalidate(struct dentry *);

/* only used at mount-time */
extern struct dentry * d_make_root(struct inode *);

/* <clickety>-<click> the ramfs-type tree */
extern void d_genocide(struct dentry *);

extern void d_tmpfile(struct file *, struct inode *);

extern struct dentry *d_find_alias(struct inode *);
extern void d_prune_aliases(struct inode *);

extern struct dentry *d_find_alias_rcu(struct inode *);

/* test whether we have any submounts in a subdir tree */
extern int path_has_submounts(const struct path *);

/*
 * This adds the entry to the hash queues.
 */
extern void d_rehash(struct dentry *);

extern void d_add(struct dentry *, struct inode *);

/* used for rename() and baskets */
extern void d_move(struct dentry *, struct dentry *);
extern void d_exchange(struct dentry *, struct dentry *);
extern struct dentry *d_ancestor(struct dentry *, struct dentry *);

/* appendix may either be NULL or be used for transname suffixes */
extern struct dentry *d_lookup(const struct dentry *, const struct qstr *);
extern struct dentry *d_hash_and_lookup(struct dentry *, struct qstr *);
extern struct dentry *__d_lookup(const struct dentry *, const struct qstr *);
extern struct dentry *__d_lookup_rcu(const struct dentry *parent,
    const struct qstr *name, unsigned *seq);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned d_count(const struct dentry *dentry)
{
 return dentry->d_lockref.count;
}

/*
 * helper function for dentry_operations.d_dname() members
 */
extern __attribute__((__format__(printf, 3, 4)))
char *dynamic_dname(char *, int, const char *, ...);

extern char *__d_path(const struct path *, const struct path *, char *, int);
extern char *d_absolute_path(const struct path *, char *, int);
extern char *d_path(const struct path *, char *, int);
extern char *dentry_path_raw(const struct dentry *, char *, int);
extern char *dentry_path(const struct dentry *, char *, int);

/* Allocation counts.. */

/**
 *	dget, dget_dlock -	get a reference to a dentry
 *	@dentry: dentry to get a reference to
 *
 *	Given a dentry or %NULL pointer increment the reference count
 *	if appropriate and return the dentry. A dentry will not be
 *	destroyed when it has references.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct dentry *dget_dlock(struct dentry *dentry)
{
 if (dentry)
  dentry->d_lockref.count++;
 return dentry;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct dentry *dget(struct dentry *dentry)
{
 if (dentry)
  lockref_get(&dentry->d_lockref);
 return dentry;
}

extern struct dentry *dget_parent(struct dentry *dentry);

/**
 *	d_unhashed -	is dentry hashed
 *	@dentry: entry to check
 *
 *	Returns true if the dentry passed is not currently hashed.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int d_unhashed(const struct dentry *dentry)
{
 return hlist_bl_unhashed(&dentry->d_hash);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int d_unlinked(const struct dentry *dentry)
{
 return d_unhashed(dentry) && !((dentry) == (dentry)->d_parent);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cant_mount(const struct dentry *dentry)
{
 return (dentry->d_flags & 0x00000100);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dont_mount(struct dentry *dentry)
{
 spin_lock(&dentry->d_lockref.lock);
 dentry->d_flags |= 0x00000100;
 spin_unlock(&dentry->d_lockref.lock);
}

extern void __d_lookup_unhash_wake(struct dentry *dentry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int d_in_lookup(const struct dentry *dentry)
{
 return dentry->d_flags & 0x10000000 /* being looked up (with parent locked shared) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void d_lookup_done(struct dentry *dentry)
{
 if (__builtin_expect(!!(d_in_lookup(dentry)), 0))
  __d_lookup_unhash_wake(dentry);
}

extern void dput(struct dentry *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_managed(const struct dentry *dentry)
{
 return dentry->d_flags & (0x00010000 /* is a mountpoint */|0x00020000 /* handle automount on this dir */|0x00040000 /* manage transit from this dirent */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_mountpoint(const struct dentry *dentry)
{
 return dentry->d_flags & 0x00010000 /* is a mountpoint */;
}

/*
 * Directory cache entry type accessor functions.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned __d_entry_type(const struct dentry *dentry)
{
 return dentry->d_flags & 0x00700000;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_miss(const struct dentry *dentry)
{
 return __d_entry_type(dentry) == 0x00000000 /* Negative dentry (maybe fallthru to nowhere) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_whiteout(const struct dentry *dentry)
{
 return __d_entry_type(dentry) == 0x00100000 /* Whiteout dentry (stop pathwalk) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_can_lookup(const struct dentry *dentry)
{
 return __d_entry_type(dentry) == 0x00200000 /* Normal directory */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_autodir(const struct dentry *dentry)
{
 return __d_entry_type(dentry) == 0x00300000 /* Lookupless directory (presumed automount) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_dir(const struct dentry *dentry)
{
 return d_can_lookup(dentry) || d_is_autodir(dentry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_symlink(const struct dentry *dentry)
{
 return __d_entry_type(dentry) == 0x00600000 /* Symlink (or fallthru to such) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_reg(const struct dentry *dentry)
{
 return __d_entry_type(dentry) == 0x00400000 /* Regular file type (or fallthru to such) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_special(const struct dentry *dentry)
{
 return __d_entry_type(dentry) == 0x00500000 /* Other file type (or fallthru to such) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_file(const struct dentry *dentry)
{
 return d_is_reg(dentry) || d_is_special(dentry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_negative(const struct dentry *dentry)
{
 // TODO: check d_is_whiteout(dentry) also.
 return d_is_miss(dentry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_flags_negative(unsigned flags)
{
 return (flags & 0x00700000) == 0x00000000 /* Negative dentry (maybe fallthru to nowhere) */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_positive(const struct dentry *dentry)
{
 return !d_is_negative(dentry);
}

/**
 * d_really_is_negative - Determine if a dentry is really negative (ignoring fallthroughs)
 * @dentry: The dentry in question
 *
 * Returns true if the dentry represents either an absent name or a name that
 * doesn't map to an inode (ie. ->d_inode is NULL).  The dentry could represent
 * a true miss, a whiteout that isn't represented by a 0,0 chardev or a
 * fallthrough marker in an opaque directory.
 *
 * Note!  (1) This should be used *only* by a filesystem to examine its own
 * dentries.  It should not be used to look at some other filesystem's
 * dentries.  (2) It should also be used in combination with d_inode() to get
 * the inode.  (3) The dentry may have something attached to ->d_lower and the
 * type field of the flags may be set to something other than miss or whiteout.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_really_is_negative(const struct dentry *dentry)
{
 return dentry->d_inode == ((void *)0);
}

/**
 * d_really_is_positive - Determine if a dentry is really positive (ignoring fallthroughs)
 * @dentry: The dentry in question
 *
 * Returns true if the dentry represents a name that maps to an inode
 * (ie. ->d_inode is not NULL).  The dentry might still represent a whiteout if
 * that is represented on medium as a 0,0 chardev.
 *
 * Note!  (1) This should be used *only* by a filesystem to examine its own
 * dentries.  It should not be used to look at some other filesystem's
 * dentries.  (2) It should also be used in combination with d_inode() to get
 * the inode.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_really_is_positive(const struct dentry *dentry)
{
 return dentry->d_inode != ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int simple_positive(const struct dentry *dentry)
{
 return d_really_is_positive(dentry) && !d_unhashed(dentry);
}

extern void d_set_fallthru(struct dentry *dentry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool d_is_fallthru(const struct dentry *dentry)
{
 return dentry->d_flags & 0x01000000 /* Fall through to lower layer */;
}


extern int sysctl_vfs_cache_pressure;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long vfs_pressure_ratio(unsigned long val)
{
 return ( { typeof(val) quot = (val) / (100); typeof(val) rem = (val) % (100); (quot * (sysctl_vfs_cache_pressure)) + ((rem * (sysctl_vfs_cache_pressure)) / (100)); } );
}

/**
 * d_inode - Get the actual inode of this dentry
 * @dentry: The dentry to query
 *
 * This is the helper normal filesystems should use to get at their own inodes
 * in their own dentries and ignore the layering superimposed upon them.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct inode *d_inode(const struct dentry *dentry)
{
 return dentry->d_inode;
}

/**
 * d_inode_rcu - Get the actual inode of this dentry with READ_ONCE()
 * @dentry: The dentry to query
 *
 * This is the helper normal filesystems should use to get at their own inodes
 * in their own dentries and ignore the layering superimposed upon them.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct inode *d_inode_rcu(const struct dentry *dentry)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_236(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(dentry->d_inode) == sizeof(char) || sizeof(dentry->d_inode) == sizeof(short) || sizeof(dentry->d_inode) == sizeof(int) || sizeof(dentry->d_inode) == sizeof(long)) || sizeof(dentry->d_inode) == sizeof(long long))) __compiletime_assert_236(); } while (0); (*(const volatile typeof( _Generic((dentry->d_inode), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (dentry->d_inode))) *)&(dentry->d_inode)); });
# 529 "./include/linux/dcache.h"
}

/**
 * d_backing_inode - Get upper or lower inode we should be using
 * @upper: The upper layer
 *
 * This is the helper that should be used to get at the inode that will be used
 * if this dentry were to be opened as a file.  The inode may be on the upper
 * dentry or it may be on a lower dentry pinned by the upper.
 *
 * Normal filesystems should not use this to access their own inodes.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct inode *d_backing_inode(const struct dentry *upper)
{
 struct inode *inode = upper->d_inode;

 return inode;
}

/**
 * d_backing_dentry - Get upper or lower dentry we should be using
 * @upper: The upper layer
 *
 * This is the helper that should be used to get the dentry of the inode that
 * will be used if this dentry were opened as a file.  It may be the upper
 * dentry or it may be a lower dentry pinned by the upper.
 *
 * Normal filesystems should not use this to access their own dentries.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct dentry *d_backing_dentry(struct dentry *upper)
{
 return upper;
}

/**
 * d_real - Return the real dentry
 * @dentry: the dentry to query
 * @inode: inode to select the dentry from multiple layers (can be NULL)
 *
 * If dentry is on a union/overlay, then return the underlying, real dentry.
 * Otherwise return the dentry itself.
 *
 * See also: Documentation/filesystems/vfs.rst
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct dentry *d_real(struct dentry *dentry,
        const struct inode *inode)
{
 if (__builtin_expect(!!(dentry->d_flags & 0x04000000), 0))
  return dentry->d_op->d_real(dentry, inode);
 else
  return dentry;
}

/**
 * d_real_inode - Return the real inode
 * @dentry: The dentry to query
 *
 * If dentry is on a union/overlay, then return the underlying, real inode.
 * Otherwise return d_inode().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct inode *d_real_inode(const struct dentry *dentry)
{
 /* This usage of d_real() results in const dentry */
 return d_backing_inode(d_real((struct dentry *) dentry, ((void *)0)));
}

struct name_snapshot {
 struct qstr name;
 unsigned char inline_name[32 /* 192 bytes */];
};
void take_dentry_name_snapshot(struct name_snapshot *, struct dentry *);
void release_dentry_name_snapshot(struct name_snapshot *);
# 9 "./include/linux/fs.h" 2
# 1 "./include/linux/path.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



struct dentry;
struct vfsmount;

struct path {
 struct vfsmount *mnt;
 struct dentry *dentry;
} ;

extern void path_get(const struct path *);
extern void path_put(const struct path *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int path_equal(const struct path *path1, const struct path *path2)
{
 return path1->mnt == path2->mnt && path1->dentry == path2->dentry;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void path_put_init(struct path *path)
{
 path_put(path);
 *path = (struct path) { };
}
# 10 "./include/linux/fs.h" 2
# 1 "./include/linux/stat.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/asm/stat.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./arch/arm64/include/generated/uapi/asm/stat.h" 1
# 1 "./include/uapi/asm-generic/stat.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/*
 * Everybody gets this wrong and has to stick with it for all
 * eternity. Hopefully, this version gets used by new architectures
 * so they don't fall into the same traps.
 *
 * stat64 is copied from powerpc64, with explicit padding added.
 * stat is the same structure layout on 64-bit, without the 'long long'
 * types.
 *
 * By convention, 64 bit architectures use the stat interface, while
 * 32 bit architectures use the stat64 interface. Note that we don't
 * provide an __old_kernel_stat here, which new architecture should
 * not have to start with.
 */





struct stat {
 unsigned long st_dev; /* Device.  */
 unsigned long st_ino; /* File serial number.  */
 unsigned int st_mode; /* File mode.  */
 unsigned int st_nlink; /* Link count.  */
 unsigned int st_uid; /* User ID of the file's owner.  */
 unsigned int st_gid; /* Group ID of the file's group. */
 unsigned long st_rdev; /* Device number, if device.  */
 unsigned long __pad1;
 long st_size; /* Size of file, in bytes.  */
 int st_blksize; /* Optimal block size for I/O.  */
 int __pad2;
 long st_blocks; /* Number 512-byte blocks allocated. */
 long st_atime; /* Time of last access.  */
 unsigned long st_atime_nsec;
 long st_mtime; /* Time of last modification.  */
 unsigned long st_mtime_nsec;
 long st_ctime; /* Time of last status change.  */
 unsigned long st_ctime_nsec;
 unsigned int __unused4;
 unsigned int __unused5;
};

/* This matches struct stat64 in glibc2.1. Only used for 32 bit. */
# 2 "./arch/arm64/include/generated/uapi/asm/stat.h" 2
# 9 "./arch/arm64/include/asm/stat.h" 2



# 1 "./include/linux/time.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







extern struct timezone sys_tz;

int get_timespec64(struct timespec64 *ts,
  const struct __kernel_timespec /* nothing */ *uts);
int put_timespec64(const struct timespec64 *ts,
  struct __kernel_timespec /* nothing */ *uts);
int get_itimerspec64(struct itimerspec64 *it,
   const struct __kernel_itimerspec /* nothing */ *uit);
int put_itimerspec64(const struct itimerspec64 *it,
   struct __kernel_itimerspec /* nothing */ *uit);

extern time64_t mktime64(const unsigned int year, const unsigned int mon,
   const unsigned int day, const unsigned int hour,
   const unsigned int min, const unsigned int sec);


extern void clear_itimer(void);




extern long do_utimes(int dfd, const char /* nothing */ *filename, struct timespec64 *times, int flags);

/*
 * Similar to the struct tm in userspace <time.h>, but it needs to be here so
 * that the kernel source is self contained.
 */
struct tm {
 /*
	 * the number of seconds after the minute, normally in the range
	 * 0 to 59, but can be up to 60 to allow for leap seconds
	 */
 int tm_sec;
 /* the number of minutes after the hour, in the range 0 to 59*/
 int tm_min;
 /* the number of hours past midnight, in the range 0 to 23 */
 int tm_hour;
 /* the day of the month, in the range 1 to 31 */
 int tm_mday;
 /* the number of months since January, in the range 0 to 11 */
 int tm_mon;
 /* the number of years since 1900 */
 long tm_year;
 /* the number of days since Sunday, in the range 0 to 6 */
 int tm_wday;
 /* the number of days since January 1, in the range 0 to 365 */
 int tm_yday;
};

void time64_to_tm(time64_t totalsecs, int offset, struct tm *result);

# 1 "./include/linux/time32.h" 1


/*
 * These are all interfaces based on the old time_t definition
 * that overflows in 2038 on 32-bit architectures. New code
 * should use the replacements based on time64_t and timespec64.
 *
 * Any interfaces in here that become unused as we migrate
 * code to time64_t should get removed.
 */


# 1 "./include/linux/timex.h" 1
/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/

/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */



# 1 "./include/uapi/linux/timex.h" 1
/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/

/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */



# 1 "./include/linux/time.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 57 "./include/uapi/linux/timex.h" 2
# 97 "./include/uapi/linux/timex.h"
struct __kernel_timex_timeval {
 __kernel_time64_t tv_sec;
 long long tv_usec;
};

struct __kernel_timex {
 unsigned int modes; /* mode selector */
 int :32; /* pad */
 long long offset; /* time offset (usec) */
 long long freq; /* frequency offset (scaled ppm) */
 long long maxerror;/* maximum error (usec) */
 long long esterror;/* estimated error (usec) */
 int status; /* clock command/status */
 int :32; /* pad */
 long long constant;/* pll time constant */
 long long precision;/* clock precision (usec) (read only) */
 long long tolerance;/* clock frequency tolerance (ppm)
				   * (read only)
				   */
 struct __kernel_timex_timeval time; /* (read only, except for ADJ_SETOFFSET) */
 long long tick; /* (modified) usecs between clock ticks */

 long long ppsfreq;/* pps frequency (scaled ppm) (ro) */
 long long jitter; /* pps jitter (us) (ro) */
 int shift; /* interval duration (s) (shift) (ro) */
 int :32; /* pad */
 long long stabil; /* pps stability (scaled ppm) (ro) */
 long long jitcnt; /* jitter limit exceeded (ro) */
 long long calcnt; /* calibration intervals (ro) */
 long long errcnt; /* calibration errors (ro) */
 long long stbcnt; /* stability limit exceeded (ro) */

 int tai; /* TAI offset (ro) */

 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32;
};

/*
 * Mode codes (timex.mode)
 */
# 156 "./include/uapi/linux/timex.h"
/* NTP userland likes the MOD_ prefix better */
# 168 "./include/uapi/linux/timex.h"
/*
 * Status codes (timex.status)
 */
# 191 "./include/uapi/linux/timex.h"
/* read-only bits */



/*
 * Clock states (time_state)
 */
# 57 "./include/linux/timex.h" 2








unsigned long random_get_entropy_fallback(void);

# 1 "./arch/arm64/include/asm/timex.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./arch/arm64/include/asm/arch_timer.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * arch/arm64/include/asm/arch_timer.h
 *
 * Copyright (C) 2012 ARM Ltd.
 * Author: Marc Zyngier <marc.zyngier@arm.com>
 */
# 21 "./arch/arm64/include/asm/arch_timer.h"
# 1 "./include/clocksource/arm_arch_timer.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */




# 1 "./include/linux/timecounter.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * linux/include/linux/timecounter.h
 *
 * based on code that migrated away from
 * linux/include/linux/clocksource.h
 */





/* simplify initialization of mask field */


/**
 * struct cyclecounter - hardware abstraction for a free running counter
 *	Provides completely state-free accessors to the underlying hardware.
 *	Depending on which hardware it reads, the cycle counter may wrap
 *	around quickly. Locking rules (if necessary) have to be defined
 *	by the implementor and user of specific instances of this API.
 *
 * @read:		returns the current cycle value
 * @mask:		bitmask for two's complement
 *			subtraction of non 64 bit counters,
 *			see CYCLECOUNTER_MASK() helper macro
 * @mult:		cycle to nanosecond multiplier
 * @shift:		cycle to nanosecond divisor (power of two)
 */
struct cyclecounter {
 u64 (*read)(const struct cyclecounter *cc);
 u64 mask;
 u32 mult;
 u32 shift;
};

/**
 * struct timecounter - layer above a %struct cyclecounter which counts nanoseconds
 *	Contains the state needed by timecounter_read() to detect
 *	cycle counter wrap around. Initialize with
 *	timecounter_init(). Also used to convert cycle counts into the
 *	corresponding nanosecond counts with timecounter_cyc2time(). Users
 *	of this code are responsible for initializing the underlying
 *	cycle counter hardware, locking issues and reading the time
 *	more often than the cycle counter wraps around. The nanosecond
 *	counter will only wrap around after ~585 years.
 *
 * @cc:			the cycle counter used by this instance
 * @cycle_last:		most recent cycle counter value seen by
 *			timecounter_read()
 * @nsec:		continuously increasing count
 * @mask:		bit mask for maintaining the 'frac' field
 * @frac:		accumulated fractional nanoseconds
 */
struct timecounter {
 const struct cyclecounter *cc;
 u64 cycle_last;
 u64 nsec;
 u64 mask;
 u64 frac;
};

/**
 * cyclecounter_cyc2ns - converts cycle counter cycles to nanoseconds
 * @cc:		Pointer to cycle counter.
 * @cycles:	Cycles
 * @mask:	bit mask for maintaining the 'frac' field
 * @frac:	pointer to storage for the fractional nanoseconds.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 cyclecounter_cyc2ns(const struct cyclecounter *cc,
          u64 cycles, u64 mask, u64 *frac)
{
 u64 ns = (u64) cycles;

 ns = (ns * cc->mult) + *frac;
 *frac = ns & mask;
 return ns >> cc->shift;
}

/**
 * timecounter_adjtime - Shifts the time of the clock.
 * @delta:	Desired change in nanoseconds.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void timecounter_adjtime(struct timecounter *tc, s64 delta)
{
 tc->nsec += delta;
}

/**
 * timecounter_init - initialize a time counter
 * @tc:			Pointer to time counter which is to be initialized/reset
 * @cc:			A cycle counter, ready to be used.
 * @start_tstamp:	Arbitrary initial time stamp.
 *
 * After this call the current cycle register (roughly) corresponds to
 * the initial time stamp. Every call to timecounter_read() increments
 * the time stamp counter by the number of elapsed nanoseconds.
 */
extern void timecounter_init(struct timecounter *tc,
        const struct cyclecounter *cc,
        u64 start_tstamp);

/**
 * timecounter_read - return nanoseconds elapsed since timecounter_init()
 *                    plus the initial time stamp
 * @tc:          Pointer to time counter.
 *
 * In other words, keeps track of time since the same epoch as
 * the function which generated the initial time stamp.
 */
extern u64 timecounter_read(struct timecounter *tc);

/**
 * timecounter_cyc2time - convert a cycle counter to same
 *                        time base as values returned by
 *                        timecounter_read()
 * @tc:		Pointer to time counter.
 * @cycle_tstamp:	a value returned by tc->cc->read()
 *
 * Cycle counts that are converted correctly as long as they
 * fall into the interval [-1/2 max cycle count, +1/2 max cycle count],
 * with "max cycle count" == cs->mask+1.
 *
 * This allows conversion of cycle counter values which were generated
 * in the past.
 */
extern u64 timecounter_cyc2time(const struct timecounter *tc,
    u64 cycle_tstamp);
# 10 "./include/clocksource/arm_arch_timer.h" 2
# 25 "./include/clocksource/arm_arch_timer.h"
enum arch_timer_reg {
 ARCH_TIMER_REG_CTRL,
 ARCH_TIMER_REG_CVAL,
};

enum arch_timer_ppi_nr {
 ARCH_TIMER_PHYS_SECURE_PPI,
 ARCH_TIMER_PHYS_NONSECURE_PPI,
 ARCH_TIMER_VIRT_PPI,
 ARCH_TIMER_HYP_PPI,
 ARCH_TIMER_HYP_VIRT_PPI,
 ARCH_TIMER_MAX_TIMER_PPI
};

enum arch_timer_spi_nr {
 ARCH_TIMER_PHYS_SPI,
 ARCH_TIMER_VIRT_SPI,
 ARCH_TIMER_MAX_TIMER_SPI
};
# 65 "./include/clocksource/arm_arch_timer.h"
struct arch_timer_kvm_info {
 struct timecounter timecounter;
 int virtual_irq;
 int physical_irq;
};

struct arch_timer_mem_frame {
 bool valid;
 phys_addr_t cntbase;
 size_t size;
 int phys_irq;
 int virt_irq;
};

struct arch_timer_mem {
 phys_addr_t cntctlbase;
 size_t size;
 struct arch_timer_mem_frame frame[8];
};



extern u32 arch_timer_get_rate(void);
extern u64 (*arch_timer_read_counter)(void);
extern struct arch_timer_kvm_info *arch_timer_get_kvm_info(void);
extern bool arch_timer_evtstrm_available(void);
# 22 "./arch/arm64/include/asm/arch_timer.h" 2
# 43 "./arch/arm64/include/asm/arch_timer.h"
enum arch_timer_erratum_match_type {
 ate_match_dt,
 ate_match_local_cap_id,
 ate_match_acpi_oem_info,
};

struct clock_event_device;

struct arch_timer_erratum_workaround {
 enum arch_timer_erratum_match_type match_type;
 const void *id;
 const char *desc;
 u64 (*read_cntpct_el0)(void);
 u64 (*read_cntvct_el0)(void);
 int (*set_next_event_phys)(unsigned long, struct clock_event_device *);
 int (*set_next_event_virt)(unsigned long, struct clock_event_device *);
 bool disable_compat_vdso;
};

extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(const struct arch_timer_erratum_workaround *) timer_unstable_counter_workaround;


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__no_instrument_function__)) u64 arch_timer_read_cntpct_el0(void)
{
 u64 cnt;

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "isb\n mrs %0, cntpct_el0" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "19" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "nop\n" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((14) << 12) | ((0) << 8) | ((5) << 5))" "\n" "	.purgem	mrs_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (cnt));

 return cnt;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__no_instrument_function__)) u64 arch_timer_read_cntvct_el0(void)
{
 u64 cnt;

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "isb\n mrs %0, cntvct_el0" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "19" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "nop\n" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((14) << 12) | ((0) << 8) | ((6) << 5))" "\n" "	.purgem	mrs_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (cnt));

 return cnt;
}
# 100 "./arch/arm64/include/asm/arch_timer.h"
/*
 * These register accessors are marked inline so the compiler can
 * nicely work out which register we want, and chuck away the rest of
 * the code.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__))
void arch_timer_reg_write_cp15(int access, enum arch_timer_reg reg, u64 val)
{
 if (access == 0) {
  switch (reg) {
  case ARCH_TIMER_REG_CTRL:
   do { u64 __val = (u64)(val); asm volatile("msr " "cntp_ctl_el0" ", %x0" : : "rZ" (__val)); } while (0);
   asm volatile("isb" : : : "memory");
   break;
  case ARCH_TIMER_REG_CVAL:
   do { u64 __val = (u64)(val); asm volatile("msr " "cntp_cval_el0" ", %x0" : : "rZ" (__val)); } while (0);
   break;
  default:
   do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_237(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_237(); } while (0);
# 119 "./arch/arm64/include/asm/arch_timer.h"
  }
 } else if (access == 1) {
  switch (reg) {
  case ARCH_TIMER_REG_CTRL:
   do { u64 __val = (u64)(val); asm volatile("msr " "cntv_ctl_el0" ", %x0" : : "rZ" (__val)); } while (0);
   asm volatile("isb" : : : "memory");
   break;
  case ARCH_TIMER_REG_CVAL:
   do { u64 __val = (u64)(val); asm volatile("msr " "cntv_cval_el0" ", %x0" : : "rZ" (__val)); } while (0);
   break;
  default:
   do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_238(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_238(); } while (0);
# 131 "./arch/arm64/include/asm/arch_timer.h"
  }
 } else {
  do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_239(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_239(); } while (0);
# 134 "./arch/arm64/include/asm/arch_timer.h"
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__))
u64 arch_timer_reg_read_cp15(int access, enum arch_timer_reg reg)
{
 if (access == 0) {
  switch (reg) {
  case ARCH_TIMER_REG_CTRL:
   return ({ u64 __val; asm volatile("mrs %0, " "cntp_ctl_el0" : "=r" (__val)); __val; });
  default:
   do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_240(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_240(); } while (0);
# 146 "./arch/arm64/include/asm/arch_timer.h"
  }
 } else if (access == 1) {
  switch (reg) {
  case ARCH_TIMER_REG_CTRL:
   return ({ u64 __val; asm volatile("mrs %0, " "cntv_ctl_el0" : "=r" (__val)); __val; });
  default:
   do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_241(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_241(); } while (0);
# 153 "./arch/arm64/include/asm/arch_timer.h"
  }
 }

 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_242(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_242(); } while (0);
# 157 "./arch/arm64/include/asm/arch_timer.h"
 do { ; __builtin_unreachable(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 arch_timer_get_cntfrq(void)
{
 return ({ u64 __val; asm volatile("mrs %0, " "cntfrq_el0" : "=r" (__val)); __val; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 arch_timer_get_cntkctl(void)
{
 return ({ u64 __val; asm volatile("mrs %0, " "cntkctl_el1" : "=r" (__val)); __val; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_timer_set_cntkctl(u32 cntkctl)
{
 do { u64 __val = (u64)(cntkctl); asm volatile("msr " "cntkctl_el1" ", %x0" : : "rZ" (__val)); } while (0);
 asm volatile("isb" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __arch_counter_get_cntpct_stable(void)
{
 u64 cnt;

 cnt = ({ u64 _val; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); _val = ({ const struct arch_timer_erratum_workaround *__wa; __wa = ({ __this_cpu_preempt_check("read"); ({ typeof(timer_unstable_counter_workaround) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(timer_unstable_counter_workaround)) { case 1: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); }); (__wa && __wa->read_cntpct_el0) ? ({ asm volatile("isb" : : : "memory"); __wa->read_cntpct_el0;}) : arch_timer_read_cntpct_el0; })(); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); _val; });
 do { u64 tmp, _val = (cnt); asm volatile( "	eor	%0, %1, %1\n" "	add	%0, sp, %0\n" "	ldr	xzr, [%0]" : "=r" (tmp) : "r" (_val)); } while (0);
 return cnt;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __arch_counter_get_cntpct(void)
{
 u64 cnt;

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "isb\n mrs %0, cntpct_el0" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "19" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "nop\n" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((14) << 12) | ((0) << 8) | ((5) << 5))" "\n" "	.purgem	mrs_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (cnt));
 do { u64 tmp, _val = (cnt); asm volatile( "	eor	%0, %1, %1\n" "	add	%0, sp, %0\n" "	ldr	xzr, [%0]" : "=r" (tmp) : "r" (_val)); } while (0);
 return cnt;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __arch_counter_get_cntvct_stable(void)
{
 u64 cnt;

 cnt = ({ u64 _val; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); _val = ({ const struct arch_timer_erratum_workaround *__wa; __wa = ({ __this_cpu_preempt_check("read"); ({ typeof(timer_unstable_counter_workaround) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(timer_unstable_counter_workaround)) { case 1: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(timer_unstable_counter_workaround)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround))); (typeof((typeof(*(&(timer_unstable_counter_workaround))) *)(&(timer_unstable_counter_workaround)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); }); (__wa && __wa->read_cntvct_el0) ? ({ asm volatile("isb" : : : "memory"); __wa->read_cntvct_el0;}) : arch_timer_read_cntvct_el0; })(); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); _val; });
 do { u64 tmp, _val = (cnt); asm volatile( "	eor	%0, %1, %1\n" "	add	%0, sp, %0\n" "	ldr	xzr, [%0]" : "=r" (tmp) : "r" (_val)); } while (0);
 return cnt;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u64 __arch_counter_get_cntvct(void)
{
 u64 cnt;

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "isb\n mrs %0, cntvct_el0" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "19" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "nop\n" "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" "	.equ	.L__gpr_num_x\\num, \\num\n" "	.equ	.L__gpr_num_w\\num, \\num\n" "	.endr\n" "	.equ	.L__gpr_num_xzr, 31\n" "	.equ	.L__gpr_num_wzr, 31\n" "	.macro	mrs_s, rt, sreg\n" ".inst " "(0xd5200000|(\\sreg)|(.L__gpr_num_\\rt))" "\n\t" "	.endm\n" "	mrs_s " "%0" ", " "(((3) << 19) | ((3) << 16) | ((14) << 12) | ((0) << 8) | ((6) << 5))" "\n" "	.purgem	mrs_s\n" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (cnt));
 do { u64 tmp, _val = (cnt); asm volatile( "	eor	%0, %1, %1\n" "	add	%0, sp, %0\n" "	ldr	xzr, [%0]" : "=r" (tmp) : "r" (_val)); } while (0);
 return cnt;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_timer_arch_init(void)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_timer_set_evtstrm_feature(void)
{
 cpu_set_feature(( __builtin_constant_p((1 << 2)) ? ( ((1 << 2)) < 2 ? 0 : ((1 << 2)) & (1ULL << 63) ? 63 : ((1 << 2)) & (1ULL << 62) ? 62 : ((1 << 2)) & (1ULL << 61) ? 61 : ((1 << 2)) & (1ULL << 60) ? 60 : ((1 << 2)) & (1ULL << 59) ? 59 : ((1 << 2)) & (1ULL << 58) ? 58 : ((1 << 2)) & (1ULL << 57) ? 57 : ((1 << 2)) & (1ULL << 56) ? 56 : ((1 << 2)) & (1ULL << 55) ? 55 : ((1 << 2)) & (1ULL << 54) ? 54 : ((1 << 2)) & (1ULL << 53) ? 53 : ((1 << 2)) & (1ULL << 52) ? 52 : ((1 << 2)) & (1ULL << 51) ? 51 : ((1 << 2)) & (1ULL << 50) ? 50 : ((1 << 2)) & (1ULL << 49) ? 49 : ((1 << 2)) & (1ULL << 48) ? 48 : ((1 << 2)) & (1ULL << 47) ? 47 : ((1 << 2)) & (1ULL << 46) ? 46 : ((1 << 2)) & (1ULL << 45) ? 45 : ((1 << 2)) & (1ULL << 44) ? 44 : ((1 << 2)) & (1ULL << 43) ? 43 : ((1 << 2)) & (1ULL << 42) ? 42 : ((1 << 2)) & (1ULL << 41) ? 41 : ((1 << 2)) & (1ULL << 40) ? 40 : ((1 << 2)) & (1ULL << 39) ? 39 : ((1 << 2)) & (1ULL << 38) ? 38 : ((1 << 2)) & (1ULL << 37) ? 37 : ((1 << 2)) & (1ULL << 36) ? 36 : ((1 << 2)) & (1ULL << 35) ? 35 : ((1 << 2)) & (1ULL << 34) ? 34 : ((1 << 2)) & (1ULL << 33) ? 33 : ((1 << 2)) & (1ULL << 32) ? 32 : ((1 << 2)) & (1ULL << 31) ? 31 : ((1 << 2)) & (1ULL << 30) ? 30 : ((1 << 2)) & (1ULL << 29) ? 29 : ((1 << 2)) & (1ULL << 28) ? 28 : ((1 << 2)) & (1ULL << 27) ? 27 : ((1 << 2)) & (1ULL << 26) ? 26 : ((1 << 2)) & (1ULL << 25) ? 25 : ((1 << 2)) & (1ULL << 24) ? 24 : ((1 << 2)) & (1ULL << 23) ? 23 : ((1 << 2)) & (1ULL << 22) ? 22 : ((1 << 2)) & (1ULL << 21) ? 21 : ((1 << 2)) & (1ULL << 20) ? 20 : ((1 << 2)) & (1ULL << 19) ? 19 : ((1 << 2)) & (1ULL << 18) ? 18 : ((1 << 2)) & (1ULL << 17) ? 17 : ((1 << 2)) & (1ULL << 16) ? 16 : ((1 << 2)) & (1ULL << 15) ? 15 : ((1 << 2)) & (1ULL << 14) ? 14 : ((1 << 2)) & (1ULL << 13) ? 13 : ((1 << 2)) & (1ULL << 12) ? 12 : ((1 << 2)) & (1ULL << 11) ? 11 : ((1 << 2)) & (1ULL << 10) ? 10 : ((1 << 2)) & (1ULL << 9) ? 9 : ((1 << 2)) & (1ULL << 8) ? 8 : ((1 << 2)) & (1ULL << 7) ? 7 : ((1 << 2)) & (1ULL << 6) ? 6 : ((1 << 2)) & (1ULL << 5) ? 5 : ((1 << 2)) & (1ULL << 4) ? 4 : ((1 << 2)) & (1ULL << 3) ? 3 : ((1 << 2)) & (1ULL << 2) ? 2 : 1) : -1));

 compat_elf_hwcap |= (1 << 21);

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_timer_have_evtstrm_feature(void)
{
 return cpu_have_feature(( __builtin_constant_p((1 << 2)) ? ( ((1 << 2)) < 2 ? 0 : ((1 << 2)) & (1ULL << 63) ? 63 : ((1 << 2)) & (1ULL << 62) ? 62 : ((1 << 2)) & (1ULL << 61) ? 61 : ((1 << 2)) & (1ULL << 60) ? 60 : ((1 << 2)) & (1ULL << 59) ? 59 : ((1 << 2)) & (1ULL << 58) ? 58 : ((1 << 2)) & (1ULL << 57) ? 57 : ((1 << 2)) & (1ULL << 56) ? 56 : ((1 << 2)) & (1ULL << 55) ? 55 : ((1 << 2)) & (1ULL << 54) ? 54 : ((1 << 2)) & (1ULL << 53) ? 53 : ((1 << 2)) & (1ULL << 52) ? 52 : ((1 << 2)) & (1ULL << 51) ? 51 : ((1 << 2)) & (1ULL << 50) ? 50 : ((1 << 2)) & (1ULL << 49) ? 49 : ((1 << 2)) & (1ULL << 48) ? 48 : ((1 << 2)) & (1ULL << 47) ? 47 : ((1 << 2)) & (1ULL << 46) ? 46 : ((1 << 2)) & (1ULL << 45) ? 45 : ((1 << 2)) & (1ULL << 44) ? 44 : ((1 << 2)) & (1ULL << 43) ? 43 : ((1 << 2)) & (1ULL << 42) ? 42 : ((1 << 2)) & (1ULL << 41) ? 41 : ((1 << 2)) & (1ULL << 40) ? 40 : ((1 << 2)) & (1ULL << 39) ? 39 : ((1 << 2)) & (1ULL << 38) ? 38 : ((1 << 2)) & (1ULL << 37) ? 37 : ((1 << 2)) & (1ULL << 36) ? 36 : ((1 << 2)) & (1ULL << 35) ? 35 : ((1 << 2)) & (1ULL << 34) ? 34 : ((1 << 2)) & (1ULL << 33) ? 33 : ((1 << 2)) & (1ULL << 32) ? 32 : ((1 << 2)) & (1ULL << 31) ? 31 : ((1 << 2)) & (1ULL << 30) ? 30 : ((1 << 2)) & (1ULL << 29) ? 29 : ((1 << 2)) & (1ULL << 28) ? 28 : ((1 << 2)) & (1ULL << 27) ? 27 : ((1 << 2)) & (1ULL << 26) ? 26 : ((1 << 2)) & (1ULL << 25) ? 25 : ((1 << 2)) & (1ULL << 24) ? 24 : ((1 << 2)) & (1ULL << 23) ? 23 : ((1 << 2)) & (1ULL << 22) ? 22 : ((1 << 2)) & (1ULL << 21) ? 21 : ((1 << 2)) & (1ULL << 20) ? 20 : ((1 << 2)) & (1ULL << 19) ? 19 : ((1 << 2)) & (1ULL << 18) ? 18 : ((1 << 2)) & (1ULL << 17) ? 17 : ((1 << 2)) & (1ULL << 16) ? 16 : ((1 << 2)) & (1ULL << 15) ? 15 : ((1 << 2)) & (1ULL << 14) ? 14 : ((1 << 2)) & (1ULL << 13) ? 13 : ((1 << 2)) & (1ULL << 12) ? 12 : ((1 << 2)) & (1ULL << 11) ? 11 : ((1 << 2)) & (1ULL << 10) ? 10 : ((1 << 2)) & (1ULL << 9) ? 9 : ((1 << 2)) & (1ULL << 8) ? 8 : ((1 << 2)) & (1ULL << 7) ? 7 : ((1 << 2)) & (1ULL << 6) ? 6 : ((1 << 2)) & (1ULL << 5) ? 5 : ((1 << 2)) & (1ULL << 4) ? 4 : ((1 << 2)) & (1ULL << 3) ? 3 : ((1 << 2)) & (1ULL << 2) ? 2 : 1) : -1));
}
# 9 "./arch/arm64/include/asm/timex.h" 2

/*
 * Use the current timer as a cycle counter since this is what we use for
 * the delay loop.
 */


# 1 "./include/asm-generic/timex.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * If you have a cycle counter, return the value here.
 */
typedef unsigned long cycles_t;







/*
 * Architectures are encouraged to implement read_current_timer
 * and define this in order to avoid the expensive delay loop
 * calibration during boot.
 */
# 17 "./arch/arm64/include/asm/timex.h" 2
# 68 "./include/linux/timex.h" 2


/*
 * The random_get_entropy() function is used by the /dev/random driver
 * in order to extract entropy via the relative unpredictability of
 * when an interrupt takes places versus a high speed, fine-grained
 * timing source or cycle counter.  Since it will be occurred on every
 * single interrupt, it must have a very low cost/overhead.
 *
 * By default we use get_cycles() for this purpose, but individual
 * architectures may override this in their asm/timex.h header file.
 * If a given arch does not have get_cycles(), then we fallback to
 * using random_get_entropy_fallback().
 */







/*
 * SHIFT_PLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in PLL mode.
 * It also used in dampening the offset correction, to define how
 * much of the current value in time_offset we correct for each
 * second. Changing this value changes the stiffness of the ntp
 * adjustment code. A lower value makes it more flexible, reducing
 * NTP convergence time. A higher value makes it stiffer, increasing
 * convergence time, but making the clock more stable.
 *
 * In David Mills' nanokernel reference implementation SHIFT_PLL is 4.
 * However this seems to increase convergence time much too long.
 *
 * https://lists.ntp.org/pipermail/hackers/2008-January/003487.html
 *
 * In the above mailing list discussion, it seems the value of 4
 * was appropriate for other Unix systems with HZ=100, and that
 * SHIFT_PLL should be decreased as HZ increases. However, Linux's
 * clock steering implementation is HZ independent.
 *
 * Through experimentation, a SHIFT_PLL value of 2 was found to allow
 * for fast convergence (very similar to the NTPv3 code used prior to
 * v2.6.19), with good clock stability.
 *
 *
 * SHIFT_FLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in FLL mode.
 * In David Mills' nanokernel reference implementation SHIFT_FLL is 2.
 *
 * MAXTC establishes the maximum time constant of the PLL.
 */




/*
 * SHIFT_USEC defines the scaling (shift) of the time_freq and
 * time_tolerance variables, which represent the current frequency
 * offset and maximum frequency tolerance.
 */
# 142 "./include/linux/timex.h"
/*
 * kernel variables
 * Note: maximum error = NTP sync distance = dispersion + delay / 2;
 * estimated error = NTP dispersion.
 */
extern unsigned long tick_usec; /* USER_HZ period (usec) */
extern unsigned long tick_nsec; /* SHIFTED_HZ period (nsec) */

/* Required to safely shift negative values */
# 162 "./include/linux/timex.h"
extern int do_adjtimex(struct __kernel_timex *);
extern int do_clock_adjtime(const clockid_t which_clock, struct __kernel_timex * ktx);

extern void hardpps(const struct timespec64 *, const struct timespec64 *);

int read_current_timer(unsigned long *timer_val);

/* The clock frequency of the i8253/i8254 PIT */
# 14 "./include/linux/time32.h" 2

# 1 "./include/vdso/time32.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



typedef s32 old_time32_t;

struct old_timespec32 {
 old_time32_t tv_sec;
 s32 tv_nsec;
};

struct old_timeval32 {
 old_time32_t tv_sec;
 s32 tv_usec;
};
# 16 "./include/linux/time32.h" 2

struct old_itimerspec32 {
 struct old_timespec32 it_interval;
 struct old_timespec32 it_value;
};

struct old_utimbuf32 {
 old_time32_t actime;
 old_time32_t modtime;
};

struct old_timex32 {
 u32 modes;
 s32 offset;
 s32 freq;
 s32 maxerror;
 s32 esterror;
 s32 status;
 s32 constant;
 s32 precision;
 s32 tolerance;
 struct old_timeval32 time;
 s32 tick;
 s32 ppsfreq;
 s32 jitter;
 s32 shift;
 s32 stabil;
 s32 jitcnt;
 s32 calcnt;
 s32 errcnt;
 s32 stbcnt;
 s32 tai;

 s32:32; s32:32; s32:32; s32:32;
 s32:32; s32:32; s32:32; s32:32;
 s32:32; s32:32; s32:32;
};

extern int get_old_timespec32(struct timespec64 *, const void /* nothing */ *);
extern int put_old_timespec32(const struct timespec64 *, void /* nothing */ *);
extern int get_old_itimerspec32(struct itimerspec64 *its,
   const struct old_itimerspec32 /* nothing */ *uits);
extern int put_old_itimerspec32(const struct itimerspec64 *its,
   struct old_itimerspec32 /* nothing */ *uits);
struct __kernel_timex;
int get_old_timex32(struct __kernel_timex *, const struct old_timex32 /* nothing */ *);
int put_old_timex32(struct old_timex32 /* nothing */ *, const struct __kernel_timex *);

/**
 * ns_to_kernel_old_timeval - Convert nanoseconds to timeval
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timeval representation of the nsec parameter.
 */
extern struct __kernel_old_timeval ns_to_kernel_old_timeval(s64 nsec);
# 61 "./include/linux/time.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool itimerspec64_valid(const struct itimerspec64 *its)
{
 if (!timespec64_valid(&(its->it_interval)) ||
  !timespec64_valid(&(its->it_value)))
  return false;

 return true;
}

/**
 * time_after32 - compare two 32-bit relative times
 * @a:	the time which may be after @b
 * @b:	the time which may be before @a
 *
 * time_after32(a, b) returns true if the time @a is after time @b.
 * time_before32(b, a) returns true if the time @b is before time @a.
 *
 * Similar to time_after(), compare two 32-bit timestamps for relative
 * times.  This is useful for comparing 32-bit seconds values that can't
 * be converted to 64-bit values (e.g. due to disk format or wire protocol
 * issues) when it is known that the times are less than 68 years apart.
 */



/**
 * time_between32 - check if a 32-bit timestamp is within a given time range
 * @t:	the time which may be within [l,h]
 * @l:	the lower bound of the range
 * @h:	the higher bound of the range
 *
 * time_before32(t, l, h) returns true if @l <= @t <= @h. All operands are
 * treated as 32-bit integers.
 *
 * Equivalent to !(time_before32(@t, @l) || time_after32(@t, @h)).
 */


# 1 "./include/vdso/time.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





struct timens_offset {
 s64 sec;
 u64 nsec;
};
# 101 "./include/linux/time.h" 2
# 13 "./arch/arm64/include/asm/stat.h" 2
# 1 "./arch/arm64/include/asm/compat.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */




typedef u16 compat_mode_t;


typedef u16 __compat_uid_t;
typedef u16 __compat_gid_t;


typedef u16 compat_ipc_pid_t;



# 1 "./include/asm-generic/compat.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 29 "./include/asm-generic/compat.h"
/* These types are common across all compat ABIs */
typedef u32 compat_size_t;
typedef s32 compat_ssize_t;
typedef s32 compat_clock_t;
typedef s32 compat_pid_t;
typedef u32 compat_ino_t;
typedef s32 compat_off_t;
typedef s64 compat_loff_t;
typedef s32 compat_daddr_t;
typedef s32 compat_timer_t;
typedef s32 compat_key_t;
typedef s16 compat_short_t;
typedef s32 compat_int_t;
typedef s32 compat_long_t;
typedef u16 compat_ushort_t;
typedef u32 compat_uint_t;
typedef u32 compat_ulong_t;
typedef u32 compat_uptr_t;
typedef u32 compat_caddr_t;
typedef u32 compat_aio_context_t;
typedef u32 compat_old_sigset_t;







typedef u32 __compat_uid32_t;
typedef u32 __compat_gid32_t;
# 69 "./include/asm-generic/compat.h"
typedef s64 compat_s64;
typedef u64 compat_u64;



typedef u32 compat_sigset_word;





typedef u32 compat_dev_t;







typedef __kernel_fsid_t compat_fsid_t;
# 109 "./include/asm-generic/compat.h"
struct compat_ipc64_perm {
 compat_key_t key;
 __compat_uid32_t uid;
 __compat_gid32_t gid;
 __compat_uid32_t cuid;
 __compat_gid32_t cgid;
 compat_mode_t mode;
 unsigned char __pad1[4 - sizeof(compat_mode_t)];
 compat_ushort_t seq;
 compat_ushort_t __pad2;
 compat_ulong_t unused1;
 compat_ulong_t unused2;
};

struct compat_semid64_ds {
 struct compat_ipc64_perm sem_perm;
 compat_ulong_t sem_otime;
 compat_ulong_t sem_otime_high;
 compat_ulong_t sem_ctime;
 compat_ulong_t sem_ctime_high;
 compat_ulong_t sem_nsems;
 compat_ulong_t __unused3;
 compat_ulong_t __unused4;
};

struct compat_msqid64_ds {
 struct compat_ipc64_perm msg_perm;
 compat_ulong_t msg_stime;
 compat_ulong_t msg_stime_high;
 compat_ulong_t msg_rtime;
 compat_ulong_t msg_rtime_high;
 compat_ulong_t msg_ctime;
 compat_ulong_t msg_ctime_high;
 compat_ulong_t msg_cbytes;
 compat_ulong_t msg_qnum;
 compat_ulong_t msg_qbytes;
 compat_pid_t msg_lspid;
 compat_pid_t msg_lrpid;
 compat_ulong_t __unused4;
 compat_ulong_t __unused5;
};

struct compat_shmid64_ds {
 struct compat_ipc64_perm shm_perm;
 compat_size_t shm_segsz;
 compat_ulong_t shm_atime;
 compat_ulong_t shm_atime_high;
 compat_ulong_t shm_dtime;
 compat_ulong_t shm_dtime_high;
 compat_ulong_t shm_ctime;
 compat_ulong_t shm_ctime_high;
 compat_pid_t shm_cpid;
 compat_pid_t shm_lpid;
 compat_ulong_t shm_nattch;
 compat_ulong_t __unused4;
 compat_ulong_t __unused5;
};
# 21 "./arch/arm64/include/asm/compat.h" 2



/*
 * Architecture specific compatibility types
 */

# 1 "./include/linux/sched.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Define 'struct task_struct' and provide the main scheduler
 * APIs (schedule(), wakeup variants, etc.)
 */

# 1 "./include/uapi/linux/sched.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





/*
 * cloning flags:
 */
# 36 "./include/uapi/linux/sched.h"
/* Flags for the clone3() syscall. */



/*
 * cloning flags intersect with CSIGNAL so can be used with unshare and clone3
 * syscalls only:
 */



/**
 * struct clone_args - arguments for the clone3 syscall
 * @flags:        Flags for the new process as listed above.
 *                All flags are valid except for CSIGNAL and
 *                CLONE_DETACHED.
 * @pidfd:        If CLONE_PIDFD is set, a pidfd will be
 *                returned in this argument.
 * @child_tid:    If CLONE_CHILD_SETTID is set, the TID of the
 *                child process will be returned in the child's
 *                memory.
 * @parent_tid:   If CLONE_PARENT_SETTID is set, the TID of
 *                the child process will be returned in the
 *                parent's memory.
 * @exit_signal:  The exit_signal the parent process will be
 *                sent when the child exits.
 * @stack:        Specify the location of the stack for the
 *                child process.
 *                Note, @stack is expected to point to the
 *                lowest address. The stack direction will be
 *                determined by the kernel and set up
 *                appropriately based on @stack_size.
 * @stack_size:   The size of the stack for the child process.
 * @tls:          If CLONE_SETTLS is set, the tls descriptor
 *                is set to tls.
 * @set_tid:      Pointer to an array of type *pid_t. The size
 *                of the array is defined using @set_tid_size.
 *                This array is used to select PIDs/TIDs for
 *                newly created processes. The first element in
 *                this defines the PID in the most nested PID
 *                namespace. Each additional element in the array
 *                defines the PID in the parent PID namespace of
 *                the original PID namespace. If the array has
 *                less entries than the number of currently
 *                nested PID namespaces only the PIDs in the
 *                corresponding namespaces are set.
 * @set_tid_size: This defines the size of the array referenced
 *                in @set_tid. This cannot be larger than the
 *                kernel's limit of nested PID namespaces.
 * @cgroup:       If CLONE_INTO_CGROUP is specified set this to
 *                a file descriptor for the cgroup.
 *
 * The structure is versioned by size and thus extensible.
 * New struct members must go at the end of the struct and
 * must be properly 64bit aligned.
 */
struct clone_args {
 __u64 __attribute__((aligned(8))) flags;
 __u64 __attribute__((aligned(8))) pidfd;
 __u64 __attribute__((aligned(8))) child_tid;
 __u64 __attribute__((aligned(8))) parent_tid;
 __u64 __attribute__((aligned(8))) exit_signal;
 __u64 __attribute__((aligned(8))) stack;
 __u64 __attribute__((aligned(8))) stack_size;
 __u64 __attribute__((aligned(8))) tls;
 __u64 __attribute__((aligned(8))) set_tid;
 __u64 __attribute__((aligned(8))) set_tid_size;
 __u64 __attribute__((aligned(8))) cgroup;
};






/*
 * Scheduling policies
 */




/* SCHED_ISO: reserved but not implemented yet */



/* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */


/*
 * For the sched_{set,get}attr() calls
 */
# 11 "./include/linux/sched.h" 2



# 1 "./include/linux/pid.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/refcount.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Variant of atomic_t specialized for reference counts.
 *
 * The interface matches the atomic_t interface (to aid in porting) but only
 * provides the few functions one should use for reference counting.
 *
 * Saturation semantics
 * ====================
 *
 * refcount_t differs from atomic_t in that the counter saturates at
 * REFCOUNT_SATURATED and will not move once there. This avoids wrapping the
 * counter and causing 'spurious' use-after-free issues. In order to avoid the
 * cost associated with introducing cmpxchg() loops into all of the saturating
 * operations, we temporarily allow the counter to take on an unchecked value
 * and then explicitly set it to REFCOUNT_SATURATED on detecting that underflow
 * or overflow has occurred. Although this is racy when multiple threads
 * access the refcount concurrently, by placing REFCOUNT_SATURATED roughly
 * equidistant from 0 and INT_MAX we minimise the scope for error:
 *
 * 	                           INT_MAX     REFCOUNT_SATURATED   UINT_MAX
 *   0                          (0x7fff_ffff)    (0xc000_0000)    (0xffff_ffff)
 *   +--------------------------------+----------------+----------------+
 *                                     <---------- bad value! ---------->
 *
 * (in a signed view of the world, the "bad value" range corresponds to
 * a negative counter value).
 *
 * As an example, consider a refcount_inc() operation that causes the counter
 * to overflow:
 *
 * 	int old = atomic_fetch_add_relaxed(r);
 *	// old is INT_MAX, refcount now INT_MIN (0x8000_0000)
 *	if (old < 0)
 *		atomic_set(r, REFCOUNT_SATURATED);
 *
 * If another thread also performs a refcount_inc() operation between the two
 * atomic operations, then the count will continue to edge closer to 0. If it
 * reaches a value of 1 before /any/ of the threads reset it to the saturated
 * value, then a concurrent refcount_dec_and_test() may erroneously free the
 * underlying object.
 * Linux limits the maximum number of tasks to PID_MAX_LIMIT, which is currently
 * 0x400000 (and can't easily be raised in the future beyond FUTEX_TID_MASK).
 * With the current PID limit, if no batched refcounting operations are used and
 * the attacker can't repeatedly trigger kernel oopses in the middle of refcount
 * operations, this makes it impossible for a saturated refcount to leave the
 * saturation range, even if it is possible for multiple uses of the same
 * refcount to nest in the context of a single task:
 *
 *     (UINT_MAX+1-REFCOUNT_SATURATED) / PID_MAX_LIMIT =
 *     0x40000000 / 0x400000 = 0x100 = 256
 *
 * If hundreds of references are added/removed with a single refcounting
 * operation, it may potentially be possible to leave the saturation range; but
 * given the precise timing details involved with the round-robin scheduling of
 * each thread manipulating the refcount and the need to hit the race multiple
 * times in succession, there doesn't appear to be a practical avenue of attack
 * even if using refcount_add() operations with larger increments.
 *
 * Memory ordering
 * ===============
 *
 * Memory ordering rules are slightly relaxed wrt regular atomic_t functions
 * and provide only what is strictly required for refcounts.
 *
 * The increments are fully relaxed; these will not provide ordering. The
 * rationale is that whatever is used to obtain the object we're increasing the
 * reference count on will provide the ordering. For locked data structures,
 * its the lock acquire, for RCU/lockless data structures its the dependent
 * load.
 *
 * Do note that inc_not_zero() provides a control dependency which will order
 * future stores against the inc, this ensures we'll never modify the object
 * if we did not in fact acquire a reference.
 *
 * The decrements will provide release order, such that all the prior loads and
 * stores will be issued before, it also provides a control dependency, which
 * will order us against the subsequent free().
 *
 * The control dependency is against the load of the cmpxchg (ll/sc) that
 * succeeded. This means the stores aren't fully ordered, but this is fine
 * because the 1->0 transition indicates no concurrency.
 *
 * Note that the allocator is responsible for ordering things between free()
 * and alloc().
 *
 * The decrements dec_and_test() and sub_and_test() also provide acquire
 * ordering on success.
 *
 */
# 101 "./include/linux/refcount.h"
struct mutex;

/**
 * typedef refcount_t - variant of atomic_t specialized for reference counts
 * @refs: atomic_t counter field
 *
 * The counter saturates at REFCOUNT_SATURATED and will not move once
 * there. This avoids wrapping the counter and causing 'spurious'
 * use-after-free bugs.
 */
typedef struct refcount_struct {
 atomic_t refs;
} refcount_t;





enum refcount_saturation_type {
 REFCOUNT_ADD_NOT_ZERO_OVF,
 REFCOUNT_ADD_OVF,
 REFCOUNT_ADD_UAF,
 REFCOUNT_SUB_UAF,
 REFCOUNT_DEC_LEAK,
};

void refcount_warn_saturate(refcount_t *r, enum refcount_saturation_type t);

/**
 * refcount_set - set a refcount's value
 * @r: the refcount
 * @n: value to which the refcount will be set
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void refcount_set(refcount_t *r, int n)
{
 atomic_set(&r->refs, n);
}

/**
 * refcount_read - get a refcount's value
 * @r: the refcount
 *
 * Return: the refcount's value
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int refcount_read(const refcount_t *r)
{
 return atomic_read(&r->refs);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool __refcount_add_not_zero(int i, refcount_t *r, int *oldp)
{
 int old = refcount_read(r);

 do {
  if (!old)
   break;
 } while (!atomic_try_cmpxchg_relaxed(&r->refs, &old, old + i));

 if (oldp)
  *oldp = old;

 if (__builtin_expect(!!(old < 0 || old + i < 0), 0))
  refcount_warn_saturate(r, REFCOUNT_ADD_NOT_ZERO_OVF);

 return old;
}

/**
 * refcount_add_not_zero - add a value to a refcount unless it is 0
 * @i: the value to add to the refcount
 * @r: the refcount
 *
 * Will saturate at REFCOUNT_SATURATED and WARN.
 *
 * Provides no memory ordering, it is assumed the caller has guaranteed the
 * object memory to be stable (RCU, etc.). It does provide a control dependency
 * and thereby orders future stores. See the comment on top.
 *
 * Use of this function is not recommended for the normal reference counting
 * use case in which references are taken and released one at a time.  In these
 * cases, refcount_inc(), or one of its variants, should instead be used to
 * increment a reference count.
 *
 * Return: false if the passed refcount is 0, true otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool refcount_add_not_zero(int i, refcount_t *r)
{
 return __refcount_add_not_zero(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __refcount_add(int i, refcount_t *r, int *oldp)
{
 int old = atomic_fetch_add_relaxed(i, &r->refs);

 if (oldp)
  *oldp = old;

 if (__builtin_expect(!!(!old), 0))
  refcount_warn_saturate(r, REFCOUNT_ADD_UAF);
 else if (__builtin_expect(!!(old < 0 || old + i < 0), 0))
  refcount_warn_saturate(r, REFCOUNT_ADD_OVF);
}

/**
 * refcount_add - add a value to a refcount
 * @i: the value to add to the refcount
 * @r: the refcount
 *
 * Similar to atomic_add(), but will saturate at REFCOUNT_SATURATED and WARN.
 *
 * Provides no memory ordering, it is assumed the caller has guaranteed the
 * object memory to be stable (RCU, etc.). It does provide a control dependency
 * and thereby orders future stores. See the comment on top.
 *
 * Use of this function is not recommended for the normal reference counting
 * use case in which references are taken and released one at a time.  In these
 * cases, refcount_inc(), or one of its variants, should instead be used to
 * increment a reference count.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void refcount_add(int i, refcount_t *r)
{
 __refcount_add(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool __refcount_inc_not_zero(refcount_t *r, int *oldp)
{
 return __refcount_add_not_zero(1, r, oldp);
}

/**
 * refcount_inc_not_zero - increment a refcount unless it is 0
 * @r: the refcount to increment
 *
 * Similar to atomic_inc_not_zero(), but will saturate at REFCOUNT_SATURATED
 * and WARN.
 *
 * Provides no memory ordering, it is assumed the caller has guaranteed the
 * object memory to be stable (RCU, etc.). It does provide a control dependency
 * and thereby orders future stores. See the comment on top.
 *
 * Return: true if the increment was successful, false otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool refcount_inc_not_zero(refcount_t *r)
{
 return __refcount_inc_not_zero(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __refcount_inc(refcount_t *r, int *oldp)
{
 __refcount_add(1, r, oldp);
}

/**
 * refcount_inc - increment a refcount
 * @r: the refcount to increment
 *
 * Similar to atomic_inc(), but will saturate at REFCOUNT_SATURATED and WARN.
 *
 * Provides no memory ordering, it is assumed the caller already has a
 * reference on the object.
 *
 * Will WARN if the refcount is 0, as this represents a possible use-after-free
 * condition.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void refcount_inc(refcount_t *r)
{
 __refcount_inc(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool __refcount_sub_and_test(int i, refcount_t *r, int *oldp)
{
 int old = atomic_fetch_sub_release(i, &r->refs);

 if (oldp)
  *oldp = old;

 if (old == i) {
  do { do { } while (0); asm volatile("dmb " "ishld" : : : "memory"); } while (0);
  return true;
 }

 if (__builtin_expect(!!(old < 0 || old - i < 0), 0))
  refcount_warn_saturate(r, REFCOUNT_SUB_UAF);

 return false;
}

/**
 * refcount_sub_and_test - subtract from a refcount and test if it is 0
 * @i: amount to subtract from the refcount
 * @r: the refcount
 *
 * Similar to atomic_dec_and_test(), but it will WARN, return false and
 * ultimately leak on underflow and will fail to decrement when saturated
 * at REFCOUNT_SATURATED.
 *
 * Provides release memory ordering, such that prior loads and stores are done
 * before, and provides an acquire ordering on success such that free()
 * must come after.
 *
 * Use of this function is not recommended for the normal reference counting
 * use case in which references are taken and released one at a time.  In these
 * cases, refcount_dec(), or one of its variants, should instead be used to
 * decrement a reference count.
 *
 * Return: true if the resulting refcount is 0, false otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool refcount_sub_and_test(int i, refcount_t *r)
{
 return __refcount_sub_and_test(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool __refcount_dec_and_test(refcount_t *r, int *oldp)
{
 return __refcount_sub_and_test(1, r, oldp);
}

/**
 * refcount_dec_and_test - decrement a refcount and test if it is 0
 * @r: the refcount
 *
 * Similar to atomic_dec_and_test(), it will WARN on underflow and fail to
 * decrement when saturated at REFCOUNT_SATURATED.
 *
 * Provides release memory ordering, such that prior loads and stores are done
 * before, and provides an acquire ordering on success such that free()
 * must come after.
 *
 * Return: true if the resulting refcount is 0, false otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool refcount_dec_and_test(refcount_t *r)
{
 return __refcount_dec_and_test(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __refcount_dec(refcount_t *r, int *oldp)
{
 int old = atomic_fetch_sub_release(1, &r->refs);

 if (oldp)
  *oldp = old;

 if (__builtin_expect(!!(old <= 1), 0))
  refcount_warn_saturate(r, REFCOUNT_DEC_LEAK);
}

/**
 * refcount_dec - decrement a refcount
 * @r: the refcount
 *
 * Similar to atomic_dec(), it will WARN on underflow and fail to decrement
 * when saturated at REFCOUNT_SATURATED.
 *
 * Provides release memory ordering, such that prior loads and stores are done
 * before.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void refcount_dec(refcount_t *r)
{
 __refcount_dec(r, ((void *)0));
}

extern __attribute__((__warn_unused_result__)) bool refcount_dec_if_one(refcount_t *r);
extern __attribute__((__warn_unused_result__)) bool refcount_dec_not_one(refcount_t *r);
extern __attribute__((__warn_unused_result__)) bool refcount_dec_and_mutex_lock(refcount_t *r, struct mutex *lock) ;
extern __attribute__((__warn_unused_result__)) bool refcount_dec_and_lock(refcount_t *r, spinlock_t *lock) ;
extern __attribute__((__warn_unused_result__)) bool refcount_dec_and_lock_irqsave(refcount_t *r,
             spinlock_t *lock,
             unsigned long *flags) ;
# 8 "./include/linux/pid.h" 2

enum pid_type
{
 PIDTYPE_PID,
 PIDTYPE_TGID,
 PIDTYPE_PGID,
 PIDTYPE_SID,
 PIDTYPE_MAX,
};

/*
 * What is struct pid?
 *
 * A struct pid is the kernel's internal notion of a process identifier.
 * It refers to individual tasks, process groups, and sessions.  While
 * there are processes attached to it the struct pid lives in a hash
 * table, so it and then the processes that it refers to can be found
 * quickly from the numeric pid value.  The attached processes may be
 * quickly accessed by following pointers from struct pid.
 *
 * Storing pid_t values in the kernel and referring to them later has a
 * problem.  The process originally with that pid may have exited and the
 * pid allocator wrapped, and another process could have come along
 * and been assigned that pid.
 *
 * Referring to user space processes by holding a reference to struct
 * task_struct has a problem.  When the user space process exits
 * the now useless task_struct is still kept.  A task_struct plus a
 * stack consumes around 10K of low kernel memory.  More precisely
 * this is THREAD_SIZE + sizeof(struct task_struct).  By comparison
 * a struct pid is about 64 bytes.
 *
 * Holding a reference to struct pid solves both of these problems.
 * It is small so holding a reference does not consume a lot of
 * resources, and since a new struct pid is allocated when the numeric pid
 * value is reused (when pids wrap around) we don't mistakenly refer to new
 * processes.
 */


/*
 * struct upid is used to get the id of the struct pid, as it is
 * seen in particular namespace. Later the struct pid is found with
 * find_pid_ns() using the int nr and struct pid_namespace *ns.
 */

struct upid {
 int nr;
 struct pid_namespace *ns;
};

struct pid
{
 refcount_t count;
 unsigned int level;
 spinlock_t lock;
 /* lists of tasks that use this pid */
 struct hlist_head tasks[PIDTYPE_MAX];
 struct hlist_head inodes;
 /* wait queue for pidfd notifications */
 wait_queue_head_t wait_pidfd;
 struct callback_head rcu;
 struct upid numbers[1];
};

extern struct pid init_struct_pid;

extern const struct file_operations pidfd_fops;

struct file;

extern struct pid *pidfd_pid(const struct file *file);
struct pid *pidfd_get_pid(unsigned int fd, unsigned int *flags);
struct task_struct *pidfd_get_task(int pidfd, unsigned int *flags);
int pidfd_create(struct pid *pid, unsigned int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid *get_pid(struct pid *pid)
{
 if (pid)
  refcount_inc(&pid->count);
 return pid;
}

extern void put_pid(struct pid *pid);
extern struct task_struct *pid_task(struct pid *pid, enum pid_type);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pid_has_task(struct pid *pid, enum pid_type type)
{
 return !hlist_empty(&pid->tasks[type]);
}
extern struct task_struct *get_pid_task(struct pid *pid, enum pid_type);

extern struct pid *get_task_pid(struct task_struct *task, enum pid_type type);

/*
 * these helpers must be called with the tasklist_lock write-held.
 */
extern void attach_pid(struct task_struct *task, enum pid_type);
extern void detach_pid(struct task_struct *task, enum pid_type);
extern void change_pid(struct task_struct *task, enum pid_type,
   struct pid *pid);
extern void exchange_tids(struct task_struct *task, struct task_struct *old);
extern void transfer_pid(struct task_struct *old, struct task_struct *new,
    enum pid_type);

struct pid_namespace;
extern struct pid_namespace init_pid_ns;

extern int pid_max;
extern int pid_max_min, pid_max_max;

/*
 * look up a PID in the hash table. Must be called with the tasklist_lock
 * or rcu_read_lock() held.
 *
 * find_pid_ns() finds the pid in the namespace specified
 * find_vpid() finds the pid by its virtual id, i.e. in the current namespace
 *
 * see also find_task_by_vpid() set in include/linux/sched.h
 */
extern struct pid *find_pid_ns(int nr, struct pid_namespace *ns);
extern struct pid *find_vpid(int nr);

/*
 * Lookup a PID in the hash table, and return with it's count elevated.
 */
extern struct pid *find_get_pid(int nr);
extern struct pid *find_ge_pid(int nr, struct pid_namespace *);

extern struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid,
        size_t set_tid_size);
extern void free_pid(struct pid *pid);
extern void disable_pid_allocation(struct pid_namespace *ns);

/*
 * ns_of_pid() returns the pid namespace in which the specified pid was
 * allocated.
 *
 * NOTE:
 * 	ns_of_pid() is expected to be called for a process (task) that has
 * 	an attached 'struct pid' (see attach_pid(), detach_pid()) i.e @pid
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid_namespace *ns_of_pid(struct pid *pid)
{
 struct pid_namespace *ns = ((void *)0);
 if (pid)
  ns = pid->numbers[pid->level].ns;
 return ns;
}

/*
 * is_child_reaper returns true if the pid is the init process
 * of the current namespace. As this one could be checked before
 * pid_ns->child_reaper is assigned in copy_process, we check
 * with the pid number.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_child_reaper(struct pid *pid)
{
 return pid->numbers[pid->level].nr == 1;
}

/*
 * the helpers to get the pid's id seen from different namespaces
 *
 * pid_nr()    : global id, i.e. the id seen from the init namespace;
 * pid_vnr()   : virtual id, i.e. the id seen from the pid namespace of
 *               current.
 * pid_nr_ns() : id seen from the ns specified.
 *
 * see also task_xid_nr() etc in include/linux/sched.h
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t pid_nr(struct pid *pid)
{
 pid_t nr = 0;
 if (pid)
  nr = pid->numbers[0].nr;
 return nr;
}

pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns);
pid_t pid_vnr(struct pid *pid);







   /*
			 * Both old and new leaders may be attached to
			 * the same pid in the middle of de_thread().
			 */
# 15 "./include/linux/sched.h" 2
# 1 "./include/linux/sem.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/sem.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./include/linux/ipc.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/uidgid.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * A set of types for the internal kernel types representing uids and gids.
 *
 * The types defined in this header allow distinguishing which uids and gids in
 * the kernel are values used by userspace and which uid and gid values are
 * the internal kernel values.  With the addition of user namespaces the values
 * can be different.  Using the type system makes it possible for the compiler
 * to detect when we overlook these differences.
 *
 */

# 1 "./include/linux/highuid.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * general notes:
 *
 * CONFIG_UID16 is defined if the given architecture needs to
 * support backwards compatibility for old system calls.
 *
 * kernel code should use uid_t and gid_t at all times when dealing with
 * kernel-private data.
 *
 * old_uid_t and old_gid_t should only be different if CONFIG_UID16 is
 * defined, else the platform should provide dummy typedefs for them
 * such that they are equivalent to __kernel_{u,g}id_t.
 *
 * uid16_t and gid16_t are used on all architectures. (when dealing
 * with structures hard coded to 16 bits, such as in filesystems)
 */


/*
 * This is the "overflow" UID and GID. They are used to signify uid/gid
 * overflow to old programs when they request uid/gid information but are
 * using the old 16 bit interfaces.
 * When you run a libc5 program, it will think that all highuid files or
 * processes are owned by this uid/gid.
 * The idea is that it's better to do so than possibly return 0 in lieu of
 * 65536, etc.
 */

extern int overflowuid;
extern int overflowgid;

extern void __bad_uid(void);
extern void __bad_gid(void);






/* prevent uid mod 65536 effect by returning a default value for high UIDs */


/*
 * -1 is different in 16 bits than it is in 32 bits
 * these macros are used by chown(), setreuid(), ...,
 */
# 69 "./include/linux/highuid.h"
/* uid/gid input should be always 32bit uid_t */



/*
 * Everything below this line is needed on all architectures, to deal with
 * filesystems that only store 16 bits of the UID/GID, etc.
 */

/*
 * This is the UID and GID that will get written to disk if a filesystem
 * only supports 16-bit UIDs and the kernel has a high UID/GID to write
 */
extern int fs_overflowuid;
extern int fs_overflowgid;




/*
 * Since these macros are used in architectures that only need limited
 * 16-bit UID back compatibility, we won't use old_uid_t and old_gid_t
 */
# 17 "./include/linux/uidgid.h" 2

struct user_namespace;
extern struct user_namespace init_user_ns;

typedef struct {
 uid_t val;
} kuid_t;


typedef struct {
 gid_t val;
} kgid_t;





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) uid_t __kuid_val(kuid_t uid)
{
 return uid.val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) gid_t __kgid_val(kgid_t gid)
{
 return gid.val;
}
# 61 "./include/linux/uidgid.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uid_eq(kuid_t left, kuid_t right)
{
 return __kuid_val(left) == __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gid_eq(kgid_t left, kgid_t right)
{
 return __kgid_val(left) == __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uid_gt(kuid_t left, kuid_t right)
{
 return __kuid_val(left) > __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gid_gt(kgid_t left, kgid_t right)
{
 return __kgid_val(left) > __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uid_gte(kuid_t left, kuid_t right)
{
 return __kuid_val(left) >= __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gid_gte(kgid_t left, kgid_t right)
{
 return __kgid_val(left) >= __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uid_lt(kuid_t left, kuid_t right)
{
 return __kuid_val(left) < __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gid_lt(kgid_t left, kgid_t right)
{
 return __kgid_val(left) < __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uid_lte(kuid_t left, kuid_t right)
{
 return __kuid_val(left) <= __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gid_lte(kgid_t left, kgid_t right)
{
 return __kgid_val(left) <= __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uid_valid(kuid_t uid)
{
 return __kuid_val(uid) != (uid_t) -1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gid_valid(kgid_t gid)
{
 return __kgid_val(gid) != (gid_t) -1;
}



extern kuid_t make_kuid(struct user_namespace *from, uid_t uid);
extern kgid_t make_kgid(struct user_namespace *from, gid_t gid);

extern uid_t from_kuid(struct user_namespace *to, kuid_t uid);
extern gid_t from_kgid(struct user_namespace *to, kgid_t gid);
extern uid_t from_kuid_munged(struct user_namespace *to, kuid_t uid);
extern gid_t from_kgid_munged(struct user_namespace *to, kgid_t gid);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kuid_has_mapping(struct user_namespace *ns, kuid_t uid)
{
 return from_kuid(ns, uid) != (uid_t) -1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kgid_has_mapping(struct user_namespace *ns, kgid_t gid)
{
 return from_kgid(ns, gid) != (gid_t) -1;
}
# 7 "./include/linux/ipc.h" 2
# 1 "./include/linux/rhashtable-types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Resizable, Scalable, Concurrent Hash Table
 *
 * Simple structures that might be needed in include
 * files.
 */







# 1 "./include/linux/workqueue.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * workqueue.h --- work queue handling for Linux.
 */




# 1 "./include/linux/timer.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/ktime.h" 1
/*
 *  include/linux/ktime.h
 *
 *  ktime_t - nanosecond-resolution time format.
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes and macros.
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  Credits:
 *
 *  	Roman Zippel provided the ideas and primary code snippets of
 *  	the ktime_t union and further simplifications of the original
 *  	code.
 *
 *  For licencing details see kernel-base/COPYING
 */




# 1 "./include/linux/jiffies.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 12 "./include/linux/jiffies.h"
# 1 "./include/vdso/jiffies.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/* TICK_NSEC is the time between ticks in nsec assuming SHIFTED_HZ */
# 13 "./include/linux/jiffies.h" 2

# 1 "./include/generated/timeconst.h" 1
/* Automatically generated by kernel/time/timeconst.bc */
/* Time conversion constants for HZ == 250 */
# 15 "./include/linux/jiffies.h" 2

/*
 * The following defines establish the engineering parameters of the PLL
 * model. The HZ variable establishes the timer interrupt frequency, 100 Hz
 * for the SunOS kernel, 256 Hz for the Ultrix kernel and 1024 Hz for the
 * OSF/1 kernel. The SHIFT_HZ define expresses the same value as the
 * nearest power of two in order to avoid hardware multiply operations.
 */
# 47 "./include/linux/jiffies.h"
/* Suppose we want to divide two numbers NOM and DEN: NOM/DEN, then we can
 * improve accuracy by shifting LSH bits, hence calculating:
 *     (NOM << LSH) / DEN
 * This however means trouble for large NOM, because (NOM << LSH) may no
 * longer fit in 32 bits. The following way of calculating this gives us
 * some slack, under the following conditions:
 *   - (NOM / DEN) fits in (32 - LSH) bits.
 *   - (NOM % DEN) fits in (32 - LSH) bits.
 */



/* LATCH is used in the interval timer and ftape setup. */


extern int register_refined_jiffies(long clock_tick_rate);

/* TICK_USEC is the time between ticks in usec assuming SHIFTED_HZ */


/* USER_TICK_USEC is the time between ticks in usec assuming fake USER_HZ */






/*
 * The 64-bit value is not atomic - you MUST NOT read it
 * without sampling the sequence number in jiffies_lock.
 * get_jiffies_64() will do this for you as appropriate.
 */
extern u64 __attribute__((__aligned__((1 << (6))), __section__(".data..cacheline_aligned"))) jiffies_64;
extern unsigned long volatile __attribute__((__aligned__((1 << (6))), __section__(".data..cacheline_aligned"))) jiffies;




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 get_jiffies_64(void)
{
 return (u64)jiffies;
}


/*
 *	These inlines deal with timer wrapping correctly. You are
 *	strongly encouraged to use them
 *	1. Because people otherwise forget
 *	2. Because if the timer wrap changes in future you won't have to
 *	   alter your driver code.
 *
 * time_after(a,b) returns true if the time a is after time b.
 *
 * Do this with "<0" and ">=0" to only test the sign of the result. A
 * good compiler would generate better code (and a really good compiler
 * wouldn't care). Gcc is currently neither.
 */
# 116 "./include/linux/jiffies.h"
/*
 * Calculate whether a is in the range of [b, c].
 */




/*
 * Calculate whether a is in the range of [b, c).
 */




/* Same as above, but does so with platform independent 64bit types.
 * These must be used when utilizing jiffies_64 (i.e. return value of
 * get_jiffies_64() */
# 149 "./include/linux/jiffies.h"
/*
 * These four macros compare jiffies and 'a' for convenience.
 */

/* time_is_before_jiffies(a) return true if a is before jiffies */



/* time_is_after_jiffies(a) return true if a is after jiffies */



/* time_is_before_eq_jiffies(a) return true if a is before or equal to jiffies*/



/* time_is_after_eq_jiffies(a) return true if a is after or equal to jiffies*/



/*
 * Have the 32 bit jiffies value wrap 5 minutes after boot
 * so jiffies wrap bugs show up earlier.
 */


/*
 * Change timeval to jiffies, trying to avoid the
 * most obvious overflows..
 *
 * And some not so obvious.
 *
 * Note that we don't want to return LONG_MAX, because
 * for various timeout reasons we often end up having
 * to wait "jiffies+1" in order to guarantee that we wait
 * at _least_ "jiffies" - so "jiffies+1" had better still
 * be positive.
 */


extern unsigned long preset_lpj;

/*
 * We want to do realistic conversions of time so we need to use the same
 * values the update wall clock code uses as the jiffies size.  This value
 * is: TICK_NSEC (which is defined in timex.h).  This
 * is a constant and is in nanoseconds.  We will use scaled math
 * with a set of scales defined here as SEC_JIFFIE_SC,  USEC_JIFFIE_SC and
 * NSEC_JIFFIE_SC.  Note that these defines contain nothing but
 * constants and so are computed at compile time.  SHIFT_HZ (computed in
 * timex.h) adjusts the scaling for different HZ values.

 * Scaled math???  What is that?
 *
 * Scaled math is a way to do integer math on values that would,
 * otherwise, either overflow, underflow, or cause undesired div
 * instructions to appear in the execution path.  In short, we "scale"
 * up the operands so they take more bits (more precision, less
 * underflow), do the desired operation and then "scale" the result back
 * by the same amount.  If we do the scaling by shifting we avoid the
 * costly mpy and the dastardly div instructions.

 * Suppose, for example, we want to convert from seconds to jiffies
 * where jiffies is defined in nanoseconds as NSEC_PER_JIFFIE.  The
 * simple math is: jiff = (sec * NSEC_PER_SEC) / NSEC_PER_JIFFIE; We
 * observe that (NSEC_PER_SEC / NSEC_PER_JIFFIE) is a constant which we
 * might calculate at compile time, however, the result will only have
 * about 3-4 bits of precision (less for smaller values of HZ).
 *
 * So, we scale as follows:
 * jiff = (sec) * (NSEC_PER_SEC / NSEC_PER_JIFFIE);
 * jiff = ((sec) * ((NSEC_PER_SEC * SCALE)/ NSEC_PER_JIFFIE)) / SCALE;
 * Then we make SCALE a power of two so:
 * jiff = ((sec) * ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE)) >> SCALE;
 * Now we define:
 * #define SEC_CONV = ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE))
 * jiff = (sec * SEC_CONV) >> SCALE;
 *
 * Often the math we use will expand beyond 32-bits so we tell C how to
 * do this and pass the 64-bit result of the mpy through the ">> SCALE"
 * which should take the result back to 32-bits.  We want this expansion
 * to capture as much precision as possible.  At the same time we don't
 * want to overflow so we pick the SCALE to avoid this.  In this file,
 * that means using a different scale for each range of HZ values (as
 * defined in timex.h).
 *
 * For those who want to know, gcc will give a 64-bit result from a "*"
 * operator if the result is a long long AND at least one of the
 * operands is cast to long long (usually just prior to the "*" so as
 * not to confuse it into thinking it really has a 64-bit operand,
 * which, buy the way, it can do, but it takes more code and at least 2
 * mpys).

 * We also need to be aware that one second in nanoseconds is only a
 * couple of bits away from overflowing a 32-bit word, so we MUST use
 * 64-bits to get the full range time in nanoseconds.

 */

/*
 * Here are the scales we will use.  One for seconds, nanoseconds and
 * microseconds.
 *
 * Within the limits of cpp we do a rough cut at the SEC_JIFFIE_SC and
 * check if the sign bit is set.  If not, we bump the shift count by 1.
 * (Gets an extra bit of precision where we can use it.)
 * We know it is set for HZ = 1024 and HZ = 100 not for 1000.
 * Haven't tested others.

 * Limits of cpp (for #if expressions) only long (no long long), but
 * then we only need the most signicant bit.
 */
# 273 "./include/linux/jiffies.h"
/*
 * The maximum jiffie value is (MAX_INT >> 1).  Here we translate that
 * into seconds.  The 64-bit case will overflow if we are not careful,
 * so use the messy SH_DIV macro to do it.  Still all constants.
 */
# 287 "./include/linux/jiffies.h"
/*
 * Convert various time units to each other:
 */
extern unsigned int jiffies_to_msecs(const unsigned long j);
extern unsigned int jiffies_to_usecs(const unsigned long j);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 jiffies_to_nsecs(const unsigned long j)
{
 return (u64)jiffies_to_usecs(j) * 1000L;
}

extern u64 jiffies64_to_nsecs(u64 j);
extern u64 jiffies64_to_msecs(u64 j);

extern unsigned long __msecs_to_jiffies(const unsigned int m);

/*
 * HZ is equal to or smaller than 1000, and 1000 is a nice round
 * multiple of HZ, divide with the factor between them, but round
 * upwards:
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long _msecs_to_jiffies(const unsigned int m)
{
 return (m + (1000L / 250 /* Internal kernel timer frequency */) - 1) / (1000L / 250 /* Internal kernel timer frequency */);
}
# 338 "./include/linux/jiffies.h"
/**
 * msecs_to_jiffies: - convert milliseconds to jiffies
 * @m:	time in milliseconds
 *
 * conversion is done as follows:
 *
 * - negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET)
 *
 * - 'too large' values [that would result in larger than
 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.
 *
 * - all other values are converted to jiffies by either multiplying
 *   the input value by a factor or dividing it with a factor and
 *   handling any 32-bit overflows.
 *   for the details see __msecs_to_jiffies()
 *
 * msecs_to_jiffies() checks for the passed in value being a constant
 * via __builtin_constant_p() allowing gcc to eliminate most of the
 * code, __msecs_to_jiffies() is called if the value passed does not
 * allow constant folding and the actual conversion must be done at
 * runtime.
 * the HZ range specific helpers _msecs_to_jiffies() are called both
 * directly here and from __msecs_to_jiffies() in the case where
 * constant folding is not possible.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long msecs_to_jiffies(const unsigned int m)
{
 if (__builtin_constant_p(m)) {
  if ((int)m < 0)
   return ((((long)(~0UL >> 1)) >> 1)-1);
  return _msecs_to_jiffies(m);
 } else {
  return __msecs_to_jiffies(m);
 }
}

extern unsigned long __usecs_to_jiffies(const unsigned int u);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long _usecs_to_jiffies(const unsigned int u)
{
 return (u + (1000000L / 250 /* Internal kernel timer frequency */) - 1) / (1000000L / 250 /* Internal kernel timer frequency */);
}
# 388 "./include/linux/jiffies.h"
/**
 * usecs_to_jiffies: - convert microseconds to jiffies
 * @u:	time in microseconds
 *
 * conversion is done as follows:
 *
 * - 'too large' values [that would result in larger than
 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.
 *
 * - all other values are converted to jiffies by either multiplying
 *   the input value by a factor or dividing it with a factor and
 *   handling any 32-bit overflows as for msecs_to_jiffies.
 *
 * usecs_to_jiffies() checks for the passed in value being a constant
 * via __builtin_constant_p() allowing gcc to eliminate most of the
 * code, __usecs_to_jiffies() is called if the value passed does not
 * allow constant folding and the actual conversion must be done at
 * runtime.
 * the HZ range specific helpers _usecs_to_jiffies() are called both
 * directly here and from __msecs_to_jiffies() in the case where
 * constant folding is not possible.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long usecs_to_jiffies(const unsigned int u)
{
 if (__builtin_constant_p(u)) {
  if (u > jiffies_to_usecs(((((long)(~0UL >> 1)) >> 1)-1)))
   return ((((long)(~0UL >> 1)) >> 1)-1);
  return _usecs_to_jiffies(u);
 } else {
  return __usecs_to_jiffies(u);
 }
}

extern unsigned long timespec64_to_jiffies(const struct timespec64 *value);
extern void jiffies_to_timespec64(const unsigned long jiffies,
      struct timespec64 *value);
extern clock_t jiffies_to_clock_t(unsigned long x);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) clock_t jiffies_delta_to_clock_t(long delta)
{
 return jiffies_to_clock_t(__builtin_choose_expr(((!!(sizeof((typeof(0L) *)1 == (typeof(delta) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(0L) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(delta) * 0l)) : (int *)8))))), ((0L) > (delta) ? (0L) : (delta)), ({ typeof(0L) __UNIQUE_ID___x243 = (0L); typeof(delta) __UNIQUE_ID___y244 = (delta); ((__UNIQUE_ID___x243) > (__UNIQUE_ID___y244) ? (__UNIQUE_ID___x243) : (__UNIQUE_ID___y244)); })));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int jiffies_delta_to_msecs(long delta)
{
 return jiffies_to_msecs(__builtin_choose_expr(((!!(sizeof((typeof(0L) *)1 == (typeof(delta) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(0L) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(delta) * 0l)) : (int *)8))))), ((0L) > (delta) ? (0L) : (delta)), ({ typeof(0L) __UNIQUE_ID___x245 = (0L); typeof(delta) __UNIQUE_ID___y246 = (delta); ((__UNIQUE_ID___x245) > (__UNIQUE_ID___y246) ? (__UNIQUE_ID___x245) : (__UNIQUE_ID___y246)); })));
}

extern unsigned long clock_t_to_jiffies(unsigned long x);
extern u64 jiffies_64_to_clock_t(u64 x);
extern u64 nsec_to_clock_t(u64 x);
extern u64 nsecs_to_jiffies64(u64 n);
extern unsigned long nsecs_to_jiffies(u64 n);
# 26 "./include/linux/ktime.h" 2


/* Nanosecond scalar representation for kernel time values */
typedef s64 ktime_t;

/**
 * ktime_set - Set a ktime_t variable from a seconds/nanoseconds value
 * @secs:	seconds to set
 * @nsecs:	nanoseconds to set
 *
 * Return: The ktime_t representation of the value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_set(const s64 secs, const unsigned long nsecs)
{
 if (__builtin_expect(!!(secs >= (((s64)~((u64)1 << 63)) / 1000000000L)), 0))
  return ((s64)~((u64)1 << 63));

 return secs * 1000000000L + (s64)nsecs;
}

/* Subtract two ktime_t variables. rem = lhs -rhs: */


/* Add two ktime_t variables. res = lhs + rhs: */


/*
 * Same as ktime_add(), but avoids undefined behaviour on overflow; however,
 * this means that you must check the result for overflow yourself.
 */


/*
 * Add a ktime_t variable and a scalar nanosecond value.
 * res = kt + nsval:
 */


/*
 * Subtract a scalar nanosecod from a ktime_t variable
 * res = kt - nsval:
 */


/* convert a timespec64 to ktime_t format: */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t timespec64_to_ktime(struct timespec64 ts)
{
 return ktime_set(ts.tv_sec, ts.tv_nsec);
}

/* Map the ktime_t to timespec conversion to ns_to_timespec function */


/* Convert ktime_t to nanoseconds */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 ktime_to_ns(const ktime_t kt)
{
 return kt;
}

/**
 * ktime_compare - Compares two ktime_t variables for less, greater or equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: ...
 *   cmp1  < cmp2: return <0
 *   cmp1 == cmp2: return 0
 *   cmp1  > cmp2: return >0
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ktime_compare(const ktime_t cmp1, const ktime_t cmp2)
{
 if (cmp1 < cmp2)
  return -1;
 if (cmp1 > cmp2)
  return 1;
 return 0;
}

/**
 * ktime_after - Compare if a ktime_t value is bigger than another one.
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: true if cmp1 happened after cmp2.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool ktime_after(const ktime_t cmp1, const ktime_t cmp2)
{
 return ktime_compare(cmp1, cmp2) > 0;
}

/**
 * ktime_before - Compare if a ktime_t value is smaller than another one.
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: true if cmp1 happened before cmp2.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool ktime_before(const ktime_t cmp1, const ktime_t cmp2)
{
 return ktime_compare(cmp1, cmp2) < 0;
}
# 148 "./include/linux/ktime.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 ktime_divns(const ktime_t kt, s64 div)
{
 /*
	 * 32-bit implementation cannot handle negative divisors,
	 * so catch them on 64bit as well.
	 */
 ({ int __ret_warn_on = !!(div < 0); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/ktime.h\"; .popsection; .long 14472b - .; .short 154; .short (1 << 0)|(((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 return kt / div;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 ktime_to_us(const ktime_t kt)
{
 return ktime_divns(kt, 1000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 ktime_to_ms(const ktime_t kt)
{
 return ktime_divns(kt, 1000000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 ktime_us_delta(const ktime_t later, const ktime_t earlier)
{
       return ktime_to_us(((later) - (earlier)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 ktime_ms_delta(const ktime_t later, const ktime_t earlier)
{
 return ktime_to_ms(((later) - (earlier)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_add_us(const ktime_t kt, const u64 usec)
{
 return ((kt) + (usec * 1000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_add_ms(const ktime_t kt, const u64 msec)
{
 return ((kt) + (msec * 1000000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_sub_us(const ktime_t kt, const u64 usec)
{
 return ((kt) - (usec * 1000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_sub_ms(const ktime_t kt, const u64 msec)
{
 return ((kt) - (msec * 1000000L));
}

extern ktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs);

/**
 * ktime_to_timespec64_cond - convert a ktime_t variable to timespec64
 *			    format only if the variable contains data
 * @kt:		the ktime_t variable to convert
 * @ts:		the timespec variable to store the result in
 *
 * Return: %true if there was a successful conversion, %false if kt was 0.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool ktime_to_timespec64_cond(const ktime_t kt,
             struct timespec64 *ts)
{
 if (kt) {
  *ts = ns_to_timespec64((kt));
  return true;
 } else {
  return false;
 }
}

# 1 "./include/vdso/ktime.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */
# 221 "./include/linux/ktime.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ns_to_ktime(u64 ns)
{
 return ns;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ms_to_ktime(u64 ms)
{
 return ms * 1000000L;
}

# 1 "./include/linux/timekeeping.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/clocksource_ids.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/* Enum to give clocksources a unique identifier */
enum clocksource_ids {
 CSID_GENERIC = 0,
 CSID_ARM_ARCH_COUNTER,
 CSID_MAX,
};
# 7 "./include/linux/timekeeping.h" 2

/* Included from linux/ktime.h */

void timekeeping_init(void);
extern int timekeeping_suspended;

/* Architecture timer tick functions: */
extern void legacy_timer_tick(unsigned long ticks);

/*
 * Get and set timeofday
 */
extern int do_settimeofday64(const struct timespec64 *ts);
extern int do_sys_settimeofday64(const struct timespec64 *tv,
     const struct timezone *tz);

/*
 * ktime_get() family: read the current time in a multitude of ways,
 *
 * The default time reference is CLOCK_MONOTONIC, starting at
 * boot time but not counting the time spent in suspend.
 * For other references, use the functions with "real", "clocktai",
 * "boottime" and "raw" suffixes.
 *
 * To get the time in a different format, use the ones wit
 * "ns", "ts64" and "seconds" suffix.
 *
 * See Documentation/core-api/timekeeping.rst for more details.
 */


/*
 * timespec64 based interfaces
 */
extern void ktime_get_raw_ts64(struct timespec64 *ts);
extern void ktime_get_ts64(struct timespec64 *ts);
extern void ktime_get_real_ts64(struct timespec64 *tv);
extern void ktime_get_coarse_ts64(struct timespec64 *ts);
extern void ktime_get_coarse_real_ts64(struct timespec64 *ts);

void getboottime64(struct timespec64 *ts);

/*
 * time64_t base interfaces
 */
extern time64_t ktime_get_seconds(void);
extern time64_t __ktime_get_real_seconds(void);
extern time64_t ktime_get_real_seconds(void);

/*
 * ktime_t based interfaces
 */

enum tk_offsets {
 TK_OFFS_REAL,
 TK_OFFS_BOOT,
 TK_OFFS_TAI,
 TK_OFFS_MAX,
};

extern ktime_t ktime_get(void);
extern ktime_t ktime_get_with_offset(enum tk_offsets offs);
extern ktime_t ktime_get_coarse_with_offset(enum tk_offsets offs);
extern ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs);
extern ktime_t ktime_get_raw(void);
extern u32 ktime_get_resolution_ns(void);

/**
 * ktime_get_real - get the real (wall-) time in ktime_t format
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_get_real(void)
{
 return ktime_get_with_offset(TK_OFFS_REAL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_get_coarse_real(void)
{
 return ktime_get_coarse_with_offset(TK_OFFS_REAL);
}

/**
 * ktime_get_boottime - Returns monotonic time since boot in ktime_t format
 *
 * This is similar to CLOCK_MONTONIC/ktime_get, but also includes the
 * time spent in suspend.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_get_boottime(void)
{
 return ktime_get_with_offset(TK_OFFS_BOOT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_get_coarse_boottime(void)
{
 return ktime_get_coarse_with_offset(TK_OFFS_BOOT);
}

/**
 * ktime_get_clocktai - Returns the TAI time of day in ktime_t format
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_get_clocktai(void)
{
 return ktime_get_with_offset(TK_OFFS_TAI);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_get_coarse_clocktai(void)
{
 return ktime_get_coarse_with_offset(TK_OFFS_TAI);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_get_coarse(void)
{
 struct timespec64 ts;

 ktime_get_coarse_ts64(&ts);
 return timespec64_to_ktime(ts);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_coarse_ns(void)
{
 return ktime_to_ns(ktime_get_coarse());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_coarse_real_ns(void)
{
 return ktime_to_ns(ktime_get_coarse_real());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_coarse_boottime_ns(void)
{
 return ktime_to_ns(ktime_get_coarse_boottime());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_coarse_clocktai_ns(void)
{
 return ktime_to_ns(ktime_get_coarse_clocktai());
}

/**
 * ktime_mono_to_real - Convert monotonic time to clock realtime
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t ktime_mono_to_real(ktime_t mono)
{
 return ktime_mono_to_any(mono, TK_OFFS_REAL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_ns(void)
{
 return ktime_to_ns(ktime_get());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_real_ns(void)
{
 return ktime_to_ns(ktime_get_real());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_boottime_ns(void)
{
 return ktime_to_ns(ktime_get_boottime());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_clocktai_ns(void)
{
 return ktime_to_ns(ktime_get_clocktai());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ktime_get_raw_ns(void)
{
 return ktime_to_ns(ktime_get_raw());
}

extern u64 ktime_get_mono_fast_ns(void);
extern u64 ktime_get_raw_fast_ns(void);
extern u64 ktime_get_boot_fast_ns(void);
extern u64 ktime_get_tai_fast_ns(void);
extern u64 ktime_get_real_fast_ns(void);

/*
 * timespec64/time64_t interfaces utilizing the ktime based ones
 * for API completeness, these could be implemented more efficiently
 * if needed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ktime_get_boottime_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_boottime()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ktime_get_coarse_boottime_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_coarse_boottime()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) time64_t ktime_get_boottime_seconds(void)
{
 return ktime_divns(ktime_get_coarse_boottime(), 1000000000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ktime_get_clocktai_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_clocktai()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ktime_get_coarse_clocktai_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_coarse_clocktai()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) time64_t ktime_get_clocktai_seconds(void)
{
 return ktime_divns(ktime_get_coarse_clocktai(), 1000000000L);
}

/*
 * RTC specific
 */
extern bool timekeeping_rtc_skipsuspend(void);
extern bool timekeeping_rtc_skipresume(void);

extern void timekeeping_inject_sleeptime64(const struct timespec64 *delta);

/*
 * struct ktime_timestanps - Simultaneous mono/boot/real timestamps
 * @mono:	Monotonic timestamp
 * @boot:	Boottime timestamp
 * @real:	Realtime timestamp
 */
struct ktime_timestamps {
 u64 mono;
 u64 boot;
 u64 real;
};

/**
 * struct system_time_snapshot - simultaneous raw/real time capture with
 *				 counter value
 * @cycles:	Clocksource counter value to produce the system times
 * @real:	Realtime system time
 * @raw:	Monotonic raw system time
 * @clock_was_set_seq:	The sequence number of clock was set events
 * @cs_was_changed_seq:	The sequence number of clocksource change events
 */
struct system_time_snapshot {
 u64 cycles;
 ktime_t real;
 ktime_t raw;
 enum clocksource_ids cs_id;
 unsigned int clock_was_set_seq;
 u8 cs_was_changed_seq;
};

/**
 * struct system_device_crosststamp - system/device cross-timestamp
 *				      (synchronized capture)
 * @device:		Device time
 * @sys_realtime:	Realtime simultaneous with device time
 * @sys_monoraw:	Monotonic raw simultaneous with device time
 */
struct system_device_crosststamp {
 ktime_t device;
 ktime_t sys_realtime;
 ktime_t sys_monoraw;
};

/**
 * struct system_counterval_t - system counter value with the pointer to the
 *				corresponding clocksource
 * @cycles:	System counter value
 * @cs:		Clocksource corresponding to system counter value. Used by
 *		timekeeping code to verify comparibility of two cycle values
 */
struct system_counterval_t {
 u64 cycles;
 struct clocksource *cs;
};

/*
 * Get cross timestamp between system clock and device clock
 */
extern int get_device_system_crosststamp(
   int (*get_time_fn)(ktime_t *device_time,
    struct system_counterval_t *system_counterval,
    void *ctx),
   void *ctx,
   struct system_time_snapshot *history,
   struct system_device_crosststamp *xtstamp);

/*
 * Simultaneously snapshot realtime and monotonic raw clocks
 */
extern void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot);

/* NMI safe mono/boot/realtime timestamps */
extern void ktime_get_fast_timestamps(struct ktime_timestamps *snap);

/*
 * Persistent clock related interfaces
 */
extern int persistent_clock_is_local;

extern void read_persistent_clock64(struct timespec64 *ts);
void read_persistent_wall_and_boot_offset(struct timespec64 *wall_clock,
       struct timespec64 *boot_offset);
# 233 "./include/linux/ktime.h" 2
# 7 "./include/linux/timer.h" 2

# 1 "./include/linux/debugobjects.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






enum debug_obj_state {
 ODEBUG_STATE_NONE,
 ODEBUG_STATE_INIT,
 ODEBUG_STATE_INACTIVE,
 ODEBUG_STATE_ACTIVE,
 ODEBUG_STATE_DESTROYED,
 ODEBUG_STATE_NOTAVAILABLE,
 ODEBUG_STATE_MAX,
};

struct debug_obj_descr;

/**
 * struct debug_obj - representation of an tracked object
 * @node:	hlist node to link the object into the tracker list
 * @state:	tracked object state
 * @astate:	current active state
 * @object:	pointer to the real object
 * @descr:	pointer to an object type specific debug description structure
 */
struct debug_obj {
 struct hlist_node node;
 enum debug_obj_state state;
 unsigned int astate;
 void *object;
 const struct debug_obj_descr *descr;
};

/**
 * struct debug_obj_descr - object type specific debug description structure
 *
 * @name:		name of the object typee
 * @debug_hint:		function returning address, which have associated
 *			kernel symbol, to allow identify the object
 * @is_static_object:	return true if the obj is static, otherwise return false
 * @fixup_init:		fixup function, which is called when the init check
 *			fails. All fixup functions must return true if fixup
 *			was successful, otherwise return false
 * @fixup_activate:	fixup function, which is called when the activate check
 *			fails
 * @fixup_destroy:	fixup function, which is called when the destroy check
 *			fails
 * @fixup_free:		fixup function, which is called when the free check
 *			fails
 * @fixup_assert_init:  fixup function, which is called when the assert_init
 *			check fails
 */
struct debug_obj_descr {
 const char *name;
 void *(*debug_hint)(void *addr);
 bool (*is_static_object)(void *addr);
 bool (*fixup_init)(void *addr, enum debug_obj_state state);
 bool (*fixup_activate)(void *addr, enum debug_obj_state state);
 bool (*fixup_destroy)(void *addr, enum debug_obj_state state);
 bool (*fixup_free)(void *addr, enum debug_obj_state state);
 bool (*fixup_assert_init)(void *addr, enum debug_obj_state state);
};
# 88 "./include/linux/debugobjects.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_object_init (void *addr, const struct debug_obj_descr *descr) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_object_init_on_stack(void *addr, const struct debug_obj_descr *descr) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
debug_object_activate (void *addr, const struct debug_obj_descr *descr) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_object_deactivate(void *addr, const struct debug_obj_descr *descr) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_object_destroy (void *addr, const struct debug_obj_descr *descr) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_object_free (void *addr, const struct debug_obj_descr *descr) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_object_assert_init(void *addr, const struct debug_obj_descr *descr) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void debug_objects_early_init(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void debug_objects_mem_init(void) { }





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
debug_check_no_obj_freed(const void *address, unsigned long size) { }
# 9 "./include/linux/timer.h" 2


struct timer_list {
 /*
	 * All fields that change during normal runtime grouped to the
	 * same cacheline
	 */
 struct hlist_node entry;
 unsigned long expires;
 void (*function)(struct timer_list *);
 u32 flags;




};
# 39 "./include/linux/timer.h"
/**
 * @TIMER_DEFERRABLE: A deferrable timer will work normally when the
 * system is busy, but will not cause a CPU to come out of idle just
 * to service it; instead, the timer will be serviced when the CPU
 * eventually wakes up with a subsequent non-deferrable timer.
 *
 * @TIMER_IRQSAFE: An irqsafe timer is executed with IRQ disabled and
 * it's safe to wait for the completion of the running instance from
 * IRQ handlers, for example, by calling del_timer_sync().
 *
 * Note: The irq disabled callback execution is a special case for
 * workqueue locking issues. It's not meant for executing random crap
 * with interrupts disabled. Abuse is monitored!
 *
 * @TIMER_PINNED: A pinned timer will not be affected by any timer
 * placement heuristics (like, NOHZ) and will always expire on the CPU
 * on which the timer was enqueued.
 *
 * Note: Because enqueuing of timers can migrate the timer from one
 * CPU to another, pinned timers are not guaranteed to stay on the
 * initialy selected CPU.  They move to the CPU on which the enqueue
 * function is invoked via mod_timer() or add_timer().  If the timer
 * should be placed on a particular CPU, then add_timer_on() has to be
 * used.
 */
# 88 "./include/linux/timer.h"
/*
 * LOCKDEP and DEBUG timer interfaces.
 */
void init_timer_key(struct timer_list *timer,
      void (*func)(struct timer_list *), unsigned int flags,
      const char *name, struct lock_class_key *key);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_timer_on_stack_key(struct timer_list *timer,
        void (*func)(struct timer_list *),
        unsigned int flags,
        const char *name,
        struct lock_class_key *key)
{
 init_timer_key(timer, func, flags, name, key);
}
# 131 "./include/linux/timer.h"
/**
 * timer_setup - prepare a timer for first use
 * @timer: the timer in question
 * @callback: the function to call when timer expires
 * @flags: any TIMER_* flags
 *
 * Regular timer initialization should use either DEFINE_TIMER() above,
 * or timer_setup(). For timers on the stack, timer_setup_on_stack() must
 * be used and must be balanced with a call to destroy_timer_on_stack().
 */
# 150 "./include/linux/timer.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void destroy_timer_on_stack(struct timer_list *timer) { }





/**
 * timer_pending - is a timer pending?
 * @timer: the timer in question
 *
 * timer_pending will tell whether a given timer is currently pending,
 * or not. Callers must ensure serialization wrt. other operations done
 * to this timer, eg. interrupt contexts, or other CPUs on SMP.
 *
 * return value: 1 if the timer is pending, 0 if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int timer_pending(const struct timer_list * timer)
{
 return !hlist_unhashed_lockless(&timer->entry);
}

extern void add_timer_on(struct timer_list *timer, int cpu);
extern int mod_timer(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);
extern int timer_reduce(struct timer_list *timer, unsigned long expires);

/*
 * The jiffies value which is added to now, when there is no timer
 * in the timer wheel:
 */


extern void add_timer(struct timer_list *timer);

extern int try_to_del_timer_sync(struct timer_list *timer);
extern int timer_delete_sync(struct timer_list *timer);
extern int timer_delete(struct timer_list *timer);
extern int timer_shutdown_sync(struct timer_list *timer);
extern int timer_shutdown(struct timer_list *timer);

/**
 * del_timer_sync - Delete a pending timer and wait for a running callback
 * @timer:	The timer to be deleted
 *
 * See timer_delete_sync() for detailed explanation.
 *
 * Do not use in new code. Use timer_delete_sync() instead.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int del_timer_sync(struct timer_list *timer)
{
 return timer_delete_sync(timer);
}

/**
 * del_timer - Delete a pending timer
 * @timer:	The timer to be deleted
 *
 * See timer_delete() for detailed explanation.
 *
 * Do not use in new code. Use timer_delete() instead.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int del_timer(struct timer_list *timer)
{
 return timer_delete(timer);
}

extern void init_timers(void);
struct hrtimer;
extern enum hrtimer_restart it_real_fn(struct hrtimer *);

unsigned long __round_jiffies(unsigned long j, int cpu);
unsigned long __round_jiffies_relative(unsigned long j, int cpu);
unsigned long round_jiffies(unsigned long j);
unsigned long round_jiffies_relative(unsigned long j);

unsigned long __round_jiffies_up(unsigned long j, int cpu);
unsigned long __round_jiffies_up_relative(unsigned long j, int cpu);
unsigned long round_jiffies_up(unsigned long j);
unsigned long round_jiffies_up_relative(unsigned long j);


int timers_prepare_cpu(unsigned int cpu);
int timers_dead_cpu(unsigned int cpu);
# 10 "./include/linux/workqueue.h" 2








struct workqueue_struct;

struct work_struct;
typedef void (*work_func_t)(struct work_struct *work);
void delayed_work_timer_fn(struct timer_list *t);

/*
 * The first word is the work queue pointer and the flags rolled into
 * one
 */


enum {
 WORK_STRUCT_PENDING_BIT = 0, /* work item is pending execution */
 WORK_STRUCT_INACTIVE_BIT= 1, /* work item is inactive */
 WORK_STRUCT_PWQ_BIT = 2, /* data points to pwq */
 WORK_STRUCT_LINKED_BIT = 3, /* next work is linked to this one */




 WORK_STRUCT_COLOR_SHIFT = 4, /* color for workqueue flushing */


 WORK_STRUCT_COLOR_BITS = 4,

 WORK_STRUCT_PENDING = 1 << WORK_STRUCT_PENDING_BIT,
 WORK_STRUCT_INACTIVE = 1 << WORK_STRUCT_INACTIVE_BIT,
 WORK_STRUCT_PWQ = 1 << WORK_STRUCT_PWQ_BIT,
 WORK_STRUCT_LINKED = 1 << WORK_STRUCT_LINKED_BIT,



 WORK_STRUCT_STATIC = 0,


 WORK_NR_COLORS = (1 << WORK_STRUCT_COLOR_BITS),

 /* not bound to any CPU, prefer the local CPU */
 WORK_CPU_UNBOUND = 256,

 /*
	 * Reserve 8 bits off of pwq pointer w/ debugobjects turned off.
	 * This makes pwqs aligned to 256 bytes and allows 16 workqueue
	 * flush colors.
	 */
 WORK_STRUCT_FLAG_BITS = WORK_STRUCT_COLOR_SHIFT +
      WORK_STRUCT_COLOR_BITS,

 /* data contains off-queue information when !WORK_STRUCT_PWQ */
 WORK_OFFQ_FLAG_BASE = WORK_STRUCT_COLOR_SHIFT,

 __WORK_OFFQ_CANCELING = WORK_OFFQ_FLAG_BASE,
 WORK_OFFQ_CANCELING = (1 << __WORK_OFFQ_CANCELING),

 /*
	 * When a work item is off queue, its high bits point to the last
	 * pool it was on.  Cap at 31 bits and use the highest number to
	 * indicate that no pool is associated.
	 */
 WORK_OFFQ_FLAG_BITS = 1,
 WORK_OFFQ_POOL_SHIFT = WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
 WORK_OFFQ_LEFT = 64 - WORK_OFFQ_POOL_SHIFT,
 WORK_OFFQ_POOL_BITS = WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,
 WORK_OFFQ_POOL_NONE = (1LU << WORK_OFFQ_POOL_BITS) - 1,

 /* convenience constants */
 WORK_STRUCT_FLAG_MASK = (1UL << WORK_STRUCT_FLAG_BITS) - 1,
 WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
 WORK_STRUCT_NO_POOL = (unsigned long)WORK_OFFQ_POOL_NONE << WORK_OFFQ_POOL_SHIFT,

 /* bit mask for work_busy() return values */
 WORK_BUSY_PENDING = 1 << 0,
 WORK_BUSY_RUNNING = 1 << 1,

 /* maximum string length for set_worker_desc() */
 WORKER_DESC_LEN = 24,
};

struct work_struct {
 atomic_long_t data;
 struct list_head entry;
 work_func_t func;



};





struct delayed_work {
 struct work_struct work;
 struct timer_list timer;

 /* target workqueue and CPU ->timer uses to queue ->work */
 struct workqueue_struct *wq;
 int cpu;
};

struct rcu_work {
 struct work_struct work;
 struct callback_head rcu;

 /* target workqueue ->rcu uses to queue ->work */
 struct workqueue_struct *wq;
};

/**
 * struct workqueue_attrs - A struct for workqueue attributes.
 *
 * This can be used to change attributes of an unbound workqueue.
 */
struct workqueue_attrs {
 /**
	 * @nice: nice level
	 */
 int nice;

 /**
	 * @cpumask: allowed CPUs
	 */
 cpumask_var_t cpumask;

 /**
	 * @no_numa: disable NUMA affinity
	 *
	 * Unlike other fields, ``no_numa`` isn't a property of a worker_pool. It
	 * only modifies how :c:func:`apply_workqueue_attrs` select pools and thus
	 * doesn't participate in pool hash calculations or equality comparisons.
	 */
 bool no_numa;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct delayed_work *to_delayed_work(struct work_struct *work)
{
 return ({ void *__mptr = (void *)(work); _Static_assert(__builtin_types_compatible_p(typeof(*(work)), typeof(((struct delayed_work *)0)->work)) || __builtin_types_compatible_p(typeof(*(work)), typeof(void)), "pointer type mismatch in container_of()"); ((struct delayed_work *)(__mptr - __builtin_offsetof(struct delayed_work, work))); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct rcu_work *to_rcu_work(struct work_struct *work)
{
 return ({ void *__mptr = (void *)(work); _Static_assert(__builtin_types_compatible_p(typeof(*(work)), typeof(((struct rcu_work *)0)->work)) || __builtin_types_compatible_p(typeof(*(work)), typeof(void)), "pointer type mismatch in container_of()"); ((struct rcu_work *)(__mptr - __builtin_offsetof(struct rcu_work, work))); });
}

struct execute_work {
 struct work_struct work;
};
# 210 "./include/linux/workqueue.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __init_work(struct work_struct *work, int onstack) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void destroy_work_on_stack(struct work_struct *work) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void destroy_delayed_work_on_stack(struct delayed_work *work) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int work_static(struct work_struct *work) { return 0; }


/*
 * initialize all of a work item in one go
 *
 * NOTE! No point in using "atomic_long_set()": using a direct
 * assignment of the work data initializer allows the compiler
 * to generate better code.
 */
# 284 "./include/linux/workqueue.h"
/**
 * work_pending - Find out whether a work item is currently pending
 * @work: The work item in question
 */



/**
 * delayed_work_pending - Find out whether a delayable work item is currently
 * pending
 * @w: The work item in question
 */



/*
 * Workqueue flags and constants.  For details, please refer to
 * Documentation/core-api/workqueue.rst.
 */
enum {
 WQ_UNBOUND = 1 << 1, /* not bound to any cpu */
 WQ_FREEZABLE = 1 << 2, /* freeze during suspend */
 WQ_MEM_RECLAIM = 1 << 3, /* may be used for memory reclaim */
 WQ_HIGHPRI = 1 << 4, /* high priority */
 WQ_CPU_INTENSIVE = 1 << 5, /* cpu intensive workqueue */
 WQ_SYSFS = 1 << 6, /* visible in sysfs, see workqueue_sysfs_register() */

 /*
	 * Per-cpu workqueues are generally preferred because they tend to
	 * show better performance thanks to cache locality.  Per-cpu
	 * workqueues exclude the scheduler from choosing the CPU to
	 * execute the worker threads, which has an unfortunate side effect
	 * of increasing power consumption.
	 *
	 * The scheduler considers a CPU idle if it doesn't have any task
	 * to execute and tries to keep idle cores idle to conserve power;
	 * however, for example, a per-cpu work item scheduled from an
	 * interrupt handler on an idle CPU will force the scheduler to
	 * execute the work item on that CPU breaking the idleness, which in
	 * turn may lead to more scheduling choices which are sub-optimal
	 * in terms of power consumption.
	 *
	 * Workqueues marked with WQ_POWER_EFFICIENT are per-cpu by default
	 * but become unbound if workqueue.power_efficient kernel param is
	 * specified.  Per-cpu workqueues which are identified to
	 * contribute significantly to power-consumption are identified and
	 * marked with this flag and enabling the power_efficient mode
	 * leads to noticeable power saving at the cost of small
	 * performance disadvantage.
	 *
	 * http://thread.gmane.org/gmane.linux.kernel/1480396
	 */
 WQ_POWER_EFFICIENT = 1 << 7,

 __WQ_DRAINING = 1 << 16, /* internal: workqueue is draining */
 __WQ_ORDERED = 1 << 17, /* internal: workqueue is ordered */
 __WQ_LEGACY = 1 << 18, /* internal: create*_workqueue() */
 __WQ_ORDERED_EXPLICIT = 1 << 19, /* internal: alloc_ordered_workqueue() */

 WQ_MAX_ACTIVE = 512, /* I like 512, better ideas? */
 WQ_MAX_UNBOUND_PER_CPU = 4, /* 4 * #cpus for unbound wq */
 WQ_DFL_ACTIVE = WQ_MAX_ACTIVE / 2,
};

/* unbound wq's aren't per-cpu, scale max_active according to #cpus */



/*
 * System-wide workqueues which are always present.
 *
 * system_wq is the one used by schedule[_delayed]_work[_on]().
 * Multi-CPU multi-threaded.  There are users which expect relatively
 * short queue flush time.  Don't queue works which can run for too
 * long.
 *
 * system_highpri_wq is similar to system_wq but for work items which
 * require WQ_HIGHPRI.
 *
 * system_long_wq is similar to system_wq but may host long running
 * works.  Queue flushing might take relatively long.
 *
 * system_unbound_wq is unbound workqueue.  Workers are not bound to
 * any specific CPU, not concurrency managed, and all queued works are
 * executed immediately as long as max_active limit is not reached and
 * resources are available.
 *
 * system_freezable_wq is equivalent to system_wq except that it's
 * freezable.
 *
 * *_power_efficient_wq are inclined towards saving power and converted
 * into WQ_UNBOUND variants if 'wq_power_efficient' is enabled; otherwise,
 * they are same as their non-power-efficient counterparts - e.g.
 * system_power_efficient_wq is identical to system_wq if
 * 'wq_power_efficient' is disabled.  See WQ_POWER_EFFICIENT for more info.
 */
extern struct workqueue_struct *system_wq;
extern struct workqueue_struct *system_highpri_wq;
extern struct workqueue_struct *system_long_wq;
extern struct workqueue_struct *system_unbound_wq;
extern struct workqueue_struct *system_freezable_wq;
extern struct workqueue_struct *system_power_efficient_wq;
extern struct workqueue_struct *system_freezable_power_efficient_wq;

/**
 * alloc_workqueue - allocate a workqueue
 * @fmt: printf format for the name of the workqueue
 * @flags: WQ_* flags
 * @max_active: max in-flight work items, 0 for default
 * remaining args: args for @fmt
 *
 * Allocate a workqueue with the specified parameters.  For detailed
 * information on WQ_* flags, please refer to
 * Documentation/core-api/workqueue.rst.
 *
 * RETURNS:
 * Pointer to the allocated workqueue on success, %NULL on failure.
 */
__attribute__((__format__(printf, 1, 4))) struct workqueue_struct *
alloc_workqueue(const char *fmt, unsigned int flags, int max_active, ...);

/**
 * alloc_ordered_workqueue - allocate an ordered workqueue
 * @fmt: printf format for the name of the workqueue
 * @flags: WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)
 * @args: args for @fmt
 *
 * Allocate an ordered workqueue.  An ordered workqueue executes at
 * most one work item at any given time in the queued order.  They are
 * implemented as unbound workqueues with @max_active of one.
 *
 * RETURNS:
 * Pointer to the allocated workqueue on success, %NULL on failure.
 */
# 430 "./include/linux/workqueue.h"
extern void destroy_workqueue(struct workqueue_struct *wq);

struct workqueue_attrs *alloc_workqueue_attrs(void);
void free_workqueue_attrs(struct workqueue_attrs *attrs);
int apply_workqueue_attrs(struct workqueue_struct *wq,
     const struct workqueue_attrs *attrs);
int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);

extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
   struct work_struct *work);
extern bool queue_work_node(int node, struct workqueue_struct *wq,
       struct work_struct *work);
extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
   struct delayed_work *work, unsigned long delay);
extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
   struct delayed_work *dwork, unsigned long delay);
extern bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork);

extern void __flush_workqueue(struct workqueue_struct *wq);
extern void drain_workqueue(struct workqueue_struct *wq);

extern int schedule_on_each_cpu(work_func_t func);

int execute_in_process_context(work_func_t fn, struct execute_work *);

extern bool flush_work(struct work_struct *work);
extern bool cancel_work(struct work_struct *work);
extern bool cancel_work_sync(struct work_struct *work);

extern bool flush_delayed_work(struct delayed_work *dwork);
extern bool cancel_delayed_work(struct delayed_work *dwork);
extern bool cancel_delayed_work_sync(struct delayed_work *dwork);

extern bool flush_rcu_work(struct rcu_work *rwork);

extern void workqueue_set_max_active(struct workqueue_struct *wq,
         int max_active);
extern struct work_struct *current_work(void);
extern bool current_is_workqueue_rescuer(void);
extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
extern unsigned int work_busy(struct work_struct *work);
extern __attribute__((__format__(printf, 1, 2))) void set_worker_desc(const char *fmt, ...);
extern void print_worker_info(const char *log_lvl, struct task_struct *task);
extern void show_all_workqueues(void);
extern void show_one_workqueue(struct workqueue_struct *wq);
extern void wq_worker_comm(char *buf, size_t size, struct task_struct *task);

/**
 * queue_work - queue work on a workqueue
 * @wq: workqueue to use
 * @work: work to queue
 *
 * Returns %false if @work was already on a queue, %true otherwise.
 *
 * We queue the work to the CPU on which it was submitted, but if the CPU dies
 * it can be processed by another CPU.
 *
 * Memory-ordering properties:  If it returns %true, guarantees that all stores
 * preceding the call to queue_work() in the program order will be visible from
 * the CPU which will execute @work by the time such work executes, e.g.,
 *
 * { x is initially 0 }
 *
 *   CPU0				CPU1
 *
 *   WRITE_ONCE(x, 1);			[ @work is being executed ]
 *   r0 = queue_work(wq, work);		  r1 = READ_ONCE(x);
 *
 * Forbids: r0 == true && r1 == 0
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool queue_work(struct workqueue_struct *wq,
         struct work_struct *work)
{
 return queue_work_on(WORK_CPU_UNBOUND, wq, work);
}

/**
 * queue_delayed_work - queue work on a workqueue after delay
 * @wq: workqueue to use
 * @dwork: delayable work to queue
 * @delay: number of jiffies to wait before queueing
 *
 * Equivalent to queue_delayed_work_on() but tries to use the local CPU.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool queue_delayed_work(struct workqueue_struct *wq,
          struct delayed_work *dwork,
          unsigned long delay)
{
 return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
}

/**
 * mod_delayed_work - modify delay of or queue a delayed work
 * @wq: workqueue to use
 * @dwork: work to queue
 * @delay: number of jiffies to wait before queueing
 *
 * mod_delayed_work_on() on local CPU.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mod_delayed_work(struct workqueue_struct *wq,
        struct delayed_work *dwork,
        unsigned long delay)
{
 return mod_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
}

/**
 * schedule_work_on - put work task on a specific cpu
 * @cpu: cpu to put the work task on
 * @work: job to be done
 *
 * This puts a job on a specific cpu
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool schedule_work_on(int cpu, struct work_struct *work)
{
 return queue_work_on(cpu, system_wq, work);
}

/**
 * schedule_work - put work task in global workqueue
 * @work: job to be done
 *
 * Returns %false if @work was already on the kernel-global workqueue and
 * %true otherwise.
 *
 * This puts a job in the kernel-global workqueue if it was not already
 * queued and leaves it in the same position on the kernel-global
 * workqueue otherwise.
 *
 * Shares the same memory-ordering properties of queue_work(), cf. the
 * DocBook header of queue_work().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool schedule_work(struct work_struct *work)
{
 return queue_work(system_wq, work);
}

/*
 * Detect attempt to flush system-wide workqueues at compile time when possible.
 *
 * See https://lkml.kernel.org/r/49925af7-78a8-a3dd-bce6-cfc02e1a9236@I-love.SAKURA.ne.jp
 * for reasons and steps for converting system-wide workqueues into local workqueues.
 */
extern void __warn_flushing_systemwide_wq(void)
 __attribute__((__warning__("Please avoid flushing system-wide workqueues.")));

/**
 * flush_scheduled_work - ensure that any scheduled work has run to completion.
 *
 * Forces execution of the kernel-global workqueue and blocks until its
 * completion.
 *
 * It's very easy to get into trouble if you don't take great care.
 * Either of the following situations will lead to deadlock:
 *
 *	One of the work items currently on the workqueue needs to acquire
 *	a lock held by your code or its caller.
 *
 *	Your code is running in the context of a work routine.
 *
 * They will be detected by lockdep when they occur, but the first might not
 * occur very often.  It depends on what work items are on the workqueue and
 * what locks they need, which you have no control over.
 *
 * In most situations flushing the entire workqueue is overkill; you merely
 * need to know that a particular work item isn't queued and isn't running.
 * In such cases you should use cancel_delayed_work_sync() or
 * cancel_work_sync() instead.
 *
 * Please stop calling this function! A conversion to stop flushing system-wide
 * workqueues is in progress. This function will be removed after all in-tree
 * users stopped calling this function.
 */
/*
 * The background of commit 771c035372a036f8 ("deprecate the
 * '__deprecated' attribute warnings entirely and for good") is that,
 * since Linus builds all modules between every single pull he does,
 * the standard kernel build needs to be _clean_ in order to be able to
 * notice when new problems happen. Therefore, don't emit warning while
 * there are in-tree users.
 */







/*
 * Although there is no longer in-tree caller, for now just emit warning
 * in order to give out-of-tree callers time to update.
 */
# 644 "./include/linux/workqueue.h"
/**
 * schedule_delayed_work_on - queue work in global workqueue on CPU after delay
 * @cpu: cpu to use
 * @dwork: job to be done
 * @delay: number of jiffies to wait
 *
 * After waiting for a given time this puts a job in the kernel-global
 * workqueue on the specified CPU.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork,
         unsigned long delay)
{
 return queue_delayed_work_on(cpu, system_wq, dwork, delay);
}

/**
 * schedule_delayed_work - put work task in global workqueue after delay
 * @dwork: job to be done
 * @delay: number of jiffies to wait or 0 for immediate execution
 *
 * After waiting for a given time this puts a job in the kernel-global
 * workqueue.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool schedule_delayed_work(struct delayed_work *dwork,
      unsigned long delay)
{
 return queue_delayed_work(system_wq, dwork, delay);
}
# 683 "./include/linux/workqueue.h"
long work_on_cpu(int cpu, long (*fn)(void *), void *arg);
long work_on_cpu_safe(int cpu, long (*fn)(void *), void *arg);



extern void freeze_workqueues_begin(void);
extern bool freeze_workqueues_busy(void);
extern void thaw_workqueues(void);



int workqueue_sysfs_register(struct workqueue_struct *wq);
# 703 "./include/linux/workqueue.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void wq_watchdog_touch(int cpu) { }



int workqueue_prepare_cpu(unsigned int cpu);
int workqueue_online_cpu(unsigned int cpu);
int workqueue_offline_cpu(unsigned int cpu);


void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) workqueue_init_early(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) workqueue_init(void);
# 16 "./include/linux/rhashtable-types.h" 2

struct rhash_head {
 struct rhash_head /* nothing */ *next;
};

struct rhlist_head {
 struct rhash_head rhead;
 struct rhlist_head /* nothing */ *next;
};

struct bucket_table;

/**
 * struct rhashtable_compare_arg - Key for the function rhashtable_compare
 * @ht: Hash table
 * @key: Key to compare against
 */
struct rhashtable_compare_arg {
 struct rhashtable *ht;
 const void *key;
};

typedef u32 (*rht_hashfn_t)(const void *data, u32 len, u32 seed);
typedef u32 (*rht_obj_hashfn_t)(const void *data, u32 len, u32 seed);
typedef int (*rht_obj_cmpfn_t)(struct rhashtable_compare_arg *arg,
          const void *obj);

/**
 * struct rhashtable_params - Hash table construction parameters
 * @nelem_hint: Hint on number of elements, should be 75% of desired size
 * @key_len: Length of key
 * @key_offset: Offset of key in struct to be hashed
 * @head_offset: Offset of rhash_head in struct to be hashed
 * @max_size: Maximum size while expanding
 * @min_size: Minimum size while shrinking
 * @automatic_shrinking: Enable automatic shrinking of tables
 * @hashfn: Hash function (default: jhash2 if !(key_len % 4), or jhash)
 * @obj_hashfn: Function to hash object
 * @obj_cmpfn: Function to compare key with object
 */
struct rhashtable_params {
 u16 nelem_hint;
 u16 key_len;
 u16 key_offset;
 u16 head_offset;
 unsigned int max_size;
 u16 min_size;
 bool automatic_shrinking;
 rht_hashfn_t hashfn;
 rht_obj_hashfn_t obj_hashfn;
 rht_obj_cmpfn_t obj_cmpfn;
};

/**
 * struct rhashtable - Hash table handle
 * @tbl: Bucket table
 * @key_len: Key length for hashfn
 * @max_elems: Maximum number of elements in table
 * @p: Configuration parameters
 * @rhlist: True if this is an rhltable
 * @run_work: Deferred worker to expand/shrink asynchronously
 * @mutex: Mutex to protect current/future table swapping
 * @lock: Spin lock to protect walker list
 * @nelems: Number of elements in table
 */
struct rhashtable {
 struct bucket_table /* nothing */ *tbl;
 unsigned int key_len;
 unsigned int max_elems;
 struct rhashtable_params p;
 bool rhlist;
 struct work_struct run_work;
 struct mutex mutex;
 spinlock_t lock;
 atomic_t nelems;
};

/**
 * struct rhltable - Hash table with duplicate objects in a list
 * @ht: Underlying rhtable
 */
struct rhltable {
 struct rhashtable ht;
};

/**
 * struct rhashtable_walker - Hash table walker
 * @list: List entry on list of walkers
 * @tbl: The table that we were walking over
 */
struct rhashtable_walker {
 struct list_head list;
 struct bucket_table *tbl;
};

/**
 * struct rhashtable_iter - Hash table iterator
 * @ht: Table to iterate through
 * @p: Current pointer
 * @list: Current hash list pointer
 * @walker: Associated rhashtable walker
 * @slot: Current slot
 * @skip: Number of entries to skip in slot
 */
struct rhashtable_iter {
 struct rhashtable *ht;
 struct rhash_head *p;
 struct rhlist_head *list;
 struct rhashtable_walker walker;
 unsigned int slot;
 unsigned int skip;
 bool end_of_table;
};

int rhashtable_init(struct rhashtable *ht,
      const struct rhashtable_params *params);
int rhltable_init(struct rhltable *hlt,
    const struct rhashtable_params *params);
# 8 "./include/linux/ipc.h" 2
# 1 "./include/uapi/linux/ipc.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */







/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct ipc_perm
{
 __kernel_key_t key;
 __kernel_uid_t uid;
 __kernel_gid_t gid;
 __kernel_uid_t cuid;
 __kernel_gid_t cgid;
 __kernel_mode_t mode;
 unsigned short seq;
};

/* Include the definition of ipc64_perm */
# 1 "./arch/arm64/include/generated/uapi/asm/ipcbuf.h" 1
# 1 "./include/uapi/asm-generic/ipcbuf.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





/*
 * The generic ipc64_perm structure:
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * ipc64_perm was originally meant to be architecture specific, but
 * everyone just ended up making identical copies without specific
 * optimizations, so we may just as well all use the same one.
 *
 * Pad space is left for:
 * - 32-bit mode_t on architectures that only had 16 bit
 * - 32-bit seq
 * - 2 miscellaneous 32-bit values
 */

struct ipc64_perm {
 __kernel_key_t key;
 __kernel_uid32_t uid;
 __kernel_gid32_t gid;
 __kernel_uid32_t cuid;
 __kernel_gid32_t cgid;
 __kernel_mode_t mode;
    /* pad if mode_t is u16: */
 unsigned char __pad1[4 - sizeof(__kernel_mode_t)];
 unsigned short seq;
 unsigned short __pad2;
 __kernel_ulong_t __unused1;
 __kernel_ulong_t __unused2;
};
# 2 "./arch/arm64/include/generated/uapi/asm/ipcbuf.h" 2
# 23 "./include/uapi/linux/ipc.h" 2

/* resource get request flags */




/* these fields are used by the DIPC package so the kernel as standard
   should avoid using them if possible */




/*
 * Control commands used with semctl, msgctl and shmctl
 * see also specific commands in sem.h, msg.h and shm.h
 */





/*
 * Version flags for semctl, msgctl, and shmctl commands
 * These are passed as bitflags or-ed with the actual command
 */





/*
 * These are used to wrap system calls.
 *
 * See architecture code for ugly details..
 */
struct ipc_kludge {
 struct msgbuf /* nothing */ *msgp;
 long msgtyp;
};
# 76 "./include/uapi/linux/ipc.h"
/* Used by the DIPC package, try and avoid reusing it */
# 9 "./include/linux/ipc.h" 2


/* used by in-kernel data structures */
struct kern_ipc_perm {
 spinlock_t lock;
 bool deleted;
 int id;
 key_t key;
 kuid_t uid;
 kgid_t gid;
 kuid_t cuid;
 kgid_t cgid;
 umode_t mode;
 unsigned long seq;
 void *security;

 struct rhash_head khtnode;

 struct callback_head rcu;
 refcount_t refcount;
} __attribute__((__aligned__((1 << (6))))) ;
# 6 "./include/uapi/linux/sem.h" 2

/* semop flags */


/* semctl Command Definitions. */
# 19 "./include/uapi/linux/sem.h"
/* ipcs ctl cmds */




/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct semid_ds {
 struct ipc_perm sem_perm; /* permissions .. see ipc.h */
 __kernel_old_time_t sem_otime; /* last semop time */
 __kernel_old_time_t sem_ctime; /* create/last semctl() time */
 struct sem *sem_base; /* ptr to first semaphore in array */
 struct sem_queue *sem_pending; /* pending operations to be processed */
 struct sem_queue **sem_pending_last; /* last pending operation */
 struct sem_undo *undo; /* undo requests on this array */
 unsigned short sem_nsems; /* no. of semaphores in array */
};

/* Include the definition of semid64_ds */
# 1 "./arch/arm64/include/generated/uapi/asm/sembuf.h" 1
# 1 "./include/uapi/asm-generic/sembuf.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */




# 1 "./arch/arm64/include/generated/uapi/asm/ipcbuf.h" 1
# 7 "./include/uapi/asm-generic/sembuf.h" 2

/*
 * The semid64_ds structure for most architectures (though it came from x86_32
 * originally). Note extra padding because this structure is passed back and
 * forth between kernel and user space.
 *
 * semid64_ds was originally meant to be architecture specific, but
 * everyone just ended up making identical copies without specific
 * optimizations, so we may just as well all use the same one.
 *
 * 64 bit architectures use a 64-bit long time field here, while
 * 32 bit architectures have a pair of unsigned long values.
 *
 * On big-endian systems, the padding is in the wrong place for
 * historic reasons, so user space has to reconstruct a time_t
 * value using
 *
 * user_semid_ds.sem_otime = kernel_semid64_ds.sem_otime +
 *		((long long)kernel_semid64_ds.sem_otime_high << 32)
 *
 * Pad space is left for 2 miscellaneous 32-bit values
 */
struct semid64_ds {
 struct ipc64_perm sem_perm; /* permissions .. see ipc.h */

 long sem_otime; /* last semop time */
 long sem_ctime; /* last change time */






 unsigned long sem_nsems; /* no. of semaphores in array */
 unsigned long __unused3;
 unsigned long __unused4;
};
# 2 "./arch/arm64/include/generated/uapi/asm/sembuf.h" 2
# 38 "./include/uapi/linux/sem.h" 2

/* semop system calls takes an array of these. */
struct sembuf {
 unsigned short sem_num; /* semaphore index in array */
 short sem_op; /* semaphore operation */
 short sem_flg; /* operation flags */
};

/* arg for semctl system calls. */
union semun {
 int val; /* value for SETVAL */
 struct semid_ds /* nothing */ *buf; /* buffer for IPC_STAT & IPC_SET */
 unsigned short /* nothing */ *array; /* array for GETALL & SETALL */
 struct seminfo /* nothing */ *__buf; /* buffer for IPC_INFO */
 void /* nothing */ *__pad;
};

struct seminfo {
 int semmap;
 int semmni;
 int semmns;
 int semmnu;
 int semmsl;
 int semopm;
 int semume;
 int semusz;
 int semvmx;
 int semaem;
};

/*
 * SEMMNI, SEMMSL and SEMMNS are default values which can be
 * modified by sysctl.
 * The values has been chosen to be larger than necessary for any
 * known configuration.
 *
 * SEMOPM should not be increased beyond 1000, otherwise there is the
 * risk that semop()/semtimedop() fails due to kernel memory fragmentation when
 * allocating the sop array.
 */
# 87 "./include/uapi/linux/sem.h"
/* unused */
# 6 "./include/linux/sem.h" 2

struct task_struct;
struct sem_undo_list;



struct sysv_sem {
 struct sem_undo_list *undo_list;
};

extern int copy_semundo(unsigned long clone_flags, struct task_struct *tsk);
extern void exit_sem(struct task_struct *tsk);
# 16 "./include/linux/sched.h" 2
# 1 "./include/linux/shm.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/asm/page.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/page.h
 *
 * Copyright (C) 1995-2003 Russell King
 * Copyright (C) 2012 ARM Ltd.
 */







# 1 "./include/linux/personality.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/personality.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */




/*
 * Flags for bug emulation.
 *
 * These occupy the top three bytes.
 */
enum {
 UNAME26 = 0x0020000,
 ADDR_NO_RANDOMIZE = 0x0040000, /* disable randomization of VA space */
 FDPIC_FUNCPTRS = 0x0080000, /* userspace function ptrs point to descriptors
						 * (signal handling)
						 */
 MMAP_PAGE_ZERO = 0x0100000,
 ADDR_COMPAT_LAYOUT = 0x0200000,
 READ_IMPLIES_EXEC = 0x0400000,
 ADDR_LIMIT_32BIT = 0x0800000,
 SHORT_INODE = 0x1000000,
 WHOLE_SECONDS = 0x2000000,
 STICKY_TIMEOUTS = 0x4000000,
 ADDR_LIMIT_3GB = 0x8000000,
};

/*
 * Security-relevant compatibility flags that must be
 * cleared upon setuid or setgid exec:
 */





/*
 * Personality types.
 *
 * These go in the low byte.  Avoid using the top bit, it will
 * conflict with error returns.
 */
enum {
 PER_LINUX = 0x0000,
 PER_LINUX_32BIT = 0x0000 | ADDR_LIMIT_32BIT,
 PER_LINUX_FDPIC = 0x0000 | FDPIC_FUNCPTRS,
 PER_SVR4 = 0x0001 | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
 PER_SVR3 = 0x0002 | STICKY_TIMEOUTS | SHORT_INODE,
 PER_SCOSVR3 = 0x0003 | STICKY_TIMEOUTS |
      WHOLE_SECONDS | SHORT_INODE,
 PER_OSR5 = 0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS,
 PER_WYSEV386 = 0x0004 | STICKY_TIMEOUTS | SHORT_INODE,
 PER_ISCR4 = 0x0005 | STICKY_TIMEOUTS,
 PER_BSD = 0x0006,
 PER_SUNOS = 0x0006 | STICKY_TIMEOUTS,
 PER_XENIX = 0x0007 | STICKY_TIMEOUTS | SHORT_INODE,
 PER_LINUX32 = 0x0008,
 PER_LINUX32_3GB = 0x0008 | ADDR_LIMIT_3GB,
 PER_IRIX32 = 0x0009 | STICKY_TIMEOUTS,/* IRIX5 32-bit */
 PER_IRIXN32 = 0x000a | STICKY_TIMEOUTS,/* IRIX6 new 32-bit */
 PER_IRIX64 = 0x000b | STICKY_TIMEOUTS,/* IRIX6 64-bit */
 PER_RISCOS = 0x000c,
 PER_SOLARIS = 0x000d | STICKY_TIMEOUTS,
 PER_UW7 = 0x000e | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
 PER_OSF4 = 0x000f, /* OSF/1 v4 */
 PER_HPUX = 0x0010,
 PER_MASK = 0x00ff,
};
# 6 "./include/linux/personality.h" 2

/*
 * Return the base personality without flags.
 */


/*
 * Change personality of the currently running process.
 */
# 16 "./arch/arm64/include/asm/page.h" 2



struct page;
struct vm_area_struct;

extern void copy_page(void *to, const void *from);
extern void clear_page(void *to);

void copy_user_highpage(struct page *to, struct page *from,
   unsigned long vaddr, struct vm_area_struct *vma);


void copy_highpage(struct page *to, struct page *from);


struct page *alloc_zeroed_user_highpage_movable(struct vm_area_struct *vma,
      unsigned long vaddr);


void tag_clear_highpage(struct page *to);





typedef struct page *pgtable_t;

int pfn_is_map_memory(unsigned long pfn);







# 1 "./include/asm-generic/getorder.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/asm-generic/getorder.h"
/**
 * get_order - Determine the allocation order of a memory size
 * @size: The size for which to get the order
 *
 * Determine the allocation order of a particular sized block of memory.  This
 * is on a logarithmic scale, where:
 *
 *	0 -> 2^0 * PAGE_SIZE and below
 *	1 -> 2^1 * PAGE_SIZE to 2^0 * PAGE_SIZE + 1
 *	2 -> 2^2 * PAGE_SIZE to 2^1 * PAGE_SIZE + 1
 *	3 -> 2^3 * PAGE_SIZE to 2^2 * PAGE_SIZE + 1
 *	4 -> 2^4 * PAGE_SIZE to 2^3 * PAGE_SIZE + 1
 *	...
 *
 * The order returned is used to find the smallest allocation granule required
 * to hold an object of the specified size.
 *
 * The result is undefined if the size is 0.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__const__)) int get_order(unsigned long size)
{
 if (__builtin_constant_p(size)) {
  if (!size)
   return 64 - 12;

  if (size < (1UL << 12))
   return 0;

  return ( __builtin_constant_p((size) - 1) ? (((size) - 1) < 2 ? 0 : 63 - __builtin_clzll((size) - 1)) : (sizeof((size) - 1) <= 4) ? __ilog2_u32((size) - 1) : __ilog2_u64((size) - 1) ) - 12 + 1;
 }

 size--;
 size >>= 12;



 return fls64(size);

}
# 53 "./arch/arm64/include/asm/page.h" 2
# 7 "./include/linux/shm.h" 2
# 1 "./include/uapi/linux/shm.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





# 1 "./include/uapi/asm-generic/hugetlb_encode.h" 1



/*
 * Several system calls take a flag to request "hugetlb" huge pages.
 * Without further specification, these system calls will use the
 * system's default huge page size.  If a system supports multiple
 * huge page sizes, the desired huge page size can be specified in
 * bits [26:31] of the flag arguments.  The value in these 6 bits
 * will encode the log2 of the huge page size.
 *
 * The following definitions are associated with this huge page size
 * encoding in flag arguments.  System call specific header files
 * that use this encoding should include this file.  They can then
 * provide definitions based on these with their own specific prefix.
 * for example:
 * #define MAP_HUGE_SHIFT HUGETLB_FLAG_ENCODE_SHIFT
 */
# 8 "./include/uapi/linux/shm.h" 2




/*
 * SHMMNI, SHMMAX and SHMALL are default upper limits which can be
 * modified by sysctl. The SHMMAX and SHMALL values have been chosen to
 * be as large possible without facilitating scenarios where userspace
 * causes overflows when adjusting the limits via operations of the form
 * "retrieve current limit; add X; update limit". It is therefore not
 * advised to make SHMMAX and SHMALL any larger. These limits are
 * suitable for both 32 and 64-bit systems.
 */






/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct shmid_ds {
 struct ipc_perm shm_perm; /* operation perms */
 int shm_segsz; /* size of segment (bytes) */
 __kernel_old_time_t shm_atime; /* last attach time */
 __kernel_old_time_t shm_dtime; /* last detach time */
 __kernel_old_time_t shm_ctime; /* last change time */
 __kernel_ipc_pid_t shm_cpid; /* pid of creator */
 __kernel_ipc_pid_t shm_lpid; /* pid of last operator */
 unsigned short shm_nattch; /* no. of current attaches */
 unsigned short shm_unused; /* compatibility */
 void *shm_unused2; /* ditto - used by DIPC */
 void *shm_unused3; /* unused */
};

/* Include the definition of shmid64_ds and shminfo64 */
# 1 "./arch/arm64/include/generated/uapi/asm/shmbuf.h" 1
# 1 "./include/uapi/asm-generic/shmbuf.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */




# 1 "./arch/arm64/include/generated/uapi/asm/ipcbuf.h" 1
# 7 "./include/uapi/asm-generic/shmbuf.h" 2


/*
 * The shmid64_ds structure for x86 architecture.
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * shmid64_ds was originally meant to be architecture specific, but
 * everyone just ended up making identical copies without specific
 * optimizations, so we may just as well all use the same one.
 *
 * 64 bit architectures use a 64-bit long time field here, while
 * 32 bit architectures have a pair of unsigned long values.
 * On big-endian systems, the lower half is in the wrong place.
 *
 *
 * Pad space is left for:
 * - 2 miscellaneous 32-bit values
 */

struct shmid64_ds {
 struct ipc64_perm shm_perm; /* operation perms */
 __kernel_size_t shm_segsz; /* size of segment (bytes) */

 long shm_atime; /* last attach time */
 long shm_dtime; /* last detach time */
 long shm_ctime; /* last change time */
# 42 "./include/uapi/asm-generic/shmbuf.h"
 __kernel_pid_t shm_cpid; /* pid of creator */
 __kernel_pid_t shm_lpid; /* pid of last operator */
 unsigned long shm_nattch; /* no. of current attaches */
 unsigned long __unused4;
 unsigned long __unused5;
};

struct shminfo64 {
 unsigned long shmmax;
 unsigned long shmmin;
 unsigned long shmmni;
 unsigned long shmseg;
 unsigned long shmall;
 unsigned long __unused1;
 unsigned long __unused2;
 unsigned long __unused3;
 unsigned long __unused4;
};
# 2 "./arch/arm64/include/generated/uapi/asm/shmbuf.h" 2
# 44 "./include/uapi/linux/shm.h" 2

/*
 * shmget() shmflg values.
 */
/* The bottom nine bits are the same as open(2) mode flags */


/* Bits 9 & 10 are IPC_CREAT and IPC_EXCL */



/*
 * Huge page size encoding when SHM_HUGETLB is specified, and a huge page
 * size other than the default is desired.  See hugetlb_encode.h
 */
# 75 "./include/uapi/linux/shm.h"
/*
 * shmat() shmflg values
 */





/* super user shmctl commands */



/* ipcs ctl commands */




/* Obsolete, used only for backwards compatibility */
struct shminfo {
 int shmmax;
 int shmmin;
 int shmmni;
 int shmseg;
 int shmall;
};

struct shm_info {
 int used_ids;
 __kernel_ulong_t shm_tot; /* total allocated shm */
 __kernel_ulong_t shm_rss; /* total resident shm */
 __kernel_ulong_t shm_swp; /* total swapped shm */
 __kernel_ulong_t swap_attempts;
 __kernel_ulong_t swap_successes;
};
# 8 "./include/linux/shm.h" 2
# 1 "./arch/arm64/include/asm/shmparam.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



/*
 * For IPC syscalls from compat tasks, we need to use the legacy 16k
 * alignment value. Since we don't have aliasing D-caches, the rest of
 * the time we can safely use PAGE_SIZE.
 */


# 1 "./include/asm-generic/shmparam.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 16 "./arch/arm64/include/asm/shmparam.h" 2
# 9 "./include/linux/shm.h" 2

struct file;


struct sysv_shm {
 struct list_head shm_clist;
};

long do_shmat(int shmid, char /* nothing */ *shmaddr, int shmflg, unsigned long *addr,
       unsigned long shmlba);
bool is_file_shm_hugepages(struct file *file);
void exit_shm(struct task_struct *task);
# 17 "./include/linux/sched.h" 2
# 1 "./include/linux/kmsan_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * A minimal header declaring types added by KMSAN to existing kernel structs.
 *
 * Copyright (C) 2017-2022 Google LLC
 * Author: Alexander Potapenko <glider@google.com>
 *
 */



/* These constants are defined in the MSan LLVM instrumentation pass. */



struct kmsan_context_state {
 char param_tls[800];
 char retval_tls[800];
 char va_arg_tls[800];
 char va_arg_origin_tls[800];
 u64 va_arg_overflow_size_tls;
 char param_origin_tls[800];
 u32 retval_origin_tls;
};




struct kmsan_ctx {
 struct kmsan_context_state cstate;
 int kmsan_in_runtime;
 bool allow_reporting;
};
# 18 "./include/linux/sched.h" 2

# 1 "./include/linux/plist.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * Descending-priority-sorted double-linked list
 *
 * (C) 2002-2003 Intel Corp
 * Inaky Perez-Gonzalez <inaky.perez-gonzalez@intel.com>.
 *
 * 2001-2005 (c) MontaVista Software, Inc.
 * Daniel Walker <dwalker@mvista.com>
 *
 * (C) 2005 Thomas Gleixner <tglx@linutronix.de>
 *
 * Simplifications of the original code by
 * Oleg Nesterov <oleg@tv-sign.ru>
 *
 * Based on simple lists (include/linux/list.h).
 *
 * This is a priority-sorted list of nodes; each node has a
 * priority from INT_MIN (highest) to INT_MAX (lowest).
 *
 * Addition is O(K), removal is O(1), change of priority of a node is
 * O(K) and K is the number of RT priority levels used in the system.
 * (1 <= K <= 99)
 *
 * This list is really a list of lists:
 *
 *  - The tier 1 list is the prio_list, different priority nodes.
 *
 *  - The tier 2 list is the node_list, serialized nodes.
 *
 * Simple ASCII art explanation:
 *
 * pl:prio_list (only for plist_node)
 * nl:node_list
 *   HEAD|             NODE(S)
 *       |
 *       ||------------------------------------|
 *       ||->|pl|<->|pl|<--------------->|pl|<-|
 *       |   |10|   |21|   |21|   |21|   |40|   (prio)
 *       |   |  |   |  |   |  |   |  |   |  |
 *       |   |  |   |  |   |  |   |  |   |  |
 * |->|nl|<->|nl|<->|nl|<->|nl|<->|nl|<->|nl|<-|
 * |-------------------------------------------|
 *
 * The nodes on the prio_list list are sorted by priority to simplify
 * the insertion of new nodes. There are no nodes with duplicate
 * priorites on the list.
 *
 * The nodes on the node_list are ordered by priority and can contain
 * entries which have the same priority. Those entries are ordered
 * FIFO
 *
 * Addition means: look for the prio_list node in the prio_list
 * for the priority of the node and insert it before the node_list
 * entry of the next prio_list node. If it is the first node of
 * that priority, add it to the prio_list in the right position and
 * insert it into the serialized node_list list
 *
 * Removal means remove it from the node_list and remove it from
 * the prio_list if the node_list list_head is non empty. In case
 * of removal from the prio_list it must be checked whether other
 * entries of the same priority are on the list or not. If there
 * is another entry of the same priority then this entry has to
 * replace the removed entry on the prio_list. If the entry which
 * is removed is the only entry of this priority then a simple
 * remove from both list is sufficient.
 *
 * INT_MIN is the highest priority, 0 is the medium highest, INT_MAX
 * is lowest priority.
 *
 * No locking is done, up to the caller.
 */
# 82 "./include/linux/plist.h"
struct plist_head {
 struct list_head node_list;
};

struct plist_node {
 int prio;
 struct list_head prio_list;
 struct list_head node_list;
};

/**
 * PLIST_HEAD_INIT - static struct plist_head initializer
 * @head:	struct plist_head variable name
 */





/**
 * PLIST_HEAD - declare and init plist_head
 * @head:	name for struct plist_head variable
 */



/**
 * PLIST_NODE_INIT - static struct plist_node initializer
 * @node:	struct plist_node variable name
 * @__prio:	initial node priority
 */







/**
 * plist_head_init - dynamic struct plist_head initializer
 * @head:	&struct plist_head pointer
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
plist_head_init(struct plist_head *head)
{
 INIT_LIST_HEAD(&head->node_list);
}

/**
 * plist_node_init - Dynamic struct plist_node initializer
 * @node:	&struct plist_node pointer
 * @prio:	initial node priority
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void plist_node_init(struct plist_node *node, int prio)
{
 node->prio = prio;
 INIT_LIST_HEAD(&node->prio_list);
 INIT_LIST_HEAD(&node->node_list);
}

extern void plist_add(struct plist_node *node, struct plist_head *head);
extern void plist_del(struct plist_node *node, struct plist_head *head);

extern void plist_requeue(struct plist_node *node, struct plist_head *head);

/**
 * plist_for_each - iterate over the plist
 * @pos:	the type * to use as a loop counter
 * @head:	the head for your list
 */



/**
 * plist_for_each_continue - continue iteration over the plist
 * @pos:	the type * to use as a loop cursor
 * @head:	the head for your list
 *
 * Continue to iterate over plist, continuing after the current position.
 */



/**
 * plist_for_each_safe - iterate safely over a plist of given type
 * @pos:	the type * to use as a loop counter
 * @n:	another type * to use as temporary storage
 * @head:	the head for your list
 *
 * Iterate over a plist of given type, safe against removal of list entry.
 */



/**
 * plist_for_each_entry	- iterate over list of given type
 * @pos:	the type * to use as a loop counter
 * @head:	the head for your list
 * @mem:	the name of the list_head within the struct
 */



/**
 * plist_for_each_entry_continue - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor
 * @head:	the head for your list
 * @m:		the name of the list_head within the struct
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */



/**
 * plist_for_each_entry_safe - iterate safely over list of given type
 * @pos:	the type * to use as a loop counter
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list
 * @m:		the name of the list_head within the struct
 *
 * Iterate over list of given type, safe against removal of list entry.
 */



/**
 * plist_head_empty - return !0 if a plist_head is empty
 * @head:	&struct plist_head pointer
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int plist_head_empty(const struct plist_head *head)
{
 return list_empty(&head->node_list);
}

/**
 * plist_node_empty - return !0 if plist_node is not on a list
 * @node:	&struct plist_node pointer
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int plist_node_empty(const struct plist_node *node)
{
 return list_empty(&node->node_list);
}

/* All functions below assume the plist_head is not empty. */

/**
 * plist_first_entry - get the struct for the first entry
 * @head:	the &struct plist_head pointer
 * @type:	the type of the struct this is embedded in
 * @member:	the name of the list_head within the struct
 */
# 246 "./include/linux/plist.h"
/**
 * plist_last_entry - get the struct for the last entry
 * @head:	the &struct plist_head pointer
 * @type:	the type of the struct this is embedded in
 * @member:	the name of the list_head within the struct
 */
# 263 "./include/linux/plist.h"
/**
 * plist_next - get the next entry in list
 * @pos:	the type * to cursor
 */



/**
 * plist_prev - get the prev entry in list
 * @pos:	the type * to cursor
 */



/**
 * plist_first - return the first node (and thus, highest priority)
 * @head:	the &struct plist_head pointer
 *
 * Assumes the plist is _not_ empty.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct plist_node *plist_first(const struct plist_head *head)
{
 return ({ void *__mptr = (void *)(head->node_list.next); _Static_assert(__builtin_types_compatible_p(typeof(*(head->node_list.next)), typeof(((struct plist_node *)0)->node_list)) || __builtin_types_compatible_p(typeof(*(head->node_list.next)), typeof(void)), "pointer type mismatch in container_of()"); ((struct plist_node *)(__mptr - __builtin_offsetof(struct plist_node, node_list))); });

}

/**
 * plist_last - return the last node (and thus, lowest priority)
 * @head:	the &struct plist_head pointer
 *
 * Assumes the plist is _not_ empty.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct plist_node *plist_last(const struct plist_head *head)
{
 return ({ void *__mptr = (void *)(head->node_list.prev); _Static_assert(__builtin_types_compatible_p(typeof(*(head->node_list.prev)), typeof(((struct plist_node *)0)->node_list)) || __builtin_types_compatible_p(typeof(*(head->node_list.prev)), typeof(void)), "pointer type mismatch in container_of()"); ((struct plist_node *)(__mptr - __builtin_offsetof(struct plist_node, node_list))); });

}
# 20 "./include/linux/sched.h" 2
# 1 "./include/linux/hrtimer.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 *  hrtimers - High-resolution kernel timers
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 */



# 1 "./include/linux/hrtimer_defs.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */
# 16 "./include/linux/hrtimer.h" 2
# 1 "./include/linux/rbtree.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
  Red Black Trees
  (C) 1999  Andrea Arcangeli <andrea@suse.de>


  linux/include/linux/rbtree.h

  To use rbtrees you'll have to implement your own insert and search cores.
  This will avoid us to use callbacks and to drop drammatically performances.
  I know it's not the cleaner way,  but in C (not in C++) to get
  performances and genericity...

  See Documentation/core-api/rbtree.rst for documentation and samples.
*/





# 1 "./include/linux/rbtree_types.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */



struct rb_node {
 unsigned long __rb_parent_color;
 struct rb_node *rb_right;
 struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));
/* The alignment might seem pointless, but allegedly CRIS needs it */

struct rb_root {
 struct rb_node *rb_node;
};

/*
 * Leftmost-cached rbtrees.
 *
 * We do not cache the rightmost node based on footprint
 * size vs number of potential users that could benefit
 * from O(1) rb_last(). Just not worth it, users that want
 * this feature can always implement the logic explicitly.
 * Furthermore, users that want to cache both pointers may
 * find it a bit asymmetric, but that's ok.
 */
struct rb_root_cached {
 struct rb_root rb_root;
 struct rb_node *rb_leftmost;
};
# 22 "./include/linux/rbtree.h" 2
# 32 "./include/linux/rbtree.h"
/* 'empty' nodes are nodes that are known not to be inserted in an rbtree */






extern void rb_insert_color(struct rb_node *, struct rb_root *);
extern void rb_erase(struct rb_node *, struct rb_root *);


/* Find logical next and previous nodes in a tree */
extern struct rb_node *rb_next(const struct rb_node *);
extern struct rb_node *rb_prev(const struct rb_node *);
extern struct rb_node *rb_first(const struct rb_root *);
extern struct rb_node *rb_last(const struct rb_root *);

/* Postorder iteration - always visit the parent after its children */
extern struct rb_node *rb_first_postorder(const struct rb_root *);
extern struct rb_node *rb_next_postorder(const struct rb_node *);

/* Fast replacement of a single node without remove/rebalance/add/rebalance */
extern void rb_replace_node(struct rb_node *victim, struct rb_node *new,
       struct rb_root *root);
extern void rb_replace_node_rcu(struct rb_node *victim, struct rb_node *new,
    struct rb_root *root);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rb_link_node(struct rb_node *node, struct rb_node *parent,
    struct rb_node **rb_link)
{
 node->__rb_parent_color = (unsigned long)parent;
 node->rb_left = node->rb_right = ((void *)0);

 *rb_link = node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rb_link_node_rcu(struct rb_node *node, struct rb_node *parent,
        struct rb_node **rb_link)
{
 node->__rb_parent_color = (unsigned long)parent;
 node->rb_left = node->rb_right = ((void *)0);

 do { uintptr_t _r_a_p__v = (uintptr_t)(node); ; if (__builtin_constant_p(node) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_247(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((*rb_link)) == sizeof(char) || sizeof((*rb_link)) == sizeof(short) || sizeof((*rb_link)) == sizeof(int) || sizeof((*rb_link)) == sizeof(long)) || sizeof((*rb_link)) == sizeof(long long))) __compiletime_assert_247(); } while (0); do { *(volatile typeof((*rb_link)) *)&((*rb_link)) = ((typeof(*rb_link))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&*rb_link) __p = (&*rb_link); union { typeof( _Generic((*&*rb_link), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&*rb_link))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&*rb_link), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&*rb_link)))) ((typeof(*((typeof(*rb_link))_r_a_p__v)) /* nothing */ *)((typeof(*rb_link))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_248(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&*rb_link) == sizeof(char) || sizeof(*&*rb_link) == sizeof(short) || sizeof(*&*rb_link) == sizeof(int) || sizeof(*&*rb_link) == sizeof(long)))) __compiletime_assert_248(); } while (0); kasan_check_write(__p, sizeof(*&*rb_link)); switch (sizeof(*&*rb_link)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 75 "./include/linux/rbtree.h"
}






/**
 * rbtree_postorder_for_each_entry_safe - iterate in post-order over rb_root of
 * given type allowing the backing memory of @pos to be invalidated
 *
 * @pos:	the 'type *' to use as a loop cursor.
 * @n:		another 'type *' to use as temporary storage
 * @root:	'rb_root *' of the rbtree.
 * @field:	the name of the rb_node field within 'type'.
 *
 * rbtree_postorder_for_each_entry_safe() provides a similar guarantee as
 * list_for_each_entry_safe() and allows the iteration to continue independent
 * of changes to @pos by the body of the loop.
 *
 * Note, however, that it cannot handle other modifications that re-order the
 * rbtree it is iterating over. This includes calling rb_erase() on @pos, as
 * rb_erase() may rebalance the tree, causing us to miss some nodes.
 */






/* Same as rb_first(), but O(1) */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rb_insert_color_cached(struct rb_node *node,
       struct rb_root_cached *root,
       bool leftmost)
{
 if (leftmost)
  root->rb_leftmost = node;
 rb_insert_color(node, &root->rb_root);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct rb_node *
rb_erase_cached(struct rb_node *node, struct rb_root_cached *root)
{
 struct rb_node *leftmost = ((void *)0);

 if (root->rb_leftmost == node)
  leftmost = root->rb_leftmost = rb_next(node);

 rb_erase(node, &root->rb_root);

 return leftmost;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rb_replace_node_cached(struct rb_node *victim,
       struct rb_node *new,
       struct rb_root_cached *root)
{
 if (root->rb_leftmost == victim)
  root->rb_leftmost = new;
 rb_replace_node(victim, new, &root->rb_root);
}

/*
 * The below helper functions use 2 operators with 3 different
 * calling conventions. The operators are related like:
 *
 *	comp(a->key,b) < 0  := less(a,b)
 *	comp(a->key,b) > 0  := less(b,a)
 *	comp(a->key,b) == 0 := !less(a,b) && !less(b,a)
 *
 * If these operators define a partial order on the elements we make no
 * guarantee on which of the elements matching the key is found. See
 * rb_find().
 *
 * The reason for this is to allow the find() interface without requiring an
 * on-stack dummy object, which might not be feasible due to object size.
 */

/**
 * rb_add_cached() - insert @node into the leftmost cached tree @tree
 * @node: node to insert
 * @tree: leftmost cached tree to insert @node into
 * @less: operator defining the (partial) node order
 *
 * Returns @node when it is the new leftmost, or NULL.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct rb_node *
rb_add_cached(struct rb_node *node, struct rb_root_cached *tree,
       bool (*less)(struct rb_node *, const struct rb_node *))
{
 struct rb_node **link = &tree->rb_root.rb_node;
 struct rb_node *parent = ((void *)0);
 bool leftmost = true;

 while (*link) {
  parent = *link;
  if (less(node, parent)) {
   link = &parent->rb_left;
  } else {
   link = &parent->rb_right;
   leftmost = false;
  }
 }

 rb_link_node(node, parent, link);
 rb_insert_color_cached(node, tree, leftmost);

 return leftmost ? node : ((void *)0);
}

/**
 * rb_add() - insert @node into @tree
 * @node: node to insert
 * @tree: tree to insert @node into
 * @less: operator defining the (partial) node order
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
rb_add(struct rb_node *node, struct rb_root *tree,
       bool (*less)(struct rb_node *, const struct rb_node *))
{
 struct rb_node **link = &tree->rb_node;
 struct rb_node *parent = ((void *)0);

 while (*link) {
  parent = *link;
  if (less(node, parent))
   link = &parent->rb_left;
  else
   link = &parent->rb_right;
 }

 rb_link_node(node, parent, link);
 rb_insert_color(node, tree);
}

/**
 * rb_find_add() - find equivalent @node in @tree, or add @node
 * @node: node to look-for / insert
 * @tree: tree to search / modify
 * @cmp: operator defining the node order
 *
 * Returns the rb_node matching @node, or NULL when no match is found and @node
 * is inserted.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct rb_node *
rb_find_add(struct rb_node *node, struct rb_root *tree,
     int (*cmp)(struct rb_node *, const struct rb_node *))
{
 struct rb_node **link = &tree->rb_node;
 struct rb_node *parent = ((void *)0);
 int c;

 while (*link) {
  parent = *link;
  c = cmp(node, parent);

  if (c < 0)
   link = &parent->rb_left;
  else if (c > 0)
   link = &parent->rb_right;
  else
   return parent;
 }

 rb_link_node(node, parent, link);
 rb_insert_color(node, tree);
 return ((void *)0);
}

/**
 * rb_find() - find @key in tree @tree
 * @key: key to match
 * @tree: tree to search
 * @cmp: operator defining the node order
 *
 * Returns the rb_node matching @key or NULL.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct rb_node *
rb_find(const void *key, const struct rb_root *tree,
 int (*cmp)(const void *key, const struct rb_node *))
{
 struct rb_node *node = tree->rb_node;

 while (node) {
  int c = cmp(key, node);

  if (c < 0)
   node = node->rb_left;
  else if (c > 0)
   node = node->rb_right;
  else
   return node;
 }

 return ((void *)0);
}

/**
 * rb_find_first() - find the first @key in @tree
 * @key: key to match
 * @tree: tree to search
 * @cmp: operator defining node order
 *
 * Returns the leftmost node matching @key, or NULL.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct rb_node *
rb_find_first(const void *key, const struct rb_root *tree,
       int (*cmp)(const void *key, const struct rb_node *))
{
 struct rb_node *node = tree->rb_node;
 struct rb_node *match = ((void *)0);

 while (node) {
  int c = cmp(key, node);

  if (c <= 0) {
   if (!c)
    match = node;
   node = node->rb_left;
  } else if (c > 0) {
   node = node->rb_right;
  }
 }

 return match;
}

/**
 * rb_next_match() - find the next @key in @tree
 * @key: key to match
 * @tree: tree to search
 * @cmp: operator defining node order
 *
 * Returns the next node matching @key, or NULL.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct rb_node *
rb_next_match(const void *key, struct rb_node *node,
       int (*cmp)(const void *key, const struct rb_node *))
{
 node = rb_next(node);
 if (node && cmp(key, node))
  node = ((void *)0);
 return node;
}

/**
 * rb_for_each() - iterates a subtree matching @key
 * @node: iterator
 * @key: key to match
 * @tree: tree to search
 * @cmp: operator defining node order
 */
# 17 "./include/linux/hrtimer.h" 2





# 1 "./include/linux/timerqueue.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







struct timerqueue_node {
 struct rb_node node;
 ktime_t expires;
};

struct timerqueue_head {
 struct rb_root_cached rb_root;
};


extern bool timerqueue_add(struct timerqueue_head *head,
      struct timerqueue_node *node);
extern bool timerqueue_del(struct timerqueue_head *head,
      struct timerqueue_node *node);
extern struct timerqueue_node *timerqueue_iterate_next(
      struct timerqueue_node *node);

/**
 * timerqueue_getnext - Returns the timer with the earliest expiration time
 *
 * @head: head of timerqueue
 *
 * Returns a pointer to the timer node that has the earliest expiration time.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
struct timerqueue_node *timerqueue_getnext(struct timerqueue_head *head)
{
 struct rb_node *leftmost = (&head->rb_root)->rb_leftmost;

 return ({ typeof(leftmost) ____ptr = (leftmost); ____ptr ? ({ void *__mptr = (void *)(____ptr); _Static_assert(__builtin_types_compatible_p(typeof(*(____ptr)), typeof(((struct timerqueue_node *)0)->node)) || __builtin_types_compatible_p(typeof(*(____ptr)), typeof(void)), "pointer type mismatch in container_of()"); ((struct timerqueue_node *)(__mptr - __builtin_offsetof(struct timerqueue_node, node))); }) : ((void *)0); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void timerqueue_init(struct timerqueue_node *node)
{
 ((&node->node)->__rb_parent_color = (unsigned long)(&node->node));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool timerqueue_node_queued(struct timerqueue_node *node)
{
 return !((&node->node)->__rb_parent_color == (unsigned long)(&node->node));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool timerqueue_node_expires(struct timerqueue_node *node)
{
 return node->expires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void timerqueue_init_head(struct timerqueue_head *head)
{
 head->rb_root = (struct rb_root_cached) { {((void *)0), }, ((void *)0) };
}
# 23 "./include/linux/hrtimer.h" 2

struct hrtimer_clock_base;
struct hrtimer_cpu_base;

/*
 * Mode arguments of xxx_hrtimer functions:
 *
 * HRTIMER_MODE_ABS		- Time value is absolute
 * HRTIMER_MODE_REL		- Time value is relative to now
 * HRTIMER_MODE_PINNED		- Timer is bound to CPU (is only considered
 *				  when starting the timer)
 * HRTIMER_MODE_SOFT		- Timer callback function will be executed in
 *				  soft irq context
 * HRTIMER_MODE_HARD		- Timer callback function will be executed in
 *				  hard irq context even on PREEMPT_RT.
 */
enum hrtimer_mode {
 HRTIMER_MODE_ABS = 0x00,
 HRTIMER_MODE_REL = 0x01,
 HRTIMER_MODE_PINNED = 0x02,
 HRTIMER_MODE_SOFT = 0x04,
 HRTIMER_MODE_HARD = 0x08,

 HRTIMER_MODE_ABS_PINNED = HRTIMER_MODE_ABS | HRTIMER_MODE_PINNED,
 HRTIMER_MODE_REL_PINNED = HRTIMER_MODE_REL | HRTIMER_MODE_PINNED,

 HRTIMER_MODE_ABS_SOFT = HRTIMER_MODE_ABS | HRTIMER_MODE_SOFT,
 HRTIMER_MODE_REL_SOFT = HRTIMER_MODE_REL | HRTIMER_MODE_SOFT,

 HRTIMER_MODE_ABS_PINNED_SOFT = HRTIMER_MODE_ABS_PINNED | HRTIMER_MODE_SOFT,
 HRTIMER_MODE_REL_PINNED_SOFT = HRTIMER_MODE_REL_PINNED | HRTIMER_MODE_SOFT,

 HRTIMER_MODE_ABS_HARD = HRTIMER_MODE_ABS | HRTIMER_MODE_HARD,
 HRTIMER_MODE_REL_HARD = HRTIMER_MODE_REL | HRTIMER_MODE_HARD,

 HRTIMER_MODE_ABS_PINNED_HARD = HRTIMER_MODE_ABS_PINNED | HRTIMER_MODE_HARD,
 HRTIMER_MODE_REL_PINNED_HARD = HRTIMER_MODE_REL_PINNED | HRTIMER_MODE_HARD,
};

/*
 * Return values for the callback function
 */
enum hrtimer_restart {
 HRTIMER_NORESTART, /* Timer is not restarted */
 HRTIMER_RESTART, /* Timer must be restarted */
};

/*
 * Values to track state of the timer
 *
 * Possible states:
 *
 * 0x00		inactive
 * 0x01		enqueued into rbtree
 *
 * The callback state is not part of the timer->state because clearing it would
 * mean touching the timer after the callback, this makes it impossible to free
 * the timer from the callback function.
 *
 * Therefore we track the callback state in:
 *
 *	timer->base->cpu_base->running == timer
 *
 * On SMP it is possible to have a "callback function running and enqueued"
 * status. It happens for example when a posix timer expired and the callback
 * queued a signal. Between dropping the lock which protects the posix timer
 * and reacquiring the base lock of the hrtimer, another CPU can deliver the
 * signal and rearm the timer.
 *
 * All state transitions are protected by cpu_base->lock.
 */



/**
 * struct hrtimer - the basic hrtimer structure
 * @node:	timerqueue node, which also manages node.expires,
 *		the absolute expiry time in the hrtimers internal
 *		representation. The time is related to the clock on
 *		which the timer is based. Is setup by adding
 *		slack to the _softexpires value. For non range timers
 *		identical to _softexpires.
 * @_softexpires: the absolute earliest expiry time of the hrtimer.
 *		The time which was given as expiry time when the timer
 *		was armed.
 * @function:	timer expiry callback function
 * @base:	pointer to the timer base (per cpu and per clock)
 * @state:	state information (See bit values above)
 * @is_rel:	Set if the timer was armed relative
 * @is_soft:	Set if hrtimer will be expired in soft interrupt context.
 * @is_hard:	Set if hrtimer will be expired in hard interrupt context
 *		even on RT.
 *
 * The hrtimer structure must be initialized by hrtimer_init()
 */
struct hrtimer {
 struct timerqueue_node node;
 ktime_t _softexpires;
 enum hrtimer_restart (*function)(struct hrtimer *);
 struct hrtimer_clock_base *base;
 u8 state;
 u8 is_rel;
 u8 is_soft;
 u8 is_hard;
};

/**
 * struct hrtimer_sleeper - simple sleeper structure
 * @timer:	embedded timer structure
 * @task:	task to wake up
 *
 * task is set to NULL, when the timer expires.
 */
struct hrtimer_sleeper {
 struct hrtimer timer;
 struct task_struct *task;
};







/**
 * struct hrtimer_clock_base - the timer base for a specific clock
 * @cpu_base:		per cpu clock base
 * @index:		clock type index for per_cpu support when moving a
 *			timer to a base on another cpu.
 * @clockid:		clock id for per_cpu support
 * @seq:		seqcount around __run_hrtimer
 * @running:		pointer to the currently running hrtimer
 * @active:		red black tree root node for the active timers
 * @get_time:		function to retrieve the current time of the clock
 * @offset:		offset of this clock to the monotonic base
 */
struct hrtimer_clock_base {
 struct hrtimer_cpu_base *cpu_base;
 unsigned int index;
 clockid_t clockid;
 seqcount_raw_spinlock_t seq;
 struct hrtimer *running;
 struct timerqueue_head active;
 ktime_t (*get_time)(void);
 ktime_t offset;
} __attribute__((__aligned__((1 << (6)))));

enum hrtimer_base_type {
 HRTIMER_BASE_MONOTONIC,
 HRTIMER_BASE_REALTIME,
 HRTIMER_BASE_BOOTTIME,
 HRTIMER_BASE_TAI,
 HRTIMER_BASE_MONOTONIC_SOFT,
 HRTIMER_BASE_REALTIME_SOFT,
 HRTIMER_BASE_BOOTTIME_SOFT,
 HRTIMER_BASE_TAI_SOFT,
 HRTIMER_MAX_CLOCK_BASES,
};

/**
 * struct hrtimer_cpu_base - the per cpu clock bases
 * @lock:		lock protecting the base and associated clock bases
 *			and timers
 * @cpu:		cpu number
 * @active_bases:	Bitfield to mark bases with active timers
 * @clock_was_set_seq:	Sequence counter of clock was set events
 * @hres_active:	State of high resolution mode
 * @in_hrtirq:		hrtimer_interrupt() is currently executing
 * @hang_detected:	The last hrtimer interrupt detected a hang
 * @softirq_activated:	displays, if the softirq is raised - update of softirq
 *			related settings is not required then.
 * @nr_events:		Total number of hrtimer interrupt events
 * @nr_retries:		Total number of hrtimer interrupt retries
 * @nr_hangs:		Total number of hrtimer interrupt hangs
 * @max_hang_time:	Maximum time spent in hrtimer_interrupt
 * @softirq_expiry_lock: Lock which is taken while softirq based hrtimer are
 *			 expired
 * @timer_waiters:	A hrtimer_cancel() invocation waits for the timer
 *			callback to finish.
 * @expires_next:	absolute time of the next event, is required for remote
 *			hrtimer enqueue; it is the total first expiry time (hard
 *			and soft hrtimer are taken into account)
 * @next_timer:		Pointer to the first expiring timer
 * @softirq_expires_next: Time to check, if soft queues needs also to be expired
 * @softirq_next_timer: Pointer to the first expiring softirq based timer
 * @clock_base:		array of clock bases for this cpu
 *
 * Note: next_timer is just an optimization for __remove_hrtimer().
 *	 Do not dereference the pointer because it is not reliable on
 *	 cross cpu removals.
 */
struct hrtimer_cpu_base {
 raw_spinlock_t lock;
 unsigned int cpu;
 unsigned int active_bases;
 unsigned int clock_was_set_seq;
 unsigned int hres_active : 1,
     in_hrtirq : 1,
     hang_detected : 1,
     softirq_activated : 1;

 unsigned int nr_events;
 unsigned short nr_retries;
 unsigned short nr_hangs;
 unsigned int max_hang_time;





 ktime_t expires_next;
 struct hrtimer *next_timer;
 ktime_t softirq_expires_next;
 struct hrtimer *softirq_next_timer;
 struct hrtimer_clock_base clock_base[HRTIMER_MAX_CLOCK_BASES];
} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_set_expires(struct hrtimer *timer, ktime_t time)
{
 timer->node.expires = time;
 timer->_softexpires = time;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_set_expires_range(struct hrtimer *timer, ktime_t time, ktime_t delta)
{
 timer->_softexpires = time;
 timer->node.expires = ktime_add_safe(time, delta);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_set_expires_range_ns(struct hrtimer *timer, ktime_t time, u64 delta)
{
 timer->_softexpires = time;
 timer->node.expires = ktime_add_safe(time, ns_to_ktime(delta));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_set_expires_tv64(struct hrtimer *timer, s64 tv64)
{
 timer->node.expires = tv64;
 timer->_softexpires = tv64;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_add_expires(struct hrtimer *timer, ktime_t time)
{
 timer->node.expires = ktime_add_safe(timer->node.expires, time);
 timer->_softexpires = ktime_add_safe(timer->_softexpires, time);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_add_expires_ns(struct hrtimer *timer, u64 ns)
{
 timer->node.expires = ((timer->node.expires) + (ns));
 timer->_softexpires = ((timer->_softexpires) + (ns));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t hrtimer_get_expires(const struct hrtimer *timer)
{
 return timer->node.expires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t hrtimer_get_softexpires(const struct hrtimer *timer)
{
 return timer->_softexpires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 hrtimer_get_expires_tv64(const struct hrtimer *timer)
{
 return timer->node.expires;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 hrtimer_get_softexpires_tv64(const struct hrtimer *timer)
{
 return timer->_softexpires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 hrtimer_get_expires_ns(const struct hrtimer *timer)
{
 return ktime_to_ns(timer->node.expires);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t hrtimer_expires_remaining(const struct hrtimer *timer)
{
 return ((timer->node.expires) - (timer->base->get_time()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t hrtimer_cb_get_time(struct hrtimer *timer)
{
 return timer->base->get_time();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int hrtimer_is_hres_active(struct hrtimer *timer)
{
 return 1 ?
  timer->base->cpu_base->hres_active : 0;
}


struct clock_event_device;

extern void hrtimer_interrupt(struct clock_event_device *dev);

extern unsigned int hrtimer_resolution;







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t
__hrtimer_expires_remaining_adjusted(const struct hrtimer *timer, ktime_t now)
{
 ktime_t rem = ((timer->node.expires) - (now));

 /*
	 * Adjust relative timers for the extra we added in
	 * hrtimer_start_range_ns() to prevent short timeouts.
	 */
 if (0 && timer->is_rel)
  rem -= hrtimer_resolution;
 return rem;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t
hrtimer_expires_remaining_adjusted(const struct hrtimer *timer)
{
 return __hrtimer_expires_remaining_adjusted(timer,
          timer->base->get_time());
}


extern void timerfd_clock_was_set(void);
extern void timerfd_resume(void);





extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct tick_device) tick_cpu_device;




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_cancel_wait_running(struct hrtimer *timer)
{
 cpu_relax();
}


/* Exported timer functions: */

/* Initialize timers: */
extern void hrtimer_init(struct hrtimer *timer, clockid_t which_clock,
    enum hrtimer_mode mode);
extern void hrtimer_init_sleeper(struct hrtimer_sleeper *sl, clockid_t clock_id,
     enum hrtimer_mode mode);
# 386 "./include/linux/hrtimer.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_init_on_stack(struct hrtimer *timer,
      clockid_t which_clock,
      enum hrtimer_mode mode)
{
 hrtimer_init(timer, which_clock, mode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_init_sleeper_on_stack(struct hrtimer_sleeper *sl,
       clockid_t clock_id,
       enum hrtimer_mode mode)
{
 hrtimer_init_sleeper(sl, clock_id, mode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void destroy_hrtimer_on_stack(struct hrtimer *timer) { }


/* Basic timer operations: */
extern void hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
       u64 range_ns, const enum hrtimer_mode mode);

/**
 * hrtimer_start - (re)start an hrtimer
 * @timer:	the timer to be added
 * @tim:	expiry time
 * @mode:	timer mode: absolute (HRTIMER_MODE_ABS) or
 *		relative (HRTIMER_MODE_REL), and pinned (HRTIMER_MODE_PINNED);
 *		softirq based mode is considered for debug purpose only!
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_start(struct hrtimer *timer, ktime_t tim,
     const enum hrtimer_mode mode)
{
 hrtimer_start_range_ns(timer, tim, 0, mode);
}

extern int hrtimer_cancel(struct hrtimer *timer);
extern int hrtimer_try_to_cancel(struct hrtimer *timer);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_start_expires(struct hrtimer *timer,
      enum hrtimer_mode mode)
{
 u64 delta;
 ktime_t soft, hard;
 soft = hrtimer_get_softexpires(timer);
 hard = hrtimer_get_expires(timer);
 delta = ktime_to_ns(((hard) - (soft)));
 hrtimer_start_range_ns(timer, soft, delta, mode);
}

void hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,
       enum hrtimer_mode mode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void hrtimer_restart(struct hrtimer *timer)
{
 hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
}

/* Query timers: */
extern ktime_t __hrtimer_get_remaining(const struct hrtimer *timer, bool adjust);

/**
 * hrtimer_get_remaining - get remaining time for the timer
 * @timer:	the timer to read
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ktime_t hrtimer_get_remaining(const struct hrtimer *timer)
{
 return __hrtimer_get_remaining(timer, false);
}

extern u64 hrtimer_get_next_event(void);
extern u64 hrtimer_next_event_without(const struct hrtimer *exclude);

extern bool hrtimer_active(const struct hrtimer *timer);

/**
 * hrtimer_is_queued - check, whether the timer is on one of the queues
 * @timer:	Timer to check
 *
 * Returns: True if the timer is queued, false otherwise
 *
 * The function can be used lockless, but it gives only a current snapshot.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool hrtimer_is_queued(struct hrtimer *timer)
{
 /* The READ_ONCE pairs with the update functions of timer->state */
 return !!(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_249(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(timer->state) == sizeof(char) || sizeof(timer->state) == sizeof(short) || sizeof(timer->state) == sizeof(int) || sizeof(timer->state) == sizeof(long)) || sizeof(timer->state) == sizeof(long long))) __compiletime_assert_249(); } while (0); (*(const volatile typeof( _Generic((timer->state), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (timer->state))) *)&(timer->state)); }) & 0x01);
# 472 "./include/linux/hrtimer.h"
}

/*
 * Helper function to check, whether the timer is running the callback
 * function
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int hrtimer_callback_running(struct hrtimer *timer)
{
 return timer->base->running == timer;
}

/* Forward a hrtimer so it expires after now: */
extern u64
hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval);

/**
 * hrtimer_forward_now - forward the timer expiry so it expires after now
 * @timer:	hrtimer to forward
 * @interval:	the interval to forward
 *
 * Forward the timer expiry so it will expire after the current time
 * of the hrtimer clock base. Returns the number of overruns.
 *
 * Can be safely called from the callback function of @timer. If
 * called from other contexts @timer must neither be enqueued nor
 * running the callback and the caller needs to take care of
 * serialization.
 *
 * Note: This only updates the timer expiry value and does not requeue
 * the timer.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 hrtimer_forward_now(struct hrtimer *timer,
          ktime_t interval)
{
 return hrtimer_forward(timer, timer->base->get_time(), interval);
}

/* Precise sleep: */

extern int nanosleep_copyout(struct restart_block *, struct timespec64 *);
extern long hrtimer_nanosleep(ktime_t rqtp, const enum hrtimer_mode mode,
         const clockid_t clockid);

extern int schedule_hrtimeout_range(ktime_t *expires, u64 delta,
        const enum hrtimer_mode mode);
extern int schedule_hrtimeout_range_clock(ktime_t *expires,
       u64 delta,
       const enum hrtimer_mode mode,
       clockid_t clock_id);
extern int schedule_hrtimeout(ktime_t *expires, const enum hrtimer_mode mode);

/* Soft interrupt function to run the hrtimer queues: */
extern void hrtimer_run_queues(void);

/* Bootup initialization: */
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) hrtimers_init(void);

/* Show pending timers: */
extern void sysrq_timer_list_show(void);

int hrtimers_prepare_cpu(unsigned int cpu);

int hrtimers_dead_cpu(unsigned int cpu);
# 21 "./include/linux/sched.h" 2

# 1 "./include/linux/seccomp.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/seccomp.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */







/* Valid values for seccomp.mode and prctl(PR_SET_SECCOMP, <mode>) */




/* Valid operations for seccomp syscall. */





/* Valid flags for SECCOMP_SET_MODE_FILTER */





/* Received notifications wait in killable state (only respond to fatal signals) */


/*
 * All BPF programs must return a 32-bit value.
 * The bottom 16-bits are for optional return data.
 * The upper 16-bits are ordered from least permissive values to most,
 * as a signed value (so 0x8000000 is negative).
 *
 * The ordering ensures that a min_t() over composed return values always
 * selects the least permissive choice.
 */
# 48 "./include/uapi/linux/seccomp.h"
/* Masks for the return value sections. */




/**
 * struct seccomp_data - the format the BPF program executes over.
 * @nr: the system call number
 * @arch: indicates system call convention as an AUDIT_ARCH_* value
 *        as defined in <linux/audit.h>.
 * @instruction_pointer: at the time of the system call.
 * @args: up to 6 system call arguments always stored as 64-bit values
 *        regardless of the architecture.
 */
struct seccomp_data {
 int nr;
 __u32 arch;
 __u64 instruction_pointer;
 __u64 args[6];
};

struct seccomp_notif_sizes {
 __u16 seccomp_notif;
 __u16 seccomp_notif_resp;
 __u16 seccomp_data;
};

struct seccomp_notif {
 __u64 id;
 __u32 pid;
 __u32 flags;
 struct seccomp_data data;
};

/*
 * Valid flags for struct seccomp_notif_resp
 *
 * Note, the SECCOMP_USER_NOTIF_FLAG_CONTINUE flag must be used with caution!
 * If set by the process supervising the syscalls of another process the
 * syscall will continue. This is problematic because of an inherent TOCTOU.
 * An attacker can exploit the time while the supervised process is waiting on
 * a response from the supervising process to rewrite syscall arguments which
 * are passed as pointers of the intercepted syscall.
 * It should be absolutely clear that this means that the seccomp notifier
 * _cannot_ be used to implement a security policy! It should only ever be used
 * in scenarios where a more privileged process supervises the syscalls of a
 * lesser privileged process to get around kernel-enforced security
 * restrictions when the privileged process deems this safe. In other words,
 * in order to continue a syscall the supervising process should be sure that
 * another security mechanism or the kernel itself will sufficiently block
 * syscalls if arguments are rewritten to something unsafe.
 *
 * Similar precautions should be applied when stacking SECCOMP_RET_USER_NOTIF
 * or SECCOMP_RET_TRACE. For SECCOMP_RET_USER_NOTIF filters acting on the
 * same syscall, the most recently added filter takes precedence. This means
 * that the new SECCOMP_RET_USER_NOTIF filter can override any
 * SECCOMP_IOCTL_NOTIF_SEND from earlier filters, essentially allowing all
 * such filtered syscalls to be executed by sending the response
 * SECCOMP_USER_NOTIF_FLAG_CONTINUE. Note that SECCOMP_RET_TRACE can equally
 * be overriden by SECCOMP_USER_NOTIF_FLAG_CONTINUE.
 */


struct seccomp_notif_resp {
 __u64 id;
 __s64 val;
 __s32 error;
 __u32 flags;
};

/* valid flags for seccomp_notif_addfd */



/**
 * struct seccomp_notif_addfd
 * @id: The ID of the seccomp notification
 * @flags: SECCOMP_ADDFD_FLAG_*
 * @srcfd: The local fd number
 * @newfd: Optional remote FD number if SETFD option is set, otherwise 0.
 * @newfd_flags: The O_* flags the remote FD should have applied
 */
struct seccomp_notif_addfd {
 __u64 id;
 __u32 flags;
 __u32 srcfd;
 __u32 newfd;
 __u32 newfd_flags;
};







/* Flags for seccomp notification fd ioctl. */




/* On success, the return value is the remote process's added fd number */
# 6 "./include/linux/seccomp.h" 2








/* sizeof() the first published struct seccomp_notif_addfd */







# 1 "./arch/arm64/include/asm/seccomp.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * arch/arm64/include/asm/seccomp.h
 *
 * Copyright (C) 2014 Linaro Limited
 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
 */



# 1 "./arch/arm64/include/asm/unistd.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 19 "./arch/arm64/include/asm/unistd.h"
/*
 * Compat syscall numbers used by the AArch64 kernel.
 */
# 34 "./arch/arm64/include/asm/unistd.h"
/*
 * The following SVCs are ARM private.
 */
# 48 "./arch/arm64/include/asm/unistd.h"
# 1 "./arch/arm64/include/uapi/asm/unistd.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
# 25 "./arch/arm64/include/uapi/asm/unistd.h"
# 1 "./include/uapi/asm-generic/unistd.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */


/*
 * This file contains the system call numbers, based on the
 * layout of the x86-64 architecture, which embeds the
 * pointer to the syscall in the table.
 *
 * As a basic principle, no duplication of functionality
 * should be added, e.g. we don't use lseek when llseek
 * is present. New architectures should use this file
 * and implement the less feature-full calls in user space.
 */
# 46 "./include/uapi/asm-generic/unistd.h"
/* fs/xattr.c */
# 72 "./include/uapi/asm-generic/unistd.h"
/* fs/dcache.c */



/* fs/cookies.c */



/* fs/eventfd.c */



/* fs/eventpoll.c */







/* fs/fcntl.c */







/* fs/inotify_user.c */







/* fs/ioctl.c */



/* fs/ioprio.c */





/* fs/locks.c */



/* fs/namei.c */
# 134 "./include/uapi/asm-generic/unistd.h"
/* renameat is superseded with flags by renameat2 */




/* fs/namespace.c */







/* fs/nfsctl.c */



/* fs/open.c */
# 190 "./include/uapi/asm-generic/unistd.h"
/* fs/pipe.c */



/* fs/quota.c */



/* fs/readdir.c */



/* fs/read_write.c */
# 222 "./include/uapi/asm-generic/unistd.h"
/* fs/sendfile.c */



/* fs/select.c */







/* fs/signalfd.c */



/* fs/splice.c */







/* fs/stat.c */
# 256 "./include/uapi/asm-generic/unistd.h"
/* fs/sync.c */
# 273 "./include/uapi/asm-generic/unistd.h"
/* fs/timerfd.c */
# 285 "./include/uapi/asm-generic/unistd.h"
/* fs/utimes.c */





/* kernel/acct.c */



/* kernel/capability.c */





/* kernel/exec_domain.c */



/* kernel/exit.c */







/* kernel/fork.c */





/* kernel/futex.c */
# 331 "./include/uapi/asm-generic/unistd.h"
/* kernel/hrtimer.c */





/* kernel/itimer.c */





/* kernel/kexec.c */



/* kernel/module.c */





/* kernel/posix-timers.c */
# 380 "./include/uapi/asm-generic/unistd.h"
/* kernel/printk.c */



/* kernel/ptrace.c */



/* kernel/sched/core.c */
# 415 "./include/uapi/asm-generic/unistd.h"
/* kernel/signal.c */
# 445 "./include/uapi/asm-generic/unistd.h"
/* kernel/sys.c */
# 494 "./include/uapi/asm-generic/unistd.h"
/* getrlimit and setrlimit are superseded with prlimit64 */
# 510 "./include/uapi/asm-generic/unistd.h"
/* kernel/time.c */
# 520 "./include/uapi/asm-generic/unistd.h"
/* kernel/sys.c */
# 538 "./include/uapi/asm-generic/unistd.h"
/* ipc/mqueue.c */
# 555 "./include/uapi/asm-generic/unistd.h"
/* ipc/msg.c */
# 565 "./include/uapi/asm-generic/unistd.h"
/* ipc/sem.c */
# 577 "./include/uapi/asm-generic/unistd.h"
/* ipc/shm.c */
# 587 "./include/uapi/asm-generic/unistd.h"
/* net/socket.c */
# 619 "./include/uapi/asm-generic/unistd.h"
/* mm/filemap.c */



/* mm/nommu.c, also with MMU */







/* security/keys/keyctl.c */







/* arch/example/kernel/sys_example.c */







/* mm/fadvise.c */



/* mm/, CONFIG_MMU only */
# 699 "./include/uapi/asm-generic/unistd.h"
/*
 * Architectures may provide up to 16 syscalls of their own
 * starting with this value.
 */
# 781 "./include/uapi/asm-generic/unistd.h"
/* 295 through 402 are unassigned to sync up with generic numbers, don't use */
# 892 "./include/uapi/asm-generic/unistd.h"
/*
 * 32 bit systems traditionally used different
 * syscalls for off_t and loff_t arguments, while
 * 64 bit systems only need the off_t version.
 * For new 32 bit platforms, there is no need to
 * implement the old 32 bit off_t syscalls, so
 * they take different names.
 * Here we map the numbers so that both versions
 * use the same syscall table layout.
 */
# 26 "./arch/arm64/include/uapi/asm/unistd.h" 2
# 49 "./arch/arm64/include/asm/unistd.h" 2
# 12 "./arch/arm64/include/asm/seccomp.h" 2








# 1 "./include/asm-generic/seccomp.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * include/asm-generic/seccomp.h
 *
 * Copyright (C) 2014 Linaro Limited
 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
 */



# 1 "./include/uapi/linux/unistd.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/*
 * Include machine specific syscall numbers
 */
# 1 "./arch/arm64/include/asm/unistd.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 19 "./arch/arm64/include/asm/unistd.h"
/*
 * Compat syscall numbers used by the AArch64 kernel.
 */
# 34 "./arch/arm64/include/asm/unistd.h"
/*
 * The following SVCs are ARM private.
 */
# 48 "./arch/arm64/include/asm/unistd.h"
# 1 "./arch/arm64/include/uapi/asm/unistd.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
# 25 "./arch/arm64/include/uapi/asm/unistd.h"
# 1 "./include/uapi/asm-generic/unistd.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */


/*
 * This file contains the system call numbers, based on the
 * layout of the x86-64 architecture, which embeds the
 * pointer to the syscall in the table.
 *
 * As a basic principle, no duplication of functionality
 * should be added, e.g. we don't use lseek when llseek
 * is present. New architectures should use this file
 * and implement the less feature-full calls in user space.
 */
# 46 "./include/uapi/asm-generic/unistd.h"
/* fs/xattr.c */
# 72 "./include/uapi/asm-generic/unistd.h"
/* fs/dcache.c */



/* fs/cookies.c */



/* fs/eventfd.c */



/* fs/eventpoll.c */







/* fs/fcntl.c */







/* fs/inotify_user.c */







/* fs/ioctl.c */



/* fs/ioprio.c */





/* fs/locks.c */



/* fs/namei.c */
# 134 "./include/uapi/asm-generic/unistd.h"
/* renameat is superseded with flags by renameat2 */




/* fs/namespace.c */







/* fs/nfsctl.c */



/* fs/open.c */
# 190 "./include/uapi/asm-generic/unistd.h"
/* fs/pipe.c */



/* fs/quota.c */



/* fs/readdir.c */



/* fs/read_write.c */
# 222 "./include/uapi/asm-generic/unistd.h"
/* fs/sendfile.c */



/* fs/select.c */







/* fs/signalfd.c */



/* fs/splice.c */







/* fs/stat.c */
# 256 "./include/uapi/asm-generic/unistd.h"
/* fs/sync.c */
# 273 "./include/uapi/asm-generic/unistd.h"
/* fs/timerfd.c */
# 285 "./include/uapi/asm-generic/unistd.h"
/* fs/utimes.c */





/* kernel/acct.c */



/* kernel/capability.c */





/* kernel/exec_domain.c */



/* kernel/exit.c */







/* kernel/fork.c */





/* kernel/futex.c */
# 331 "./include/uapi/asm-generic/unistd.h"
/* kernel/hrtimer.c */





/* kernel/itimer.c */





/* kernel/kexec.c */



/* kernel/module.c */





/* kernel/posix-timers.c */
# 380 "./include/uapi/asm-generic/unistd.h"
/* kernel/printk.c */



/* kernel/ptrace.c */



/* kernel/sched/core.c */
# 415 "./include/uapi/asm-generic/unistd.h"
/* kernel/signal.c */
# 445 "./include/uapi/asm-generic/unistd.h"
/* kernel/sys.c */
# 494 "./include/uapi/asm-generic/unistd.h"
/* getrlimit and setrlimit are superseded with prlimit64 */
# 510 "./include/uapi/asm-generic/unistd.h"
/* kernel/time.c */
# 520 "./include/uapi/asm-generic/unistd.h"
/* kernel/sys.c */
# 538 "./include/uapi/asm-generic/unistd.h"
/* ipc/mqueue.c */
# 555 "./include/uapi/asm-generic/unistd.h"
/* ipc/msg.c */
# 565 "./include/uapi/asm-generic/unistd.h"
/* ipc/sem.c */
# 577 "./include/uapi/asm-generic/unistd.h"
/* ipc/shm.c */
# 587 "./include/uapi/asm-generic/unistd.h"
/* net/socket.c */
# 619 "./include/uapi/asm-generic/unistd.h"
/* mm/filemap.c */



/* mm/nommu.c, also with MMU */







/* security/keys/keyctl.c */







/* arch/example/kernel/sys_example.c */







/* mm/fadvise.c */



/* mm/, CONFIG_MMU only */
# 699 "./include/uapi/asm-generic/unistd.h"
/*
 * Architectures may provide up to 16 syscalls of their own
 * starting with this value.
 */
# 781 "./include/uapi/asm-generic/unistd.h"
/* 295 through 402 are unassigned to sync up with generic numbers, don't use */
# 892 "./include/uapi/asm-generic/unistd.h"
/*
 * 32 bit systems traditionally used different
 * syscalls for off_t and loff_t arguments, while
 * 64 bit systems only need the off_t version.
 * For new 32 bit platforms, there is no need to
 * implement the old 32 bit off_t syscalls, so
 * they take different names.
 * Here we map the numbers so that both versions
 * use the same syscall table layout.
 */
# 26 "./arch/arm64/include/uapi/asm/unistd.h" 2
# 49 "./arch/arm64/include/asm/unistd.h" 2
# 9 "./include/uapi/linux/unistd.h" 2
# 12 "./include/asm-generic/seccomp.h" 2
# 31 "./include/asm-generic/seccomp.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const int *get_compat_mode1_syscalls(void)
{
 static const int mode1_syscalls_32[] = {
  3, 4,
  1, 173,
  -1, /* negative terminated */
 };
 return mode1_syscalls_32;
}
# 21 "./arch/arm64/include/asm/seccomp.h" 2
# 23 "./include/linux/seccomp.h" 2

struct seccomp_filter;
/**
 * struct seccomp - the state of a seccomp'ed process
 *
 * @mode:  indicates one of the valid values above for controlled
 *         system calls available to a process.
 * @filter_count: number of seccomp filters
 * @filter: must always point to a valid seccomp-filter or NULL as it is
 *          accessed without locking during system call entry.
 *
 *          @filter must only be accessed from the context of current as there
 *          is no read locking.
 */
struct seccomp {
 int mode;
 atomic_t filter_count;
 struct seccomp_filter *filter;
};


extern int __secure_computing(const struct seccomp_data *sd);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int secure_computing(void)
{
 if (__builtin_expect(!!(test_ti_thread_flag(((struct thread_info *)get_current()), 11 /* syscall secure computing */)), 0))
  return __secure_computing(((void *)0));
 return 0;
}




extern long prctl_get_seccomp(void);
extern long prctl_set_seccomp(unsigned long, void /* nothing */ *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int seccomp_mode(struct seccomp *s)
{
 return s->mode;
}
# 95 "./include/linux/seccomp.h"
extern void seccomp_filter_release(struct task_struct *tsk);
extern void get_seccomp_filter(struct task_struct *tsk);
# 114 "./include/linux/seccomp.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long seccomp_get_filter(struct task_struct *task,
          unsigned long n, void /* nothing */ *data)
{
 return -22 /* Invalid argument */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long seccomp_get_metadata(struct task_struct *task,
     unsigned long filter_off,
     void /* nothing */ *data)
{
 return -22 /* Invalid argument */;
}
# 23 "./include/linux/sched.h" 2
# 1 "./include/linux/nodemask.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Nodemasks provide a bitmap suitable for representing the
 * set of Node's in a system, one bit position per Node number.
 *
 * See detailed comments in the file linux/bitmap.h describing the
 * data type on which these nodemasks are based.
 *
 * For details of nodemask_parse_user(), see bitmap_parse_user() in
 * lib/bitmap.c.  For details of nodelist_parse(), see bitmap_parselist(),
 * also in bitmap.c.  For details of node_remap(), see bitmap_bitremap in
 * lib/bitmap.c.  For details of nodes_remap(), see bitmap_remap in
 * lib/bitmap.c.  For details of nodes_onto(), see bitmap_onto in
 * lib/bitmap.c.  For details of nodes_fold(), see bitmap_fold in
 * lib/bitmap.c.
 *
 * The available nodemask operations are:
 *
 * void node_set(node, mask)		turn on bit 'node' in mask
 * void node_clear(node, mask)		turn off bit 'node' in mask
 * void nodes_setall(mask)		set all bits
 * void nodes_clear(mask)		clear all bits
 * int node_isset(node, mask)		true iff bit 'node' set in mask
 * int node_test_and_set(node, mask)	test and set bit 'node' in mask
 *
 * void nodes_and(dst, src1, src2)	dst = src1 & src2  [intersection]
 * void nodes_or(dst, src1, src2)	dst = src1 | src2  [union]
 * void nodes_xor(dst, src1, src2)	dst = src1 ^ src2
 * void nodes_andnot(dst, src1, src2)	dst = src1 & ~src2
 * void nodes_complement(dst, src)	dst = ~src
 *
 * int nodes_equal(mask1, mask2)	Does mask1 == mask2?
 * int nodes_intersects(mask1, mask2)	Do mask1 and mask2 intersect?
 * int nodes_subset(mask1, mask2)	Is mask1 a subset of mask2?
 * int nodes_empty(mask)		Is mask empty (no bits sets)?
 * int nodes_full(mask)			Is mask full (all bits sets)?
 * int nodes_weight(mask)		Hamming weight - number of set bits
 *
 * void nodes_shift_right(dst, src, n)	Shift right
 * void nodes_shift_left(dst, src, n)	Shift left
 *
 * unsigned int first_node(mask)	Number lowest set bit, or MAX_NUMNODES
 * unsigend int next_node(node, mask)	Next node past 'node', or MAX_NUMNODES
 * unsigned int next_node_in(node, mask) Next node past 'node', or wrap to first,
 *					or MAX_NUMNODES
 * unsigned int first_unset_node(mask)	First node not set in mask, or
 *					MAX_NUMNODES
 *
 * nodemask_t nodemask_of_node(node)	Return nodemask with bit 'node' set
 * NODE_MASK_ALL			Initializer - all bits set
 * NODE_MASK_NONE			Initializer - no bits set
 * unsigned long *nodes_addr(mask)	Array of unsigned long's in mask
 *
 * int nodemask_parse_user(ubuf, ulen, mask)	Parse ascii string as nodemask
 * int nodelist_parse(buf, map)		Parse ascii string as nodelist
 * int node_remap(oldbit, old, new)	newbit = map(old, new)(oldbit)
 * void nodes_remap(dst, src, old, new)	*dst = map(old, new)(src)
 * void nodes_onto(dst, orig, relmap)	*dst = orig relative to relmap
 * void nodes_fold(dst, orig, sz)	dst bits = orig bits mod sz
 *
 * for_each_node_mask(node, mask)	for-loop node over mask
 *
 * int num_online_nodes()		Number of online Nodes
 * int num_possible_nodes()		Number of all possible Nodes
 *
 * int node_random(mask)		Random node with set bit in mask
 *
 * int node_online(node)		Is some node online?
 * int node_possible(node)		Is some node possible?
 *
 * node_set_online(node)		set bit 'node' in node_online_map
 * node_set_offline(node)		clear bit 'node' in node_online_map
 *
 * for_each_node(node)			for-loop node over node_possible_map
 * for_each_online_node(node)		for-loop node over node_online_map
 *
 * Subtlety:
 * 1) The 'type-checked' form of node_isset() causes gcc (3.3.2, anyway)
 *    to generate slightly worse code.  So use a simple one-line #define
 *    for node_isset(), instead of wrapping an inline inside a macro, the
 *    way we do the other calls.
 *
 * NODEMASK_SCRATCH
 * When doing above logical AND, OR, XOR, Remap operations the callers tend to
 * need temporary nodemask_t's on the stack. But if NODES_SHIFT is large,
 * nodemask_t's consume too much stack space.  NODEMASK_SCRATCH is a helper
 * for such situations. See below and CPUMASK_ALLOC also.
 */







typedef struct { unsigned long bits[((((1 << 4)) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))]; } nodemask_t;
extern nodemask_t _unused_nodemask_arg_;

/**
 * nodemask_pr_args - printf args to output a nodemask
 * @maskp: nodemask to be printed
 *
 * Can be used to provide arguments for '%*pb[l]' when printing a nodemask.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __nodemask_pr_numnodes(const nodemask_t *m)
{
 return m ? (1 << 4) : 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const unsigned long *__nodemask_pr_bits(const nodemask_t *m)
{
 return m ? m->bits : ((void *)0);
}

/*
 * The inline keyword gives the compiler room to decide to inline, or
 * not inline a function as it sees best.  However, as these functions
 * are called in both __init and non-__init functions, if they are not
 * inlined we will end up with a section mismatch error (of the type of
 * freeable items not being freed).  So we must use __always_inline here
 * to fix the problem.  If other functions in the future also end up in
 * this situation they will also need to be annotated as __always_inline
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __node_set(int node, volatile nodemask_t *dstp)
{
 set_bit(node, dstp->bits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __node_clear(int node, volatile nodemask_t *dstp)
{
 clear_bit(node, dstp->bits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_setall(nodemask_t *dstp, unsigned int nbits)
{
 bitmap_fill(dstp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_clear(nodemask_t *dstp, unsigned int nbits)
{
 bitmap_zero(dstp->bits, nbits);
}

/* No static inline type checking - see Subtlety (1) above. */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __node_test_and_set(int node, nodemask_t *addr)
{
 return test_and_set_bit(node, addr->bits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_and(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_and(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_or(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_or(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_xor(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_xor(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_andnot(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_andnot(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_complement(nodemask_t *dstp,
     const nodemask_t *srcp, unsigned int nbits)
{
 bitmap_complement(dstp->bits, srcp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __nodes_equal(const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 return bitmap_equal(src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __nodes_intersects(const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 return bitmap_intersects(src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __nodes_subset(const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 return bitmap_subset(src1p->bits, src2p->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __nodes_empty(const nodemask_t *srcp, unsigned int nbits)
{
 return bitmap_empty(srcp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __nodes_full(const nodemask_t *srcp, unsigned int nbits)
{
 return bitmap_full(srcp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __nodes_weight(const nodemask_t *srcp, unsigned int nbits)
{
 return bitmap_weight(srcp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_shift_right(nodemask_t *dstp,
     const nodemask_t *srcp, int n, int nbits)
{
 bitmap_shift_right(dstp->bits, srcp->bits, n, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_shift_left(nodemask_t *dstp,
     const nodemask_t *srcp, int n, int nbits)
{
 bitmap_shift_left(dstp->bits, srcp->bits, n, nbits);
}

/* FIXME: better would be to fix all architectures to never return
          > MAX_NUMNODES, then the silly min_ts could be dropped. */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __first_node(const nodemask_t *srcp)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)((1 << 4))) *)1 == (typeof((unsigned int)(find_first_bit(srcp->bits, (1 << 4)))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)((1 << 4))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(find_first_bit(srcp->bits, (1 << 4)))) * 0l)) : (int *)8))))), (((unsigned int)((1 << 4))) < ((unsigned int)(find_first_bit(srcp->bits, (1 << 4)))) ? ((unsigned int)((1 << 4))) : ((unsigned int)(find_first_bit(srcp->bits, (1 << 4))))), ({ typeof((unsigned int)((1 << 4))) __UNIQUE_ID___x250 = ((unsigned int)((1 << 4))); typeof((unsigned int)(find_first_bit(srcp->bits, (1 << 4)))) __UNIQUE_ID___y251 = ((unsigned int)(find_first_bit(srcp->bits, (1 << 4)))); ((__UNIQUE_ID___x250) < (__UNIQUE_ID___y251) ? (__UNIQUE_ID___x250) : (__UNIQUE_ID___y251)); }));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __next_node(int n, const nodemask_t *srcp)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)((1 << 4))) *)1 == (typeof((unsigned int)(find_next_bit(srcp->bits, (1 << 4), n+1))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)((1 << 4))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(find_next_bit(srcp->bits, (1 << 4), n+1))) * 0l)) : (int *)8))))), (((unsigned int)((1 << 4))) < ((unsigned int)(find_next_bit(srcp->bits, (1 << 4), n+1))) ? ((unsigned int)((1 << 4))) : ((unsigned int)(find_next_bit(srcp->bits, (1 << 4), n+1)))), ({ typeof((unsigned int)((1 << 4))) __UNIQUE_ID___x252 = ((unsigned int)((1 << 4))); typeof((unsigned int)(find_next_bit(srcp->bits, (1 << 4), n+1))) __UNIQUE_ID___y253 = ((unsigned int)(find_next_bit(srcp->bits, (1 << 4), n+1))); ((__UNIQUE_ID___x252) < (__UNIQUE_ID___y253) ? (__UNIQUE_ID___x252) : (__UNIQUE_ID___y253)); }));
}

/*
 * Find the next present node in src, starting after node n, wrapping around to
 * the first node in src if needed.  Returns MAX_NUMNODES if src is empty.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __next_node_in(int node, const nodemask_t *srcp)
{
 unsigned int ret = __next_node(node, srcp);

 if (ret == (1 << 4))
  ret = __first_node(srcp);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_nodemask_of_node(nodemask_t *mask, int node)
{
 __nodes_clear(&(*mask), (1 << 4));
 __node_set((node), &(*mask));
}
# 307 "./include/linux/nodemask.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __first_unset_node(const nodemask_t *maskp)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)((1 << 4))) *)1 == (typeof((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 4)))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)((1 << 4))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 4)))) * 0l)) : (int *)8))))), (((unsigned int)((1 << 4))) < ((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 4)))) ? ((unsigned int)((1 << 4))) : ((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 4))))), ({ typeof((unsigned int)((1 << 4))) __UNIQUE_ID___x254 = ((unsigned int)((1 << 4))); typeof((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 4)))) __UNIQUE_ID___y255 = ((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 4)))); ((__UNIQUE_ID___x254) < (__UNIQUE_ID___y255) ? (__UNIQUE_ID___x254) : (__UNIQUE_ID___y255)); }));

}
# 341 "./include/linux/nodemask.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __nodemask_parse_user(const char /* nothing */ *buf, int len,
     nodemask_t *dstp, int nbits)
{
 return bitmap_parse_user(buf, len, dstp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __nodelist_parse(const char *buf, nodemask_t *dstp, int nbits)
{
 return bitmap_parselist(buf, dstp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __node_remap(int oldbit,
  const nodemask_t *oldp, const nodemask_t *newp, int nbits)
{
 return bitmap_bitremap(oldbit, oldp->bits, newp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_remap(nodemask_t *dstp, const nodemask_t *srcp,
  const nodemask_t *oldp, const nodemask_t *newp, int nbits)
{
 bitmap_remap(dstp->bits, srcp->bits, oldp->bits, newp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_onto(nodemask_t *dstp, const nodemask_t *origp,
  const nodemask_t *relmapp, int nbits)
{
 bitmap_onto(dstp->bits, origp->bits, relmapp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __nodes_fold(nodemask_t *dstp, const nodemask_t *origp,
  int sz, int nbits)
{
 bitmap_fold(dstp->bits, origp->bits, sz, nbits);
}
# 395 "./include/linux/nodemask.h"
/*
 * Bitmasks that are kept for all the nodes.
 */
enum node_states {
 N_POSSIBLE, /* The node could become online at some point */
 N_ONLINE, /* The node is online */
 N_NORMAL_MEMORY, /* The node has regular memory */



 N_HIGH_MEMORY = N_NORMAL_MEMORY,

 N_MEMORY, /* The node has memory(regular, high, movable) */
 N_CPU, /* The node has one or more cpus */
 N_GENERIC_INITIATOR, /* The node has one or more Generic Initiators */
 NR_NODE_STATES
};

/*
 * The following particular system nodemasks and operations
 * on them manage all possible and online nodes.
 */

extern nodemask_t node_states[NR_NODE_STATES];


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int node_state(int node, enum node_states state)
{
 return ((__builtin_constant_p((node)) && __builtin_constant_p((uintptr_t)((node_states[state]).bits) != (uintptr_t)((void *)0)) && (uintptr_t)((node_states[state]).bits) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)((node_states[state]).bits))) ? const_test_bit((node), (node_states[state]).bits) : generic_test_bit((node), (node_states[state]).bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_set_state(int node, enum node_states state)
{
 __node_set(node, &node_states[state]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_clear_state(int node, enum node_states state)
{
 __node_clear(node, &node_states[state]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int num_node_state(enum node_states state)
{
 return __nodes_weight(&(node_states[state]), (1 << 4));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int next_online_node(int nid)
{
 return __next_node((nid), &(node_states[N_ONLINE]));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int next_memory_node(int nid)
{
 return __next_node((nid), &(node_states[N_MEMORY]));
}

extern unsigned int nr_node_ids;
extern unsigned int nr_online_nodes;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_set_online(int nid)
{
 node_set_state(nid, N_ONLINE);
 nr_online_nodes = num_node_state(N_ONLINE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_set_offline(int nid)
{
 node_clear_state(nid, N_ONLINE);
 nr_online_nodes = num_node_state(N_ONLINE);
}
# 505 "./include/linux/nodemask.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int node_random(const nodemask_t *maskp)
{

 int w, bit;

 w = __nodes_weight(&(*maskp), (1 << 4));
 switch (w) {
 case 0:
  bit = (-1);
  break;
 case 1:
  bit = __first_node(&(*maskp));
  break;
 default:
  bit = find_nth_bit(maskp->bits, (1 << 4), get_random_u32_below(w));
  break;
 }
 return bit;



}
# 539 "./include/linux/nodemask.h"
/*
 * For nodemask scratch area.
 * NODEMASK_ALLOC(type, name) allocates an object with a specified type and
 * name.
 */
# 553 "./include/linux/nodemask.h"
/* Example structure for using NODEMASK_ALLOC, used in mempolicy. */
struct nodemask_scratch {
 nodemask_t mask1;
 nodemask_t mask2;
};
# 24 "./include/linux/sched.h" 2


# 1 "./include/linux/resource.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/resource.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */






/*
 * Resource control/accounting header file for linux
 */

/*
 * Definition of struct rusage taken from BSD 4.3 Reno
 *
 * We don't support all of these yet, but we might as well have them....
 * Otherwise, each time we add new items, programs which depend on this
 * structure will lose.  This reduces the chances of that happening.
 */





struct rusage {
 struct __kernel_old_timeval ru_utime; /* user time used */
 struct __kernel_old_timeval ru_stime; /* system time used */
 __kernel_long_t ru_maxrss; /* maximum resident set size */
 __kernel_long_t ru_ixrss; /* integral shared memory size */
 __kernel_long_t ru_idrss; /* integral unshared data size */
 __kernel_long_t ru_isrss; /* integral unshared stack size */
 __kernel_long_t ru_minflt; /* page reclaims */
 __kernel_long_t ru_majflt; /* page faults */
 __kernel_long_t ru_nswap; /* swaps */
 __kernel_long_t ru_inblock; /* block input operations */
 __kernel_long_t ru_oublock; /* block output operations */
 __kernel_long_t ru_msgsnd; /* messages sent */
 __kernel_long_t ru_msgrcv; /* messages received */
 __kernel_long_t ru_nsignals; /* signals received */
 __kernel_long_t ru_nvcsw; /* voluntary context switches */
 __kernel_long_t ru_nivcsw; /* involuntary " */
};

struct rlimit {
 __kernel_ulong_t rlim_cur;
 __kernel_ulong_t rlim_max;
};



struct rlimit64 {
 __u64 rlim_cur;
 __u64 rlim_max;
};
# 62 "./include/uapi/linux/resource.h"
/*
 * Limit the stack by to some sane default: root can always
 * increase this limit if needed..  8MB seems reasonable.
 */


/*
 * Limit the amount of locked memory by some sane default:
 * root can always increase this limit if needed.
 *
 * The main use-cases are (1) preventing sensitive memory
 * from being swapped; (2) real-time operations; (3) via
 * IOURING_REGISTER_BUFFERS.
 *
 * The first two don't need much. The latter will take as
 * much as it can get. 8MB is a reasonably sane default.
 */


/*
 * Due to binary compatibility, the actual resource numbers
 * may be different for different linux versions..
 */
# 1 "./arch/arm64/include/generated/uapi/asm/resource.h" 1
# 1 "./include/asm-generic/resource.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/asm-generic/resource.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/*
 * Resource limit IDs
 *
 * ( Compatibility detail: there are architectures that have
 *   a different rlimit ID order in the 5-9 range and want
 *   to keep that order for binary compatibility. The reasons
 *   are historic and all new rlimits are identical across all
 *   arches. If an arch has such special order for some rlimits
 *   then it defines them prior including asm-generic/resource.h. )
 */
# 51 "./include/uapi/asm-generic/resource.h"
/*
 * SuS says limits have to be unsigned.
 * Which makes a ton more sense anyway.
 *
 * Some architectures override this (for compatibility reasons):
 */
# 6 "./include/asm-generic/resource.h" 2


/*
 * boot-time rlimit defaults for the init task:
 */
# 2 "./arch/arm64/include/generated/uapi/asm/resource.h" 2
# 86 "./include/uapi/linux/resource.h" 2
# 6 "./include/linux/resource.h" 2


struct task_struct;

void getrusage(struct task_struct *p, int who, struct rusage *ru);
# 27 "./include/linux/sched.h" 2
# 1 "./include/linux/latencytop.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * latencytop.h: Infrastructure for displaying latency
 *
 * (C) Copyright 2008 Intel Corporation
 * Author: Arjan van de Ven <arjan@linux.intel.com>
 *
 */





struct task_struct;
# 43 "./include/linux/latencytop.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
account_scheduler_latency(struct task_struct *task, int usecs, int inter)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_tsk_latency_tracing(struct task_struct *p)
{
}
# 28 "./include/linux/sched.h" 2
# 1 "./include/linux/sched/prio.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







/*
 * Priority of a process goes from 0..MAX_PRIO-1, valid RT
 * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
 * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
 * values are inverted: lower p->prio value means higher priority.
 */






/*
 * Convert user-nice values [ -20 ... 0 ... 19 ]
 * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
 * and back.
 */



/*
 * Convert nice value [19,-20] to rlimit style value [1,40].
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long nice_to_rlimit(long nice)
{
 return (19 - nice + 1);
}

/*
 * Convert rlimit style value [1,40] to nice value [-20, 19].
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long rlimit_to_nice(long prio)
{
 return (19 - prio + 1);
}
# 29 "./include/linux/sched.h" 2
# 1 "./include/linux/sched/types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/**
 * struct task_cputime - collected CPU time counts
 * @stime:		time spent in kernel mode, in nanoseconds
 * @utime:		time spent in user mode, in nanoseconds
 * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
 *
 * This structure groups together three kinds of CPU time that are tracked for
 * threads and thread groups.  Most things considering CPU time want to group
 * these counts together and treat all three of them in parallel.
 */
struct task_cputime {
 u64 stime;
 u64 utime;
 unsigned long long sum_exec_runtime;
};
# 30 "./include/linux/sched.h" 2
# 1 "./include/linux/signal_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Basic signal handling related data type definitions:
 */


# 1 "./include/uapi/linux/signal.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./arch/arm64/include/asm/signal.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/uapi/asm/signal.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */



/* Required for AArch32 compatibility. */





# 1 "./include/asm-generic/signal.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/asm-generic/signal.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 42 "./include/uapi/asm-generic/signal.h"
/*
#define SIGLOST		29
*/




/* These should not be considered constants from userland.  */
# 61 "./include/uapi/asm-generic/signal.h"
typedef struct {
 unsigned long sig[(64 / 64)];
} sigset_t;

/* not actually used, but required for linux/syscalls.h */
typedef unsigned long old_sigset_t;

# 1 "./include/uapi/asm-generic/signal-defs.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





/*
 * SA_FLAGS values:
 *
 * SA_NOCLDSTOP flag to turn off SIGCHLD when children stop.
 * SA_NOCLDWAIT flag on SIGCHLD to inhibit zombies.
 * SA_SIGINFO delivers the signal with SIGINFO structs.
 * SA_ONSTACK indicates that a registered stack_t will be used.
 * SA_RESTART flag to get restarting signals (which were the default long ago)
 * SA_NODEFER prevents the current signal from being masked in the handler.
 * SA_RESETHAND clears the handler when the signal is delivered.
 * SA_UNSUPPORTED is a flag bit that will never be supported. Kernels from
 * before the introduction of SA_UNSUPPORTED did not clear unknown bits from
 * sa_flags when read using the oldact argument to sigaction and rt_sigaction,
 * so this bit allows flag bit support to be detected from userspace while
 * allowing an old kernel to be distinguished from a kernel that supports every
 * flag bit.
 * SA_EXPOSE_TAGBITS exposes an architecture-defined set of tag bits in
 * siginfo.si_addr.
 *
 * SA_ONESHOT and SA_NOMASK are the historical Linux names for the Single
 * Unix names RESETHAND and NODEFER respectively.
 */
# 38 "./include/uapi/asm-generic/signal-defs.h"
/* 0x00000008 used on alpha, mips, parisc */
/* 0x00000010 used on alpha, parisc */
/* 0x00000020 used on alpha, parisc, sparc */
/* 0x00000040 used on alpha, parisc */
/* 0x00000080 used on parisc */
/* 0x00000100 used on sparc */
/* 0x00000200 used on sparc */


/* 0x00010000 used on mips */
/* 0x00800000 used for internal SA_IMMUTABLE */
/* 0x01000000 used on x86 */
/* 0x02000000 used on x86 */
/*
 * New architectures should not define the obsolete
 *	SA_RESTORER	0x04000000
 */
# 82 "./include/uapi/asm-generic/signal-defs.h"
typedef void __signalfn_t(int);
typedef __signalfn_t /* nothing */ *__sighandler_t;

typedef void __restorefn_t(void);
typedef __restorefn_t /* nothing */ *__sigrestore_t;
# 69 "./include/uapi/asm-generic/signal.h" 2
# 85 "./include/uapi/asm-generic/signal.h"
typedef struct sigaltstack {
 void /* nothing */ *ss_sp;
 int ss_flags;
 __kernel_size_t ss_size;
} stack_t;
# 6 "./include/asm-generic/signal.h" 2
# 27 "./arch/arm64/include/uapi/asm/signal.h" 2
# 7 "./arch/arm64/include/asm/signal.h" 2
# 1 "./arch/arm64/include/generated/uapi/asm/siginfo.h" 1
# 1 "./include/uapi/asm-generic/siginfo.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */






typedef union sigval {
 int sival_int;
 void /* nothing */ *sival_ptr;
} sigval_t;



/*
 * The default "si_band" type is "long", as specified by POSIX.
 * However, some architectures want to override this to "int"
 * for historical compatibility reasons, so we allow that.
 */
# 32 "./include/uapi/asm-generic/siginfo.h"
/*
 * Be careful when extending this union.  On 32bit siginfo_t is 32bit
 * aligned.  Which means that a 64bit field or any other field that
 * would increase the alignment of siginfo_t will break the ABI.
 */
union __sifields {
 /* kill() */
 struct {
  __kernel_pid_t _pid; /* sender's pid */
  __kernel_uid32_t _uid; /* sender's uid */
 } _kill;

 /* POSIX.1b timers */
 struct {
  __kernel_timer_t _tid; /* timer id */
  int _overrun; /* overrun count */
  sigval_t _sigval; /* same as below */
  int _sys_private; /* not to be passed to user */
 } _timer;

 /* POSIX.1b signals */
 struct {
  __kernel_pid_t _pid; /* sender's pid */
  __kernel_uid32_t _uid; /* sender's uid */
  sigval_t _sigval;
 } _rt;

 /* SIGCHLD */
 struct {
  __kernel_pid_t _pid; /* which child */
  __kernel_uid32_t _uid; /* sender's uid */
  int _status; /* exit code */
  __kernel_clock_t _utime;
  __kernel_clock_t _stime;
 } _sigchld;

 /* SIGILL, SIGFPE, SIGSEGV, SIGBUS, SIGTRAP, SIGEMT */
 struct {
  void /* nothing */ *_addr; /* faulting insn/memory ref. */
# 79 "./include/uapi/asm-generic/siginfo.h"
  union {
   /* used on alpha and sparc */
   int _trapno; /* TRAP # which caused the signal */
   /*
			 * used when si_code=BUS_MCEERR_AR or
			 * used when si_code=BUS_MCEERR_AO
			 */
   short _addr_lsb; /* LSB of the reported address */
   /* used when si_code=SEGV_BNDERR */
   struct {
    char _dummy_bnd[(__alignof__(void *) < sizeof(short) ? sizeof(short) : __alignof__(void *))];
    void /* nothing */ *_lower;
    void /* nothing */ *_upper;
   } _addr_bnd;
   /* used when si_code=SEGV_PKUERR */
   struct {
    char _dummy_pkey[(__alignof__(void *) < sizeof(short) ? sizeof(short) : __alignof__(void *))];
    __u32 _pkey;
   } _addr_pkey;
   /* used when si_code=TRAP_PERF */
   struct {
    unsigned long _data;
    __u32 _type;
    __u32 _flags;
   } _perf;
  };
 } _sigfault;

 /* SIGPOLL */
 struct {
  long _band; /* POLL_IN, POLL_OUT, POLL_MSG */
  int _fd;
 } _sigpoll;

 /* SIGSYS */
 struct {
  void /* nothing */ *_call_addr; /* calling user insn */
  int _syscall; /* triggering system call number */
  unsigned int _arch; /* AUDIT_ARCH_* of syscall */
 } _sigsys;
};
# 139 "./include/uapi/asm-generic/siginfo.h"
typedef struct siginfo {
 union {
  struct { int si_signo; int si_errno; int si_code; union __sifields _sifields; };
  int _si_pad[128/sizeof(int)];
 };
} siginfo_t;

/*
 * How these fields are to be accessed.
 */
# 175 "./include/uapi/asm-generic/siginfo.h"
/*
 * si_code values
 * Digital reserves positive values for kernel-generated signals.
 */
# 193 "./include/uapi/asm-generic/siginfo.h"
/*
 * SIGILL si_codes
 */
# 209 "./include/uapi/asm-generic/siginfo.h"
/*
 * SIGFPE si_codes
 */
# 229 "./include/uapi/asm-generic/siginfo.h"
/*
 * SIGSEGV si_codes
 */
# 247 "./include/uapi/asm-generic/siginfo.h"
/*
 * SIGBUS si_codes
 */



/* hardware memory error consumed on a machine check: action required */

/* hardware memory error detected in process but not consumed: action optional*/



/*
 * SIGTRAP si_codes
 */
# 270 "./include/uapi/asm-generic/siginfo.h"
/*
 * There is an additional set of SIGTRAP si_codes used by ptrace
 * that are of the form: ((PTRACE_EVENT_XXX << 8) | SIGTRAP)
 */

/*
 * Flags for si_perf_flags if SIGTRAP si_code is TRAP_PERF.
 */


/*
 * SIGCHLD si_codes
 */
# 291 "./include/uapi/asm-generic/siginfo.h"
/*
 * SIGPOLL (or any other signal without signal specific si_codes) si_codes
 */
# 302 "./include/uapi/asm-generic/siginfo.h"
/*
 * SIGSYS si_codes
 */




/*
 * SIGEMT si_codes
 */



/*
 * sigevent definitions
 *
 * It seems likely that SIGEV_THREAD will have to be handled from
 * userspace, libpthread transmuting it to SIGEV_SIGNAL, which the
 * thread manager then catches and does the appropriate nonsense.
 * However, everything is written out here so as to not get lost.
 */





/*
 * This works because the alignment is ok on all current architectures
 * but we leave open this being overridden in the future
 */
# 340 "./include/uapi/asm-generic/siginfo.h"
typedef struct sigevent {
 sigval_t sigev_value;
 int sigev_signo;
 int sigev_notify;
 union {
  int _pad[((64 - (sizeof(int) * 2 + sizeof(sigval_t))) / sizeof(int))];
   int _tid;

  struct {
   void (*_function)(sigval_t);
   void *_attribute; /* really pthread_attr_t */
  } _sigev_thread;
 } _sigev_un;
} sigevent_t;
# 2 "./arch/arm64/include/generated/uapi/asm/siginfo.h" 2
# 8 "./arch/arm64/include/asm/signal.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void /* nothing */ *arch_untagged_si_addr(void /* nothing */ *addr,
       unsigned long sig,
       unsigned long si_code)
{
 /*
	 * For historical reasons, all bits of the fault address are exposed as
	 * address bits for watchpoint exceptions. New architectures should
	 * handle the tag bits consistently.
	 */
 if (sig == 5 && si_code == 1 /* process breakpoint */)
  return addr;

 return ({ u64 __addr = ( u64)(addr); __addr &= (( __typeof__(__addr))sign_extend64(( u64)(__addr), 55)); ( __typeof__(addr))__addr; });
}
# 6 "./include/uapi/linux/signal.h" 2
# 1 "./arch/arm64/include/generated/uapi/asm/siginfo.h" 1
# 7 "./include/uapi/linux/signal.h" 2




/* bit-flags */

/* mask for all SS_xxx flags */
# 11 "./include/linux/signal_types.h" 2

typedef struct kernel_siginfo {
 struct { int si_signo; int si_errno; int si_code; union __sifields _sifields; };
} kernel_siginfo_t;

struct ucounts;

/*
 * Real Time signals may be queued.
 */

struct sigqueue {
 struct list_head list;
 int flags;
 kernel_siginfo_t info;
 struct ucounts *ucounts;
};

/* flags values. */


struct sigpending {
 struct list_head list;
 sigset_t signal;
};

struct sigaction {

 __sighandler_t sa_handler;
 unsigned long sa_flags;





 __sigrestore_t sa_restorer;

 sigset_t sa_mask; /* mask last for extensibility */
};

struct k_sigaction {
 struct sigaction sa;



};
# 67 "./include/linux/signal_types.h"
struct ksignal {
 struct k_sigaction ka;
 kernel_siginfo_t info;
 int sig;
};

/* Used to kill the race between sigaction and forced signals */
# 31 "./include/linux/sched.h" 2
# 1 "./include/linux/syscall_user_dispatch.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright (C) 2020 Collabora Ltd.
 */
# 26 "./include/linux/syscall_user_dispatch.h"
struct syscall_user_dispatch {};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int set_syscall_user_dispatch(unsigned long mode, unsigned long offset,
         unsigned long len, char /* nothing */ *selector)
{
 return -22 /* Invalid argument */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_syscall_work_syscall_user_dispatch(struct task_struct *tsk)
{
}
# 32 "./include/linux/sched.h" 2
# 1 "./include/linux/mm_types_task.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Here are the definitions of the MM data types that are embedded in 'struct task_struct'.
 *
 * (These are defined separately to decouple sched.h from mm_types.h as much as possible.)
 */
# 27 "./include/linux/mm_types_task.h"
/*
 * When updating this, please also update struct resident_page_types[] in
 * kernel/fork.c
 */
enum {
 MM_FILEPAGES, /* Resident file mapping pages */
 MM_ANONPAGES, /* Resident anonymous pages */
 MM_SWAPENTS, /* Anonymous swap entries */
 MM_SHMEMPAGES, /* Resident shared memory pages */
 NR_MM_COUNTERS
};

struct page_frag {
 struct page *page;

 __u32 offset;
 __u32 size;




};

/* Track pages that require TLB flushes */
struct tlbflush_unmap_batch {
# 72 "./include/linux/mm_types_task.h"
};
# 33 "./include/linux/sched.h" 2
# 1 "./include/linux/task_io_accounting.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * task_io_accounting: a structure which is used for recording a single task's
 * IO statistics.
 *
 * Don't include this header file directly - it is designed to be dragged in via
 * sched.h.
 *
 * Blame Andrew Morton for all this.
 */

struct task_io_accounting {

 /* bytes read */
 u64 rchar;
 /*  bytes written */
 u64 wchar;
 /* # of read syscalls */
 u64 syscr;
 /* # of write syscalls */
 u64 syscw;



 /*
	 * The number of bytes which this task has caused to be read from
	 * storage.
	 */
 u64 read_bytes;

 /*
	 * The number of bytes which this task has caused, or shall cause to be
	 * written to disk.
	 */
 u64 write_bytes;

 /*
	 * A task can cause "negative" IO too.  If this task truncates some
	 * dirty pagecache, some IO which another task has been accounted for
	 * (in its write_bytes) will not be happening.  We _could_ just
	 * subtract that from the truncating task's write_bytes, but there is
	 * information loss in doing that.
	 */
 u64 cancelled_write_bytes;

};
# 34 "./include/linux/sched.h" 2
# 1 "./include/linux/posix-timers.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/alarmtimer.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







struct rtc_device;

enum alarmtimer_type {
 ALARM_REALTIME,
 ALARM_BOOTTIME,

 /* Supported types end here */
 ALARM_NUMTYPE,

 /* Used for tracing information. No usable types. */
 ALARM_REALTIME_FREEZER,
 ALARM_BOOTTIME_FREEZER,
};

enum alarmtimer_restart {
 ALARMTIMER_NORESTART,
 ALARMTIMER_RESTART,
};





/**
 * struct alarm - Alarm timer structure
 * @node:	timerqueue node for adding to the event list this value
 *		also includes the expiration time.
 * @timer:	hrtimer used to schedule events while running
 * @function:	Function pointer to be executed when the timer fires.
 * @type:	Alarm type (BOOTTIME/REALTIME).
 * @state:	Flag that represents if the alarm is set to fire or not.
 * @data:	Internal data value.
 */
struct alarm {
 struct timerqueue_node node;
 struct hrtimer timer;
 enum alarmtimer_restart (*function)(struct alarm *, ktime_t now);
 enum alarmtimer_type type;
 int state;
 void *data;
};

void alarm_init(struct alarm *alarm, enum alarmtimer_type type,
  enum alarmtimer_restart (*function)(struct alarm *, ktime_t));
void alarm_start(struct alarm *alarm, ktime_t start);
void alarm_start_relative(struct alarm *alarm, ktime_t start);
void alarm_restart(struct alarm *alarm);
int alarm_try_to_cancel(struct alarm *alarm);
int alarm_cancel(struct alarm *alarm);

u64 alarm_forward(struct alarm *alarm, ktime_t now, ktime_t interval);
u64 alarm_forward_now(struct alarm *alarm, ktime_t interval);
ktime_t alarm_expires_remaining(const struct alarm *alarm);


/* Provide way to access the rtc device being used by alarmtimers */
struct rtc_device *alarmtimer_get_rtcdev(void);
# 8 "./include/linux/posix-timers.h" 2


struct kernel_siginfo;
struct task_struct;

/*
 * Bit fields within a clockid:
 *
 * The most significant 29 bits hold either a pid or a file descriptor.
 *
 * Bit 2 indicates whether a cpu clock refers to a thread or a process.
 *
 * Bits 1 and 0 give the type: PROF=0, VIRT=1, SCHED=2, or FD=3.
 *
 * A clockid is invalid if bits 2, 1, and 0 are all set.
 */
# 38 "./include/linux/posix-timers.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) clockid_t make_process_cpuclock(const unsigned int pid,
  const clockid_t clock)
{
 return ((~pid) << 3) | clock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) clockid_t make_thread_cpuclock(const unsigned int tid,
  const clockid_t clock)
{
 return make_process_cpuclock(tid, clock | 4);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) clockid_t fd_to_clockid(const int fd)
{
 return make_process_cpuclock((unsigned int) fd, 3);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int clockid_to_fd(const clockid_t clk)
{
 return ~(clk >> 3);
}



/**
 * cpu_timer - Posix CPU timer representation for k_itimer
 * @node:	timerqueue node to queue in the task/sig
 * @head:	timerqueue head on which this timer is queued
 * @task:	Pointer to target task
 * @elist:	List head for the expiry list
 * @firing:	Timer is currently firing
 */
struct cpu_timer {
 struct timerqueue_node node;
 struct timerqueue_head *head;
 struct pid *pid;
 struct list_head elist;
 int firing;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_timer_enqueue(struct timerqueue_head *head,
         struct cpu_timer *ctmr)
{
 ctmr->head = head;
 return timerqueue_add(head, &ctmr->node);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_timer_queued(struct cpu_timer *ctmr)
{
 return !!ctmr->head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpu_timer_dequeue(struct cpu_timer *ctmr)
{
 if (cpu_timer_queued(ctmr)) {
  timerqueue_del(ctmr->head, &ctmr->node);
  ctmr->head = ((void *)0);
  return true;
 }
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 cpu_timer_getexpires(struct cpu_timer *ctmr)
{
 return ctmr->node.expires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cpu_timer_setexpires(struct cpu_timer *ctmr, u64 exp)
{
 ctmr->node.expires = exp;
}

/**
 * posix_cputimer_base - Container per posix CPU clock
 * @nextevt:		Earliest-expiration cache
 * @tqhead:		timerqueue head for cpu_timers
 */
struct posix_cputimer_base {
 u64 nextevt;
 struct timerqueue_head tqhead;
};

/**
 * posix_cputimers - Container for posix CPU timer related data
 * @bases:		Base container for posix CPU clocks
 * @timers_active:	Timers are queued.
 * @expiry_active:	Timer expiry is active. Used for
 *			process wide timers to avoid multiple
 *			task trying to handle expiry concurrently
 *
 * Used in task_struct and signal_struct
 */
struct posix_cputimers {
 struct posix_cputimer_base bases[3];
 unsigned int timers_active;
 unsigned int expiry_active;
};

/**
 * posix_cputimers_work - Container for task work based posix CPU timer expiry
 * @work:	The task work to be scheduled
 * @scheduled:  @work has been scheduled already, no further processing
 */
struct posix_cputimers_work {
 struct callback_head work;
 unsigned int scheduled;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void posix_cputimers_init(struct posix_cputimers *pct)
{
 memset(pct, 0, sizeof(*pct));
 pct->bases[0].nextevt = ((u64)~0ULL);
 pct->bases[1].nextevt = ((u64)~0ULL);
 pct->bases[2].nextevt = ((u64)~0ULL);
}

void posix_cputimers_group_init(struct posix_cputimers *pct, u64 cpu_limit);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void posix_cputimers_rt_watchdog(struct posix_cputimers *pct,
            u64 runtime)
{
 pct->bases[2].nextevt = runtime;
}

/* Init task static initializer */
# 186 "./include/linux/posix-timers.h"
void clear_posix_cputimers_work(struct task_struct *p);
void posix_cputimers_init_work(void);







/**
 * struct k_itimer - POSIX.1b interval timer structure.
 * @list:		List head for binding the timer to signals->posix_timers
 * @t_hash:		Entry in the posix timer hash table
 * @it_lock:		Lock protecting the timer
 * @kclock:		Pointer to the k_clock struct handling this timer
 * @it_clock:		The posix timer clock id
 * @it_id:		The posix timer id for identifying the timer
 * @it_active:		Marker that timer is active
 * @it_overrun:		The overrun counter for pending signals
 * @it_overrun_last:	The overrun at the time of the last delivered signal
 * @it_requeue_pending:	Indicator that timer waits for being requeued on
 *			signal delivery
 * @it_sigev_notify:	The notify word of sigevent struct for signal delivery
 * @it_interval:	The interval for periodic timers
 * @it_signal:		Pointer to the creators signal struct
 * @it_pid:		The pid of the process/task targeted by the signal
 * @it_process:		The task to wakeup on clock_nanosleep (CPU timers)
 * @sigq:		Pointer to preallocated sigqueue
 * @it:			Union representing the various posix timer type
 *			internals.
 * @rcu:		RCU head for freeing the timer.
 */
struct k_itimer {
 struct list_head list;
 struct hlist_node t_hash;
 spinlock_t it_lock;
 const struct k_clock *kclock;
 clockid_t it_clock;
 timer_t it_id;
 int it_active;
 s64 it_overrun;
 s64 it_overrun_last;
 int it_requeue_pending;
 int it_sigev_notify;
 ktime_t it_interval;
 struct signal_struct *it_signal;
 union {
  struct pid *it_pid;
  struct task_struct *it_process;
 };
 struct sigqueue *sigq;
 union {
  struct {
   struct hrtimer timer;
  } real;
  struct cpu_timer cpu;
  struct {
   struct alarm alarmtimer;
  } alarm;
 } it;
 struct callback_head rcu;
};

void run_posix_cpu_timers(void);
void posix_cpu_timers_exit(struct task_struct *task);
void posix_cpu_timers_exit_group(struct task_struct *task);
void set_process_cpu_timer(struct task_struct *task, unsigned int clock_idx,
      u64 *newval, u64 *oldval);

int update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new);

void posixtimer_rearm(struct kernel_siginfo *info);
# 35 "./include/linux/sched.h" 2
# 1 "./include/uapi/linux/rseq.h" 1
/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */



/*
 * linux/rseq.h
 *
 * Restartable sequences system call API
 *
 * Copyright (c) 2015-2018 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
 */




enum rseq_cpu_id_state {
 RSEQ_CPU_ID_UNINITIALIZED = -1,
 RSEQ_CPU_ID_REGISTRATION_FAILED = -2,
};

enum rseq_flags {
 RSEQ_FLAG_UNREGISTER = (1 << 0),
};

enum rseq_cs_flags_bit {
 RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT = 0,
 RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT = 1,
 RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT = 2,
};

enum rseq_cs_flags {
 RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT =
  (1U << RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT),
 RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL =
  (1U << RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT),
 RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE =
  (1U << RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT),
};

/*
 * struct rseq_cs is aligned on 4 * 8 bytes to ensure it is always
 * contained within a single cache-line. It is usually declared as
 * link-time constant data.
 */
struct rseq_cs {
 /* Version of this structure. */
 __u32 version;
 /* enum rseq_cs_flags */
 __u32 flags;
 __u64 start_ip;
 /* Offset from start_ip. */
 __u64 post_commit_offset;
 __u64 abort_ip;
} __attribute__((aligned(4 * sizeof(__u64))));

/*
 * struct rseq is aligned on 4 * 8 bytes to ensure it is always
 * contained within a single cache-line.
 *
 * A single struct rseq per thread is allowed.
 */
struct rseq {
 /*
	 * Restartable sequences cpu_id_start field. Updated by the
	 * kernel. Read by user-space with single-copy atomicity
	 * semantics. This field should only be read by the thread which
	 * registered this data structure. Aligned on 32-bit. Always
	 * contains a value in the range of possible CPUs, although the
	 * value may not be the actual current CPU (e.g. if rseq is not
	 * initialized). This CPU number value should always be compared
	 * against the value of the cpu_id field before performing a rseq
	 * commit or returning a value read from a data structure indexed
	 * using the cpu_id_start value.
	 */
 __u32 cpu_id_start;
 /*
	 * Restartable sequences cpu_id field. Updated by the kernel.
	 * Read by user-space with single-copy atomicity semantics. This
	 * field should only be read by the thread which registered this
	 * data structure. Aligned on 32-bit. Values
	 * RSEQ_CPU_ID_UNINITIALIZED and RSEQ_CPU_ID_REGISTRATION_FAILED
	 * have a special semantic: the former means "rseq uninitialized",
	 * and latter means "rseq initialization failed". This value is
	 * meant to be read within rseq critical sections and compared
	 * with the cpu_id_start value previously read, before performing
	 * the commit instruction, or read and compared with the
	 * cpu_id_start value before returning a value loaded from a data
	 * structure indexed using the cpu_id_start value.
	 */
 __u32 cpu_id;
 /*
	 * Restartable sequences rseq_cs field.
	 *
	 * Contains NULL when no critical section is active for the current
	 * thread, or holds a pointer to the currently active struct rseq_cs.
	 *
	 * Updated by user-space, which sets the address of the currently
	 * active rseq_cs at the beginning of assembly instruction sequence
	 * block, and set to NULL by the kernel when it restarts an assembly
	 * instruction sequence block, as well as when the kernel detects that
	 * it is preempting or delivering a signal outside of the range
	 * targeted by the rseq_cs. Also needs to be set to NULL by user-space
	 * before reclaiming memory that contains the targeted struct rseq_cs.
	 *
	 * Read and set by the kernel. Set by user-space with single-copy
	 * atomicity semantics. This field should only be updated by the
	 * thread which registered this data structure. Aligned on 64-bit.
	 *
	 * 32-bit architectures should update the low order bits of the
	 * rseq_cs field, leaving the high order bits initialized to 0.
	 */
 __u64 rseq_cs;

 /*
	 * Restartable sequences flags field.
	 *
	 * This field should only be updated by the thread which
	 * registered this data structure. Read by the kernel.
	 * Mainly used for single-stepping through rseq critical sections
	 * with debuggers.
	 *
	 * - RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT
	 *     Inhibit instruction sequence block restart on preemption
	 *     for this thread.
	 * - RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL
	 *     Inhibit instruction sequence block restart on signal
	 *     delivery for this thread.
	 * - RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE
	 *     Inhibit instruction sequence block restart on migration for
	 *     this thread.
	 */
 __u32 flags;
} __attribute__((aligned(4 * sizeof(__u64))));
# 36 "./include/linux/sched.h" 2

# 1 "./include/linux/kcsan.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * The Kernel Concurrency Sanitizer (KCSAN) infrastructure. Public interface and
 * data structures to set up runtime. See kcsan-checks.h for explicit checks and
 * modifiers. For more info please see Documentation/dev-tools/kcsan.rst.
 *
 * Copyright (C) 2019, Google LLC.
 */
# 71 "./include/linux/kcsan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kcsan_init(void) { }
# 38 "./include/linux/sched.h" 2
# 1 "./include/linux/rv.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Runtime Verification.
 *
 * For futher information, see: kernel/trace/rv/rv.c.
 */
# 39 "./include/linux/sched.h" 2
# 1 "./arch/arm64/include/generated/asm/kmap_size.h" 1
# 1 "./include/asm-generic/kmap_size.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/* For debug this provides guard pages between the maps */
# 2 "./arch/arm64/include/generated/asm/kmap_size.h" 2
# 40 "./include/linux/sched.h" 2

/* task_struct member predeclarations (sorted alphabetically): */
struct audit_context;
struct backing_dev_info;
struct bio_list;
struct blk_plug;
struct bpf_local_storage;
struct bpf_run_ctx;
struct capture_control;
struct cfs_rq;
struct fs_struct;
struct futex_pi_state;
struct io_context;
struct io_uring_task;
struct mempolicy;
struct nameidata;
struct nsproxy;
struct perf_event_context;
struct pid_namespace;
struct pipe_inode_info;
struct rcu_node;
struct reclaim_state;
struct robust_list_head;
struct root_domain;
struct rq;
struct sched_attr;
struct sched_param;
struct seq_file;
struct sighand_struct;
struct signal_struct;
struct task_delay_info;
struct task_group;

/*
 * Task state bitmask. NOTE! These bits are also
 * encoded in fs/proc/array.c: get_task_state().
 *
 * We have two separate sets of flags: task->state
 * is about runnability, while task->exit_state are
 * about the task exiting. Confusing, but this way
 * modifying one set can't modify the other one by
 * mistake.
 */

/* Used in tsk->state: */





/* Used in tsk->exit_state: */



/* Used in tsk->state again: */
# 109 "./include/linux/sched.h"
/*
 * DO NOT ADD ANY NEW USERS !
 */


/* Convenience macros for the sake of set_current_state: */






/* Convenience macros for the sake of wake_up(): */


/* get_task_state(): */
# 136 "./include/linux/sched.h"
/*
 * Special states are those that do not use the normal wait-loop pattern. See
 * the comment with set_special_state().
 */
# 174 "./include/linux/sched.h"
/*
 * set_current_state() includes a barrier so that the write of current->state
 * is correctly serialised wrt the caller's subsequent test of whether to
 * actually sleep:
 *
 *   for (;;) {
 *	set_current_state(TASK_UNINTERRUPTIBLE);
 *	if (CONDITION)
 *	   break;
 *
 *	schedule();
 *   }
 *   __set_current_state(TASK_RUNNING);
 *
 * If the caller does not need such serialisation (because, for instance, the
 * CONDITION test and condition change and wakeup are under the same lock) then
 * use __set_current_state().
 *
 * The above is typically ordered against the wakeup, which does:
 *
 *   CONDITION = 1;
 *   wake_up_state(p, TASK_UNINTERRUPTIBLE);
 *
 * where wake_up_state()/try_to_wake_up() executes a full memory barrier before
 * accessing p->state.
 *
 * Wakeup will do: if (@state & p->state) p->state = TASK_RUNNING, that is,
 * once it observes the TASK_UNINTERRUPTIBLE store the waking CPU can issue a
 * TASK_RUNNING store which can collide with __set_current_state(TASK_RUNNING).
 *
 * However, with slightly different timing the wakeup TASK_RUNNING store can
 * also collide with the TASK_UNINTERRUPTIBLE store. Losing that store is not
 * a problem either because that will result in one extra go around the loop
 * and our @cond test will save the day.
 *
 * Also see the comments of try_to_wake_up().
 */
# 223 "./include/linux/sched.h"
/*
 * set_special_state() should be used for those states when the blocking task
 * can not use the regular condition based wait-loop. In that case we must
 * serialize against wakeups such that any possible in-flight TASK_RUNNING
 * stores will not collide with our state change.
 */
# 239 "./include/linux/sched.h"
/*
 * PREEMPT_RT specific variants for "sleeping" spin/rwlocks
 *
 * RT's spin/rwlock substitutions are state preserving. The state of the
 * task when blocking on the lock is saved in task_struct::saved_state and
 * restored after the lock has been acquired.  These operations are
 * serialized by task_struct::pi_lock against try_to_wake_up(). Any non RT
 * lock related wakeups while the task is blocked on the lock are
 * redirected to operate on task_struct::saved_state to ensure that these
 * are not dropped. On restore task_struct::saved_state is set to
 * TASK_RUNNING so any wakeup attempt redirected to saved_state will fail.
 *
 * The lock operation looks like this:
 *
 *	current_save_and_set_rtlock_wait_state();
 *	for (;;) {
 *		if (try_lock())
 *			break;
 *		raw_spin_unlock_irq(&lock->wait_lock);
 *		schedule_rtlock();
 *		raw_spin_lock_irq(&lock->wait_lock);
 *		set_current_state(TASK_RTLOCK_WAIT);
 *	}
 *	current_restore_rtlock_saved_state();
 */
# 286 "./include/linux/sched.h"
/*
 * Define the task command name length as enum, then it can be visible to
 * BPF programs.
 */
enum {
 TASK_COMM_LEN = 16,
};

extern void scheduler_tick(void);



extern long schedule_timeout(long timeout);
extern long schedule_timeout_interruptible(long timeout);
extern long schedule_timeout_killable(long timeout);
extern long schedule_timeout_uninterruptible(long timeout);
extern long schedule_timeout_idle(long timeout);
           void schedule(void);
extern void schedule_preempt_disabled(void);
           void preempt_schedule_irq(void);




extern int __attribute__((__warn_unused_result__)) io_schedule_prepare(void);
extern void io_schedule_finish(int token);
extern long io_schedule_timeout(long timeout);
extern void io_schedule(void);

/**
 * struct prev_cputime - snapshot of system and user cputime
 * @utime: time spent in user mode
 * @stime: time spent in system mode
 * @lock: protects the above two fields
 *
 * Stores previous user/system time values such that we can guarantee
 * monotonicity.
 */
struct prev_cputime {

 u64 utime;
 u64 stime;
 raw_spinlock_t lock;

};

enum vtime_state {
 /* Task is sleeping or running in a CPU with VTIME inactive: */
 VTIME_INACTIVE = 0,
 /* Task is idle */
 VTIME_IDLE,
 /* Task runs in kernelspace in a CPU with VTIME active: */
 VTIME_SYS,
 /* Task runs in userspace in a CPU with VTIME active: */
 VTIME_USER,
 /* Task runs as guests in a CPU with VTIME active: */
 VTIME_GUEST,
};

struct vtime {
 seqcount_t seqcount;
 unsigned long long starttime;
 enum vtime_state state;
 unsigned int cpu;
 u64 utime;
 u64 stime;
 u64 gtime;
};

/*
 * Utilization clamp constraints.
 * @UCLAMP_MIN:	Minimum utilization
 * @UCLAMP_MAX:	Maximum utilization
 * @UCLAMP_CNT:	Utilization clamp constraints count
 */
enum uclamp_id {
 UCLAMP_MIN = 0,
 UCLAMP_MAX,
 UCLAMP_CNT
};


extern struct root_domain def_root_domain;
extern struct mutex sched_domains_mutex;


struct sched_info {

 /* Cumulative counters: */

 /* # of times we have run on this CPU: */
 unsigned long pcount;

 /* Time spent waiting on a runqueue: */
 unsigned long long run_delay;

 /* Timestamps: */

 /* When did we last run on a CPU? */
 unsigned long long last_arrival;

 /* When were we last queued to run? */
 unsigned long long last_queued;


};

/*
 * Integer metrics need fixed point arithmetic, e.g., sched/fair
 * has a few: load, load_avg, util_avg, freq, and capacity.
 *
 * We define a basic fixed point arithmetic range, and then formalize
 * all these metrics based on that basic range.
 */



/* Increase resolution of cpu_capacity calculations */



struct load_weight {
 unsigned long weight;
 u32 inv_weight;
};

/**
 * struct util_est - Estimation utilization of FAIR tasks
 * @enqueued: instantaneous estimated utilization of a task/cpu
 * @ewma:     the Exponential Weighted Moving Average (EWMA)
 *            utilization of a task
 *
 * Support data structure to track an Exponential Weighted Moving Average
 * (EWMA) of a FAIR task's utilization. New samples are added to the moving
 * average each time a task completes an activation. Sample's weight is chosen
 * so that the EWMA will be relatively insensitive to transient changes to the
 * task's workload.
 *
 * The enqueued attribute has a slightly different meaning for tasks and cpus:
 * - task:   the task's util_avg at last task dequeue time
 * - cfs_rq: the sum of util_est.enqueued for each RUNNABLE task on that CPU
 * Thus, the util_est.enqueued of a task represents the contribution on the
 * estimated utilization of the CPU where that task is currently enqueued.
 *
 * Only for tasks we track a moving average of the past instantaneous
 * estimated utilization. This allows to absorb sporadic drops in utilization
 * of an otherwise almost periodic task.
 *
 * The UTIL_AVG_UNCHANGED flag is used to synchronize util_est with util_avg
 * updates. When a task is dequeued, its util_est should not be updated if its
 * util_avg has not been updated in the meantime.
 * This information is mapped into the MSB bit of util_est.enqueued at dequeue
 * time. Since max value of util_est.enqueued for a task is 1024 (PELT util_avg
 * for a task) it is safe to use MSB.
 */
struct util_est {
 unsigned int enqueued;
 unsigned int ewma;


} __attribute__((__aligned__(sizeof(u64))));

/*
 * The load/runnable/util_avg accumulates an infinite geometric series
 * (see __update_load_avg_cfs_rq() in kernel/sched/pelt.c).
 *
 * [load_avg definition]
 *
 *   load_avg = runnable% * scale_load_down(load)
 *
 * [runnable_avg definition]
 *
 *   runnable_avg = runnable% * SCHED_CAPACITY_SCALE
 *
 * [util_avg definition]
 *
 *   util_avg = running% * SCHED_CAPACITY_SCALE
 *
 * where runnable% is the time ratio that a sched_entity is runnable and
 * running% the time ratio that a sched_entity is running.
 *
 * For cfs_rq, they are the aggregated values of all runnable and blocked
 * sched_entities.
 *
 * The load/runnable/util_avg doesn't directly factor frequency scaling and CPU
 * capacity scaling. The scaling is done through the rq_clock_pelt that is used
 * for computing those signals (see update_rq_clock_pelt())
 *
 * N.B., the above ratios (runnable% and running%) themselves are in the
 * range of [0, 1]. To do fixed point arithmetics, we therefore scale them
 * to as large a range as necessary. This is for example reflected by
 * util_avg's SCHED_CAPACITY_SCALE.
 *
 * [Overflow issue]
 *
 * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities
 * with the highest load (=88761), always runnable on a single cfs_rq,
 * and should not overflow as the number already hits PID_MAX_LIMIT.
 *
 * For all other cases (including 32-bit kernels), struct load_weight's
 * weight will overflow first before we do, because:
 *
 *    Max(load_avg) <= Max(load.weight)
 *
 * Then it is the load_weight's responsibility to consider overflow
 * issues.
 */
struct sched_avg {
 u64 last_update_time;
 u64 load_sum;
 u64 runnable_sum;
 u32 util_sum;
 u32 period_contrib;
 unsigned long load_avg;
 unsigned long runnable_avg;
 unsigned long util_avg;
 struct util_est util_est;
} __attribute__((__aligned__((1 << (6)))));

struct sched_statistics {
# 545 "./include/linux/sched.h"
} __attribute__((__aligned__((1 << (6)))));

struct sched_entity {
 /* For load-balancing: */
 struct load_weight load;
 struct rb_node run_node;
 struct list_head group_node;
 unsigned int on_rq;

 u64 exec_start;
 u64 sum_exec_runtime;
 u64 vruntime;
 u64 prev_sum_exec_runtime;

 u64 nr_migrations;


 int depth;
 struct sched_entity *parent;
 /* rq on which this entity is (to be) queued: */
 struct cfs_rq *cfs_rq;
 /* rq "owned" by this entity/group: */
 struct cfs_rq *my_q;
 /* cached value of my_q->h_nr_running */
 unsigned long runnable_weight;



 /*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
 struct sched_avg avg;

};

struct sched_rt_entity {
 struct list_head run_list;
 unsigned long timeout;
 unsigned long watchdog_stamp;
 unsigned int time_slice;
 unsigned short on_rq;
 unsigned short on_list;

 struct sched_rt_entity *back;







} ;

struct sched_dl_entity {
 struct rb_node rb_node;

 /*
	 * Original scheduling parameters. Copied here from sched_attr
	 * during sched_setattr(), they will remain the same until
	 * the next sched_setattr().
	 */
 u64 dl_runtime; /* Maximum runtime for each instance	*/
 u64 dl_deadline; /* Relative deadline of each instance	*/
 u64 dl_period; /* Separation of two instances (period) */
 u64 dl_bw; /* dl_runtime / dl_period		*/
 u64 dl_density; /* dl_runtime / dl_deadline		*/

 /*
	 * Actual scheduling parameters. Initialized with the values above,
	 * they are continuously updated during task execution. Note that
	 * the remaining runtime could be < 0 in case we are in overrun.
	 */
 s64 runtime; /* Remaining runtime for this instance	*/
 u64 deadline; /* Absolute deadline for this instance	*/
 unsigned int flags; /* Specifying the scheduler behaviour	*/

 /*
	 * Some bool flags:
	 *
	 * @dl_throttled tells if we exhausted the runtime. If so, the
	 * task has to wait for a replenishment to be performed at the
	 * next firing of dl_timer.
	 *
	 * @dl_yielded tells if task gave up the CPU before consuming
	 * all its available runtime during the last job.
	 *
	 * @dl_non_contending tells if the task is inactive while still
	 * contributing to the active utilization. In other words, it
	 * indicates if the inactive timer has been armed and its handler
	 * has not been executed yet. This flag is useful to avoid race
	 * conditions between the inactive timer handler and the wakeup
	 * code.
	 *
	 * @dl_overrun tells if the task asked to be informed about runtime
	 * overruns.
	 */
 unsigned int dl_throttled : 1;
 unsigned int dl_yielded : 1;
 unsigned int dl_non_contending : 1;
 unsigned int dl_overrun : 1;

 /*
	 * Bandwidth enforcement timer. Each -deadline task has its
	 * own bandwidth to be enforced, thus we need one timer per task.
	 */
 struct hrtimer dl_timer;

 /*
	 * Inactive timer, responsible for decreasing the active utilization
	 * at the "0-lag time". When a -deadline task blocks, it contributes
	 * to GRUB's active utilization until the "0-lag time", hence a
	 * timer is needed to decrease the active utilization at the correct
	 * time.
	 */
 struct hrtimer inactive_timer;


 /*
	 * Priority Inheritance. When a DEADLINE scheduling entity is boosted
	 * pi_se points to the donor, otherwise points to the dl_se it belongs
	 * to (the original one/itself).
	 */
 struct sched_dl_entity *pi_se;

};
# 709 "./include/linux/sched.h"
union rcu_special {
 struct {
  u8 blocked;
  u8 need_qs;
  u8 exp_hint; /* Hint for performance. */
  u8 need_mb; /* Readers need smp_mb(). */
 } b; /* Bits. */
 u32 s; /* Set of bits. */
};

enum perf_event_task_context {
 perf_invalid_context = -1,
 perf_hw_context = 0,
 perf_sw_context,
 perf_nr_task_contexts,
};

struct wake_q_node {
 struct wake_q_node *next;
};

struct kmap_ctrl {




};

struct task_struct {

 /*
	 * For reasons of header soup (see current_thread_info()), this
	 * must be the first element of task_struct.
	 */
 struct thread_info thread_info;

 unsigned int __state;






 /*
	 * This begins the randomizable portion of task_struct. Only
	 * scheduling-critical items should be added above here.
	 */


 void *stack;
 refcount_t usage;
 /* Per task flags (PF_*), defined further below: */
 unsigned int flags;
 unsigned int ptrace;


 int on_cpu;
 struct __call_single_node wake_entry;
 unsigned int wakee_flips;
 unsigned long wakee_flip_decay_ts;
 struct task_struct *last_wakee;

 /*
	 * recent_used_cpu is initially set as the last CPU used by a task
	 * that wakes affine another task. Waker/wakee relationships can
	 * push tasks around a CPU where each wakeup moves to the next one.
	 * Tracking a recently used CPU allows a quick search for a recently
	 * used CPU that may be idle.
	 */
 int recent_used_cpu;
 int wake_cpu;

 int on_rq;

 int prio;
 int static_prio;
 int normal_prio;
 unsigned int rt_priority;

 struct sched_entity se;
 struct sched_rt_entity rt;
 struct sched_dl_entity dl;
 const struct sched_class *sched_class;
# 800 "./include/linux/sched.h"
 struct task_group *sched_task_group;
# 816 "./include/linux/sched.h"
 struct sched_statistics stats;


 /* List of struct preempt_notifier: */
 struct hlist_head preempt_notifiers;






 unsigned int policy;
 int nr_cpus_allowed;
 const cpumask_t *cpus_ptr;
 cpumask_t *user_cpus_ptr;
 cpumask_t cpus_mask;
 void *migration_pending;

 unsigned short migration_disabled;

 unsigned short migration_flags;


 int rcu_read_lock_nesting;
 union rcu_special rcu_read_unlock_special;
 struct list_head rcu_node_entry;
 struct rcu_node *rcu_blocked_node;



 unsigned long rcu_tasks_nvcsw;
 u8 rcu_tasks_holdout;
 u8 rcu_tasks_idx;
 int rcu_tasks_idle_cpu;
 struct list_head rcu_tasks_holdout_list;



 int trc_reader_nesting;
 int trc_ipi_to_cpu;
 union rcu_special trc_reader_special;
 struct list_head trc_holdout_list;
 struct list_head trc_blkd_node;
 int trc_blkd_cpu;


 struct sched_info sched_info;

 struct list_head tasks;

 struct plist_node pushable_tasks;
 struct rb_node pushable_dl_tasks;


 struct mm_struct *mm;
 struct mm_struct *active_mm;

 int exit_state;
 int exit_code;
 int exit_signal;
 /* The signal sent when the parent dies: */
 int pdeath_signal;
 /* JOBCTL_*, siglock protected: */
 unsigned long jobctl;

 /* Used for emulating ABI behavior of previous Linux versions: */
 unsigned int personality;

 /* Scheduler bits, serialized by scheduler locks: */
 unsigned sched_reset_on_fork:1;
 unsigned sched_contributes_to_load:1;
 unsigned sched_migrated:1;

 /* Force alignment to the next boundary: */
 unsigned :0;

 /* Unserialized, strictly 'current' */

 /*
	 * This field must not be in the scheduler word above due to wakelist
	 * queueing no longer being serialized by p->on_cpu. However:
	 *
	 * p->XXX = X;			ttwu()
	 * schedule()			  if (p->on_rq && ..) // false
	 *   smp_mb__after_spinlock();	  if (smp_load_acquire(&p->on_cpu) && //true
	 *   deactivate_task()		      ttwu_queue_wakelist())
	 *     p->on_rq = 0;			p->sched_remote_wakeup = Y;
	 *
	 * guarantees all stores of 'current' are visible before
	 * ->sched_remote_wakeup gets used, so it can be in this word.
	 */
 unsigned sched_remote_wakeup:1;

 /* Bit to tell LSMs we're in execve(): */
 unsigned in_execve:1;
 unsigned in_iowait:1;




 unsigned in_user_fault:1;
# 926 "./include/linux/sched.h"
 /* disallow userland-initiated cgroup migration */
 unsigned no_cgroup_migration:1;
 /* task is frozen/stopped (used by the cgroup freezer) */
 unsigned frozen:1;


 unsigned use_memdelay:1;
# 943 "./include/linux/sched.h"
 /* Recursion prevention for eventfd_signal() */
 unsigned in_eventfd:1;
# 957 "./include/linux/sched.h"
 unsigned long atomic_flags; /* Flags requiring atomic access. */

 struct restart_block restart_block;

 pid_t pid;
 pid_t tgid;


 /* Canary value for the -fstack-protector GCC feature: */
 unsigned long stack_canary;

 /*
	 * Pointers to the (original) parent process, youngest child, younger sibling,
	 * older sibling, respectively.  (p->father can be replaced with
	 * p->real_parent->pid)
	 */

 /* Real parent process: */
 struct task_struct /* nothing */ *real_parent;

 /* Recipient of SIGCHLD, wait4() reports: */
 struct task_struct /* nothing */ *parent;

 /*
	 * Children/sibling form the list of natural children:
	 */
 struct list_head children;
 struct list_head sibling;
 struct task_struct *group_leader;

 /*
	 * 'ptraced' is the list of tasks this task is using ptrace() on.
	 *
	 * This includes both natural children and PTRACE_ATTACH targets.
	 * 'ptrace_entry' is this task's link on the p->parent->ptraced list.
	 */
 struct list_head ptraced;
 struct list_head ptrace_entry;

 /* PID/PID hash table linkage. */
 struct pid *thread_pid;
 struct hlist_node pid_links[PIDTYPE_MAX];
 struct list_head thread_group;
 struct list_head thread_node;

 struct completion *vfork_done;

 /* CLONE_CHILD_SETTID: */
 int /* nothing */ *set_child_tid;

 /* CLONE_CHILD_CLEARTID: */
 int /* nothing */ *clear_child_tid;

 /* PF_KTHREAD | PF_IO_WORKER */
 void *worker_private;

 u64 utime;
 u64 stime;




 u64 gtime;
 struct prev_cputime prev_cputime;







 /* Context switch counts: */
 unsigned long nvcsw;
 unsigned long nivcsw;

 /* Monotonic time in nsecs: */
 u64 start_time;

 /* Boot based time in nsecs: */
 u64 start_boottime;

 /* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */
 unsigned long min_flt;
 unsigned long maj_flt;

 /* Empty if CONFIG_POSIX_CPUTIMERS=n */
 struct posix_cputimers posix_cputimers;


 struct posix_cputimers_work posix_cputimers_work;


 /* Process credentials: */

 /* Tracer's credentials at attach: */
 const struct cred /* nothing */ *ptracer_cred;

 /* Objective and real subjective task credentials (COW): */
 const struct cred /* nothing */ *real_cred;

 /* Effective (overridable) subjective task credentials (COW): */
 const struct cred /* nothing */ *cred;


 /* Cached requested key. */
 struct key *cached_requested_key;


 /*
	 * executable name, excluding path.
	 *
	 * - normally initialized setup_new_exec()
	 * - access it with [gs]et_task_comm()
	 * - lock it with task_lock()
	 */
 char comm[TASK_COMM_LEN];

 struct nameidata *nameidata;


 struct sysv_sem sysvsem;
 struct sysv_shm sysvshm;





 /* Filesystem information: */
 struct fs_struct *fs;

 /* Open file information: */
 struct files_struct *files;


 struct io_uring_task *io_uring;


 /* Namespaces: */
 struct nsproxy *nsproxy;

 /* Signal handlers: */
 struct signal_struct *signal;
 struct sighand_struct /* nothing */ *sighand;
 sigset_t blocked;
 sigset_t real_blocked;
 /* Restored if set_restore_sigmask() was used: */
 sigset_t saved_sigmask;
 struct sigpending pending;
 unsigned long sas_ss_sp;
 size_t sas_ss_size;
 unsigned int sas_ss_flags;

 struct callback_head *task_works;



 struct audit_context *audit_context;

 kuid_t loginuid;
 unsigned int sessionid;

 struct seccomp seccomp;
 struct syscall_user_dispatch syscall_dispatch;

 /* Thread group tracking: */
 u64 parent_exec_id;
 u64 self_exec_id;

 /* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */
 spinlock_t alloc_lock;

 /* Protection of the PI data structures: */
 raw_spinlock_t pi_lock;

 struct wake_q_node wake_q;


 /* PI waiters blocked on a rt_mutex held by this task: */
 struct rb_root_cached pi_waiters;
 /* Updated under owner's pi_lock and rq lock */
 struct task_struct *pi_top_task;
 /* Deadlock detection and priority inheritance handling: */
 struct rt_mutex_waiter *pi_blocked_on;
# 1175 "./include/linux/sched.h"
 /* Journalling filesystem info: */
 void *journal_info;

 /* Stacked block device info: */
 struct bio_list *bio_list;

 /* Stack plugging: */
 struct blk_plug *plug;

 /* VM state: */
 struct reclaim_state *reclaim_state;

 struct backing_dev_info *backing_dev_info;

 struct io_context *io_context;


 struct capture_control *capture_control;

 /* Ptrace state: */
 unsigned long ptrace_message;
 kernel_siginfo_t *last_siginfo;

 struct task_io_accounting ioac;





 /* Accumulated RSS usage: */
 u64 acct_rss_mem1;
 /* Accumulated virtual memory usage: */
 u64 acct_vm_mem1;
 /* stime + utime since last update: */
 u64 acct_timexpd;


 /* Protected by ->alloc_lock: */
 nodemask_t mems_allowed;
 /* Sequence number to catch updates: */
 seqcount_spinlock_t mems_allowed_seq;
 int cpuset_mem_spread_rotor;
 int cpuset_slab_spread_rotor;


 /* Control Group info protected by css_set_lock: */
 struct css_set /* nothing */ *cgroups;
 /* cg_list protected by css_set_lock and tsk->alloc_lock: */
 struct list_head cg_list;






 struct robust_list_head /* nothing */ *robust_list;

 struct compat_robust_list_head /* nothing */ *compat_robust_list;

 struct list_head pi_state_list;
 struct futex_pi_state *pi_state_cache;
 struct mutex futex_exit_mutex;
 unsigned int futex_state;


 struct perf_event_context *perf_event_ctxp;
 struct mutex perf_event_mutex;
 struct list_head perf_event_list;





 /* Protected by alloc_lock: */
 struct mempolicy *mempolicy;
 short il_prev;
 short pref_node_fork;


 int numa_scan_seq;
 unsigned int numa_scan_period;
 unsigned int numa_scan_period_max;
 int numa_preferred_nid;
 unsigned long numa_migrate_retry;
 /* Migration stamp: */
 u64 node_stamp;
 u64 last_task_numa_placement;
 u64 last_sum_exec_runtime;
 struct callback_head numa_work;

 /*
	 * This pointer is only modified for current in syscall and
	 * pagefault context (and for tasks being destroyed), so it can be read
	 * from any of the following contexts:
	 *  - RCU read-side critical section
	 *  - current->numa_group from everywhere
	 *  - task's runqueue locked, task not running
	 */
 struct numa_group /* nothing */ *numa_group;

 /*
	 * numa_faults is an array split into four regions:
	 * faults_memory, faults_cpu, faults_memory_buffer, faults_cpu_buffer
	 * in this precise order.
	 *
	 * faults_memory: Exponential decaying average of faults on a per-node
	 * basis. Scheduling placement decisions are made based on these
	 * counts. The values remain static for the duration of a PTE scan.
	 * faults_cpu: Track the nodes the process was running on when a NUMA
	 * hinting fault was incurred.
	 * faults_memory_buffer and faults_cpu_buffer: Record faults per node
	 * during the current scan window. When the scan completes, the counts
	 * in faults_memory and faults_cpu decay and these values are copied.
	 */
 unsigned long *numa_faults;
 unsigned long total_numa_faults;

 /*
	 * numa_faults_locality tracks if faults recorded during the last
	 * scan window were remote/local or failed to migrate. The task scan
	 * period is adapted based on the locality of the faults with different
	 * weights depending on whether they were shared or private faults
	 */
 unsigned long numa_faults_locality[3];

 unsigned long numa_pages_migrated;



 struct rseq /* nothing */ *rseq;
 u32 rseq_sig;
 /*
	 * RmW on rseq_event_mask must be performed atomically
	 * with respect to preemption.
	 */
 unsigned long rseq_event_mask;


 struct tlbflush_unmap_batch tlb_ubc;

 union {
  refcount_t rcu_users;
  struct callback_head rcu;
 };

 /* Cache last used pipe for splice(): */
 struct pipe_inode_info *splice_pipe;

 struct page_frag task_frag;
# 1333 "./include/linux/sched.h"
 /*
	 * When (nr_dirtied >= nr_dirtied_pause), it's time to call
	 * balance_dirty_pages() for a dirty throttling pause:
	 */
 int nr_dirtied;
 int nr_dirtied_pause;
 /* Start of a write-and-pause period: */
 unsigned long dirty_paused_when;





 /*
	 * Time slack values; these are used to round up poll() and
	 * select() etc timeout values. These are in nanoseconds.
	 */
 u64 timer_slack_ns;
 u64 default_timer_slack_ns;
# 1427 "./include/linux/sched.h"
 struct mem_cgroup *memcg_in_oom;
 gfp_t memcg_oom_gfp_mask;
 int memcg_oom_order;

 /* Number of pages to reclaim on returning to userland: */
 unsigned int memcg_nr_pages_over_high;

 /* Used by memcontrol for targeted memcg charge: */
 struct mem_cgroup *active_memcg;



 struct request_queue *throttle_queue;
# 1449 "./include/linux/sched.h"
 struct kmap_ctrl kmap_ctrl;






 int pagefault_disabled;

 struct task_struct *oom_reaper_list;
 struct timer_list oom_reaper_timer;


 struct vm_struct *stack_vm_area;


 /* A live task holds one reference: */
 refcount_t stack_refcount;





 /* Used by LSM modules for access restriction: */
 void *security;


 /* Used by BPF task local storage */
 struct bpf_local_storage /* nothing */ *bpf_storage;
 /* Used for BPF run context */
 struct bpf_run_ctx *bpf_ctx;
# 1525 "./include/linux/sched.h"
 /*
	 * New fields for task_struct should be added above here, so that
	 * they are included in the randomized portion of task_struct.
	 */


 /* CPU-specific state of this task: */
 struct thread_struct thread;

 /*
	 * WARNING: on x86, 'thread_struct' contains a variable-sized
	 * structure.  It *MUST* be at the end of 'task_struct'.
	 *
	 * Do not put anything below here!
	 */
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid *task_pid(struct task_struct *task)
{
 return task->thread_pid;
}

/*
 * the helpers to get the task's different pids as they are seen
 * from various namespaces
 *
 * task_xid_nr()     : global id, i.e. the id seen from the init namespace;
 * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of
 *                     current.
 * task_xid_nr_ns()  : id seen from the ns specified;
 *
 * see also pid_nr() etc in include/linux/pid.h
 */
pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_pid_nr(struct task_struct *tsk)
{
 return tsk->pid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_pid_vnr(struct task_struct *tsk)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_PID, ((void *)0));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_tgid_nr(struct task_struct *tsk)
{
 return tsk->tgid;
}

/**
 * pid_alive - check that a task structure is not stale
 * @p: Task structure to be checked.
 *
 * Test if a process is not yet dead (at most zombie state)
 * If pid_alive fails, then pointers within the task structure
 * can be stale and must not be dereferenced.
 *
 * Return: 1 if the process is alive. 0 otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pid_alive(const struct task_struct *p)
{
 return p->thread_pid != ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_pgrp_vnr(struct task_struct *tsk)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ((void *)0));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_session_vnr(struct task_struct *tsk)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_SID, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_TGID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_tgid_vnr(struct task_struct *tsk)
{
 return __task_pid_nr_ns(tsk, PIDTYPE_TGID, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
{
 pid_t pid = 0;

 rcu_read_lock();
 if (pid_alive(tsk))
  pid = task_tgid_nr_ns(({ /* Dependency order vs. p above. */ typeof(*(tsk->real_parent)) *__UNIQUE_ID_rcu256 = (typeof(*(tsk->real_parent)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_257(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((tsk->real_parent)) == sizeof(char) || sizeof((tsk->real_parent)) == sizeof(short) || sizeof((tsk->real_parent)) == sizeof(int) || sizeof((tsk->real_parent)) == sizeof(long)) || sizeof((tsk->real_parent)) == sizeof(long long))) __compiletime_assert_257(); } while (0); (*(const volatile typeof( _Generic(((tsk->real_parent)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((tsk->real_parent)))) *)&((tsk->real_parent))); }); do { } while (0 && (!((0) || rcu_read_lock_held()))); ; ((typeof(*(tsk->real_parent)) *)(__UNIQUE_ID_rcu256)); }), ns);
# 1634 "./include/linux/sched.h"
 rcu_read_unlock();

 return pid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_ppid_nr(const struct task_struct *tsk)
{
 return task_ppid_nr_ns(tsk, &init_pid_ns);
}

/* Obsolete, do not use: */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pid_t task_pgrp_nr(struct task_struct *tsk)
{
 return task_pgrp_nr_ns(tsk, &init_pid_ns);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __task_state_index(unsigned int tsk_state,
           unsigned int tsk_exit_state)
{
 unsigned int state = (tsk_state | tsk_exit_state) & (0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040);

 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_258(void) __attribute__((__error__("BUILD_BUG_ON failed: " "((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) == 0 || ((((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) & (((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) - 1)) != 0)"))); if (!(!(((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) == 0 || ((((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) & (((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) - 1)) != 0)))) __compiletime_assert_258(); } while (0);
# 1660 "./include/linux/sched.h"
 if (tsk_state == (0x00000002 | 0x00000400))
  state = ((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1);

 /*
	 * We're lying here, but rather than expose a completely new task state
	 * to userspace, we can make this appear as if the task has gone through
	 * a regular rt_mutex_lock() call.
	 */
 if (tsk_state == 0x00001000)
  state = 0x00000002;

 return fls(state);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int task_state_index(struct task_struct *tsk)
{
 return __task_state_index(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_259(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(tsk->__state) == sizeof(char) || sizeof(tsk->__state) == sizeof(short) || sizeof(tsk->__state) == sizeof(int) || sizeof(tsk->__state) == sizeof(long)) || sizeof(tsk->__state) == sizeof(long long))) __compiletime_assert_259(); } while (0); (*(const volatile typeof( _Generic((tsk->__state), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (tsk->__state))) *)&(tsk->__state)); }), tsk->exit_state);
# 1677 "./include/linux/sched.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) char task_index_to_char(unsigned int state)
{
 static const char state_char[] = "RSDTtXZPI";

 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_260(void) __attribute__((__error__("BUILD_BUG_ON failed: " "1 + ilog2(TASK_REPORT_MAX) != sizeof(state_char) - 1"))); if (!(!(1 + ( __builtin_constant_p((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) ? (((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) < 2 ? 0 : 63 - __builtin_clzll((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1))) : (sizeof((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) <= 4) ? __ilog2_u32((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) : __ilog2_u64((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) ) != sizeof(state_char) - 1))) __compiletime_assert_260(); } while (0);
# 1685 "./include/linux/sched.h"
 return state_char[state];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) char task_state_to_char(struct task_struct *tsk)
{
 return task_index_to_char(task_state_index(tsk));
}

/**
 * is_global_init - check if a task structure is init. Since init
 * is free to have sub-threads we need to check tgid.
 * @tsk: Task structure to be checked.
 *
 * Check if a task structure is the first user space task the kernel created.
 *
 * Return: 1 if the task structure is init. 0 otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_global_init(struct task_struct *tsk)
{
 return task_tgid_nr(tsk) == 1;
}

extern struct pid *cad_pid;

/*
 * Per process flags
 */
# 1746 "./include/linux/sched.h"
/*
 * Only the _current_ task can read/write to tsk->flags, but other
 * tasks can access tsk->flags in readonly mode for example
 * with tsk_used_math (like during threaded core dumping).
 * There is however an exception to this rule during ptrace
 * or during fork: the ptracer task is allowed to write to the
 * child->flags of its traced child (same goes for fork, the parent
 * can write to the child->flags), because we're guaranteed the
 * child is not running and in turn not changing child->flags
 * at the same time the parent does it.
 */
# 1770 "./include/linux/sched.h"
/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_percpu_thread(void)
{

 return (get_current()->flags & 0x04000000 /* Userland is not allowed to meddle with cpus_mask */) &&
  (get_current()->nr_cpus_allowed == 1);



}

/* Per-process atomic flags. */
# 1806 "./include/linux/sched.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_no_new_privs(struct task_struct *p) { return ((__builtin_constant_p(0 /* May not gain new privileges. */) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(0 /* May not gain new privileges. */, &p->atomic_flags) : generic_test_bit(0 /* May not gain new privileges. */, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_no_new_privs(struct task_struct *p) { set_bit(0 /* May not gain new privileges. */, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_spread_page(struct task_struct *p) { return ((__builtin_constant_p(1 /* Spread page cache over cpuset */) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(1 /* Spread page cache over cpuset */, &p->atomic_flags) : generic_test_bit(1 /* Spread page cache over cpuset */, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_spread_page(struct task_struct *p) { set_bit(1 /* Spread page cache over cpuset */, &p->atomic_flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_clear_spread_page(struct task_struct *p) { clear_bit(1 /* Spread page cache over cpuset */, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_spread_slab(struct task_struct *p) { return ((__builtin_constant_p(2 /* Spread some slab caches over cpuset */) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(2 /* Spread some slab caches over cpuset */, &p->atomic_flags) : generic_test_bit(2 /* Spread some slab caches over cpuset */, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_spread_slab(struct task_struct *p) { set_bit(2 /* Spread some slab caches over cpuset */, &p->atomic_flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_clear_spread_slab(struct task_struct *p) { clear_bit(2 /* Spread some slab caches over cpuset */, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_spec_ssb_disable(struct task_struct *p) { return ((__builtin_constant_p(3 /* Speculative Store Bypass disabled */) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(3 /* Speculative Store Bypass disabled */, &p->atomic_flags) : generic_test_bit(3 /* Speculative Store Bypass disabled */, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_spec_ssb_disable(struct task_struct *p) { set_bit(3 /* Speculative Store Bypass disabled */, &p->atomic_flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_clear_spec_ssb_disable(struct task_struct *p) { clear_bit(3 /* Speculative Store Bypass disabled */, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_spec_ssb_noexec(struct task_struct *p) { return ((__builtin_constant_p(7 /* Speculative Store Bypass clear on execve() */) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(7 /* Speculative Store Bypass clear on execve() */, &p->atomic_flags) : generic_test_bit(7 /* Speculative Store Bypass clear on execve() */, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_spec_ssb_noexec(struct task_struct *p) { set_bit(7 /* Speculative Store Bypass clear on execve() */, &p->atomic_flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_clear_spec_ssb_noexec(struct task_struct *p) { clear_bit(7 /* Speculative Store Bypass clear on execve() */, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_spec_ssb_force_disable(struct task_struct *p) { return ((__builtin_constant_p(4 /* Speculative Store Bypass force disabled*/) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(4 /* Speculative Store Bypass force disabled*/, &p->atomic_flags) : generic_test_bit(4 /* Speculative Store Bypass force disabled*/, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_spec_ssb_force_disable(struct task_struct *p) { set_bit(4 /* Speculative Store Bypass force disabled*/, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_spec_ib_disable(struct task_struct *p) { return ((__builtin_constant_p(5 /* Indirect branch speculation restricted */) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(5 /* Indirect branch speculation restricted */, &p->atomic_flags) : generic_test_bit(5 /* Indirect branch speculation restricted */, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_spec_ib_disable(struct task_struct *p) { set_bit(5 /* Indirect branch speculation restricted */, &p->atomic_flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_clear_spec_ib_disable(struct task_struct *p) { clear_bit(5 /* Indirect branch speculation restricted */, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_spec_ib_force_disable(struct task_struct *p) { return ((__builtin_constant_p(6 /* Indirect branch speculation permanently restricted */) && __builtin_constant_p((uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&p->atomic_flags))) ? const_test_bit(6 /* Indirect branch speculation permanently restricted */, &p->atomic_flags) : generic_test_bit(6 /* Indirect branch speculation permanently restricted */, &p->atomic_flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_set_spec_ib_force_disable(struct task_struct *p) { set_bit(6 /* Indirect branch speculation permanently restricted */, &p->atomic_flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
current_restore_flags(unsigned long orig_flags, unsigned long flags)
{
 get_current()->flags &= ~flags;
 get_current()->flags |= orig_flags & flags;
}

extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
extern int task_can_attach(struct task_struct *p, const struct cpumask *cs_effective_cpus);

extern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);
extern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);
extern int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src, int node);
extern void release_user_cpus_ptr(struct task_struct *p);
extern int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask);
extern void force_compatible_cpus_allowed_ptr(struct task_struct *p);
extern void relax_compatible_cpus_allowed_ptr(struct task_struct *p);
# 1879 "./include/linux/sched.h"
extern int yield_to(struct task_struct *p, bool preempt);
extern void set_user_nice(struct task_struct *p, long nice);
extern int task_prio(const struct task_struct *p);

/**
 * task_nice - return the nice value of a given task.
 * @p: the task in question.
 *
 * Return: The nice value [ -20 ... 0 ... 19 ].
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int task_nice(const struct task_struct *p)
{
 return (((p)->static_prio) - (100 + (19 - -20 + 1) / 2));
}

extern int can_nice(const struct task_struct *p, const int nice);
extern int task_curr(const struct task_struct *p);
extern int idle_cpu(int cpu);
extern int available_idle_cpu(int cpu);
extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);
extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);
extern void sched_set_fifo(struct task_struct *p);
extern void sched_set_fifo_low(struct task_struct *p);
extern void sched_set_normal(struct task_struct *p, int nice);
extern int sched_setattr(struct task_struct *, const struct sched_attr *);
extern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);
extern struct task_struct *idle_task(int cpu);

/**
 * is_idle_task - is the specified task an idle task?
 * @p: the task in question.
 *
 * Return: 1 if @p is an idle task. 0 otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool is_idle_task(const struct task_struct *p)
{
 return !!(p->flags & 0x00000002 /* I am an IDLE thread */);
}

extern struct task_struct *curr_task(int cpu);
extern void ia64_set_curr_task(int cpu, struct task_struct *p);

void yield(void);

union thread_union {

 struct task_struct task;




 unsigned long stack[((((1UL))) << (14 + 0))/sizeof(long)];
};





extern unsigned long init_stack[((((1UL))) << (14 + 0)) / sizeof(unsigned long)];







/*
 * find a task by one of its numerical ids
 *
 * find_task_by_pid_ns():
 *      finds a task by its pid in the specified namespace
 * find_task_by_vpid():
 *      finds a task by its virtual pid
 *
 * see also find_vpid() etc in include/linux/pid.h
 */

extern struct task_struct *find_task_by_vpid(pid_t nr);
extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);

/*
 * find a task by its virtual pid and get the task struct
 */
extern struct task_struct *find_get_task_by_vpid(pid_t nr);

extern int wake_up_state(struct task_struct *tsk, unsigned int state);
extern int wake_up_process(struct task_struct *tsk);
extern void wake_up_new_task(struct task_struct *tsk);


extern void kick_process(struct task_struct *tsk);




extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_task_comm(struct task_struct *tsk, const char *from)
{
 __set_task_comm(tsk, from, false);
}

extern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void scheduler_ipi(void)
{
 /*
	 * Fold TIF_NEED_RESCHED into the preempt_count; anybody setting
	 * TIF_NEED_RESCHED remotely (for the first time) will also send
	 * this IPI.
	 */
 do { if (test_ti_thread_flag(((struct thread_info *)get_current()), 1 /* rescheduling necessary */)) set_preempt_need_resched(); } while (0);
}
extern unsigned long wait_task_inactive(struct task_struct *, unsigned int match_state);
# 2006 "./include/linux/sched.h"
/*
 * Set thread flags in other task's structures.
 * See asm/thread_info.h for TIF_xxxx flags available:
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_tsk_thread_flag(struct task_struct *tsk, int flag)
{
 set_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_tsk_thread_flag(struct task_struct *tsk, int flag)
{
 clear_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void update_tsk_thread_flag(struct task_struct *tsk, int flag,
       bool value)
{
 update_ti_thread_flag((&(tsk)->thread_info), flag, value);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)
{
 return test_and_set_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)
{
 return test_and_clear_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_tsk_thread_flag(struct task_struct *tsk, int flag)
{
 return test_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_tsk_need_resched(struct task_struct *tsk)
{
 set_tsk_thread_flag(tsk,1 /* rescheduling necessary */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_tsk_need_resched(struct task_struct *tsk)
{
 clear_tsk_thread_flag(tsk,1 /* rescheduling necessary */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int test_tsk_need_resched(struct task_struct *tsk)
{
 return __builtin_expect(!!(test_tsk_thread_flag(tsk,1 /* rescheduling necessary */)), 0);
}

/*
 * cond_resched() and cond_resched_lock(): latency reduction via
 * explicit rescheduling in places that are safe. The return
 * value indicates whether a reschedule was done in fact.
 * cond_resched_lock() will drop the spinlock before scheduling,
 */
# 2093 "./include/linux/sched.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int _cond_resched(void) { return 0; }
# 2102 "./include/linux/sched.h"
extern int __cond_resched_lock(spinlock_t *lock);
extern int __cond_resched_rwlock_read(rwlock_t *lock);
extern int __cond_resched_rwlock_write(rwlock_t *lock);





/*
 * Non RT kernels have an elevated preempt count due to the held lock,
 * but are not allowed to be inside a RCU read side critical section
 */
# 2140 "./include/linux/sched.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void cond_resched_rcu(void)
{





}
# 2157 "./include/linux/sched.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool preempt_model_none(void)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool preempt_model_voluntary(void)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool preempt_model_full(void)
{
 return 1;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool preempt_model_rt(void)
{
 return 0;
}

/*
 * Does the preemption model allow non-cooperative preemption?
 *
 * For !CONFIG_PREEMPT_DYNAMIC kernels this is an exact match with
 * CONFIG_PREEMPTION; for CONFIG_PREEMPT_DYNAMIC this doesn't work as the
 * kernel is *built* with CONFIG_PREEMPTION=y but may run with e.g. the
 * PREEMPT_NONE model.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool preempt_model_preemptible(void)
{
 return preempt_model_full() || preempt_model_rt();
}

/*
 * Does a critical section need to be broken due to another
 * task waiting?: (technically does not depend on CONFIG_PREEMPTION,
 * but a general need for low latency)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int spin_needbreak(spinlock_t *lock)
{

 return spin_is_contended(lock);



}

/*
 * Check if a rwlock is contended.
 * Returns non-zero if there is another task waiting on the rwlock.
 * Returns zero if the lock is not contended or the system / underlying
 * rwlock implementation does not support contention detection.
 * Technically does not depend on CONFIG_PREEMPTION, but a general need
 * for low latency.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rwlock_needbreak(rwlock_t *lock)
{

 return queued_rwlock_is_contended(&(lock)->raw_lock);



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool need_resched(void)
{
 return __builtin_expect(!!(test_ti_thread_flag(((struct thread_info *)get_current()), 1 /* rescheduling necessary */)), 0);
}

/*
 * Wrappers for p->thread_info->cpu access. No-op on UP.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int task_cpu(const struct task_struct *p)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_261(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((&(p)->thread_info)->cpu) == sizeof(char) || sizeof((&(p)->thread_info)->cpu) == sizeof(short) || sizeof((&(p)->thread_info)->cpu) == sizeof(int) || sizeof((&(p)->thread_info)->cpu) == sizeof(long)) || sizeof((&(p)->thread_info)->cpu) == sizeof(long long))) __compiletime_assert_261(); } while (0); (*(const volatile typeof( _Generic(((&(p)->thread_info)->cpu), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((&(p)->thread_info)->cpu))) *)&((&(p)->thread_info)->cpu)); });
# 2234 "./include/linux/sched.h"
}

extern void set_task_cpu(struct task_struct *p, unsigned int cpu);
# 2251 "./include/linux/sched.h"
extern bool sched_task_on_rq(struct task_struct *p);
extern unsigned long get_wchan(struct task_struct *p);
extern struct task_struct *cpu_curr_snapshot(int cpu);

/*
 * In order to reduce various lock holder preemption latencies provide an
 * interface to see if a vCPU is currently running or not.
 *
 * This allows us to terminate optimistic spin loops and block, analogous to
 * the native optimistic spin heuristic of testing if the lock owner task is
 * running or not.
 */







extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
extern long sched_getaffinity(pid_t pid, struct cpumask *mask);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool owner_on_cpu(struct task_struct *owner)
{
 /*
	 * As lock holder preemption issue, we both skip spinning if
	 * task is not on cpu or its cpu is preempted
	 */
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_262(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(owner->on_cpu) == sizeof(char) || sizeof(owner->on_cpu) == sizeof(short) || sizeof(owner->on_cpu) == sizeof(int) || sizeof(owner->on_cpu) == sizeof(long)) || sizeof(owner->on_cpu) == sizeof(long long))) __compiletime_assert_262(); } while (0); (*(const volatile typeof( _Generic((owner->on_cpu), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (owner->on_cpu))) *)&(owner->on_cpu)); }) && !vcpu_is_preempted(task_cpu(owner));
# 2285 "./include/linux/sched.h"
}

/* Returns effective CPU energy utilization, as seen by the scheduler */
unsigned long sched_cpu_util(int cpu);




/*
 * Map the event mask on the user-space ABI enum rseq_cs_flags
 * for direct mask checks.
 */
enum rseq_event_mask_bits {
 RSEQ_EVENT_PREEMPT_BIT = RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT,
 RSEQ_EVENT_SIGNAL_BIT = RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT,
 RSEQ_EVENT_MIGRATE_BIT = RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT,
};

enum rseq_event_mask {
 RSEQ_EVENT_PREEMPT = (1U << RSEQ_EVENT_PREEMPT_BIT),
 RSEQ_EVENT_SIGNAL = (1U << RSEQ_EVENT_SIGNAL_BIT),
 RSEQ_EVENT_MIGRATE = (1U << RSEQ_EVENT_MIGRATE_BIT),
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_set_notify_resume(struct task_struct *t)
{
 if (t->rseq)
  set_tsk_thread_flag(t, 2 /* callback before returning to user */);
}

void __rseq_handle_notify_resume(struct ksignal *sig, struct pt_regs *regs);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_handle_notify_resume(struct ksignal *ksig,
          struct pt_regs *regs)
{
 if (get_current()->rseq)
  __rseq_handle_notify_resume(ksig, regs);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_signal_deliver(struct ksignal *ksig,
           struct pt_regs *regs)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 ((__builtin_constant_p(RSEQ_EVENT_SIGNAL_BIT) && __builtin_constant_p((uintptr_t)(&get_current()->rseq_event_mask) != (uintptr_t)((void *)0)) && (uintptr_t)(&get_current()->rseq_event_mask) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&get_current()->rseq_event_mask))) ? generic___set_bit(RSEQ_EVENT_SIGNAL_BIT, &get_current()->rseq_event_mask) : generic___set_bit(RSEQ_EVENT_SIGNAL_BIT, &get_current()->rseq_event_mask));
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 rseq_handle_notify_resume(ksig, regs);
}

/* rseq_preempt() requires preemption to be disabled. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_preempt(struct task_struct *t)
{
 ((__builtin_constant_p(RSEQ_EVENT_PREEMPT_BIT) && __builtin_constant_p((uintptr_t)(&t->rseq_event_mask) != (uintptr_t)((void *)0)) && (uintptr_t)(&t->rseq_event_mask) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&t->rseq_event_mask))) ? generic___set_bit(RSEQ_EVENT_PREEMPT_BIT, &t->rseq_event_mask) : generic___set_bit(RSEQ_EVENT_PREEMPT_BIT, &t->rseq_event_mask));
 rseq_set_notify_resume(t);
}

/* rseq_migrate() requires preemption to be disabled. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_migrate(struct task_struct *t)
{
 ((__builtin_constant_p(RSEQ_EVENT_MIGRATE_BIT) && __builtin_constant_p((uintptr_t)(&t->rseq_event_mask) != (uintptr_t)((void *)0)) && (uintptr_t)(&t->rseq_event_mask) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&t->rseq_event_mask))) ? generic___set_bit(RSEQ_EVENT_MIGRATE_BIT, &t->rseq_event_mask) : generic___set_bit(RSEQ_EVENT_MIGRATE_BIT, &t->rseq_event_mask));
 rseq_set_notify_resume(t);
}

/*
 * If parent process has a registered restartable sequences area, the
 * child inherits. Unregister rseq for a clone with CLONE_VM set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_fork(struct task_struct *t, unsigned long clone_flags)
{
 if (clone_flags & 0x00000100 /* set if VM shared between processes */) {
  t->rseq = ((void *)0);
  t->rseq_sig = 0;
  t->rseq_event_mask = 0;
 } else {
  t->rseq = get_current()->rseq;
  t->rseq_sig = get_current()->rseq_sig;
  t->rseq_event_mask = get_current()->rseq_event_mask;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_execve(struct task_struct *t)
{
 t->rseq = ((void *)0);
 t->rseq_sig = 0;
 t->rseq_event_mask = 0;
}
# 2405 "./include/linux/sched.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rseq_syscall(struct pt_regs *regs)
{
}
# 2417 "./include/linux/sched.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sched_core_free(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sched_core_fork(struct task_struct *p) { }


extern void sched_set_stop_task(int cpu, struct task_struct *stop);
# 29 "./arch/arm64/include/asm/compat.h" 2
# 1 "./include/linux/sched/task_stack.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * task->stack (kernel stack) handling interfaces:
 */


# 1 "./include/uapi/linux/magic.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 59 "./include/uapi/linux/magic.h"
     /* used by file system utilities that
	                                   look at the superblock, etc.  */
# 99 "./include/uapi/linux/magic.h"
/* Since UDF 2.01 is ISO 13346 based... */
# 11 "./include/linux/sched/task_stack.h" 2



/*
 * When accessing the stack of a non-current task that might exit, use
 * try_get_task_stack() instead.  task_stack_page will return a pointer
 * that could get freed out from under you.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void *task_stack_page(const struct task_struct *task)
{
 return task->stack;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long *end_of_stack(const struct task_struct *task)
{



 return task->stack;

}
# 66 "./include/linux/sched/task_stack.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *try_get_task_stack(struct task_struct *tsk)
{
 return refcount_inc_not_zero(&tsk->stack_refcount) ?
  task_stack_page(tsk) : ((void *)0);
}

extern void put_task_stack(struct task_struct *tsk);
# 82 "./include/linux/sched/task_stack.h"
void exit_task_stack_account(struct task_struct *tsk);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int object_is_on_stack(const void *obj)
{
 void *stack = task_stack_page(get_current());

 return (obj >= stack) && (obj < (stack + ((((1UL))) << (14 + 0))));
}

extern void thread_stack_cache_init(void);
# 116 "./include/linux/sched/task_stack.h"
extern void set_task_stack_end_magic(struct task_struct *tsk);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kstack_end(void *addr)
{
 /* Reliable end of stack detection:
	 * Some APM bios versions misalign the stack
	 */
 return !(((unsigned long)addr+sizeof(void*)-1) & (((((1UL))) << (14 + 0))-sizeof(void*)));
}
# 30 "./arch/arm64/include/asm/compat.h" 2







typedef u16 __compat_uid16_t;
typedef u16 __compat_gid16_t;
typedef s32 compat_nlink_t;

struct compat_stat {




 compat_dev_t st_dev;

 compat_ino_t st_ino;
 compat_mode_t st_mode;
 compat_ushort_t st_nlink;
 __compat_uid16_t st_uid;
 __compat_gid16_t st_gid;




 compat_dev_t st_rdev;

 compat_off_t st_size;
 compat_off_t st_blksize;
 compat_off_t st_blocks;
 old_time32_t st_atime;
 compat_ulong_t st_atime_nsec;
 old_time32_t st_mtime;
 compat_ulong_t st_mtime_nsec;
 old_time32_t st_ctime;
 compat_ulong_t st_ctime_nsec;
 compat_ulong_t __unused4[2];
};

struct compat_statfs {
 int f_type;
 int f_bsize;
 int f_blocks;
 int f_bfree;
 int f_bavail;
 int f_files;
 int f_ffree;
 compat_fsid_t f_fsid;
 int f_namelen; /* SunOS ignores this field. */
 int f_frsize;
 int f_flags;
 int f_spare[4];
};
# 93 "./arch/arm64/include/asm/compat.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_compat_task(void)
{
 return test_ti_thread_flag(((struct thread_info *)get_current()), 22 /* 32bit process */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_compat_thread(struct thread_info *thread)
{
 return test_ti_thread_flag(thread, 22 /* 32bit process */);
}
# 14 "./arch/arm64/include/asm/stat.h" 2

/*
 * struct stat64 is needed for compat tasks only. Its definition is different
 * from the generic struct stat64.
 */
struct stat64 {
 compat_u64 st_dev;
 unsigned char __pad0[4];


 compat_ulong_t __st_ino;
 compat_uint_t st_mode;
 compat_uint_t st_nlink;

 compat_ulong_t st_uid;
 compat_ulong_t st_gid;

 compat_u64 st_rdev;
 unsigned char __pad3[4];

 compat_s64 st_size;
 compat_ulong_t st_blksize;
 compat_u64 st_blocks; /* Number of 512-byte blocks allocated. */

 compat_ulong_t st_atime;
 compat_ulong_t st_atime_nsec;

 compat_ulong_t st_mtime;
 compat_ulong_t st_mtime_nsec;

 compat_ulong_t st_ctime;
 compat_ulong_t st_ctime_nsec;

 compat_u64 st_ino;
};
# 7 "./include/linux/stat.h" 2
# 1 "./include/uapi/linux/stat.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 46 "./include/uapi/linux/stat.h"
/*
 * Timestamp structure for the timestamps in struct statx.
 *
 * tv_sec holds the number of seconds before (negative) or after (positive)
 * 00:00:00 1st January 1970 UTC.
 *
 * tv_nsec holds a number of nanoseconds (0..999,999,999) after the tv_sec time.
 *
 * __reserved is held in case we need a yet finer resolution.
 */
struct statx_timestamp {
 __s64 tv_sec;
 __u32 tv_nsec;
 __s32 __reserved;
};

/*
 * Structures for the extended file attribute retrieval system call
 * (statx()).
 *
 * The caller passes a mask of what they're specifically interested in as a
 * parameter to statx().  What statx() actually got will be indicated in
 * st_mask upon return.
 *
 * For each bit in the mask argument:
 *
 * - if the datum is not supported:
 *
 *   - the bit will be cleared, and
 *
 *   - the datum will be set to an appropriate fabricated value if one is
 *     available (eg. CIFS can take a default uid and gid), otherwise
 *
 *   - the field will be cleared;
 *
 * - otherwise, if explicitly requested:
 *
 *   - the datum will be synchronised to the server if AT_STATX_FORCE_SYNC is
 *     set or if the datum is considered out of date, and
 *
 *   - the field will be filled in and the bit will be set;
 *
 * - otherwise, if not requested, but available in approximate form without any
 *   effort, it will be filled in anyway, and the bit will be set upon return
 *   (it might not be up to date, however, and no attempt will be made to
 *   synchronise the internal state first);
 *
 * - otherwise the field and the bit will be cleared before returning.
 *
 * Items in STATX_BASIC_STATS may be marked unavailable on return, but they
 * will have values installed for compatibility purposes so that stat() and
 * co. can be emulated in userspace.
 */
struct statx {
 /* 0x00 */
 __u32 stx_mask; /* What results were written [uncond] */
 __u32 stx_blksize; /* Preferred general I/O size [uncond] */
 __u64 stx_attributes; /* Flags conveying information about the file [uncond] */
 /* 0x10 */
 __u32 stx_nlink; /* Number of hard links */
 __u32 stx_uid; /* User ID of owner */
 __u32 stx_gid; /* Group ID of owner */
 __u16 stx_mode; /* File mode */
 __u16 __spare0[1];
 /* 0x20 */
 __u64 stx_ino; /* Inode number */
 __u64 stx_size; /* File size */
 __u64 stx_blocks; /* Number of 512-byte blocks allocated */
 __u64 stx_attributes_mask; /* Mask to show what's supported in stx_attributes */
 /* 0x40 */
 struct statx_timestamp stx_atime; /* Last access time */
 struct statx_timestamp stx_btime; /* File creation time */
 struct statx_timestamp stx_ctime; /* Last attribute change time */
 struct statx_timestamp stx_mtime; /* Last data modification time */
 /* 0x80 */
 __u32 stx_rdev_major; /* Device ID of special file [if bdev/cdev] */
 __u32 stx_rdev_minor;
 __u32 stx_dev_major; /* ID of device containing file [uncond] */
 __u32 stx_dev_minor;
 /* 0x90 */
 __u64 stx_mnt_id;
 __u32 stx_dio_mem_align; /* Memory buffer alignment for direct I/O */
 __u32 stx_dio_offset_align; /* File offset alignment for direct I/O */
 /* 0xa0 */
 __u64 __spare3[12]; /* Spare space for future expansion */
 /* 0x100 */
};

/*
 * Flags to be stx_mask
 *
 * Query request/result mask for statx() and struct statx::stx_mask.
 *
 * These bits should be set in the mask argument of statx() to request
 * particular items when calling statx().
 */
# 169 "./include/uapi/linux/stat.h"
/*
 * Attributes to be found in stx_attributes and masked in stx_attributes_mask.
 *
 * These give information about the features or the state of a file that might
 * be of use to ordinary userspace programs such as GUIs or ls rather than
 * specialised tools.
 *
 * Note that the flags marked [I] correspond to the FS_IOC_SETFLAGS flags
 * semantically.  Where possible, the numerical value is picked to correspond
 * also.  Note that the DAX attribute indicates that the file is in the CPU
 * direct access state.  It does not correspond to the per-inode flag that
 * some filesystems support.
 *
 */
# 8 "./include/linux/stat.h" 2
# 22 "./include/linux/stat.h"
struct kstat {
 u32 result_mask; /* What fields the user got */
 umode_t mode;
 unsigned int nlink;
 uint32_t blksize; /* Preferred I/O size */
 u64 attributes;
 u64 attributes_mask;
# 41 "./include/linux/stat.h"
 u64 ino;
 dev_t dev;
 dev_t rdev;
 kuid_t uid;
 kgid_t gid;
 loff_t size;
 struct timespec64 atime;
 struct timespec64 mtime;
 struct timespec64 ctime;
 struct timespec64 btime; /* File creation time */
 u64 blocks;
 u64 mnt_id;
 u32 dio_mem_align;
 u32 dio_offset_align;
};
# 11 "./include/linux/fs.h" 2


# 1 "./include/linux/list_lru.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.
 * Authors: David Chinner and Glauber Costa
 *
 * Generic LRU infrastructure
 */





# 1 "./include/linux/shrinker.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/*
 * This struct is used to pass information from page reclaim to the shrinkers.
 * We consolidate the values for easier extension later.
 *
 * The 'gfpmask' refers to the allocation we are currently trying to
 * fulfil.
 */
struct shrink_control {
 gfp_t gfp_mask;

 /* current node being shrunk (for NUMA aware shrinkers) */
 int nid;

 /*
	 * How many objects scan_objects should scan and try to reclaim.
	 * This is reset before every call, so it is safe for callees
	 * to modify.
	 */
 unsigned long nr_to_scan;

 /*
	 * How many objects did scan_objects process?
	 * This defaults to nr_to_scan before every call, but the callee
	 * should track its actual progress.
	 */
 unsigned long nr_scanned;

 /* current memcg being shrunk (for memcg aware shrinkers) */
 struct mem_cgroup *memcg;
};



/*
 * A callback you can register to apply pressure to ageable caches.
 *
 * @count_objects should return the number of freeable items in the cache. If
 * there are no objects to free, it should return SHRINK_EMPTY, while 0 is
 * returned in cases of the number of freeable items cannot be determined
 * or shrinker should skip this cache for this time (e.g., their number
 * is below shrinkable limit). No deadlock checks should be done during the
 * count callback - the shrinker relies on aggregating scan counts that couldn't
 * be executed due to potential deadlocks to be run at a later call when the
 * deadlock condition is no longer pending.
 *
 * @scan_objects will only be called if @count_objects returned a non-zero
 * value for the number of freeable objects. The callout should scan the cache
 * and attempt to free items from the cache. It should then return the number
 * of objects freed during the scan, or SHRINK_STOP if progress cannot be made
 * due to potential deadlocks. If SHRINK_STOP is returned, then no further
 * attempts to call the @scan_objects will be made from the current reclaim
 * context.
 *
 * @flags determine the shrinker abilities, like numa awareness
 */
struct shrinker {
 unsigned long (*count_objects)(struct shrinker *,
           struct shrink_control *sc);
 unsigned long (*scan_objects)(struct shrinker *,
          struct shrink_control *sc);

 long batch; /* reclaim batch size, 0 = default */
 int seeks; /* seeks to recreate an obj */
 unsigned flags;

 /* These are for internal use */
 struct list_head list;

 /* ID in shrinker_idr */
 int id;






 /* objs pending delete, per node */
 atomic_long_t *nr_deferred;
};


/* Flags */



/*
 * It just makes sense when the shrinker is also MEMCG_AWARE for now,
 * non-MEMCG_AWARE shrinker should not have this flag set.
 */


extern int __attribute__((__format__(printf, 2, 3))) prealloc_shrinker(struct shrinker *shrinker,
         const char *fmt, ...);
extern void register_shrinker_prepared(struct shrinker *shrinker);
extern int __attribute__((__format__(printf, 2, 3))) register_shrinker(struct shrinker *shrinker,
         const char *fmt, ...);
extern void unregister_shrinker(struct shrinker *shrinker);
extern void free_prealloced_shrinker(struct shrinker *shrinker);
extern void synchronize_shrinkers(void);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int shrinker_debugfs_add(struct shrinker *shrinker)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void shrinker_debugfs_remove(struct shrinker *shrinker)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__format__(printf, 2, 3)))
int shrinker_debugfs_rename(struct shrinker *shrinker, const char *fmt, ...)
{
 return 0;
}
# 14 "./include/linux/list_lru.h" 2
# 1 "./include/linux/xarray.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */


/*
 * eXtensible Arrays
 * Copyright (c) 2017 Microsoft Corporation
 * Author: Matthew Wilcox <willy@infradead.org>
 *
 * See Documentation/core-api/xarray.rst for how to use the XArray.
 */




# 1 "./include/linux/gfp.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/mmzone.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 18 "./include/linux/mmzone.h"
# 1 "./include/linux/pageblock-flags.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Macros for manipulating and testing flags related to a
 * pageblock_nr_pages number of pages.
 *
 * Copyright (C) IBM Corporation, 2006
 *
 * Original author, Mel Gorman
 * Major cleanups and reduction of bit operations, Andy Whitcroft
 */






/* Bit indices that affect a whole block of pages */
enum pageblock_bits {
 PB_migrate,
 PB_migrate_end = PB_migrate + 3 - 1,
   /* 3 bits required for migrate types */
 PB_migrate_skip,/* If set the block is skipped by compaction */

 /*
	 * Assume the bits will always align on a word. If this assumption
	 * changes then get/set pageblock needs updating.
	 */
 NR_PAGEBLOCK_BITS
};
# 40 "./include/linux/pageblock-flags.h"
/*
 * Huge pages are a constant size, but don't exceed the maximum allocation
 * granularity.
 */
# 61 "./include/linux/pageblock-flags.h"
/* Forward declaration */
struct page;

unsigned long get_pfnblock_flags_mask(const struct page *page,
    unsigned long pfn,
    unsigned long mask);

void set_pfnblock_flags_mask(struct page *page,
    unsigned long flags,
    unsigned long pfn,
    unsigned long mask);

/* Declarations for getting and setting flags. See mm/page_alloc.c */
# 19 "./include/linux/mmzone.h" 2
# 1 "./include/linux/page-flags-layout.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/*
 * When a memory allocation must conform to specific limitations (such
 * as being suitable for DMA) the caller will pass in hints to the
 * allocator in the gfp_mask, in the zone modifier bits.  These bits
 * are used to select a priority ordered list of memory zones which
 * match the requested limits. See gfp_zone() in include/linux/gfp.h
 */
# 37 "./include/linux/page-flags-layout.h"
/*
 * page->flags layout:
 *
 * There are five possibilities for how page->flags get laid out.  The first
 * pair is for the normal case without sparsemem. The second pair is for
 * sparsemem when there is plenty of space for node and section information.
 * The last is when there is insufficient space in page->flags and a separate
 * lookup is necessary.
 *
 * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE |             ... | FLAGS |
 *      " plus space for last_cpupid: |       NODE     | ZONE | LAST_CPUPID ... | FLAGS |
 * classic sparse with space for node:| SECTION | NODE | ZONE |             ... | FLAGS |
 *      " plus space for last_cpupid: | SECTION | NODE | ZONE | LAST_CPUPID ... | FLAGS |
 * classic sparse no space for node:  | SECTION |     ZONE    | ... | FLAGS |
 */
# 67 "./include/linux/page-flags-layout.h"
/*
 * Note that this #define MUST have a value so that it can be tested with
 * the IS_ENABLED() macro.
 */
# 109 "./include/linux/page-flags-layout.h"
/* see the comment on MAX_NR_TIERS */
# 20 "./include/linux/mmzone.h" 2

# 1 "./include/linux/mm_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/auxvec.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/uapi/linux/auxvec.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./arch/arm64/include/uapi/asm/auxvec.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */



/* vDSO location */
# 6 "./include/uapi/linux/auxvec.h" 2

/* Symbolic values for the entries in the auxiliary table
   put on the initial stack */
# 27 "./include/uapi/linux/auxvec.h"
/* AT_* values 18 through 22 are reserved */
# 6 "./include/linux/auxvec.h" 2


  /* number of "#define AT_.*" above, minus {AT_NULL, AT_IGNORE, AT_NOTELF} */
# 8 "./include/linux/mm_types.h" 2
# 1 "./include/linux/kref.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * kref.h - library routines for handling generic reference counted objects
 *
 * Copyright (C) 2004 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (C) 2004 IBM Corp.
 *
 * based on kobject.h which was:
 * Copyright (C) 2002-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (C) 2002-2003 Open Source Development Labs
 */







struct kref {
 refcount_t refcount;
};



/**
 * kref_init - initialize object.
 * @kref: object in question.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kref_init(struct kref *kref)
{
 refcount_set(&kref->refcount, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int kref_read(const struct kref *kref)
{
 return refcount_read(&kref->refcount);
}

/**
 * kref_get - increment refcount for object.
 * @kref: object.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kref_get(struct kref *kref)
{
 refcount_inc(&kref->refcount);
}

/**
 * kref_put - decrement refcount for object.
 * @kref: object.
 * @release: pointer to the function that will clean up the object when the
 *	     last reference to the object is released.
 *	     This pointer is required, and it is not acceptable to pass kfree
 *	     in as this function.
 *
 * Decrement the refcount, and if 0, call release().
 * Return 1 if the object was removed, otherwise return 0.  Beware, if this
 * function returns 0, you still can not count on the kref from remaining in
 * memory.  Only use the return value if you want to see if the kref is now
 * gone, not present.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kref_put(struct kref *kref, void (*release)(struct kref *kref))
{
 if (refcount_dec_and_test(&kref->refcount)) {
  release(kref);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kref_put_mutex(struct kref *kref,
     void (*release)(struct kref *kref),
     struct mutex *lock)
{
 if (refcount_dec_and_mutex_lock(&kref->refcount, lock)) {
  release(kref);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kref_put_lock(struct kref *kref,
    void (*release)(struct kref *kref),
    spinlock_t *lock)
{
 if (refcount_dec_and_lock(&kref->refcount, lock)) {
  release(kref);
  return 1;
 }
 return 0;
}

/**
 * kref_get_unless_zero - Increment refcount for object unless it is zero.
 * @kref: object.
 *
 * Return non-zero if the increment succeeded. Otherwise return 0.
 *
 * This function is intended to simplify locking around refcounting for
 * objects that can be looked up from a lookup structure, and which are
 * removed from that lookup structure in the object destructor.
 * Operations on such objects require at least a read lock around
 * lookup + kref_get, and a write lock around kref_put + remove from lookup
 * structure. Furthermore, RCU implementations become extremely tricky.
 * With a lookup followed by a kref_get_unless_zero *with return value check*
 * locking in the kref_put path can be deferred to the actual removal from
 * the lookup structure and RCU lookups become trivial.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) kref_get_unless_zero(struct kref *kref)
{
 return refcount_inc_not_zero(&kref->refcount);
}
# 9 "./include/linux/mm_types.h" 2



# 1 "./include/linux/maple_tree.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */


/*
 * Maple Tree - An RCU-safe adaptive tree for storing ranges
 * Copyright (c) 2018-2022 Oracle
 * Authors:     Liam R. Howlett <Liam.Howlett@Oracle.com>
 *              Matthew Wilcox <willy@infradead.org>
 */




/* #define CONFIG_MAPLE_RCU_DISABLED */
/* #define CONFIG_DEBUG_MAPLE_TREE_VERBOSE */

/*
 * Allocated nodes are mutable until they have been inserted into the tree,
 * at which time they cannot change their type until they have been removed
 * from the tree and an RCU grace period has passed.
 *
 * Removed nodes have their ->parent set to point to themselves.  RCU readers
 * check ->parent before relying on the value that they loaded from the
 * slots array.  This lets us reuse the slots array for the RCU head.
 *
 * Nodes in the tree point to their parent unless bit 0 is set.
 */

/* 64bit sizes */
# 46 "./include/linux/maple_tree.h"
/*
 * The node->parent of the root node has bit 0 set and the rest of the pointer
 * is a pointer to the tree itself.  No more bits are available in this pointer
 * (on m68k, the data structure may only be 2-byte aligned).
 *
 * Internal non-root nodes can only have maple_range_* nodes as parents.  The
 * parent pointer is 256B aligned like all other tree nodes.  When storing a 32
 * or 64 bit values, the offset can fit into 4 bits.  The 16 bit values need an
 * extra bit to store the offset.  This extra bit comes from a reuse of the last
 * bit in the node type.  This is possible by using bit 1 to indicate if bit 2
 * is part of the type or the slot.
 *
 * Once the type is decided, the decision of an allocation range type or a range
 * type is done by examining the immutable tree flag for the MAPLE_ALLOC_RANGE
 * flag.
 *
 *  Node types:
 *   0x??1 = Root
 *   0x?00 = 16 bit nodes
 *   0x010 = 32 bit nodes
 *   0x110 = 64 bit nodes
 *
 *  Slot size and location in the parent pointer:
 *   type  : slot location
 *   0x??1 : Root
 *   0x?00 : 16 bit values, type in 0-1, slot in 2-6
 *   0x010 : 32 bit values, type in 0-2, slot in 3-6
 *   0x110 : 64 bit values, type in 0-2, slot in 3-6
 */

/*
 * This metadata is used to optimize the gap updating code and in reverse
 * searching for gaps or any other code that needs to find the end of the data.
 */
struct maple_metadata {
 unsigned char end;
 unsigned char gap;
};

/*
 * Leaf nodes do not store pointers to nodes, they store user data.  Users may
 * store almost any bit pattern.  As noted above, the optimisation of storing an
 * entry at 0 in the root pointer cannot be done for data which have the bottom
 * two bits set to '10'.  We also reserve values with the bottom two bits set to
 * '10' which are below 4096 (ie 2, 6, 10 .. 4094) for internal use.  Some APIs
 * return errnos as a negative errno shifted right by two bits and the bottom
 * two bits set to '10', and while choosing to store these values in the array
 * is not an error, it may lead to confusion if you're testing for an error with
 * mas_is_err().
 *
 * Non-leaf nodes store the type of the node pointed to (enum maple_type in bits
 * 3-6), bit 2 is reserved.  That leaves bits 0-1 unused for now.
 *
 * In regular B-Tree terms, pivots are called keys.  The term pivot is used to
 * indicate that the tree is specifying ranges,  Pivots may appear in the
 * subtree with an entry attached to the value whereas keys are unique to a
 * specific position of a B-tree.  Pivot values are inclusive of the slot with
 * the same index.
 */

struct maple_range_64 {
 struct maple_pnode *parent;
 unsigned long pivot[16 /* 256 bytes */ - 1];
 union {
  void /* nothing */ *slot[16 /* 256 bytes */];
  struct {
   void /* nothing */ *pad[16 /* 256 bytes */ - 1];
   struct maple_metadata meta;
  };
 };
};

/*
 * At tree creation time, the user can specify that they're willing to trade off
 * storing fewer entries in a tree in return for storing more information in
 * each node.
 *
 * The maple tree supports recording the largest range of NULL entries available
 * in this node, also called gaps.  This optimises the tree for allocating a
 * range.
 */
struct maple_arange_64 {
 struct maple_pnode *parent;
 unsigned long pivot[10 /* 240 bytes */ - 1];
 void /* nothing */ *slot[10 /* 240 bytes */];
 unsigned long gap[10 /* 240 bytes */];
 struct maple_metadata meta;
};

struct maple_alloc {
 unsigned long total;
 unsigned char node_count;
 unsigned int request_count;
 struct maple_alloc *slot[(31 /* 256 bytes including ->parent */ - 1)];
};

struct maple_topiary {
 struct maple_pnode *parent;
 struct maple_enode *next; /* Overlaps the pivot */
};

enum maple_type {
 maple_dense,
 maple_leaf_64,
 maple_range_64,
 maple_arange_64,
};


/**
 * DOC: Maple tree flags
 *
 * * MT_FLAGS_ALLOC_RANGE	- Track gaps in this tree
 * * MT_FLAGS_USE_RCU		- Operate in RCU mode
 * * MT_FLAGS_HEIGHT_OFFSET	- The position of the tree height in the flags
 * * MT_FLAGS_HEIGHT_MASK	- The mask for the maple tree height value
 * * MT_FLAGS_LOCK_MASK		- How the mt_lock is used
 * * MT_FLAGS_LOCK_IRQ		- Acquired irq-safe
 * * MT_FLAGS_LOCK_BH		- Acquired bh-safe
 * * MT_FLAGS_LOCK_EXTERN	- mt_lock is not used
 *
 * MAPLE_HEIGHT_MAX	The largest height that can be stored
 */
# 192 "./include/linux/maple_tree.h"
typedef struct { /* nothing */ } lockdep_map_p;




/*
 * If the tree contains a single entry at index 0, it is usually stored in
 * tree->ma_root.  To optimise for the page cache, an entry which ends in '00',
 * '01' or '11' is stored in the root, but an entry which ends in '10' will be
 * stored in a node.  Bits 3-6 are used to store enum maple_type.
 *
 * The flags are used both to store some immutable information about this tree
 * (set at tree creation time) and dynamic information set under the spinlock.
 *
 * Another use of flags are to indicate global states of the tree.  This is the
 * case with the MAPLE_USE_RCU flag, which indicates the tree is currently in
 * RCU mode.  This mode was added to allow the tree to reuse nodes instead of
 * re-allocating and RCU freeing nodes when there is a single user.
 */
struct maple_tree {
 union {
  spinlock_t ma_lock;
  lockdep_map_p ma_external_lock;
 };
 void /* nothing */ *ma_root;
 unsigned int ma_flags;
};

/**
 * MTREE_INIT() - Initialize a maple tree
 * @name: The maple tree name
 * @__flags: The maple tree flags
 *
 */






/**
 * MTREE_INIT_EXT() - Initialize a maple tree with an external lock.
 * @name: The tree name
 * @__flags: The maple tree flags
 * @__lock: The external lock
 */
# 254 "./include/linux/maple_tree.h"
/*
 * The Maple Tree squeezes various bits in at various points which aren't
 * necessarily obvious.  Usually, this is done by observing that pointers are
 * N-byte aligned and thus the bottom log_2(N) bits are available for use.  We
 * don't use the high bits of pointers to store additional information because
 * we don't know what bits are unused on any given architecture.
 *
 * Nodes are 256 bytes in size and are also aligned to 256 bytes, giving us 8
 * low bits for our own purposes.  Nodes are currently of 4 types:
 * 1. Single pointer (Range is 0-0)
 * 2. Non-leaf Allocation Range nodes
 * 3. Non-leaf Range nodes
 * 4. Leaf Range nodes All nodes consist of a number of node slots,
 *    pivots, and a parent pointer.
 */

struct maple_node {
 union {
  struct {
   struct maple_pnode *parent;
   void /* nothing */ *slot[31 /* 256 bytes including ->parent */];
  };
  struct {
   void *pad;
   struct callback_head rcu;
   struct maple_enode *piv_parent;
   unsigned char parent_slot;
   enum maple_type type;
   unsigned char slot_len;
   unsigned int ma_flags;
  };
  struct maple_range_64 mr64;
  struct maple_arange_64 ma64;
  struct maple_alloc alloc;
 };
};

/*
 * More complicated stores can cause two nodes to become one or three and
 * potentially alter the height of the tree.  Either half of the tree may need
 * to be rebalanced against the other.  The ma_topiary struct is used to track
 * which nodes have been 'cut' from the tree so that the change can be done
 * safely at a later date.  This is done to support RCU.
 */
struct ma_topiary {
 struct maple_enode *head;
 struct maple_enode *tail;
 struct maple_tree *mtree;
};

void *mtree_load(struct maple_tree *mt, unsigned long index);

int mtree_insert(struct maple_tree *mt, unsigned long index,
  void *entry, gfp_t gfp);
int mtree_insert_range(struct maple_tree *mt, unsigned long first,
  unsigned long last, void *entry, gfp_t gfp);
int mtree_alloc_range(struct maple_tree *mt, unsigned long *startp,
  void *entry, unsigned long size, unsigned long min,
  unsigned long max, gfp_t gfp);
int mtree_alloc_rrange(struct maple_tree *mt, unsigned long *startp,
  void *entry, unsigned long size, unsigned long min,
  unsigned long max, gfp_t gfp);

int mtree_store_range(struct maple_tree *mt, unsigned long first,
        unsigned long last, void *entry, gfp_t gfp);
int mtree_store(struct maple_tree *mt, unsigned long index,
  void *entry, gfp_t gfp);
void *mtree_erase(struct maple_tree *mt, unsigned long index);

void mtree_destroy(struct maple_tree *mt);
void __mt_destroy(struct maple_tree *mt);

/**
 * mtree_empty() - Determine if a tree has any present entries.
 * @mt: Maple Tree.
 *
 * Context: Any context.
 * Return: %true if the tree contains only NULL pointers.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mtree_empty(const struct maple_tree *mt)
{
 return mt->ma_root == ((void *)0);
}

/* Advanced API */

/*
 * The maple state is defined in the struct ma_state and is used to keep track
 * of information during operations, and even between operations when using the
 * advanced API.
 *
 * If state->node has bit 0 set then it references a tree location which is not
 * a node (eg the root).  If bit 1 is set, the rest of the bits are a negative
 * errno.  Bit 2 (the 'unallocated slots' bit) is clear.  Bits 3-6 indicate the
 * node type.
 *
 * state->alloc either has a request number of nodes or an allocated node.  If
 * stat->alloc has a requested number of nodes, the first bit will be set (0x1)
 * and the remaining bits are the value.  If state->alloc is a node, then the
 * node will be of type maple_alloc.  maple_alloc has MAPLE_NODE_SLOTS - 1 for
 * storing more allocated nodes, a total number of nodes allocated, and the
 * node_count in this node.  node_count is the number of allocated nodes in this
 * node.  The scaling beyond MAPLE_NODE_SLOTS - 1 is handled by storing further
 * nodes into state->alloc->slot[0]'s node.  Nodes are taken from state->alloc
 * by removing a node from the state->alloc node until state->alloc->node_count
 * is 1, when state->alloc is returned and the state->alloc->slot[0] is promoted
 * to state->alloc.  Nodes are pushed onto state->alloc by putting the current
 * state->alloc into the pushed node's slot[0].
 *
 * The state also contains the implied min/max of the state->node, the depth of
 * this search, and the offset. The implied min/max are either from the parent
 * node or are 0-oo for the root node.  The depth is incremented or decremented
 * every time a node is walked down or up.  The offset is the slot/pivot of
 * interest in the node - either for reading or writing.
 *
 * When returning a value the maple state index and last respectively contain
 * the start and end of the range for the entry.  Ranges are inclusive in the
 * Maple Tree.
 */
struct ma_state {
 struct maple_tree *tree; /* The tree we're operating in */
 unsigned long index; /* The index we're operating on - range start */
 unsigned long last; /* The last index we're operating on - range end */
 struct maple_enode *node; /* The node containing this entry */
 unsigned long min; /* The minimum index of this node - implied pivot min */
 unsigned long max; /* The maximum index of this node - implied pivot max */
 struct maple_alloc *alloc; /* Allocated nodes for this operation */
 unsigned char depth; /* depth of tree descent during write */
 unsigned char offset;
 unsigned char mas_flags;
};

struct ma_wr_state {
 struct ma_state *mas;
 struct maple_node *node; /* Decoded mas->node */
 unsigned long r_min; /* range min */
 unsigned long r_max; /* range max */
 enum maple_type type; /* mas->node type */
 unsigned char offset_end; /* The offset where the write ends */
 unsigned char node_end; /* mas->node end */
 unsigned long *pivots; /* mas->node->pivots pointer */
 unsigned long end_piv; /* The pivot at the offset end */
 void /* nothing */ **slots; /* mas->node->slots pointer */
 void *entry; /* The entry to write */
 void *content; /* The existing entry that is being overwritten */
};





/*
 * Special values for ma_state.node.
 * MAS_START means we have not searched the tree.
 * MAS_ROOT means we have searched the tree and the entry we found lives in
 * the root of the tree (ie it has index 0, length 1 and is the only entry in
 * the tree).
 * MAS_NONE means we have searched the tree and there is no node in the
 * tree for this entry.  For example, we searched for index 1 in an empty
 * tree.  Or we have a tree which points to a full leaf node and we
 * searched for an entry which is larger than can be contained in that
 * leaf node.
 * MA_ERROR represents an errno.  After dropping the lock and attempting
 * to resolve the error, the walk would have to be restarted from the
 * top of the tree as the tree may have been modified.
 */
# 452 "./include/linux/maple_tree.h"
void *mas_walk(struct ma_state *mas);
void *mas_store(struct ma_state *mas, void *entry);
void *mas_erase(struct ma_state *mas);
int mas_store_gfp(struct ma_state *mas, void *entry, gfp_t gfp);
void mas_store_prealloc(struct ma_state *mas, void *entry);
void *mas_find(struct ma_state *mas, unsigned long max);
void *mas_find_rev(struct ma_state *mas, unsigned long min);
int mas_preallocate(struct ma_state *mas, void *entry, gfp_t gfp);
bool mas_is_err(struct ma_state *mas);

bool mas_nomem(struct ma_state *mas, gfp_t gfp);
void mas_pause(struct ma_state *mas);
void maple_tree_init(void);
void mas_destroy(struct ma_state *mas);
int mas_expected_entries(struct ma_state *mas, unsigned long nr_entries);

void *mas_prev(struct ma_state *mas, unsigned long min);
void *mas_next(struct ma_state *mas, unsigned long max);

int mas_empty_area(struct ma_state *mas, unsigned long min, unsigned long max,
     unsigned long size);

/* Checks if a mas has not found anything */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mas_is_none(struct ma_state *mas)
{
 return mas->node == ((struct maple_enode *)9UL);
}

/* Checks if a mas has been paused */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mas_is_paused(struct ma_state *mas)
{
 return mas->node == ((struct maple_enode *)17UL);
}

void mas_dup_tree(struct ma_state *oldmas, struct ma_state *mas);
void mas_dup_store(struct ma_state *mas, void *entry);

/*
 * This finds an empty area from the highest address to the lowest.
 * AKA "Topdown" version,
 */
int mas_empty_area_rev(struct ma_state *mas, unsigned long min,
         unsigned long max, unsigned long size);
/**
 * mas_reset() - Reset a Maple Tree operation state.
 * @mas: Maple Tree operation state.
 *
 * Resets the error or walk state of the @mas so future walks of the
 * array will start from the root.  Use this if you have dropped the
 * lock and want to reuse the ma_state.
 *
 * Context: Any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mas_reset(struct ma_state *mas)
{
 mas->node = ((struct maple_enode *)1UL);
}

/**
 * mas_for_each() - Iterate over a range of the maple tree.
 * @__mas: Maple Tree operation state (maple_state)
 * @__entry: Entry retrieved from the tree
 * @__max: maximum index to retrieve from the tree
 *
 * When returned, mas->index and mas->last will hold the entire range for the
 * entry.
 *
 * Note: may return the zero entry.
 *
 */




/**
 * mas_set_range() - Set up Maple Tree operation state for a different index.
 * @mas: Maple Tree operation state.
 * @start: New start of range in the Maple Tree.
 * @last: New end of range in the Maple Tree.
 *
 * Move the operation state to refer to a different range.  This will
 * have the effect of starting a walk from the top; see mas_next()
 * to move to an adjacent index.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
void mas_set_range(struct ma_state *mas, unsigned long start, unsigned long last)
{
        mas->index = start;
        mas->last = last;
        mas->node = ((struct maple_enode *)1UL);
}

/**
 * mas_set() - Set up Maple Tree operation state for a different index.
 * @mas: Maple Tree operation state.
 * @index: New index into the Maple Tree.
 *
 * Move the operation state to refer to a different index.  This will
 * have the effect of starting a walk from the top; see mas_next()
 * to move to an adjacent index.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mas_set(struct ma_state *mas, unsigned long index)
{

 mas_set_range(mas, index, index);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mt_external_lock(const struct maple_tree *mt)
{
 return (mt->ma_flags & 0x300) == 0x300;
}

/**
 * mt_init_flags() - Initialise an empty maple tree with flags.
 * @mt: Maple Tree
 * @flags: maple tree flags.
 *
 * If you need to initialise a Maple Tree with special flags (eg, an
 * allocation tree), use this function.
 *
 * Context: Any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mt_init_flags(struct maple_tree *mt, unsigned int flags)
{
 mt->ma_flags = flags;
 if (!mt_external_lock(mt))
  do { spinlock_check(&mt->ma_lock); *(&mt->ma_lock) = (spinlock_t) { { .rlock = { .raw_lock = { { .val = { (0) } } }, } } }; } while (0);
 do { uintptr_t _r_a_p__v = (uintptr_t)(((void *)0)); ; if (__builtin_constant_p(((void *)0)) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_263(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((mt->ma_root)) == sizeof(char) || sizeof((mt->ma_root)) == sizeof(short) || sizeof((mt->ma_root)) == sizeof(int) || sizeof((mt->ma_root)) == sizeof(long)) || sizeof((mt->ma_root)) == sizeof(long long))) __compiletime_assert_263(); } while (0); do { *(volatile typeof((mt->ma_root)) *)&((mt->ma_root)) = ((typeof(mt->ma_root))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&mt->ma_root) __p = (&mt->ma_root); union { typeof( _Generic((*&mt->ma_root), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&mt->ma_root))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&mt->ma_root), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&mt->ma_root)))) ((typeof(*((typeof(mt->ma_root))_r_a_p__v)) /* nothing */ *)((typeof(mt->ma_root))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_264(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&mt->ma_root) == sizeof(char) || sizeof(*&mt->ma_root) == sizeof(short) || sizeof(*&mt->ma_root) == sizeof(int) || sizeof(*&mt->ma_root) == sizeof(long)))) __compiletime_assert_264(); } while (0); kasan_check_write(__p, sizeof(*&mt->ma_root)); switch (sizeof(*&mt->ma_root)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 580 "./include/linux/maple_tree.h"
}

/**
 * mt_init() - Initialise an empty maple tree.
 * @mt: Maple Tree
 *
 * An empty Maple Tree.
 *
 * Context: Any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mt_init(struct maple_tree *mt)
{
 mt_init_flags(mt, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mt_in_rcu(struct maple_tree *mt)
{



 return mt->ma_flags & 0x02;
}

/**
 * mt_clear_in_rcu() - Switch the tree to non-RCU mode.
 * @mt: The Maple Tree
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mt_clear_in_rcu(struct maple_tree *mt)
{
 if (!mt_in_rcu(mt))
  return;

 if (mt_external_lock(mt)) {
  do { if (__builtin_expect(!!(!1), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/maple_tree.h\"; .popsection; .long 14472b - .; .short 613; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
  mt->ma_flags &= ~0x02;
 } else {
  spin_lock((&(mt)->ma_lock));
  mt->ma_flags &= ~0x02;
  spin_unlock((&(mt)->ma_lock));
 }
}

/**
 * mt_set_in_rcu() - Switch the tree to RCU safe mode.
 * @mt: The Maple Tree
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mt_set_in_rcu(struct maple_tree *mt)
{
 if (mt_in_rcu(mt))
  return;

 if (mt_external_lock(mt)) {
  do { if (__builtin_expect(!!(!1), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/maple_tree.h\"; .popsection; .long 14472b - .; .short 632; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
  mt->ma_flags |= 0x02;
 } else {
  spin_lock((&(mt)->ma_lock));
  mt->ma_flags |= 0x02;
  spin_unlock((&(mt)->ma_lock));
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int mt_height(const struct maple_tree *mt)

{
 return (mt->ma_flags & 0x7C) >> 0x02;
}

void *mt_find(struct maple_tree *mt, unsigned long *index, unsigned long max);
void *mt_find_after(struct maple_tree *mt, unsigned long *index,
      unsigned long max);
void *mt_prev(struct maple_tree *mt, unsigned long index, unsigned long min);
void *mt_next(struct maple_tree *mt, unsigned long index, unsigned long max);

/**
 * mt_for_each - Iterate over each entry starting at index until max.
 * @__tree: The Maple Tree
 * @__entry: The current entry
 * @__index: The index to update to track the location in the tree
 * @__max: The maximum limit for @index
 *
 * Note: Will not return the zero entry.
 */
# 13 "./include/linux/mm_types.h" 2
# 1 "./include/linux/rwsem.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/* rwsem.h: R/W semaphores, public interface
 *
 * Written by David Howells (dhowells@redhat.com).
 * Derived from asm-i386/semaphore.h
 */
# 35 "./include/linux/rwsem.h"
/*
 * For an uncontended rwsem, count and owner are the only fields a task
 * needs to touch when acquiring the rwsem. So they are put next to each
 * other to increase the chance that they will share the same cacheline.
 *
 * In a contended rwsem, the owner is likely the most frequently accessed
 * field in the structure as the optimistic waiter that holds the osq lock
 * will spin on owner. For an embedded rwsem, other hot fields in the
 * containing structure should be moved further away from the rwsem to
 * reduce the chance that they will share the same cacheline causing
 * cacheline bouncing problem.
 */
struct rw_semaphore {
 atomic_long_t count;
 /*
	 * Write owner or one of the read owners as well flags regarding
	 * the current state of the rwsem. Can be used as a speculative
	 * check to see if the write owner is running on the cpu.
	 */
 atomic_long_t owner;

 struct optimistic_spin_queue osq; /* spinner MCS lock */

 raw_spinlock_t wait_lock;
 struct list_head wait_list;






};

/* In all implementations count != 0 means locked */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rwsem_is_locked(struct rw_semaphore *sem)
{
 return atomic_long_read(&sem->count) != 0;
}




/* Common initializer macros and functions */
# 103 "./include/linux/rwsem.h"
extern void __init_rwsem(struct rw_semaphore *sem, const char *name,
    struct lock_class_key *key);
# 113 "./include/linux/rwsem.h"
/*
 * This is the same regardless of which rwsem implementation that is being used.
 * It is just a heuristic meant to be called by somebody already holding the
 * rwsem to see if somebody from an incompatible type is wanting access to the
 * lock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rwsem_is_contended(struct rw_semaphore *sem)
{
 return !list_empty(&sem->wait_list);
}
# 166 "./include/linux/rwsem.h"
/*
 * The functions below are the same for all rwsem implementations including
 * the RT specific variant.
 */

/*
 * lock for reading
 */
extern void down_read(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_read_interruptible(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_read_killable(struct rw_semaphore *sem);

/*
 * trylock for reading -- returns 1 if successful, 0 if contention
 */
extern int down_read_trylock(struct rw_semaphore *sem);

/*
 * lock for writing
 */
extern void down_write(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_write_killable(struct rw_semaphore *sem);

/*
 * trylock for writing -- returns 1 if successful, 0 if contention
 */
extern int down_write_trylock(struct rw_semaphore *sem);

/*
 * release a read lock
 */
extern void up_read(struct rw_semaphore *sem);

/*
 * release a write lock
 */
extern void up_write(struct rw_semaphore *sem);

/*
 * downgrade write lock to read lock
 */
extern void downgrade_write(struct rw_semaphore *sem);
# 14 "./include/linux/mm_types.h" 2
# 1 "./include/linux/completion.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * (C) Copyright 2001 Linus Torvalds
 *
 * Atomic wait-for-completion handler data structures.
 * See kernel/sched/completion.c for details.
 */

# 1 "./include/linux/swait.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./include/linux/swait.h"
/*
 * Simple waitqueues are semantically very different to regular wait queues
 * (wait.h). The most important difference is that the simple waitqueue allows
 * for deterministic behaviour -- IOW it has strictly bounded IRQ and lock hold
 * times.
 *
 * Mainly, this is accomplished by two things. Firstly not allowing swake_up_all
 * from IRQ disabled, and dropping the lock upon every wakeup, giving a higher
 * priority task a chance to run.
 *
 * Secondly, we had to drop a fair number of features of the other waitqueue
 * code; notably:
 *
 *  - mixing INTERRUPTIBLE and UNINTERRUPTIBLE sleeps on the same waitqueue;
 *    all wakeups are TASK_NORMAL in order to avoid O(n) lookups for the right
 *    sleeper state.
 *
 *  - the !exclusive mode; because that leads to O(n) wakeups, everything is
 *    exclusive. As such swake_up_one will only ever awake _one_ waiter.
 *
 *  - custom wake callback functions; because you cannot give any guarantees
 *    about random code. This also allows swait to be used in RT, such that
 *    raw spinlock can be used for the swait queue head.
 *
 * As a side effect of these; the data structures are slimmer albeit more ad-hoc.
 * For all the above, note that simple wait queues should _only_ be used under
 * very specific realtime constraints -- it is best to stick with the regular
 * wait queues in most cases.
 */

struct task_struct;

struct swait_queue_head {
 raw_spinlock_t lock;
 struct list_head task_list;
};

struct swait_queue {
 struct task_struct *task;
 struct list_head task_list;
};
# 69 "./include/linux/swait.h"
extern void __init_swait_queue_head(struct swait_queue_head *q, const char *name,
        struct lock_class_key *key);
# 88 "./include/linux/swait.h"
/**
 * swait_active -- locklessly test for waiters on the queue
 * @wq: the waitqueue to test for waiters
 *
 * returns true if the wait list is not empty
 *
 * NOTE: this function is lockless and requires care, incorrect usage _will_
 * lead to sporadic and non-obvious failure.
 *
 * NOTE2: this function has the same above implications as regular waitqueues.
 *
 * Use either while holding swait_queue_head::lock or when used for wakeups
 * with an extra smp_mb() like:
 *
 *      CPU0 - waker                    CPU1 - waiter
 *
 *                                      for (;;) {
 *      @cond = true;                     prepare_to_swait_exclusive(&wq_head, &wait, state);
 *      smp_mb();                         // smp_mb() from set_current_state()
 *      if (swait_active(wq_head))        if (@cond)
 *        wake_up(wq_head);                      break;
 *                                        schedule();
 *                                      }
 *                                      finish_swait(&wq_head, &wait);
 *
 * Because without the explicit smp_mb() it's possible for the
 * swait_active() load to get hoisted over the @cond store such that we'll
 * observe an empty wait list while the waiter might not observe @cond.
 * This, in turn, can trigger missing wakeups.
 *
 * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
 * which (when the lock is uncontended) are of roughly equal cost.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int swait_active(struct swait_queue_head *wq)
{
 return !list_empty(&wq->task_list);
}

/**
 * swq_has_sleeper - check if there are any waiting processes
 * @wq: the waitqueue to test for waiters
 *
 * Returns true if @wq has waiting processes
 *
 * Please refer to the comment for swait_active.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool swq_has_sleeper(struct swait_queue_head *wq)
{
 /*
	 * We need to be sure we are in sync with the list_add()
	 * modifications to the wait queue (task_list).
	 *
	 * This memory barrier should be paired with one on the
	 * waiting side.
	 */
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 return swait_active(wq);
}

extern void swake_up_one(struct swait_queue_head *q);
extern void swake_up_all(struct swait_queue_head *q);
extern void swake_up_locked(struct swait_queue_head *q);

extern void prepare_to_swait_exclusive(struct swait_queue_head *q, struct swait_queue *wait, int state);
extern long prepare_to_swait_event(struct swait_queue_head *q, struct swait_queue *wait, int state);

extern void __finish_swait(struct swait_queue_head *q, struct swait_queue *wait);
extern void finish_swait(struct swait_queue_head *q, struct swait_queue *wait);

/* as per ___wait_event() but for swait, therefore "exclusive == 1" */
# 235 "./include/linux/swait.h"
/**
 * swait_event_idle_exclusive - wait without system load contribution
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_IDLE) until the @condition evaluates to
 * true. The @condition is checked each time the waitqueue @wq is woken up.
 *
 * This function is mostly used when a kthread or workqueue waits for some
 * condition and doesn't want to contribute to system load. Signals are
 * ignored.
 */
# 259 "./include/linux/swait.h"
/**
 * swait_event_idle_timeout_exclusive - wait up to timeout without load contribution
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout at which we'll give up in jiffies
 *
 * The process is put to sleep (TASK_IDLE) until the @condition evaluates to
 * true. The @condition is checked each time the waitqueue @wq is woken up.
 *
 * This function is mostly used when a kthread or workqueue waits for some
 * condition and doesn't want to contribute to system load. Signals are
 * ignored.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * or the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed.
 */
# 13 "./include/linux/completion.h" 2

/*
 * struct completion - structure used to maintain state for a "completion"
 *
 * This is the opaque structure used to maintain the state for a "completion".
 * Completions currently use a FIFO to queue threads that have to wait for
 * the "completion" event.
 *
 * See also:  complete(), wait_for_completion() (and friends _timeout,
 * _interruptible, _interruptible_timeout, and _killable), init_completion(),
 * reinit_completion(), and macros DECLARE_COMPLETION(),
 * DECLARE_COMPLETION_ONSTACK().
 */
struct completion {
 unsigned int done;
 struct swait_queue_head wait;
};


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void complete_acquire(struct completion *x) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void complete_release(struct completion *x) {}
# 44 "./include/linux/completion.h"
/**
 * DECLARE_COMPLETION - declare and initialize a completion structure
 * @work:  identifier for the completion structure
 *
 * This macro declares and initializes a completion structure. Generally used
 * for static declarations. You should use the _ONSTACK variant for automatic
 * variables.
 */



/*
 * Lockdep needs to run a non-constant initializer for on-stack
 * completions - so we use the _ONSTACK() variant for those that
 * are on the kernel stack:
 */
/**
 * DECLARE_COMPLETION_ONSTACK - declare and initialize a completion structure
 * @work:  identifier for the completion structure
 *
 * This macro declares and initializes a completion structure on the kernel
 * stack.
 */
# 77 "./include/linux/completion.h"
/**
 * init_completion - Initialize a dynamically allocated completion
 * @x:  pointer to completion structure that is to be initialized
 *
 * This inline function will initialize a dynamically created completion
 * structure.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_completion(struct completion *x)
{
 x->done = 0;
 do { static struct lock_class_key __key; __init_swait_queue_head((&x->wait), "&x->wait", &__key); } while (0);
}

/**
 * reinit_completion - reinitialize a completion structure
 * @x:  pointer to completion structure that is to be reinitialized
 *
 * This inline function should be used to reinitialize a completion structure so it can
 * be reused. This is especially important after complete_all() is used.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void reinit_completion(struct completion *x)
{
 x->done = 0;
}

extern void wait_for_completion(struct completion *);
extern void wait_for_completion_io(struct completion *);
extern int wait_for_completion_interruptible(struct completion *x);
extern int wait_for_completion_killable(struct completion *x);
extern int wait_for_completion_state(struct completion *x, unsigned int state);
extern unsigned long wait_for_completion_timeout(struct completion *x,
         unsigned long timeout);
extern unsigned long wait_for_completion_io_timeout(struct completion *x,
          unsigned long timeout);
extern long wait_for_completion_interruptible_timeout(
 struct completion *x, unsigned long timeout);
extern long wait_for_completion_killable_timeout(
 struct completion *x, unsigned long timeout);
extern bool try_wait_for_completion(struct completion *x);
extern bool completion_done(struct completion *x);

extern void complete(struct completion *);
extern void complete_all(struct completion *);
# 15 "./include/linux/mm_types.h" 2

# 1 "./include/linux/uprobes.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */


/*
 * User-space Probes (UProbes)
 *
 * Copyright (C) IBM Corporation, 2008-2012
 * Authors:
 *	Srikar Dronamraju
 *	Jim Keniston
 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra
 */






struct vm_area_struct;
struct mm_struct;
struct inode;
struct notifier_block;
struct page;






enum uprobe_filter_ctx {
 UPROBE_FILTER_REGISTER,
 UPROBE_FILTER_UNREGISTER,
 UPROBE_FILTER_MMAP,
};

struct uprobe_consumer {
 int (*handler)(struct uprobe_consumer *self, struct pt_regs *regs);
 int (*ret_handler)(struct uprobe_consumer *self,
    unsigned long func,
    struct pt_regs *regs);
 bool (*filter)(struct uprobe_consumer *self,
    enum uprobe_filter_ctx ctx,
    struct mm_struct *mm);

 struct uprobe_consumer *next;
};
# 142 "./include/linux/uprobes.h"
struct uprobes_state {
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uprobes_init(void)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)
{
 return -38 /* Invalid system call number */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int uprobe_register_refctr(struct inode *inode, loff_t offset, loff_t ref_ctr_offset, struct uprobe_consumer *uc)
{
 return -38 /* Invalid system call number */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
uprobe_apply(struct inode *inode, loff_t offset, struct uprobe_consumer *uc, bool add)
{
 return -38 /* Invalid system call number */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int uprobe_mmap(struct vm_area_struct *vma)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uprobe_start_dup_mmap(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uprobe_end_dup_mmap(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uprobe_notify_resume(struct pt_regs *regs)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uprobe_deny_signal(void)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uprobe_free_utask(struct task_struct *t)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uprobe_copy_process(struct task_struct *t, unsigned long flags)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uprobe_clear_state(struct mm_struct *mm)
{
}
# 17 "./include/linux/mm_types.h" 2




# 1 "./include/linux/percpu_counter.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/*
 * A simple "approximate counter" for use in ext2 and ext3 superblocks.
 *
 * WARNING: these things are HUGE.  4 kbytes per counter on 32-way P4.
 */
# 17 "./include/linux/percpu_counter.h"
/* percpu_counter batch for local add or sub */




struct percpu_counter {
 raw_spinlock_t lock;
 s64 count;

 struct list_head list; /* All percpu_counters are on a list */

 s32 /* nothing */ *counters;
};

extern int percpu_counter_batch;

int __percpu_counter_init(struct percpu_counter *fbc, s64 amount, gfp_t gfp,
     struct lock_class_key *key);
# 43 "./include/linux/percpu_counter.h"
void percpu_counter_destroy(struct percpu_counter *fbc);
void percpu_counter_set(struct percpu_counter *fbc, s64 amount);
void percpu_counter_add_batch(struct percpu_counter *fbc, s64 amount,
         s32 batch);
s64 __percpu_counter_sum(struct percpu_counter *fbc);
s64 percpu_counter_sum_all(struct percpu_counter *fbc);
int __percpu_counter_compare(struct percpu_counter *fbc, s64 rhs, s32 batch);
void percpu_counter_sync(struct percpu_counter *fbc);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int percpu_counter_compare(struct percpu_counter *fbc, s64 rhs)
{
 return __percpu_counter_compare(fbc, rhs, percpu_counter_batch);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_counter_add(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add_batch(fbc, amount, percpu_counter_batch);
}

/*
 * With percpu_counter_add_local() and percpu_counter_sub_local(), counts
 * are accumulated in local per cpu counter and not in fbc->count until
 * local count overflows PERCPU_COUNTER_LOCAL_BATCH. This makes counter
 * write efficient.
 * But percpu_counter_sum(), instead of percpu_counter_read(), needs to be
 * used to add up the counts from each CPU to account for all the local
 * counts. So percpu_counter_add_local() and percpu_counter_sub_local()
 * should be used when a counter is updated frequently and read rarely.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
percpu_counter_add_local(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add_batch(fbc, amount, ((int)(~0U >> 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 percpu_counter_sum_positive(struct percpu_counter *fbc)
{
 s64 ret = __percpu_counter_sum(fbc);
 return ret < 0 ? 0 : ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 percpu_counter_sum(struct percpu_counter *fbc)
{
 return __percpu_counter_sum(fbc);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 percpu_counter_read(struct percpu_counter *fbc)
{
 return fbc->count;
}

/*
 * It is possible for the percpu_counter_read() to return a small negative
 * number for some counter which should never be negative.
 *
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) s64 percpu_counter_read_positive(struct percpu_counter *fbc)
{
 /* Prevent reloads of fbc->count */
 s64 ret = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_265(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(fbc->count) == sizeof(char) || sizeof(fbc->count) == sizeof(short) || sizeof(fbc->count) == sizeof(int) || sizeof(fbc->count) == sizeof(long)) || sizeof(fbc->count) == sizeof(long long))) __compiletime_assert_265(); } while (0); (*(const volatile typeof( _Generic((fbc->count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (fbc->count))) *)&(fbc->count)); });
# 104 "./include/linux/percpu_counter.h"
 if (ret >= 0)
  return ret;
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_counter_initialized(struct percpu_counter *fbc)
{
 return (fbc->counters != ((void *)0));
}
# 212 "./include/linux/percpu_counter.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_counter_inc(struct percpu_counter *fbc)
{
 percpu_counter_add(fbc, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_counter_dec(struct percpu_counter *fbc)
{
 percpu_counter_add(fbc, -1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_counter_sub(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add(fbc, -amount);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
percpu_counter_sub_local(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add_local(fbc, -amount);
}
# 22 "./include/linux/mm_types.h" 2

# 1 "./arch/arm64/include/asm/mmu.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 20 "./arch/arm64/include/asm/mmu.h"
typedef struct {
 atomic64_t id;

 void *sigpage;

 refcount_t pinned;
 void *vdso;
 unsigned long flags;
} mm_context_t;

/*
 * We use atomic64_read() here because the ASID for an 'mm_struct' can
 * be reallocated when scheduling one of its threads following a
 * rollover event (see new_context() and flush_context()). In this case,
 * a concurrent TLBI (e.g. via try_to_unmap_one() and ptep_clear_flush())
 * may use a stale ASID. This is fine in principle as the new ASID is
 * guaranteed to be clean in the TLB, but the TLBI routines have to take
 * care to handle the following race:
 *
 *    CPU 0                    CPU 1                          CPU 2
 *
 *    // ptep_clear_flush(mm)
 *    xchg_relaxed(pte, 0)
 *    DSB ISHST
 *    old = ASID(mm)
 *         |                                                  <rollover>
 *         |                   new = new_context(mm)
 *         \-----------------> atomic_set(mm->context.id, new)
 *                             cpu_switch_mm(mm)
 *                             // Hardware walk of pte using new ASID
 *    TLBI(old)
 *
 * In this scenario, the barrier on CPU 0 and the dependency on CPU 1
 * ensure that the page-table walker on CPU 1 *must* see the invalid PTE
 * written by CPU 0.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arm64_kernel_unmapped_at_el0(void)
{
 return cpus_have_const_cap(53);
}

extern void arm64_memblock_init(void);
extern void paging_init(void);
extern void bootmem_init(void);
extern void *early_io_map(phys_addr_t phys, unsigned long virt);
extern void init_mem_pgprot(void);
extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
          unsigned long virt, phys_addr_t size,
          pgprot_t prot, bool page_mappings_only);
extern void *fixmap_remap_fdt(phys_addr_t dt_phys, int *size, pgprot_t prot);
extern void mark_linear_text_alias_ro(void);
extern bool kaslr_requires_kpti(void);
# 24 "./include/linux/mm_types.h" 2








struct address_space;
struct mem_cgroup;

/*
 * Each physical page in the system has a struct page associated with
 * it to keep track of whatever it is we are using the page for at the
 * moment. Note that we have no way to track which tasks are using
 * a page, though if it is a pagecache page, rmap structures can tell us
 * who is mapping it.
 *
 * If you allocate the page using alloc_pages(), you can use some of the
 * space in struct page for your own purposes.  The five words in the main
 * union are available, except for bit 0 of the first word which must be
 * kept clear.  Many users use this word to store a pointer to an object
 * which is guaranteed to be aligned.  If you use the same storage as
 * page->mapping, you must restore it to NULL before freeing the page.
 *
 * If your page will not be mapped to userspace, you can also use the four
 * bytes in the mapcount union, but you must call page_mapcount_reset()
 * before freeing it.
 *
 * If you want to use the refcount field, it must be used in such a way
 * that other CPUs temporarily incrementing and then decrementing the
 * refcount does not cause problems.  On receiving the page from
 * alloc_pages(), the refcount will be positive.
 *
 * If you allocate pages of order > 0, you can use some of the fields
 * in each subpage, but you may need to restore some of their values
 * afterwards.
 *
 * SLUB uses cmpxchg_double() to atomically update its freelist and counters.
 * That requires that freelist & counters in struct slab be adjacent and
 * double-word aligned. Because struct slab currently just reinterprets the
 * bits of struct page, we align all struct pages to double-word boundaries,
 * and ensure that 'freelist' is aligned within struct slab.
 */






struct page {
 unsigned long flags; /* Atomic flags, some possibly
					 * updated asynchronously */
 /*
	 * Five words (20/40 bytes) are available in this union.
	 * WARNING: bit 0 of the first word is used for PageTail(). That
	 * means the other users of this union MUST NOT use the bit to
	 * avoid collision and false-positive PageTail().
	 */
 union {
  struct { /* Page cache and anonymous pages */
   /**
			 * @lru: Pageout list, eg. active_list protected by
			 * lruvec->lru_lock.  Sometimes used as a generic list
			 * by the page owner.
			 */
   union {
    struct list_head lru;

    /* Or, for the Unevictable "LRU list" slot */
    struct {
     /* Always even, to negate PageTail */
     void *__filler;
     /* Count page's or folio's mlocks */
     unsigned int mlock_count;
    };

    /* Or, free page */
    struct list_head buddy_list;
    struct list_head pcp_list;
   };
   /* See page-flags.h for PAGE_MAPPING_FLAGS */
   struct address_space *mapping;
   union {
    unsigned long index; /* Our offset within mapping. */
    unsigned long share; /* share count for fsdax */
   };
   /**
			 * @private: Mapping-private opaque data.
			 * Usually used for buffer_heads if PagePrivate.
			 * Used for swp_entry_t if PageSwapCache.
			 * Indicates order in the buddy system if PageBuddy.
			 */
   unsigned long private;
  };
  struct { /* page_pool used by netstack */
   /**
			 * @pp_magic: magic value to avoid recycling non
			 * page_pool allocated pages.
			 */
   unsigned long pp_magic;
   struct page_pool *pp;
   unsigned long _pp_mapping_pad;
   unsigned long dma_addr;
   union {
    /**
				 * dma_addr_upper: might require a 64-bit
				 * value on 32-bit architectures.
				 */
    unsigned long dma_addr_upper;
    /**
				 * For frag page support, not supported in
				 * 32-bit architectures with 64-bit DMA.
				 */
    atomic_long_t pp_frag_count;
   };
  };
  struct { /* Tail pages of compound page */
   unsigned long compound_head; /* Bit zero is set */

   /* First tail page only */
   unsigned char compound_dtor;
   unsigned char compound_order;
   atomic_t compound_mapcount;
   atomic_t subpages_mapcount;
   atomic_t compound_pincount;

   unsigned int compound_nr; /* 1 << compound_order */

  };
  struct { /* Second tail page of transparent huge page */
   unsigned long _compound_pad_1; /* compound_head */
   unsigned long _compound_pad_2;
   /* For both global and memcg */
   struct list_head deferred_list;
  };
  struct { /* Second tail page of hugetlb page */
   unsigned long _hugetlb_pad_1; /* compound_head */
   void *hugetlb_subpool;
   void *hugetlb_cgroup;
   void *hugetlb_cgroup_rsvd;
   void *hugetlb_hwpoison;
   /* No more space on 32-bit: use third tail if more */
  };
  struct { /* Page table pages */
   unsigned long _pt_pad_1; /* compound_head */
   pgtable_t pmd_huge_pte; /* protected by page->ptl */
   unsigned long _pt_pad_2; /* mapping */
   union {
    struct mm_struct *pt_mm; /* x86 pgds only */
    atomic_t pt_frag_refcount; /* powerpc */
   };



   spinlock_t ptl;

  };
  struct { /* ZONE_DEVICE pages */
   /** @pgmap: Points to the hosting device page map. */
   struct dev_pagemap *pgmap;
   void *zone_device_data;
   /*
			 * ZONE_DEVICE private pages are counted as being
			 * mapped so the next 3 words hold the mapping, index,
			 * and private fields from the source anonymous or
			 * page cache page while the page is migrated to device
			 * private memory.
			 * ZONE_DEVICE MEMORY_DEVICE_FS_DAX pages also
			 * use the mapping, index, and private fields when
			 * pmem backed DAX files are mapped.
			 */
  };

  /** @rcu_head: You can use this to free a page by RCU. */
  struct callback_head callback_head;
 };

 union { /* This union is 4 bytes in size. */
  /*
		 * If the page can be mapped to userspace, encodes the number
		 * of times this page is referenced by a page table.
		 */
  atomic_t _mapcount;

  /*
		 * If the page is neither PageSlab nor mappable to userspace,
		 * the value stored here may help determine what this page
		 * is used for.  See page-flags.h for a list of page types
		 * which are currently stored here.
		 */
  unsigned int page_type;
 };

 /* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */
 atomic_t _refcount;


 unsigned long memcg_data;


 /*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 */
# 255 "./include/linux/mm_types.h"
} __attribute__((__aligned__(2 * sizeof(unsigned long))));

/*
 * struct encoded_page - a nonexistent type marking this pointer
 *
 * An 'encoded_page' pointer is a pointer to a regular 'struct page', but
 * with the low bits of the pointer indicating extra context-dependent
 * information. Not super-common, but happens in mmu_gather and mlock
 * handling, and this acts as a type system check on that use.
 *
 * We only really have two guaranteed bits in general, although you could
 * play with 'struct page' alignment (see CONFIG_HAVE_ALIGNED_STRUCT_PAGE)
 * for more.
 *
 * Use the supplied helper functions to endcode/decode the pointer and bits.
 */
struct encoded_page;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct encoded_page *encode_page(struct page *page, unsigned long flags)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_266(void) __attribute__((__error__("BUILD_BUG_ON failed: " "flags > ENCODE_PAGE_BITS"))); if (!(!(flags > 3ul))) __compiletime_assert_266(); } while (0);
# 276 "./include/linux/mm_types.h"
 return (struct encoded_page *)(flags | (unsigned long)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long encoded_page_flags(struct encoded_page *page)
{
 return 3ul & (unsigned long)page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *encoded_page_ptr(struct encoded_page *page)
{
 return (struct page *)(~3ul & (unsigned long)page);
}

/**
 * struct folio - Represents a contiguous set of bytes.
 * @flags: Identical to the page flags.
 * @lru: Least Recently Used list; tracks how recently this folio was used.
 * @mlock_count: Number of times this folio has been pinned by mlock().
 * @mapping: The file this page belongs to, or refers to the anon_vma for
 *    anonymous memory.
 * @index: Offset within the file, in units of pages.  For anonymous memory,
 *    this is the index from the beginning of the mmap.
 * @private: Filesystem per-folio data (see folio_attach_private()).
 *    Used for swp_entry_t if folio_test_swapcache().
 * @_mapcount: Do not access this member directly.  Use folio_mapcount() to
 *    find out how many times this folio is mapped by userspace.
 * @_refcount: Do not access this member directly.  Use folio_ref_count()
 *    to find how many references there are to this folio.
 * @memcg_data: Memory Control Group data.
 * @_flags_1: For large folios, additional page flags.
 * @_head_1: Points to the folio.  Do not use.
 * @_folio_dtor: Which destructor to use for this folio.
 * @_folio_order: Do not use directly, call folio_order().
 * @_compound_mapcount: Do not use directly, call folio_entire_mapcount().
 * @_subpages_mapcount: Do not use directly, call folio_mapcount().
 * @_pincount: Do not use directly, call folio_maybe_dma_pinned().
 * @_folio_nr_pages: Do not use directly, call folio_nr_pages().
 * @_flags_2: For alignment.  Do not use.
 * @_head_2: Points to the folio.  Do not use.
 * @_hugetlb_subpool: Do not use directly, use accessor in hugetlb.h.
 * @_hugetlb_cgroup: Do not use directly, use accessor in hugetlb_cgroup.h.
 * @_hugetlb_cgroup_rsvd: Do not use directly, use accessor in hugetlb_cgroup.h.
 * @_hugetlb_hwpoison: Do not use directly, call raw_hwp_list_head().
 *
 * A folio is a physically, virtually and logically contiguous set
 * of bytes.  It is a power-of-two in size, and it is aligned to that
 * same power-of-two.  It is at least as large as %PAGE_SIZE.  If it is
 * in the page cache, it is at a file offset which is a multiple of that
 * power-of-two.  It may be mapped into userspace at an address which is
 * at an arbitrary page offset, but its kernel virtual address is aligned
 * to its size.
 */
struct folio {
 /* private: don't document the anon union */
 union {
  struct {
 /* public: */
   unsigned long flags;
   union {
    struct list_head lru;
 /* private: avoid cluttering the output */
    struct {
     void *__filler;
 /* public: */
     unsigned int mlock_count;
 /* private: */
    };
 /* public: */
   };
   struct address_space *mapping;
   unsigned long index;
   void *private;
   atomic_t _mapcount;
   atomic_t _refcount;

   unsigned long memcg_data;

 /* private: the union with struct page is transitional */
  };
  struct page page;
 };
 union {
  struct {
   unsigned long _flags_1;
   unsigned long _head_1;
   unsigned char _folio_dtor;
   unsigned char _folio_order;
   atomic_t _compound_mapcount;
   atomic_t _subpages_mapcount;
   atomic_t _pincount;

   unsigned int _folio_nr_pages;

  };
  struct page __page_1;
 };
 union {
  struct {
   unsigned long _flags_2;
   unsigned long _head_2;
   void *_hugetlb_subpool;
   void *_hugetlb_cgroup;
   void *_hugetlb_cgroup_rsvd;
   void *_hugetlb_hwpoison;
  };
  struct page __page_2;
 };
};



_Static_assert(__builtin_offsetof(struct page, flags) == __builtin_offsetof(struct folio, flags), "offsetof(struct page, flags) == offsetof(struct folio, flags)");
_Static_assert(__builtin_offsetof(struct page, lru) == __builtin_offsetof(struct folio, lru), "offsetof(struct page, lru) == offsetof(struct folio, lru)");
_Static_assert(__builtin_offsetof(struct page, mapping) == __builtin_offsetof(struct folio, mapping), "offsetof(struct page, mapping) == offsetof(struct folio, mapping)");
_Static_assert(__builtin_offsetof(struct page, compound_head) == __builtin_offsetof(struct folio, lru), "offsetof(struct page, compound_head) == offsetof(struct folio, lru)");
_Static_assert(__builtin_offsetof(struct page, index) == __builtin_offsetof(struct folio, index), "offsetof(struct page, index) == offsetof(struct folio, index)");
_Static_assert(__builtin_offsetof(struct page, private) == __builtin_offsetof(struct folio, private), "offsetof(struct page, private) == offsetof(struct folio, private)");
_Static_assert(__builtin_offsetof(struct page, _mapcount) == __builtin_offsetof(struct folio, _mapcount), "offsetof(struct page, _mapcount) == offsetof(struct folio, _mapcount)");
_Static_assert(__builtin_offsetof(struct page, _refcount) == __builtin_offsetof(struct folio, _refcount), "offsetof(struct page, _refcount) == offsetof(struct folio, _refcount)");

_Static_assert(__builtin_offsetof(struct page, memcg_data) == __builtin_offsetof(struct folio, memcg_data), "offsetof(struct page, memcg_data) == offsetof(struct folio, memcg_data)");





_Static_assert(__builtin_offsetof(struct folio, _flags_1) == __builtin_offsetof(struct page, flags) + sizeof(struct page), "offsetof(struct folio, _flags_1) == offsetof(struct page, flags) + sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _head_1) == __builtin_offsetof(struct page, compound_head) + sizeof(struct page), "offsetof(struct folio, _head_1) == offsetof(struct page, compound_head) + sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _folio_dtor) == __builtin_offsetof(struct page, compound_dtor) + sizeof(struct page), "offsetof(struct folio, _folio_dtor) == offsetof(struct page, compound_dtor) + sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _folio_order) == __builtin_offsetof(struct page, compound_order) + sizeof(struct page), "offsetof(struct folio, _folio_order) == offsetof(struct page, compound_order) + sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _compound_mapcount) == __builtin_offsetof(struct page, compound_mapcount) + sizeof(struct page), "offsetof(struct folio, _compound_mapcount) == offsetof(struct page, compound_mapcount) + sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _subpages_mapcount) == __builtin_offsetof(struct page, subpages_mapcount) + sizeof(struct page), "offsetof(struct folio, _subpages_mapcount) == offsetof(struct page, subpages_mapcount) + sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _pincount) == __builtin_offsetof(struct page, compound_pincount) + sizeof(struct page), "offsetof(struct folio, _pincount) == offsetof(struct page, compound_pincount) + sizeof(struct page)");

_Static_assert(__builtin_offsetof(struct folio, _folio_nr_pages) == __builtin_offsetof(struct page, compound_nr) + sizeof(struct page), "offsetof(struct folio, _folio_nr_pages) == offsetof(struct page, compound_nr) + sizeof(struct page)");





_Static_assert(__builtin_offsetof(struct folio, _flags_2) == __builtin_offsetof(struct page, flags) + 2 * sizeof(struct page), "offsetof(struct folio, _flags_2) == offsetof(struct page, flags) + 2 * sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _head_2) == __builtin_offsetof(struct page, compound_head) + 2 * sizeof(struct page), "offsetof(struct folio, _head_2) == offsetof(struct page, compound_head) + 2 * sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _hugetlb_subpool) == __builtin_offsetof(struct page, hugetlb_subpool) + 2 * sizeof(struct page), "offsetof(struct folio, _hugetlb_subpool) == offsetof(struct page, hugetlb_subpool) + 2 * sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _hugetlb_cgroup) == __builtin_offsetof(struct page, hugetlb_cgroup) + 2 * sizeof(struct page), "offsetof(struct folio, _hugetlb_cgroup) == offsetof(struct page, hugetlb_cgroup) + 2 * sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _hugetlb_cgroup_rsvd) == __builtin_offsetof(struct page, hugetlb_cgroup_rsvd) + 2 * sizeof(struct page), "offsetof(struct folio, _hugetlb_cgroup_rsvd) == offsetof(struct page, hugetlb_cgroup_rsvd) + 2 * sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _hugetlb_hwpoison) == __builtin_offsetof(struct page, hugetlb_hwpoison) + 2 * sizeof(struct page), "offsetof(struct folio, _hugetlb_hwpoison) == offsetof(struct page, hugetlb_hwpoison) + 2 * sizeof(struct page)");


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) atomic_t *folio_mapcount_ptr(struct folio *folio)
{
 struct page *tail = &folio->page + 1;
 return &tail->compound_mapcount;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) atomic_t *folio_subpages_mapcount_ptr(struct folio *folio)
{
 struct page *tail = &folio->page + 1;
 return &tail->subpages_mapcount;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) atomic_t *compound_mapcount_ptr(struct page *page)
{
 return &page[1].compound_mapcount;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) atomic_t *subpages_mapcount_ptr(struct page *page)
{
 return &page[1].subpages_mapcount;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) atomic_t *compound_pincount_ptr(struct page *page)
{
 return &page[1].compound_pincount;
}

/*
 * Used for sizing the vmemmap region on some architectures
 */





/*
 * page_private can be used on tail pages.  However, PagePrivate is only
 * checked by the VM on the head page.  So page_private on the tail pages
 * should be used for data that's ancillary to the head page (eg attaching
 * buffer heads to tail pages after attaching buffer heads to the head page)
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_page_private(struct page *page, unsigned long private)
{
 page->private = private;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *folio_get_private(struct folio *folio)
{
 return folio->private;
}

struct page_frag_cache {
 void * va;

 __u16 offset;
 __u16 size;



 /* we maintain a pagecount bias, so that we dont dirty cache line
	 * containing page->_refcount every time we allocate a fragment.
	 */
 unsigned int pagecnt_bias;
 bool pfmemalloc;
};

typedef unsigned long vm_flags_t;

/*
 * A region containing a mapping of a non-memory backed file under NOMMU
 * conditions.  These are held in a global tree and are pinned by the VMAs that
 * map parts of them.
 */
struct vm_region {
 struct rb_node vm_rb; /* link in global region tree */
 vm_flags_t vm_flags; /* VMA vm_flags */
 unsigned long vm_start; /* start address of region */
 unsigned long vm_end; /* region initialised to here */
 unsigned long vm_top; /* region allocated to here */
 unsigned long vm_pgoff; /* the offset in vm_file corresponding to vm_start */
 struct file *vm_file; /* the backing file or NULL */

 int vm_usage; /* region usage count (access under nommu_region_sem) */
 bool vm_icache_flushed : 1; /* true if the icache has been flushed for
						* this region */
};
# 520 "./include/linux/mm_types.h"
struct vm_userfaultfd_ctx {};


struct anon_vma_name {
 struct kref kref;
 /* The name needs to be at the end because it is dynamically sized. */
 char name[];
};

/*
 * This struct describes a virtual memory area. There is one of these
 * per VM-area/task. A VM area is any part of the process virtual memory
 * space that has a special rule for the page-fault handlers (ie a shared
 * library, the executable area etc).
 */
struct vm_area_struct {
 /* The first cache line has the info for VMA tree walking. */

 unsigned long vm_start; /* Our start address within vm_mm. */
 unsigned long vm_end; /* The first byte after our end address
					   within vm_mm. */

 struct mm_struct *vm_mm; /* The address space we belong to. */

 /*
	 * Access permissions of this VMA.
	 * See vmf_insert_mixed_prot() for discussion.
	 */
 pgprot_t vm_page_prot;
 unsigned long vm_flags; /* Flags, see mm.h. */

 /*
	 * For areas with an address space and backing store,
	 * linkage into the address_space->i_mmap interval tree.
	 *
	 */
 struct {
  struct rb_node rb;
  unsigned long rb_subtree_last;
 } shared;

 /*
	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
	 * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack
	 * or brk vma (with NULL file) can only be in an anon_vma list.
	 */
 struct list_head anon_vma_chain; /* Serialized by mmap_lock &
					  * page_table_lock */
 struct anon_vma *anon_vma; /* Serialized by page_table_lock */

 /* Function pointers to deal with this struct. */
 const struct vm_operations_struct *vm_ops;

 /* Information about our backing store: */
 unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE
					   units */
 struct file * vm_file; /* File we map to (can be NULL). */
 void * vm_private_data; /* was vm_pte (shared mem) */
# 589 "./include/linux/mm_types.h"
 atomic_long_t swap_readahead_info;





 struct mempolicy *vm_policy; /* NUMA policy for the VMA */

 struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
} ;

struct kioctx_table;
struct mm_struct {
 struct {
  struct maple_tree mm_mt;

  unsigned long (*get_unmapped_area) (struct file *filp,
    unsigned long addr, unsigned long len,
    unsigned long pgoff, unsigned long flags);

  unsigned long mmap_base; /* base of mmap area */
  unsigned long mmap_legacy_base; /* base of mmap area in bottom-up allocations */





  unsigned long task_size; /* size of task vm space */
  pgd_t * pgd;


  /**
		 * @membarrier_state: Flags controlling membarrier behavior.
		 *
		 * This field is close to @pgd to hopefully fit in the same
		 * cache-line, which needs to be touched by switch_mm().
		 */
  atomic_t membarrier_state;


  /**
		 * @mm_users: The number of users including userspace.
		 *
		 * Use mmget()/mmget_not_zero()/mmput() to modify. When this
		 * drops to 0 (i.e. when the task exits and there are no other
		 * temporary reference holders), we also release a reference on
		 * @mm_count (which may then free the &struct mm_struct if
		 * @mm_count also drops to 0).
		 */
  atomic_t mm_users;

  /**
		 * @mm_count: The number of references to &struct mm_struct
		 * (@mm_users count as 1).
		 *
		 * Use mmgrab()/mmdrop() to modify. When this drops to 0, the
		 * &struct mm_struct is freed.
		 */
  atomic_t mm_count;


  atomic_long_t pgtables_bytes; /* PTE page table pages */

  int map_count; /* number of VMAs */

  spinlock_t page_table_lock; /* Protects page tables and some
					     * counters
					     */
  /*
		 * With some kernel config, the current mmap_lock's offset
		 * inside 'mm_struct' is at 0x120, which is very optimal, as
		 * its two hot fields 'count' and 'owner' sit in 2 different
		 * cachelines,  and when mmap_lock is highly contended, both
		 * of the 2 fields will be accessed frequently, current layout
		 * will help to reduce cache bouncing.
		 *
		 * So please be careful with adding new fields before
		 * mmap_lock, which can easily push the 2 fields into one
		 * cacheline.
		 */
  struct rw_semaphore mmap_lock;

  struct list_head mmlist; /* List of maybe swapped mm's.	These
					  * are globally strung together off
					  * init_mm.mmlist, and are protected
					  * by mmlist_lock
					  */


  unsigned long hiwater_rss; /* High-watermark of RSS usage */
  unsigned long hiwater_vm; /* High-water virtual memory usage */

  unsigned long total_vm; /* Total pages mapped */
  unsigned long locked_vm; /* Pages that have PG_mlocked set */
  atomic64_t pinned_vm; /* Refcount permanently increased */
  unsigned long data_vm; /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
  unsigned long exec_vm; /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
  unsigned long stack_vm; /* VM_STACK */
  unsigned long def_flags;

  /**
		 * @write_protect_seq: Locked when any thread is write
		 * protecting pages mapped by this mm to enforce a later COW,
		 * for instance during page table copying for fork().
		 */
  seqcount_t write_protect_seq;

  spinlock_t arg_lock; /* protect the below fields */

  unsigned long start_code, end_code, start_data, end_data;
  unsigned long start_brk, brk, start_stack;
  unsigned long arg_start, arg_end, env_start, env_end;

  unsigned long saved_auxv[(2*(2 /* entries in ARCH_DLINFO */ + 20 /* NEW_AUX_ENT entries in auxiliary table */ + 1))]; /* for /proc/PID/auxv */

  struct percpu_counter rss_stat[NR_MM_COUNTERS];

  struct linux_binfmt *binfmt;

  /* Architecture-specific MM context */
  mm_context_t context;

  unsigned long flags; /* Must use atomic bitops to access */


  spinlock_t ioctx_lock;
  struct kioctx_table /* nothing */ *ioctx_table;


  /*
		 * "owner" points to a task that is regarded as the canonical
		 * user/owner of this mm. All of the following must be true in
		 * order for it to be changed:
		 *
		 * current == mm->owner
		 * current->mm != mm
		 * new_owner->mm == mm
		 * new_owner->alloc_lock is held
		 */
  struct task_struct /* nothing */ *owner;

  struct user_namespace *user_ns;

  /* store ref to file /proc/<pid>/exe symlink points to */
  struct file /* nothing */ *exe_file;

  struct mmu_notifier_subscriptions *notifier_subscriptions;





  /*
		 * numa_next_scan is the next time that PTEs will be remapped
		 * PROT_NONE to trigger NUMA hinting faults; such faults gather
		 * statistics and migrate pages to new nodes if necessary.
		 */
  unsigned long numa_next_scan;

  /* Restart point for scanning and remapping PTEs. */
  unsigned long numa_scan_offset;

  /* numa_scan_seq prevents two threads remapping PTEs. */
  int numa_scan_seq;

  /*
		 * An operation with batched TLB flushing is going on. Anything
		 * that can move process memory needs to flush the TLB when
		 * moving a PROT_NONE mapped page.
		 */
  atomic_t tlb_flush_pending;




  struct uprobes_state uprobes_state;




  atomic_long_t hugetlb_usage;

  struct work_struct async_put_work;





  /*
		 * Represent how many pages of this process are involved in KSM
		 * merging.
		 */
  unsigned long ksm_merging_pages;
  /*
		 * Represent how many pages are checked for ksm merging
		 * including merged and not merged.
		 */
  unsigned long ksm_rmap_items;
# 804 "./include/linux/mm_types.h"
 } ;

 /*
	 * The mm_cpumask needs to be at the end of mm_struct, because it
	 * is dynamically sized based on nr_cpu_ids.
	 */
 unsigned long cpu_bitmap[];
};


extern struct mm_struct init_mm;

/* Pointer magic because the dynamic array size confuses some compilers. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_init_cpumask(struct mm_struct *mm)
{
 unsigned long cpu_bitmap = (unsigned long)mm;

 cpu_bitmap += __builtin_offsetof(struct mm_struct, cpu_bitmap);
 cpumask_clear((struct cpumask *)cpu_bitmap);
}

/* Future-safe accessor for struct mm_struct's cpu_vm_mask. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) cpumask_t *mm_cpumask(struct mm_struct *mm)
{
 return (struct cpumask *)&mm->cpu_bitmap;
}
# 867 "./include/linux/mm_types.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_add_mm(struct mm_struct *mm)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_del_mm(struct mm_struct *mm)
{
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_migrate_mm(struct mm_struct *mm)
{
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_init_mm(struct mm_struct *mm)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_use_mm(struct mm_struct *mm)
{
}



struct vma_iterator {
 struct ma_state mas;
};
# 904 "./include/linux/mm_types.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vma_iter_init(struct vma_iterator *vmi,
  struct mm_struct *mm, unsigned long addr)
{
 vmi->mas.tree = &mm->mm_mt;
 vmi->mas.index = addr;
 vmi->mas.node = ((struct maple_enode *)1UL);
}

struct mmu_gather;
extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm);
extern void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm);
extern void tlb_finish_mmu(struct mmu_gather *tlb);

struct vm_fault;

/**
 * typedef vm_fault_t - Return type for page fault handlers.
 *
 * Page fault handlers return a bitmask of %VM_FAULT values.
 */
typedef unsigned int vm_fault_t;

/**
 * enum vm_fault_reason - Page fault handlers return a bitmask of
 * these values to tell the core VM what happened when handling the
 * fault. Used to decide whether a process gets delivered SIGBUS or
 * just gets major/minor fault counters bumped up.
 *
 * @VM_FAULT_OOM:		Out Of Memory
 * @VM_FAULT_SIGBUS:		Bad access
 * @VM_FAULT_MAJOR:		Page read from storage
 * @VM_FAULT_HWPOISON:		Hit poisoned small page
 * @VM_FAULT_HWPOISON_LARGE:	Hit poisoned large page. Index encoded
 *				in upper bits
 * @VM_FAULT_SIGSEGV:		segmentation fault
 * @VM_FAULT_NOPAGE:		->fault installed the pte, not return page
 * @VM_FAULT_LOCKED:		->fault locked the returned page
 * @VM_FAULT_RETRY:		->fault blocked, must retry
 * @VM_FAULT_FALLBACK:		huge page fault failed, fall back to small
 * @VM_FAULT_DONE_COW:		->fault has fully handled COW
 * @VM_FAULT_NEEDDSYNC:		->fault did not modify page tables and needs
 *				fsync() to complete (for synchronous page faults
 *				in DAX)
 * @VM_FAULT_COMPLETED:		->fault completed, meanwhile mmap lock released
 * @VM_FAULT_HINDEX_MASK:	mask HINDEX value
 *
 */
enum vm_fault_reason {
 VM_FAULT_OOM = ( vm_fault_t)0x000001,
 VM_FAULT_SIGBUS = ( vm_fault_t)0x000002,
 VM_FAULT_MAJOR = ( vm_fault_t)0x000004,
 VM_FAULT_HWPOISON = ( vm_fault_t)0x000010,
 VM_FAULT_HWPOISON_LARGE = ( vm_fault_t)0x000020,
 VM_FAULT_SIGSEGV = ( vm_fault_t)0x000040,
 VM_FAULT_NOPAGE = ( vm_fault_t)0x000100,
 VM_FAULT_LOCKED = ( vm_fault_t)0x000200,
 VM_FAULT_RETRY = ( vm_fault_t)0x000400,
 VM_FAULT_FALLBACK = ( vm_fault_t)0x000800,
 VM_FAULT_DONE_COW = ( vm_fault_t)0x001000,
 VM_FAULT_NEEDDSYNC = ( vm_fault_t)0x002000,
 VM_FAULT_COMPLETED = ( vm_fault_t)0x004000,
 VM_FAULT_HINDEX_MASK = ( vm_fault_t)0x0f0000,
};

/* Encode hstate index for a hwpoisoned large page */
# 990 "./include/linux/mm_types.h"
struct vm_special_mapping {
 const char *name; /* The name, e.g. "[vdso]". */

 /*
	 * If .fault is not provided, this points to a
	 * NULL-terminated array of pages that back the special mapping.
	 *
	 * This must not be NULL unless .fault is provided.
	 */
 struct page **pages;

 /*
	 * If non-NULL, then this is called to resolve page faults
	 * on the special mapping.  If used, .pages is not checked.
	 */
 vm_fault_t (*fault)(const struct vm_special_mapping *sm,
    struct vm_area_struct *vma,
    struct vm_fault *vmf);

 int (*mremap)(const struct vm_special_mapping *sm,
       struct vm_area_struct *new_vma);
};

enum tlb_flush_reason {
 TLB_FLUSH_ON_TASK_SWITCH,
 TLB_REMOTE_SHOOTDOWN,
 TLB_LOCAL_SHOOTDOWN,
 TLB_LOCAL_MM_SHOOTDOWN,
 TLB_REMOTE_SEND_IPI,
 NR_TLB_FLUSH_REASONS,
};

 /*
  * A swap entry has to fit into a "unsigned long", as the entry is hidden
  * in the "index" field of the swapper address space.
  */
typedef struct {
 unsigned long val;
} swp_entry_t;

/**
 * enum fault_flag - Fault flag definitions.
 * @FAULT_FLAG_WRITE: Fault was a write fault.
 * @FAULT_FLAG_MKWRITE: Fault was mkwrite of existing PTE.
 * @FAULT_FLAG_ALLOW_RETRY: Allow to retry the fault if blocked.
 * @FAULT_FLAG_RETRY_NOWAIT: Don't drop mmap_lock and wait when retrying.
 * @FAULT_FLAG_KILLABLE: The fault task is in SIGKILL killable region.
 * @FAULT_FLAG_TRIED: The fault has been tried once.
 * @FAULT_FLAG_USER: The fault originated in userspace.
 * @FAULT_FLAG_REMOTE: The fault is not for current task/mm.
 * @FAULT_FLAG_INSTRUCTION: The fault was during an instruction fetch.
 * @FAULT_FLAG_INTERRUPTIBLE: The fault can be interrupted by non-fatal signals.
 * @FAULT_FLAG_UNSHARE: The fault is an unsharing request to break COW in a
 *                      COW mapping, making sure that an exclusive anon page is
 *                      mapped after the fault.
 * @FAULT_FLAG_ORIG_PTE_VALID: whether the fault has vmf->orig_pte cached.
 *                        We should only access orig_pte if this flag set.
 *
 * About @FAULT_FLAG_ALLOW_RETRY and @FAULT_FLAG_TRIED: we can specify
 * whether we would allow page faults to retry by specifying these two
 * fault flags correctly.  Currently there can be three legal combinations:
 *
 * (a) ALLOW_RETRY and !TRIED:  this means the page fault allows retry, and
 *                              this is the first try
 *
 * (b) ALLOW_RETRY and TRIED:   this means the page fault allows retry, and
 *                              we've already tried at least once
 *
 * (c) !ALLOW_RETRY and !TRIED: this means the page fault does not allow retry
 *
 * The unlisted combination (!ALLOW_RETRY && TRIED) is illegal and should never
 * be used.  Note that page faults can be allowed to retry for multiple times,
 * in which case we'll have an initial fault with flags (a) then later on
 * continuous faults with flags (b).  We should always try to detect pending
 * signals before a retry to make sure the continuous page faults can still be
 * interrupted if necessary.
 *
 * The combination FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE is illegal.
 * FAULT_FLAG_UNSHARE is ignored and treated like an ordinary read fault when
 * applied to mappings that are not COW mappings.
 */
enum fault_flag {
 FAULT_FLAG_WRITE = 1 << 0,
 FAULT_FLAG_MKWRITE = 1 << 1,
 FAULT_FLAG_ALLOW_RETRY = 1 << 2,
 FAULT_FLAG_RETRY_NOWAIT = 1 << 3,
 FAULT_FLAG_KILLABLE = 1 << 4,
 FAULT_FLAG_TRIED = 1 << 5,
 FAULT_FLAG_USER = 1 << 6,
 FAULT_FLAG_REMOTE = 1 << 7,
 FAULT_FLAG_INSTRUCTION = 1 << 8,
 FAULT_FLAG_INTERRUPTIBLE = 1 << 9,
 FAULT_FLAG_UNSHARE = 1 << 10,
 FAULT_FLAG_ORIG_PTE_VALID = 1 << 11,
};

typedef unsigned int zap_flags_t;
# 22 "./include/linux/mmzone.h" 2
# 1 "./include/linux/page-flags.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Macros for manipulating and testing page->flags
 */
# 17 "./include/linux/page-flags.h"
/*
 * Various page->flags bits:
 *
 * PG_reserved is set for special pages. The "struct page" of such a page
 * should in general not be touched (e.g. set dirty) except by its owner.
 * Pages marked as PG_reserved include:
 * - Pages part of the kernel image (including vDSO) and similar (e.g. BIOS,
 *   initrd, HW tables)
 * - Pages reserved or allocated early during boot (before the page allocator
 *   was initialized). This includes (depending on the architecture) the
 *   initial vmemmap, initial page tables, crashkernel, elfcorehdr, and much
 *   much more. Once (if ever) freed, PG_reserved is cleared and they will
 *   be given to the page allocator.
 * - Pages falling into physical memory gaps - not IORESOURCE_SYSRAM. Trying
 *   to read/write these pages might end badly. Don't touch!
 * - The zero page(s)
 * - Pages not added to the page allocator when onlining a section because
 *   they were excluded via the online_page_callback() or because they are
 *   PG_hwpoison.
 * - Pages allocated in the context of kexec/kdump (loaded kernel image,
 *   control pages, vmcoreinfo)
 * - MMIO/DMA pages. Some architectures don't allow to ioremap pages that are
 *   not marked PG_reserved (as they might be in use by somebody else who does
 *   not respect the caching strategy).
 * - Pages part of an offline section (struct pages of offline sections should
 *   not be trusted as they will be initialized when first onlined).
 * - MCA pages on ia64
 * - Pages holding CPU notes for POWER Firmware Assisted Dump
 * - Device memory (e.g. PMEM, DAX, HMM)
 * Some PG_reserved pages will be excluded from the hibernation image.
 * PG_reserved does in general not hinder anybody from dumping or swapping
 * and is no longer required for remap_pfn_range(). ioremap might require it.
 * Consequently, PG_reserved for a page mapped into user space can indicate
 * the zero page, the vDSO, MMIO pages or device memory.
 *
 * The PG_private bitflag is set on pagecache pages if they contain filesystem
 * specific data (which is normally at page->private). It can be used by
 * private allocations for its own usage.
 *
 * During initiation of disk I/O, PG_locked is set. This bit is set before I/O
 * and cleared when writeback _starts_ or when read _completes_. PG_writeback
 * is set before writeback starts and cleared when it finishes.
 *
 * PG_locked also pins a page in pagecache, and blocks truncation of the file
 * while it is held.
 *
 * page_waitqueue(page) is a wait queue of all tasks waiting for the page
 * to become unlocked.
 *
 * PG_swapbacked is set when a page uses swap as a backing storage.  This are
 * usually PageAnon or shmem pages but please note that even anonymous pages
 * might lose their PG_swapbacked flag when they simply can be dropped (e.g. as
 * a result of MADV_FREE).
 *
 * PG_referenced, PG_reclaim are used for page reclaim for anonymous and
 * file-backed pagecache (see mm/vmscan.c).
 *
 * PG_error is set to indicate that an I/O error occurred on this page.
 *
 * PG_arch_1 is an architecture specific page state bit.  The generic code
 * guarantees that this bit is cleared for a page when it first is entered into
 * the page cache.
 *
 * PG_hwpoison indicates that a page got corrupted in hardware and contains
 * data with incorrect ECC bits that triggered a machine check. Accessing is
 * not safe since it may cause another machine check. Don't touch!
 */

/*
 * Don't use the pageflags directly.  Use the PageFoo macros.
 *
 * The page flags field is split into two parts, the main flags area
 * which extends from the low bits upwards, and the fields area which
 * extends from the high bits downwards.
 *
 *  | FIELD | ... | FLAGS |
 *  N-1           ^       0
 *               (NR_PAGEFLAGS)
 *
 * The fields area is reserved for fields mapping zone, node (for NUMA) and
 * SPARSEMEM section (for variants of SPARSEMEM that require section ids like
 * SPARSEMEM_EXTREME with !SPARSEMEM_VMEMMAP).
 */
enum pageflags {
 PG_locked, /* Page is locked. Don't touch. */
 PG_referenced,
 PG_uptodate,
 PG_dirty,
 PG_lru,
 PG_active,
 PG_workingset,
 PG_waiters, /* Page has waiters, check its waitqueue. Must be bit #7 and in the same byte as "PG_locked" */
 PG_error,
 PG_slab,
 PG_owner_priv_1, /* Owner use. If pagecache, fs may use*/
 PG_arch_1,
 PG_reserved,
 PG_private, /* If pagecache, has fs-private data */
 PG_private_2, /* If pagecache, has fs aux data */
 PG_writeback, /* Page is under writeback */
 PG_head, /* A head page */
 PG_mappedtodisk, /* Has blocks allocated on-disk */
 PG_reclaim, /* To be reclaimed asap */
 PG_swapbacked, /* Page is backed by RAM/swap */
 PG_unevictable, /* Page is "unevictable"  */

 PG_mlocked, /* Page is vma mlocked */





 PG_hwpoison, /* hardware poisoned page. Don't touch */






 PG_arch_2,




 __NR_PAGEFLAGS,

 PG_readahead = PG_reclaim,

 /*
	 * Depending on the way an anonymous folio can be mapped into a page
	 * table (e.g., single PMD/PUD/CONT of the head page vs. PTE-mapped
	 * THP), PG_anon_exclusive may be set only for the head page or for
	 * tail pages of an anonymous folio. For now, we only expect it to be
	 * set on tail pages for PTE-mapped THP.
	 */
 PG_anon_exclusive = PG_mappedtodisk,

 /* Filesystems */
 PG_checked = PG_owner_priv_1,

 /* SwapBacked */
 PG_swapcache = PG_owner_priv_1, /* Swap page: swp_entry_t in private */

 /* Two page bits are conscripted by FS-Cache to maintain local caching
	 * state.  These bits are set on pages belonging to the netfs's inodes
	 * when those inodes are being locally cached.
	 */
 PG_fscache = PG_private_2, /* page backed by cache */

 /* XEN */
 /* Pinned in Xen as a read-only pagetable page. */
 PG_pinned = PG_owner_priv_1,
 /* Pinned as part of domain save (see xen_mm_pin_all()). */
 PG_savepinned = PG_dirty,
 /* Has a grant mapping of another (foreign) domain's page. */
 PG_foreign = PG_owner_priv_1,
 /* Remapped by swiotlb-xen. */
 PG_xen_remapped = PG_owner_priv_1,

 /* SLOB */
 PG_slob_free = PG_private,


 /*
	 * Compound pages. Stored in first tail page's flags.
	 * Indicates that at least one subpage is hwpoisoned in the
	 * THP.
	 */
 PG_has_hwpoisoned = PG_error,


 /* non-lru isolated movable page */
 PG_isolated = PG_reclaim,

 /* Only valid for buddy pages. Used to track pages that are reported */
 PG_reported = PG_uptodate,


 /* For self-hosted memmap pages */
 PG_vmemmap_self_hosted = PG_owner_priv_1,

};






extern struct static_key_false hugetlb_optimize_vmemmap_key;

/*
 * Return the real head page struct iff the @page is a fake head page, otherwise
 * return the @page itself. See Documentation/mm/vmemmap_dedup.rst.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) const struct page *page_fixed_fake_head(const struct page *page)
{
 if (!({ bool branch; if (__builtin_types_compatible_p(typeof(*&hugetlb_optimize_vmemmap_key), struct static_key_true)) branch = arch_static_branch_jump(&(&hugetlb_optimize_vmemmap_key)->key, false); else if (__builtin_types_compatible_p(typeof(*&hugetlb_optimize_vmemmap_key), struct static_key_false)) branch = arch_static_branch(&(&hugetlb_optimize_vmemmap_key)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); }))
  return page;

 /*
	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
	 * struct page. The alignment check aims to avoid access the fields (
	 * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)
	 * cold cacheline in some cases.
	 */
 if (((((unsigned long)page) & ((typeof((unsigned long)page))(((1UL) << 12)) - 1)) == 0) &&
     ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags))) {
  /*
		 * We can safely access the field of the @page[1] with PG_head
		 * because the @page is a compound page composed with at least
		 * two contiguous pages.
		 */
  unsigned long head = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_267(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page[1].compound_head) == sizeof(char) || sizeof(page[1].compound_head) == sizeof(short) || sizeof(page[1].compound_head) == sizeof(int) || sizeof(page[1].compound_head) == sizeof(long)) || sizeof(page[1].compound_head) == sizeof(long long))) __compiletime_assert_267(); } while (0); (*(const volatile typeof( _Generic((page[1].compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page[1].compound_head))) *)&(page[1].compound_head)); });
# 231 "./include/linux/page-flags.h"
  if (__builtin_expect(!!(head & 1), 1))
   return (const struct page *)(head - 1);
 }
 return page;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int page_is_fake_head(struct page *page)
{
 return page_fixed_fake_head(page) != page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long _compound_head(const struct page *page)
{
 unsigned long head = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_268(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_268(); } while (0); (*(const volatile typeof( _Generic((page->compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->compound_head))) *)&(page->compound_head)); });
# 252 "./include/linux/page-flags.h"
 if (__builtin_expect(!!(head & 1), 0))
  return head - 1;
 return (unsigned long)page_fixed_fake_head(page);
}



/**
 * page_folio - Converts from page to folio.
 * @p: The page.
 *
 * Every page is part of a folio.  This function cannot be called on a
 * NULL pointer.
 *
 * Context: No reference, nor lock is required on @page.  If the caller
 * does not hold a reference, this call may race with a folio split, so
 * it should re-check the folio still contains this page after gaining
 * a reference on the folio.
 * Return: The folio which contains this page.
 */




/**
 * folio_page - Return a page from a folio.
 * @folio: The folio.
 * @n: The page number to return.
 *
 * @n is relative to the start of the folio.  This function does not
 * check that the page number lies within @folio; the caller is presumed
 * to have a reference to the page.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageTail(struct page *page)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_269(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_269(); } while (0); (*(const volatile typeof( _Generic((page->compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->compound_head))) *)&(page->compound_head)); }) & 1 || page_is_fake_head(page);
# 290 "./include/linux/page-flags.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageCompound(struct page *page)
{
 return ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags)) ||
        ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_270(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_270(); } while (0); (*(const volatile typeof( _Generic((page->compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->compound_head))) *)&(page->compound_head)); }) & 1;
# 296 "./include/linux/page-flags.h"
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PagePoisoned(const struct page *page)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_271(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->flags) == sizeof(char) || sizeof(page->flags) == sizeof(short) || sizeof(page->flags) == sizeof(int) || sizeof(page->flags) == sizeof(long)) || sizeof(page->flags) == sizeof(long long))) __compiletime_assert_271(); } while (0); (*(const volatile typeof( _Generic((page->flags), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->flags))) *)&(page->flags)); }) == -1l;
# 302 "./include/linux/page-flags.h"
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_init_poison(struct page *page, size_t size)
{
}


static unsigned long *folio_flags(struct folio *folio, unsigned n)
{
 struct page *page = &folio->page;

 ((void)(sizeof(( long)(PageTail(page)))));
 ((void)(sizeof(( long)(n > 0 && !((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags))))));
 return &page[n].flags;
}

/*
 * Page flags policies wrt compound pages
 *
 * PF_POISONED_CHECK
 *     check if this struct page poisoned/uninitialized
 *
 * PF_ANY:
 *     the page flag is relevant for small, head and tail pages.
 *
 * PF_HEAD:
 *     for compound page all operations related to the page flag applied to
 *     head page.
 *
 * PF_ONLY_HEAD:
 *     for compound page, callers only ever operate on the head page.
 *
 * PF_NO_TAIL:
 *     modifications of the page flag must be done on small or head pages,
 *     checks can be done on tail pages too.
 *
 * PF_NO_COMPOUND:
 *     the page flag is not relevant for compound pages.
 *
 * PF_SECOND:
 *     the page flag is stored in the first tail page.
 */
# 365 "./include/linux/page-flags.h"
/* Which page is the flag stored in */







/*
 * Macros to create function definitions for page flags
 */
# 470 "./include/linux/page-flags.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_locked(struct folio *folio) { return ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_locked, folio_flags(folio, 0)) : generic_test_bit(PG_locked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageLocked(struct page *page) { return ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_locked, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_locked, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_locked(struct folio *folio) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_locked, folio_flags(folio, 0)) : generic___set_bit(PG_locked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageLocked(struct page *page) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___set_bit(PG_locked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___set_bit(PG_locked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_locked(struct folio *folio) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_locked, folio_flags(folio, 0)) : generic___clear_bit(PG_locked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageLocked(struct page *page) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_locked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_locked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_waiters(struct folio *folio) { return ((__builtin_constant_p(PG_waiters) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_waiters, folio_flags(folio, 0)) : generic_test_bit(PG_waiters, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageWaiters(struct page *page) { return ((__builtin_constant_p(PG_waiters) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_waiters, &({ ((void)(sizeof(( long)(PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_waiters, &({ ((void)(sizeof(( long)(PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_waiters(struct folio *folio) { set_bit(PG_waiters, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageWaiters(struct page *page) { set_bit(PG_waiters, &({ ((void)(sizeof(( long)(PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_waiters(struct folio *folio) { clear_bit(PG_waiters, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageWaiters(struct page *page) { clear_bit(PG_waiters, &({ ((void)(sizeof(( long)(PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_error(struct folio *folio) { return ((__builtin_constant_p(PG_error) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_error, folio_flags(folio, 0)) : generic_test_bit(PG_error, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageError(struct page *page) { return ((__builtin_constant_p(PG_error) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_error, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_error, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_error(struct folio *folio) { set_bit(PG_error, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageError(struct page *page) { set_bit(PG_error, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_error(struct folio *folio) { clear_bit(PG_error, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageError(struct page *page) { clear_bit(PG_error, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_error(struct folio *folio) { return test_and_clear_bit(PG_error, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageError(struct page *page) { return test_and_clear_bit(PG_error, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_referenced(struct folio *folio) { return ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_referenced, folio_flags(folio, 0)) : generic_test_bit(PG_referenced, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageReferenced(struct page *page) { return ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_referenced, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_referenced, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_referenced(struct folio *folio) { set_bit(PG_referenced, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageReferenced(struct page *page) { set_bit(PG_referenced, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_referenced(struct folio *folio) { clear_bit(PG_referenced, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageReferenced(struct page *page) { clear_bit(PG_referenced, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_referenced(struct folio *folio) { return test_and_clear_bit(PG_referenced, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageReferenced(struct page *page) { return test_and_clear_bit(PG_referenced, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_referenced(struct folio *folio) { ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_referenced, folio_flags(folio, 0)) : generic___set_bit(PG_referenced, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageReferenced(struct page *page) { ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? generic___set_bit(PG_referenced, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic___set_bit(PG_referenced, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_dirty(struct folio *folio) { return ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_dirty, folio_flags(folio, 0)) : generic_test_bit(PG_dirty, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageDirty(struct page *page) { return ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_dirty(struct folio *folio) { set_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageDirty(struct page *page) { set_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_dirty(struct folio *folio) { clear_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageDirty(struct page *page) { clear_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_set_dirty(struct folio *folio) { return test_and_set_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestSetPageDirty(struct page *page) { return test_and_set_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_dirty(struct folio *folio) { return test_and_clear_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageDirty(struct page *page) { return test_and_clear_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_dirty(struct folio *folio) { ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_dirty, folio_flags(folio, 0)) : generic___clear_bit(PG_dirty, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageDirty(struct page *page) { ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_dirty, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_lru(struct folio *folio) { return ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_lru, folio_flags(folio, 0)) : generic_test_bit(PG_lru, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageLRU(struct page *page) { return ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_lru, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_lru, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_lru(struct folio *folio) { set_bit(PG_lru, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageLRU(struct page *page) { set_bit(PG_lru, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_lru(struct folio *folio) { clear_bit(PG_lru, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageLRU(struct page *page) { clear_bit(PG_lru, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_lru(struct folio *folio) { ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_lru, folio_flags(folio, 0)) : generic___clear_bit(PG_lru, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageLRU(struct page *page) { ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_lru, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_lru, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_lru(struct folio *folio) { return test_and_clear_bit(PG_lru, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageLRU(struct page *page) { return test_and_clear_bit(PG_lru, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_active(struct folio *folio) { return ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_active, folio_flags(folio, 0)) : generic_test_bit(PG_active, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageActive(struct page *page) { return ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_active, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_active, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_active(struct folio *folio) { set_bit(PG_active, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageActive(struct page *page) { set_bit(PG_active, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_active(struct folio *folio) { clear_bit(PG_active, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageActive(struct page *page) { clear_bit(PG_active, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_active(struct folio *folio) { ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_active, folio_flags(folio, 0)) : generic___clear_bit(PG_active, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageActive(struct page *page) { ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_active, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_active, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_active(struct folio *folio) { return test_and_clear_bit(PG_active, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageActive(struct page *page) { return test_and_clear_bit(PG_active, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_workingset(struct folio *folio) { return ((__builtin_constant_p(PG_workingset) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_workingset, folio_flags(folio, 0)) : generic_test_bit(PG_workingset, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageWorkingset(struct page *page) { return ((__builtin_constant_p(PG_workingset) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_workingset, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_workingset, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_workingset(struct folio *folio) { set_bit(PG_workingset, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageWorkingset(struct page *page) { set_bit(PG_workingset, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_workingset(struct folio *folio) { clear_bit(PG_workingset, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageWorkingset(struct page *page) { clear_bit(PG_workingset, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_workingset(struct folio *folio) { return test_and_clear_bit(PG_workingset, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageWorkingset(struct page *page) { return test_and_clear_bit(PG_workingset, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_slab(struct folio *folio) { return ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_slab, folio_flags(folio, 0)) : generic_test_bit(PG_slab, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageSlab(struct page *page) { return ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_slab, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_slab, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_slab(struct folio *folio) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_slab, folio_flags(folio, 0)) : generic___set_bit(PG_slab, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageSlab(struct page *page) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___set_bit(PG_slab, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___set_bit(PG_slab, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_slab(struct folio *folio) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_slab, folio_flags(folio, 0)) : generic___clear_bit(PG_slab, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageSlab(struct page *page) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_slab, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_slab, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_slob_free(struct folio *folio) { return ((__builtin_constant_p(PG_slob_free) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_slob_free, folio_flags(folio, 0)) : generic_test_bit(PG_slob_free, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageSlobFree(struct page *page) { return ((__builtin_constant_p(PG_slob_free) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_slob_free, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_slob_free, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_slob_free(struct folio *folio) { ((__builtin_constant_p(PG_slob_free) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_slob_free, folio_flags(folio, 0)) : generic___set_bit(PG_slob_free, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageSlobFree(struct page *page) { ((__builtin_constant_p(PG_slob_free) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___set_bit(PG_slob_free, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___set_bit(PG_slob_free, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_slob_free(struct folio *folio) { ((__builtin_constant_p(PG_slob_free) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_slob_free, folio_flags(folio, 0)) : generic___clear_bit(PG_slob_free, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageSlobFree(struct page *page) { ((__builtin_constant_p(PG_slob_free) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_slob_free, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_slob_free, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_checked(struct folio *folio) { return ((__builtin_constant_p(PG_checked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_checked, folio_flags(folio, 0)) : generic_test_bit(PG_checked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageChecked(struct page *page) { return ((__builtin_constant_p(PG_checked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_checked, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_checked, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_checked(struct folio *folio) { set_bit(PG_checked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageChecked(struct page *page) { set_bit(PG_checked, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_checked(struct folio *folio) { clear_bit(PG_checked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageChecked(struct page *page) { clear_bit(PG_checked, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } /* Used by some filesystems */

/* Xen */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_pinned(struct folio *folio) { return ((__builtin_constant_p(PG_pinned) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_pinned, folio_flags(folio, 0)) : generic_test_bit(PG_pinned, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PagePinned(struct page *page) { return ((__builtin_constant_p(PG_pinned) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_pinned, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_pinned, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_pinned(struct folio *folio) { set_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPagePinned(struct page *page) { set_bit(PG_pinned, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_pinned(struct folio *folio) { clear_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPagePinned(struct page *page) { clear_bit(PG_pinned, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_set_pinned(struct folio *folio) { return test_and_set_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestSetPagePinned(struct page *page) { return test_and_set_bit(PG_pinned, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_pinned(struct folio *folio) { return test_and_clear_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPagePinned(struct page *page) { return test_and_clear_bit(PG_pinned, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_savepinned(struct folio *folio) { return ((__builtin_constant_p(PG_savepinned) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_savepinned, folio_flags(folio, 0)) : generic_test_bit(PG_savepinned, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageSavePinned(struct page *page) { return ((__builtin_constant_p(PG_savepinned) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_savepinned, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_savepinned, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_savepinned(struct folio *folio) { set_bit(PG_savepinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageSavePinned(struct page *page) { set_bit(PG_savepinned, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_savepinned(struct folio *folio) { clear_bit(PG_savepinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageSavePinned(struct page *page) { clear_bit(PG_savepinned, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_foreign(struct folio *folio) { return ((__builtin_constant_p(PG_foreign) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_foreign, folio_flags(folio, 0)) : generic_test_bit(PG_foreign, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageForeign(struct page *page) { return ((__builtin_constant_p(PG_foreign) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_foreign, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_foreign, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_foreign(struct folio *folio) { set_bit(PG_foreign, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageForeign(struct page *page) { set_bit(PG_foreign, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_foreign(struct folio *folio) { clear_bit(PG_foreign, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageForeign(struct page *page) { clear_bit(PG_foreign, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_xen_remapped(struct folio *folio) { return ((__builtin_constant_p(PG_xen_remapped) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_xen_remapped, folio_flags(folio, 0)) : generic_test_bit(PG_xen_remapped, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageXenRemapped(struct page *page) { return ((__builtin_constant_p(PG_xen_remapped) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_xen_remapped, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_xen_remapped, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_xen_remapped(struct folio *folio) { set_bit(PG_xen_remapped, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageXenRemapped(struct page *page) { set_bit(PG_xen_remapped, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_xen_remapped(struct folio *folio) { clear_bit(PG_xen_remapped, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageXenRemapped(struct page *page) { clear_bit(PG_xen_remapped, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_xen_remapped(struct folio *folio) { return test_and_clear_bit(PG_xen_remapped, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageXenRemapped(struct page *page) { return test_and_clear_bit(PG_xen_remapped, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_reserved(struct folio *folio) { return ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_reserved, folio_flags(folio, 0)) : generic_test_bit(PG_reserved, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageReserved(struct page *page) { return ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_reserved, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_reserved, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_reserved(struct folio *folio) { set_bit(PG_reserved, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageReserved(struct page *page) { set_bit(PG_reserved, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_reserved(struct folio *folio) { clear_bit(PG_reserved, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageReserved(struct page *page) { clear_bit(PG_reserved, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_reserved(struct folio *folio) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_reserved, folio_flags(folio, 0)) : generic___clear_bit(PG_reserved, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageReserved(struct page *page) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? generic___clear_bit(PG_reserved, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic___clear_bit(PG_reserved, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_reserved(struct folio *folio) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_reserved, folio_flags(folio, 0)) : generic___set_bit(PG_reserved, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageReserved(struct page *page) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? generic___set_bit(PG_reserved, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic___set_bit(PG_reserved, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_swapbacked(struct folio *folio) { return ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_swapbacked, folio_flags(folio, 0)) : generic_test_bit(PG_swapbacked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageSwapBacked(struct page *page) { return ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_swapbacked(struct folio *folio) { set_bit(PG_swapbacked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageSwapBacked(struct page *page) { set_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_swapbacked(struct folio *folio) { clear_bit(PG_swapbacked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageSwapBacked(struct page *page) { clear_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_swapbacked(struct folio *folio) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_swapbacked, folio_flags(folio, 0)) : generic___clear_bit(PG_swapbacked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageSwapBacked(struct page *page) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_swapbacked(struct folio *folio) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_swapbacked, folio_flags(folio, 0)) : generic___set_bit(PG_swapbacked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageSwapBacked(struct page *page) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___set_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___set_bit(PG_swapbacked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); }

/*
 * Private page markings that may be used by the filesystem that owns the page
 * for its own purposes.
 * - PG_private and PG_private_2 cause release_folio() and co to be invoked
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_private(struct folio *folio) { return ((__builtin_constant_p(PG_private) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_private, folio_flags(folio, 0)) : generic_test_bit(PG_private, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PagePrivate(struct page *page) { return ((__builtin_constant_p(PG_private) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? const_test_bit(PG_private, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic_test_bit(PG_private, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_private(struct folio *folio) { set_bit(PG_private, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPagePrivate(struct page *page) { set_bit(PG_private, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_private(struct folio *folio) { clear_bit(PG_private, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPagePrivate(struct page *page) { clear_bit(PG_private, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_private_2(struct folio *folio) { return ((__builtin_constant_p(PG_private_2) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_private_2, folio_flags(folio, 0)) : generic_test_bit(PG_private_2, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PagePrivate2(struct page *page) { return ((__builtin_constant_p(PG_private_2) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? const_test_bit(PG_private_2, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic_test_bit(PG_private_2, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_private_2(struct folio *folio) { set_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPagePrivate2(struct page *page) { set_bit(PG_private_2, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_private_2(struct folio *folio) { clear_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPagePrivate2(struct page *page) { clear_bit(PG_private_2, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_set_private_2(struct folio *folio) { return test_and_set_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestSetPagePrivate2(struct page *page) { return test_and_set_bit(PG_private_2, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_private_2(struct folio *folio) { return test_and_clear_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPagePrivate2(struct page *page) { return test_and_clear_bit(PG_private_2, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_owner_priv_1(struct folio *folio) { return ((__builtin_constant_p(PG_owner_priv_1) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_owner_priv_1, folio_flags(folio, 0)) : generic_test_bit(PG_owner_priv_1, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageOwnerPriv1(struct page *page) { return ((__builtin_constant_p(PG_owner_priv_1) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? const_test_bit(PG_owner_priv_1, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic_test_bit(PG_owner_priv_1, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_owner_priv_1(struct folio *folio) { set_bit(PG_owner_priv_1, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageOwnerPriv1(struct page *page) { set_bit(PG_owner_priv_1, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_owner_priv_1(struct folio *folio) { clear_bit(PG_owner_priv_1, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageOwnerPriv1(struct page *page) { clear_bit(PG_owner_priv_1, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_owner_priv_1(struct folio *folio) { return test_and_clear_bit(PG_owner_priv_1, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageOwnerPriv1(struct page *page) { return test_and_clear_bit(PG_owner_priv_1, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }

/*
 * Only test-and-set exist for PG_writeback.  The unconditional operators are
 * risky: they bypass page accounting.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_writeback(struct folio *folio) { return ((__builtin_constant_p(PG_writeback) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_writeback, folio_flags(folio, 0)) : generic_test_bit(PG_writeback, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageWriteback(struct page *page) { return ((__builtin_constant_p(PG_writeback) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_writeback, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_writeback, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_set_writeback(struct folio *folio) { return test_and_set_bit(PG_writeback, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestSetPageWriteback(struct page *page) { return test_and_set_bit(PG_writeback, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_writeback(struct folio *folio) { return test_and_clear_bit(PG_writeback, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageWriteback(struct page *page) { return test_and_clear_bit(PG_writeback, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_mappedtodisk(struct folio *folio) { return ((__builtin_constant_p(PG_mappedtodisk) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_mappedtodisk, folio_flags(folio, 0)) : generic_test_bit(PG_mappedtodisk, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageMappedToDisk(struct page *page) { return ((__builtin_constant_p(PG_mappedtodisk) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_mappedtodisk, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_mappedtodisk, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_mappedtodisk(struct folio *folio) { set_bit(PG_mappedtodisk, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageMappedToDisk(struct page *page) { set_bit(PG_mappedtodisk, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_mappedtodisk(struct folio *folio) { clear_bit(PG_mappedtodisk, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageMappedToDisk(struct page *page) { clear_bit(PG_mappedtodisk, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }

/* PG_readahead is only used for reads; PG_reclaim is only for writes */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_reclaim(struct folio *folio) { return ((__builtin_constant_p(PG_reclaim) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_reclaim, folio_flags(folio, 0)) : generic_test_bit(PG_reclaim, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageReclaim(struct page *page) { return ((__builtin_constant_p(PG_reclaim) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_reclaim, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_reclaim, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_reclaim(struct folio *folio) { set_bit(PG_reclaim, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageReclaim(struct page *page) { set_bit(PG_reclaim, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_reclaim(struct folio *folio) { clear_bit(PG_reclaim, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageReclaim(struct page *page) { clear_bit(PG_reclaim, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_reclaim(struct folio *folio) { return test_and_clear_bit(PG_reclaim, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageReclaim(struct page *page) { return test_and_clear_bit(PG_reclaim, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_readahead(struct folio *folio) { return ((__builtin_constant_p(PG_readahead) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_readahead, folio_flags(folio, 0)) : generic_test_bit(PG_readahead, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageReadahead(struct page *page) { return ((__builtin_constant_p(PG_readahead) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_readahead, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_readahead, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_readahead(struct folio *folio) { set_bit(PG_readahead, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageReadahead(struct page *page) { set_bit(PG_readahead, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_readahead(struct folio *folio) { clear_bit(PG_readahead, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageReadahead(struct page *page) { clear_bit(PG_readahead, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_readahead(struct folio *folio) { return test_and_clear_bit(PG_readahead, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageReadahead(struct page *page) { return test_and_clear_bit(PG_readahead, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags); }
# 534 "./include/linux/page-flags.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_highmem(const struct folio *folio) { return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PageHighMem(const struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_set_highmem(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void SetPageHighMem(struct page *page) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_clear_highmem(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ClearPageHighMem(struct page *page) { }



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_swapcache(struct folio *folio)
{
 return folio_test_swapbacked(folio) &&
   ((__builtin_constant_p(PG_swapcache) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_swapcache, folio_flags(folio, 0)) : generic_test_bit(PG_swapcache, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool PageSwapCache(struct page *page)
{
 return folio_test_swapcache((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_swapcache(struct folio *folio) { set_bit(PG_swapcache, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageSwapCache(struct page *page) { set_bit(PG_swapcache, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_swapcache(struct folio *folio) { clear_bit(PG_swapcache, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageSwapCache(struct page *page) { clear_bit(PG_swapcache, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_unevictable(struct folio *folio) { return ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_unevictable, folio_flags(folio, 0)) : generic_test_bit(PG_unevictable, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageUnevictable(struct page *page) { return ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_unevictable, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_unevictable, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_unevictable(struct folio *folio) { set_bit(PG_unevictable, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageUnevictable(struct page *page) { set_bit(PG_unevictable, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_unevictable(struct folio *folio) { clear_bit(PG_unevictable, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageUnevictable(struct page *page) { clear_bit(PG_unevictable, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_unevictable(struct folio *folio) { ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_unevictable, folio_flags(folio, 0)) : generic___clear_bit(PG_unevictable, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageUnevictable(struct page *page) { ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_unevictable, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_unevictable, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_unevictable(struct folio *folio) { return test_and_clear_bit(PG_unevictable, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageUnevictable(struct page *page) { return test_and_clear_bit(PG_unevictable, &({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); })->flags); }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_mlocked(struct folio *folio) { return ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_mlocked, folio_flags(folio, 0)) : generic_test_bit(PG_mlocked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageMlocked(struct page *page) { return ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_mlocked, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_mlocked, &({ ((void)(sizeof(( long)(0 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_mlocked(struct folio *folio) { set_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageMlocked(struct page *page) { set_bit(PG_mlocked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_mlocked(struct folio *folio) { clear_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageMlocked(struct page *page) { clear_bit(PG_mlocked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_mlocked(struct folio *folio) { ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_mlocked, folio_flags(folio, 0)) : generic___clear_bit(PG_mlocked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageMlocked(struct page *page) { ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_mlocked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_mlocked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_set_mlocked(struct folio *folio) { return test_and_set_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestSetPageMlocked(struct page *page) { return test_and_set_bit(PG_mlocked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_mlocked(struct folio *folio) { return test_and_clear_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageMlocked(struct page *page) { return test_and_clear_bit(PG_mlocked, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }
# 571 "./include/linux/page-flags.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_uncached(const struct folio *folio) { return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PageUncached(const struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_set_uncached(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void SetPageUncached(struct page *page) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_clear_uncached(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ClearPageUncached(struct page *page) { }



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_hwpoison(struct folio *folio) { return ((__builtin_constant_p(PG_hwpoison) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_hwpoison, folio_flags(folio, 0)) : generic_test_bit(PG_hwpoison, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageHWPoison(struct page *page) { return ((__builtin_constant_p(PG_hwpoison) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? const_test_bit(PG_hwpoison, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic_test_bit(PG_hwpoison, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_hwpoison(struct folio *folio) { set_bit(PG_hwpoison, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageHWPoison(struct page *page) { set_bit(PG_hwpoison, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_hwpoison(struct folio *folio) { clear_bit(PG_hwpoison, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageHWPoison(struct page *page) { clear_bit(PG_hwpoison, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_set_hwpoison(struct folio *folio) { return test_and_set_bit(PG_hwpoison, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestSetPageHWPoison(struct page *page) { return test_and_set_bit(PG_hwpoison, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_hwpoison(struct folio *folio) { return test_and_clear_bit(PG_hwpoison, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageHWPoison(struct page *page) { return test_and_clear_bit(PG_hwpoison, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }


extern void SetPageHWPoisonTakenOff(struct page *page);
extern void ClearPageHWPoisonTakenOff(struct page *page);
extern bool take_page_off_buddy(struct page *page);
extern bool put_page_back_buddy(struct page *page);
# 598 "./include/linux/page-flags.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_skip_kasan_poison(const struct folio *folio) { return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PageSkipKASanPoison(const struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_set_skip_kasan_poison(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void SetPageSkipKASanPoison(struct page *page) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_clear_skip_kasan_poison(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ClearPageSkipKASanPoison(struct page *page) { }


/*
 * PageReported() is used to track reported free pages within the Buddy
 * allocator. We can use the non-atomic version of the test and set
 * operations as both should be shielded with the zone lock to prevent
 * any possible races on the setting or clearing of the bit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_reported(struct folio *folio) { return ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_reported, folio_flags(folio, 0)) : generic_test_bit(PG_reported, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageReported(struct page *page) { return ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? const_test_bit(PG_reported, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic_test_bit(PG_reported, &({ ((void)(sizeof(( long)(0 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_reported(struct folio *folio) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_reported, folio_flags(folio, 0)) : generic___set_bit(PG_reported, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageReported(struct page *page) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? generic___set_bit(PG_reported, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic___set_bit(PG_reported, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_reported(struct folio *folio) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_reported, folio_flags(folio, 0)) : generic___clear_bit(PG_reported, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageReported(struct page *page) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags))) ? generic___clear_bit(PG_reported, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags) : generic___clear_bit(PG_reported, &({ ((void)(sizeof(( long)(1 && PageCompound(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; }); })->flags)); }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_vmemmap_self_hosted(struct folio *folio) { return ((__builtin_constant_p(PG_vmemmap_self_hosted) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0)) : generic_test_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageVmemmapSelfHosted(struct page *page) { return ((__builtin_constant_p(PG_vmemmap_self_hosted) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? const_test_bit(PG_vmemmap_self_hosted, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic_test_bit(PG_vmemmap_self_hosted, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_vmemmap_self_hosted(struct folio *folio) { set_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageVmemmapSelfHosted(struct page *page) { set_bit(PG_vmemmap_self_hosted, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_vmemmap_self_hosted(struct folio *folio) { clear_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageVmemmapSelfHosted(struct page *page) { clear_bit(PG_vmemmap_self_hosted, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }




/*
 * On an anonymous page mapped into a user virtual memory area,
 * page->mapping points to its anon_vma, not to a struct address_space;
 * with the PAGE_MAPPING_ANON bit set to distinguish it.  See rmap.h.
 *
 * On an anonymous page in a VM_MERGEABLE area, if CONFIG_KSM is enabled,
 * the PAGE_MAPPING_MOVABLE bit may be set along with the PAGE_MAPPING_ANON
 * bit; and then page->mapping points, not to an anon_vma, but to a private
 * structure which KSM associates with that merged page.  See ksm.h.
 *
 * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is used for non-lru movable
 * page and then page->mapping points to a struct movable_operations.
 *
 * Please note that, confusingly, "page_mapping" refers to the inode
 * address_space which maps the page from disk; whereas "page_mapped"
 * refers to user virtual address space into which the page is mapped.
 */





/*
 * Different with flags above, this flag is used only for fsdax mode.  It
 * indicates that this page->mapping is now under reflink case.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_mapping_flags(struct folio *folio)
{
 return ((unsigned long)folio->mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageMappingFlags(struct page *page)
{
 return ((unsigned long)page->mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_anon(struct folio *folio)
{
 return ((unsigned long)folio->mapping & 0x1) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool PageAnon(struct page *page)
{
 return folio_test_anon((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool __folio_test_movable(const struct folio *folio)
{
 return ((unsigned long)folio->mapping & (0x1 | 0x2)) ==
   0x2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __PageMovable(struct page *page)
{
 return ((unsigned long)page->mapping & (0x1 | 0x2)) ==
    0x2;
}


/*
 * A KSM page is one of those write-protected "shared pages" or "merged pages"
 * which KSM maps into multiple mms, wherever identical anonymous page content
 * is found in VM_MERGEABLE vmas.  It's a PageAnon page, pointing not to any
 * anon_vma, but to that page's node of the stable tree.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_ksm(struct folio *folio)
{
 return ((unsigned long)folio->mapping & (0x1 | 0x2)) ==
    (0x1 | 0x2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool PageKsm(struct page *page)
{
 return folio_test_ksm((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}




u64 stable_page_flags(struct page *page);

/**
 * folio_test_uptodate - Is this folio up to date?
 * @folio: The folio.
 *
 * The uptodate flag is set on a folio when every byte in the folio is
 * at least as new as the corresponding bytes on storage.  Anonymous
 * and CoW folios are always uptodate.  If the folio is not uptodate,
 * some of the bytes in it may be; see the is_partially_uptodate()
 * address_space operation.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_uptodate(struct folio *folio)
{
 bool ret = ((__builtin_constant_p(PG_uptodate) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_uptodate, folio_flags(folio, 0)) : generic_test_bit(PG_uptodate, folio_flags(folio, 0)));
 /*
	 * Must ensure that the data we read out of the folio is loaded
	 * _after_ we've loaded folio->flags to check the uptodate bit.
	 * We can skip the barrier if the folio is not uptodate, because
	 * we wouldn't be reading anything from it.
	 *
	 * See folio_mark_uptodate() for the other side of the story.
	 */
 if (ret)
  do { do { } while (0); asm volatile("dmb " "ishld" : : : "memory"); } while (0);

 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PageUptodate(struct page *page)
{
 return folio_test_uptodate((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_mark_uptodate(struct folio *folio)
{
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0);
 ((__builtin_constant_p(PG_uptodate) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_uptodate, folio_flags(folio, 0)) : generic___set_bit(PG_uptodate, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_mark_uptodate(struct folio *folio)
{
 /*
	 * Memory barrier must be issued before setting the PG_uptodate bit,
	 * so that all previous stores issued in order to bring the folio
	 * uptodate are actually visible before folio_test_uptodate becomes true.
	 */
 do { do { } while (0); asm volatile("dmb " "ishst" : : : "memory"); } while (0);
 set_bit(PG_uptodate, folio_flags(folio, 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageUptodate(struct page *page)
{
 __folio_mark_uptodate((struct folio *)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageUptodate(struct page *page)
{
 folio_mark_uptodate((struct folio *)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_uptodate(struct folio *folio) { clear_bit(PG_uptodate, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageUptodate(struct page *page) { clear_bit(PG_uptodate, &({ ((void)(sizeof(( long)(1 && PageTail(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(((typeof(page))_compound_head(page))))))); ((typeof(page))_compound_head(page)); }); })->flags); }

bool __folio_start_writeback(struct folio *folio, bool keep_write);
bool set_page_writeback(struct page *page);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_page_writeback_keepwrite(struct page *page)
{
 __folio_start_writeback((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))), true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool test_set_page_writeback(struct page *page)
{
 return set_page_writeback(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_head(struct folio *folio)
{
 return ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_head, folio_flags(folio, 0)) : generic_test_bit(PG_head, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageHead(struct page *page)
{
 ({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; });
 return ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags)) && !page_is_fake_head(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_set_head(struct folio *folio) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_head, folio_flags(folio, 0)) : generic___set_bit(PG_head, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageHead(struct page *page) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? generic___set_bit(PG_head, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic___set_bit(PG_head, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __folio_clear_head(struct folio *folio) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_head, folio_flags(folio, 0)) : generic___clear_bit(PG_head, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageHead(struct page *page) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? generic___clear_bit(PG_head, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic___clear_bit(PG_head, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_head(struct folio *folio) { clear_bit(PG_head, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageHead(struct page *page) { clear_bit(PG_head, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); }

/**
 * folio_test_large() - Does this folio contain more than one page?
 * @folio: The folio to test.
 *
 * Return: True if the folio is larger than one page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_large(struct folio *folio)
{
 return folio_test_head(folio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void set_compound_head(struct page *page, struct page *head)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_272(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_272(); } while (0); do { *(volatile typeof(page->compound_head) *)&(page->compound_head) = ((unsigned long)head + 1); } while (0); } while (0);
# 806 "./include/linux/page-flags.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void clear_compound_head(struct page *page)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_273(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_273(); } while (0); do { *(volatile typeof(page->compound_head) *)&(page->compound_head) = (0); } while (0); } while (0);
# 811 "./include/linux/page-flags.h"
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ClearPageCompound(struct page *page)
{
 do { if (__builtin_expect(!!(!PageHead(page)), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/page-flags.h\"; .popsection; .long 14472b - .; .short 816; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 ClearPageHead(page);
}





int PageHuge(struct page *page);
int PageHeadHuge(struct page *page);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_hugetlb(struct folio *folio)
{
 return PageHeadHuge(&folio->page);
}






/*
 * PageHuge() only returns true for hugetlbfs pages, but not for
 * normal or transparent huge pages.
 *
 * PageTransHuge() returns true for both transparent huge and
 * hugetlbfs pages, but not normal pages. PageTransHuge() can only be
 * called only in the core VM paths where hugetlbfs pages can't exist.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PageTransHuge(struct page *page)
{
 ((void)(sizeof(( long)(PageTail(page)))));
 return PageHead(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_transhuge(struct folio *folio)
{
 return folio_test_head(folio);
}

/*
 * PageTransCompound returns true for both transparent huge pages
 * and hugetlbfs pages, so it should only be called when it's known
 * that hugetlbfs pages aren't involved.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PageTransCompound(struct page *page)
{
 return PageCompound(page);
}

/*
 * PageTransTail returns true for both transparent huge pages
 * and hugetlbfs pages, so it should only be called when it's known
 * that hugetlbfs pages aren't involved.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int PageTransTail(struct page *page)
{
 return PageTail(page);
}
# 882 "./include/linux/page-flags.h"
/*
 * PageHasHWPoisoned indicates that at least one subpage is hwpoisoned in the
 * compound page.
 *
 * This flag is set by hwpoison handler.  Cleared by THP split or free page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_has_hwpoisoned(struct folio *folio) { return ((__builtin_constant_p(PG_has_hwpoisoned) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 1)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 1)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 1)))) ? const_test_bit(PG_has_hwpoisoned, folio_flags(folio, 1)) : generic_test_bit(PG_has_hwpoisoned, folio_flags(folio, 1))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageHasHWPoisoned(struct page *page) { return ((__builtin_constant_p(PG_has_hwpoisoned) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags))) ? const_test_bit(PG_has_hwpoisoned, &({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags) : generic_test_bit(PG_has_hwpoisoned, &({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_has_hwpoisoned(struct folio *folio) { set_bit(PG_has_hwpoisoned, folio_flags(folio, 1)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageHasHWPoisoned(struct page *page) { set_bit(PG_has_hwpoisoned, &({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_has_hwpoisoned(struct folio *folio) { clear_bit(PG_has_hwpoisoned, folio_flags(folio, 1)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageHasHWPoisoned(struct page *page) { clear_bit(PG_has_hwpoisoned, &({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_set_has_hwpoisoned(struct folio *folio) { return test_and_set_bit(PG_has_hwpoisoned, folio_flags(folio, 1)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestSetPageHasHWPoisoned(struct page *page) { return test_and_set_bit(PG_has_hwpoisoned, &({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_clear_has_hwpoisoned(struct folio *folio) { return test_and_clear_bit(PG_has_hwpoisoned, folio_flags(folio, 1)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int TestClearPageHasHWPoisoned(struct page *page) { return test_and_clear_bit(PG_has_hwpoisoned, &({ ((void)(sizeof(( long)(!PageHead(page))))); ({ ((void)(sizeof(( long)(PagePoisoned(&page[1]))))); &page[1]; }); })->flags); }





/*
 * Check if a page is currently marked HWPoisoned. Note that this check is
 * best effort only and inherently racy: there is no way to synchronize with
 * failing hardware.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_page_hwpoison(struct page *page)
{
 if (PageHWPoison(page))
  return true;
 return PageHuge(page) && PageHWPoison(((typeof(page))_compound_head(page)));
}

/*
 * For pages that are never mapped to userspace (and aren't PageSlab),
 * page_type may be used.  Because it is initialised to -1, we invert the
 * sense of the bit, so __SetPageFoo *clears* the bit used for PageFoo, and
 * __ClearPageFoo *sets* the bit used for PageFoo.  We reserve a few high and
 * low bits so that an underflow or overflow of page_mapcount() won't be
 * mistaken for a page type value.
 */


/* Reserve		0x0000007f to catch underflows of page_mapcount */
# 927 "./include/linux/page-flags.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_has_type(struct page *page)
{
 return (int)page->page_type < -128;
}
# 948 "./include/linux/page-flags.h"
/*
 * PageBuddy() indicates that the page is free and in the buddy system
 * (see mm/page_alloc.c).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageBuddy(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000080)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageBuddy(struct page *page) { ((void)(sizeof(( long)(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000))))); page->page_type &= ~0x00000080; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageBuddy(struct page *page) { ((void)(sizeof(( long)(!PageBuddy(page))))); page->page_type |= 0x00000080; }

/*
 * PageOffline() indicates that the page is logically offline although the
 * containing section is online. (e.g. inflated in a balloon driver or
 * not onlined when onlining the section).
 * The content of these pages is effectively stale. Such pages should not
 * be touched (read/write/dump/save) except by their owner.
 *
 * If a driver wants to allow to offline unmovable PageOffline() pages without
 * putting them back to the buddy, it can do so via the memory notifier by
 * decrementing the reference count in MEM_GOING_OFFLINE and incrementing the
 * reference count in MEM_CANCEL_OFFLINE. When offlining, the PageOffline()
 * pages (now with a reference count of zero) are treated like free pages,
 * allowing the containing memory block to get offlined. A driver that
 * relies on this feature is aware that re-onlining the memory block will
 * require to re-set the pages PageOffline() and not giving them to the
 * buddy via online_page_callback_t.
 *
 * There are drivers that mark a page PageOffline() and expect there won't be
 * any further access to page content. PFN walkers that read content of random
 * pages should check PageOffline() and synchronize with such drivers using
 * page_offline_freeze()/page_offline_thaw().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageOffline(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000100)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageOffline(struct page *page) { ((void)(sizeof(( long)(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000))))); page->page_type &= ~0x00000100; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageOffline(struct page *page) { ((void)(sizeof(( long)(!PageOffline(page))))); page->page_type |= 0x00000100; }

extern void page_offline_freeze(void);
extern void page_offline_thaw(void);
extern void page_offline_begin(void);
extern void page_offline_end(void);

/*
 * Marks pages in use as page tables.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageTable(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000200)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageTable(struct page *page) { ((void)(sizeof(( long)(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000))))); page->page_type &= ~0x00000200; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageTable(struct page *page) { ((void)(sizeof(( long)(!PageTable(page))))); page->page_type |= 0x00000200; }

/*
 * Marks guardpages used with debug_pagealloc.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageGuard(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000400)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __SetPageGuard(struct page *page) { ((void)(sizeof(( long)(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000))))); page->page_type &= ~0x00000400; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageGuard(struct page *page) { ((void)(sizeof(( long)(!PageGuard(page))))); page->page_type |= 0x00000400; }

extern bool is_free_buddy_page(struct page *page);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool folio_test_isolated(struct folio *folio) { return ((__builtin_constant_p(PG_isolated) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_isolated, folio_flags(folio, 0)) : generic_test_bit(PG_isolated, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageIsolated(struct page *page) { return ((__builtin_constant_p(PG_isolated) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? const_test_bit(PG_isolated, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic_test_bit(PG_isolated, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_set_isolated(struct folio *folio) { set_bit(PG_isolated, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageIsolated(struct page *page) { set_bit(PG_isolated, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void folio_clear_isolated(struct folio *folio) { clear_bit(PG_isolated, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageIsolated(struct page *page) { clear_bit(PG_isolated, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags); };

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int PageAnonExclusive(struct page *page)
{
 ((void)(sizeof(( long)(!PageAnon(page)))));
 ((void)(sizeof(( long)(PageHuge(page) && !PageHead(page)))));
 return ((__builtin_constant_p(PG_anon_exclusive) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? const_test_bit(PG_anon_exclusive, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic_test_bit(PG_anon_exclusive, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void SetPageAnonExclusive(struct page *page)
{
 ((void)(sizeof(( long)(!PageAnon(page) || PageKsm(page)))));
 ((void)(sizeof(( long)(PageHuge(page) && !PageHead(page)))));
 set_bit(PG_anon_exclusive, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void ClearPageAnonExclusive(struct page *page)
{
 ((void)(sizeof(( long)(!PageAnon(page) || PageKsm(page)))));
 ((void)(sizeof(( long)(PageHuge(page) && !PageHead(page)))));
 clear_bit(PG_anon_exclusive, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __ClearPageAnonExclusive(struct page *page)
{
 ((void)(sizeof(( long)(!PageAnon(page)))));
 ((void)(sizeof(( long)(PageHuge(page) && !PageHead(page)))));
 ((__builtin_constant_p(PG_anon_exclusive) && __builtin_constant_p((uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags))) ? generic___clear_bit(PG_anon_exclusive, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags) : generic___clear_bit(PG_anon_exclusive, &({ ((void)(sizeof(( long)(PagePoisoned(page))))); page; })->flags));
}







/*
 * Flags checked when a page is freed.  Pages being freed should not have
 * these flags set.  If they are, there is a problem.
 */







/*
 * Flags checked when a page is prepped for return by the page allocator.
 * Pages being prepped should not have these flags set.  If they are set,
 * there has been a kernel bug or struct page corruption.
 *
 * __PG_HWPOISON is exceptional because it needs to be kept beyond page's
 * alloc-free cycle to prevent from reusing the page.
 */





/**
 * page_has_private - Determine if page has private stuff
 * @page: The page to be checked
 *
 * Determine if a page has private stuff, indicating that release routines
 * should be invoked upon it.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_has_private(struct page *page)
{
 return !!(page->flags & (1UL << PG_private | 1UL << PG_private_2));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_has_private(struct folio *folio)
{
 return page_has_private(&folio->page);
}
# 23 "./include/linux/mmzone.h" 2
# 1 "./include/linux/local_lock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/local_lock_internal.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./include/linux/local_lock_internal.h"
typedef struct {




} local_lock_t;
# 47 "./include/linux/local_lock_internal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void local_lock_acquire(local_lock_t *l) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void local_lock_release(local_lock_t *l) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void local_lock_debug_init(local_lock_t *l) { }
# 6 "./include/linux/local_lock.h" 2

/**
 * local_lock_init - Runtime initialize a lock instance
 */


/**
 * local_lock - Acquire a per CPU local lock
 * @lock:	The lock variable
 */


/**
 * local_lock_irq - Acquire a per CPU local lock and disable interrupts
 * @lock:	The lock variable
 */


/**
 * local_lock_irqsave - Acquire a per CPU local lock, save and disable
 *			 interrupts
 * @lock:	The lock variable
 * @flags:	Storage for interrupt flags
 */



/**
 * local_unlock - Release a per CPU local lock
 * @lock:	The lock variable
 */


/**
 * local_unlock_irq - Release a per CPU local lock and enable interrupts
 * @lock:	The lock variable
 */


/**
 * local_unlock_irqrestore - Release a per CPU local lock and restore
 *			      interrupt flags
 * @lock:	The lock variable
 * @flags:      Interrupt flags to restore
 */
# 24 "./include/linux/mmzone.h" 2


/* Free memory management - zoned buddy allocator.  */







/*
 * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed
 * costly to service.  That is between allocation orders which should
 * coalesce naturally under reasonable reclaim pressure and those which
 * will not.
 */


enum migratetype {
 MIGRATE_UNMOVABLE,
 MIGRATE_MOVABLE,
 MIGRATE_RECLAIMABLE,
 MIGRATE_PCPTYPES, /* the number of types on the pcp lists */
 MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,

 /*
	 * MIGRATE_CMA migration type is designed to mimic the way
	 * ZONE_MOVABLE works.  Only movable pages can be allocated
	 * from MIGRATE_CMA pageblocks and page allocator never
	 * implicitly change migration type of MIGRATE_CMA pageblock.
	 *
	 * The way to use it is to change migratetype of a range of
	 * pageblocks to MIGRATE_CMA which can be done by
	 * __free_pageblock_cma() function.
	 */
 MIGRATE_CMA,


 MIGRATE_ISOLATE, /* can't allocate from here */

 MIGRATE_TYPES
};

/* In mm/page_alloc.c; keep in sync also with show_migration_types() there */
extern const char * const migratetype_names[MIGRATE_TYPES];
# 78 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_migrate_movable(int mt)
{
 return __builtin_expect(!!((mt) == MIGRATE_CMA), 0) || mt == MIGRATE_MOVABLE;
}

/*
 * Check whether a migratetype can be merged with another migratetype.
 *
 * It is only mergeable when it can fall back to other migratetypes for
 * allocation. See fallbacks[MIGRATE_TYPES][3] in page_alloc.c.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool migratetype_is_mergeable(int mt)
{
 return mt < MIGRATE_PCPTYPES;
}





extern int page_group_by_mobility_disabled;






struct free_area {
 struct list_head free_list[MIGRATE_TYPES];
 unsigned long nr_free;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *get_page_from_free_area(struct free_area *area,
         int migratetype)
{
 return ({ struct list_head *head__ = (&area->free_list[migratetype]); struct list_head *pos__ = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_274(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head__->next) == sizeof(char) || sizeof(head__->next) == sizeof(short) || sizeof(head__->next) == sizeof(int) || sizeof(head__->next) == sizeof(long)) || sizeof(head__->next) == sizeof(long long))) __compiletime_assert_274(); } while (0); (*(const volatile typeof( _Generic((head__->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head__->next))) *)&(head__->next)); }); pos__ != head__ ? ({ void *__mptr = (void *)(pos__); _Static_assert(__builtin_types_compatible_p(typeof(*(pos__)), typeof(((struct page *)0)->lru)) || __builtin_types_compatible_p(typeof(*(pos__)), typeof(void)), "pointer type mismatch in container_of()"); ((struct page *)(__mptr - __builtin_offsetof(struct page, lru))); }) : ((void *)0); });
# 115 "./include/linux/mmzone.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool free_area_empty(struct free_area *area, int migratetype)
{
 return list_empty(&area->free_list[migratetype]);
}

struct pglist_data;


enum numa_stat_item {
 NUMA_HIT, /* allocated in intended node */
 NUMA_MISS, /* allocated in non intended node */
 NUMA_FOREIGN, /* was intended here, hit elsewhere */
 NUMA_INTERLEAVE_HIT, /* interleaver preferred this zone */
 NUMA_LOCAL, /* allocation from local node */
 NUMA_OTHER, /* allocation from other node */
 NR_VM_NUMA_EVENT_ITEMS
};




enum zone_stat_item {
 /* First 128 byte cacheline (assuming 64 bit words) */
 NR_FREE_PAGES,
 NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
 NR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE,
 NR_ZONE_ACTIVE_ANON,
 NR_ZONE_INACTIVE_FILE,
 NR_ZONE_ACTIVE_FILE,
 NR_ZONE_UNEVICTABLE,
 NR_ZONE_WRITE_PENDING, /* Count of dirty, writeback and unstable pages */
 NR_MLOCK, /* mlock()ed pages found and moved off LRU */
 /* Second 128 byte cacheline */
 NR_BOUNCE,



 NR_FREE_CMA_PAGES,
 NR_VM_ZONE_STAT_ITEMS };

enum node_stat_item {
 NR_LRU_BASE,
 NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
 NR_ACTIVE_ANON, /*  "     "     "   "       "         */
 NR_INACTIVE_FILE, /*  "     "     "   "       "         */
 NR_ACTIVE_FILE, /*  "     "     "   "       "         */
 NR_UNEVICTABLE, /*  "     "     "   "       "         */
 NR_SLAB_RECLAIMABLE_B,
 NR_SLAB_UNRECLAIMABLE_B,
 NR_ISOLATED_ANON, /* Temporary isolated pages from anon lru */
 NR_ISOLATED_FILE, /* Temporary isolated pages from file lru */
 WORKINGSET_NODES,
 WORKINGSET_REFAULT_BASE,
 WORKINGSET_REFAULT_ANON = WORKINGSET_REFAULT_BASE,
 WORKINGSET_REFAULT_FILE,
 WORKINGSET_ACTIVATE_BASE,
 WORKINGSET_ACTIVATE_ANON = WORKINGSET_ACTIVATE_BASE,
 WORKINGSET_ACTIVATE_FILE,
 WORKINGSET_RESTORE_BASE,
 WORKINGSET_RESTORE_ANON = WORKINGSET_RESTORE_BASE,
 WORKINGSET_RESTORE_FILE,
 WORKINGSET_NODERECLAIM,
 NR_ANON_MAPPED, /* Mapped anonymous pages */
 NR_FILE_MAPPED, /* pagecache pages mapped into pagetables.
			   only modified from process context */
 NR_FILE_PAGES,
 NR_FILE_DIRTY,
 NR_WRITEBACK,
 NR_WRITEBACK_TEMP, /* Writeback using temporary buffers */
 NR_SHMEM, /* shmem pages (included tmpfs/GEM pages) */
 NR_SHMEM_THPS,
 NR_SHMEM_PMDMAPPED,
 NR_FILE_THPS,
 NR_FILE_PMDMAPPED,
 NR_ANON_THPS,
 NR_VMSCAN_WRITE,
 NR_VMSCAN_IMMEDIATE, /* Prioritise for reclaim when writeback ends */
 NR_DIRTIED, /* page dirtyings since bootup */
 NR_WRITTEN, /* page writings since bootup */
 NR_THROTTLED_WRITTEN, /* NR_WRITTEN while reclaim throttled */
 NR_KERNEL_MISC_RECLAIMABLE, /* reclaimable non-slab kernel pages */
 NR_FOLL_PIN_ACQUIRED, /* via: pin_user_page(), gup flag: FOLL_PIN */
 NR_FOLL_PIN_RELEASED, /* pages returned via unpin_user_page() */
 NR_KERNEL_STACK_KB, /* measured in KiB */



 NR_PAGETABLE, /* used for pagetables */
 NR_SECONDARY_PAGETABLE, /* secondary pagetables, e.g. KVM pagetables */

 NR_SWAPCACHE,


 PGPROMOTE_SUCCESS, /* promote successfully */
 PGPROMOTE_CANDIDATE, /* candidate pages to promote */

 NR_VM_NODE_STAT_ITEMS
};

/*
 * Returns true if the item should be printed in THPs (/proc/vmstat
 * currently prints number of anon, file and shmem THPs. But the item
 * is charged in pages).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool vmstat_item_print_in_thp(enum node_stat_item item)
{
 if (!1)
  return false;

 return item == NR_ANON_THPS ||
        item == NR_FILE_THPS ||
        item == NR_SHMEM_THPS ||
        item == NR_SHMEM_PMDMAPPED ||
        item == NR_FILE_PMDMAPPED;
}

/*
 * Returns true if the value is measured in bytes (most vmstat values are
 * measured in pages). This defines the API part, the internal representation
 * might be different.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool vmstat_item_in_bytes(int idx)
{
 /*
	 * Global and per-node slab counters track slab pages.
	 * It's expected that changes are multiples of PAGE_SIZE.
	 * Internally values are stored in pages.
	 *
	 * Per-memcg and per-lruvec counters track memory, consumed
	 * by individual slab objects. These counters are actually
	 * byte-precise.
	 */
 return (idx == NR_SLAB_RECLAIMABLE_B ||
  idx == NR_SLAB_UNRECLAIMABLE_B);
}

/*
 * We do arithmetic on the LRU lists in various places in the code,
 * so it is important to keep the active lists LRU_ACTIVE higher in
 * the array than the corresponding inactive lists, and to keep
 * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.
 *
 * This has to be kept in sync with the statistics in zone_stat_item
 * above and the descriptions in vmstat_text in mm/vmstat.c
 */




enum lru_list {
 LRU_INACTIVE_ANON = 0,
 LRU_ACTIVE_ANON = 0 + 1,
 LRU_INACTIVE_FILE = 0 + 2,
 LRU_ACTIVE_FILE = 0 + 2 + 1,
 LRU_UNEVICTABLE,
 NR_LRU_LISTS
};

enum vmscan_throttle_state {
 VMSCAN_THROTTLE_WRITEBACK,
 VMSCAN_THROTTLE_ISOLATED,
 VMSCAN_THROTTLE_NOPROGRESS,
 VMSCAN_THROTTLE_CONGESTED,
 NR_VMSCAN_THROTTLE,
};





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_file_lru(enum lru_list lru)
{
 return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_active_lru(enum lru_list lru)
{
 return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
}





enum lruvec_flags {
 LRUVEC_CONGESTED, /* lruvec has many dirty pages
					 * backed by a congested BDI
					 */
};



/*
 * Evictable pages are divided into multiple generations. The youngest and the
 * oldest generation numbers, max_seq and min_seq, are monotonically increasing.
 * They form a sliding window of a variable size [MIN_NR_GENS, MAX_NR_GENS]. An
 * offset within MAX_NR_GENS, i.e., gen, indexes the LRU list of the
 * corresponding generation. The gen counter in folio->flags stores gen+1 while
 * a page is on one of lrugen->lists[]. Otherwise it stores 0.
 *
 * A page is added to the youngest generation on faulting. The aging needs to
 * check the accessed bit at least twice before handing this page over to the
 * eviction. The first check takes care of the accessed bit set on the initial
 * fault; the second check makes sure this page hasn't been used since then.
 * This process, AKA second chance, requires a minimum of two generations,
 * hence MIN_NR_GENS. And to maintain ABI compatibility with the active/inactive
 * LRU, e.g., /proc/vmstat, these two generations are considered active; the
 * rest of generations, if they exist, are considered inactive. See
 * lru_gen_is_active().
 *
 * PG_active is always cleared while a page is on one of lrugen->lists[] so that
 * the aging needs not to worry about it. And it's set again when a page
 * considered active is isolated for non-reclaiming purposes, e.g., migration.
 * See lru_gen_add_folio() and lru_gen_del_folio().
 *
 * MAX_NR_GENS is set to 4 so that the multi-gen LRU can support twice the
 * number of categories of the active/inactive LRU when keeping track of
 * accesses through page tables. This requires order_base_2(MAX_NR_GENS+1) bits
 * in folio->flags.
 */



/*
 * Each generation is divided into multiple tiers. A page accessed N times
 * through file descriptors is in tier order_base_2(N). A page in the first tier
 * (N=0,1) is marked by PG_referenced unless it was faulted in through page
 * tables or read ahead. A page in any other tier (N>1) is marked by
 * PG_referenced and PG_workingset. This implies a minimum of two tiers is
 * supported without using additional bits in folio->flags.
 *
 * In contrast to moving across generations which requires the LRU lock, moving
 * across tiers only involves atomic operations on folio->flags and therefore
 * has a negligible cost in the buffered access path. In the eviction path,
 * comparisons of refaulted/(evicted+protected) from the first tier and the
 * rest infer whether pages accessed multiple times through file descriptors
 * are statistically hot and thus worth protecting.
 *
 * MAX_NR_TIERS is set to 4 so that the multi-gen LRU can support twice the
 * number of categories of the active/inactive LRU when keeping track of
 * accesses through file descriptors. This uses MAX_NR_TIERS-2 spare bits in
 * folio->flags.
 */




struct lruvec;
struct page_vma_mapped_walk;
# 488 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_init_lruvec(struct lruvec *lruvec)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_look_around(struct page_vma_mapped_walk *pvmw)
{
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_init_memcg(struct mem_cgroup *memcg)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lru_gen_exit_memcg(struct mem_cgroup *memcg)
{
}




struct lruvec {
 struct list_head lists[NR_LRU_LISTS];
 /* per lruvec lru_lock for memcg */
 spinlock_t lru_lock;
 /*
	 * These track the cost of reclaiming one LRU - file or anon -
	 * over the other. As the observed cost of reclaiming one LRU
	 * increases, the reclaim scan balance tips toward the other.
	 */
 unsigned long anon_cost;
 unsigned long file_cost;
 /* Non-resident age, driven by LRU movement */
 atomic_long_t nonresident_age;
 /* Refaults at the time of last reclaim cycle */
 unsigned long refaults[2];
 /* Various lruvec state flags (enum lruvec_flags) */
 unsigned long flags;







 struct pglist_data *pgdat;

};

/* Isolate unmapped pages */

/* Isolate for asynchronous migration */

/* Isolate unevictable pages */


/* LRU Isolation modes. */
typedef unsigned isolate_mode_t;

enum zone_watermarks {
 WMARK_MIN,
 WMARK_LOW,
 WMARK_HIGH,
 WMARK_PROMO,
 NR_WMARK
};

/*
 * One per migratetype for each PAGE_ALLOC_COSTLY_ORDER. One additional list
 * for THP which will usually be GFP_MOVABLE. Even if it is another type,
 * it should not contribute to serious fragmentation causing THP allocation
 * failures.
 */
# 573 "./include/linux/mmzone.h"
/* Fields and list protected by pagesets local_lock in page_alloc.c */
struct per_cpu_pages {
 spinlock_t lock; /* Protects lists field */
 int count; /* number of pages in the list */
 int high; /* high watermark, emptying needed */
 int batch; /* chunk size for buddy add/remove */
 short free_factor; /* batch scaling factor during free */

 short expire; /* When 0, remote pagesets are drained */


 /* Lists of pages, one per migrate type stored on the pcp-lists */
 struct list_head lists[((MIGRATE_PCPTYPES * (3 + 1)) + 1)];
} __attribute__((__aligned__((1 << (6)))));

struct per_cpu_zonestat {

 s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
 s8 stat_threshold;


 /*
	 * Low priority inaccurate counters that are only folded
	 * on demand. Use a large type to avoid the overhead of
	 * folding during refresh_cpu_vm_stats.
	 */
 unsigned long vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];

};

struct per_cpu_nodestat {
 s8 stat_threshold;
 s8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];
};



enum zone_type {
 /*
	 * ZONE_DMA and ZONE_DMA32 are used when there are peripherals not able
	 * to DMA to all of the addressable memory (ZONE_NORMAL).
	 * On architectures where this area covers the whole 32 bit address
	 * space ZONE_DMA32 is used. ZONE_DMA is left for the ones with smaller
	 * DMA addressing constraints. This distinction is important as a 32bit
	 * DMA mask is assumed when ZONE_DMA32 is defined. Some 64-bit
	 * platforms may need both zones as they support peripherals with
	 * different DMA addressing limitations.
	 */

 ZONE_DMA,


 ZONE_DMA32,

 /*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 */
 ZONE_NORMAL,
# 644 "./include/linux/mmzone.h"
 /*
	 * ZONE_MOVABLE is similar to ZONE_NORMAL, except that it contains
	 * movable pages with few exceptional cases described below. Main use
	 * cases for ZONE_MOVABLE are to make memory offlining/unplug more
	 * likely to succeed, and to locally limit unmovable allocations - e.g.,
	 * to increase the number of THP/huge pages. Notable special cases are:
	 *
	 * 1. Pinned pages: (long-term) pinning of movable pages might
	 *    essentially turn such pages unmovable. Therefore, we do not allow
	 *    pinning long-term pages in ZONE_MOVABLE. When pages are pinned and
	 *    faulted, they come from the right zone right away. However, it is
	 *    still possible that address space already has pages in
	 *    ZONE_MOVABLE at the time when pages are pinned (i.e. user has
	 *    touches that memory before pinning). In such case we migrate them
	 *    to a different zone. When migration fails - pinning fails.
	 * 2. memblock allocations: kernelcore/movablecore setups might create
	 *    situations where ZONE_MOVABLE contains unmovable allocations
	 *    after boot. Memory offlining and allocations fail early.
	 * 3. Memory holes: kernelcore/movablecore setups might create very rare
	 *    situations where ZONE_MOVABLE contains memory holes after boot,
	 *    for example, if we have sections that are only partially
	 *    populated. Memory offlining and allocations fail early.
	 * 4. PG_hwpoison pages: while poisoned pages can be skipped during
	 *    memory offlining, such pages cannot be allocated.
	 * 5. Unmovable PG_offline pages: in paravirtualized environments,
	 *    hotplugged memory blocks might only partially be managed by the
	 *    buddy (e.g., via XEN-balloon, Hyper-V balloon, virtio-mem). The
	 *    parts not manged by the buddy are unmovable PG_offline pages. In
	 *    some cases (virtio-mem), such pages can be skipped during
	 *    memory offlining, however, cannot be moved/allocated. These
	 *    techniques might use alloc_contig_range() to hide previously
	 *    exposed pages from the buddy again (e.g., to implement some sort
	 *    of memory unplug in virtio-mem).
	 * 6. ZERO_PAGE(0), kernelcore/movablecore setups might create
	 *    situations where ZERO_PAGE(0) which is allocated differently
	 *    on different platforms may end up in a movable zone. ZERO_PAGE(0)
	 *    cannot be migrated.
	 * 7. Memory-hotplug: when using memmap_on_memory and onlining the
	 *    memory to the MOVABLE zone, the vmemmap pages are also placed in
	 *    such zone. Such pages cannot be really moved around as they are
	 *    self-stored in the range, but they are treated as movable when
	 *    the range they describe is about to be offlined.
	 *
	 * In general, no unmovable allocations that degrade memory offlining
	 * should end up in ZONE_MOVABLE. Allocators (like alloc_contig_range())
	 * have to expect that migrating pages in ZONE_MOVABLE can fail (even
	 * if has_unmovable_pages() states that there are no unmovable pages,
	 * there can be false negatives).
	 */
 ZONE_MOVABLE,



 __MAX_NR_ZONES

};





struct zone {
 /* Read-mostly fields */

 /* zone watermarks, access with *_wmark_pages(zone) macros */
 unsigned long _watermark[NR_WMARK];
 unsigned long watermark_boost;

 unsigned long nr_reserved_highatomic;

 /*
	 * We don't know if the memory that we're going to allocate will be
	 * freeable or/and it will be released eventually, so to avoid totally
	 * wasting several GB of ram we must reserve some of the lower zone
	 * memory (otherwise we risk to run OOM on the lower zones despite
	 * there being tons of freeable ram on the higher zones).  This array is
	 * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl
	 * changes.
	 */
 long lowmem_reserve[4 /* __MAX_NR_ZONES */];


 int node;

 struct pglist_data *zone_pgdat;
 struct per_cpu_pages /* nothing */ *per_cpu_pageset;
 struct per_cpu_zonestat /* nothing */ *per_cpu_zonestats;
 /*
	 * the high and batch values are copied to individual pagesets for
	 * faster access
	 */
 int pageset_high;
 int pageset_batch;
# 746 "./include/linux/mmzone.h"
 /* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
 unsigned long zone_start_pfn;

 /*
	 * spanned_pages is the total pages spanned by the zone, including
	 * holes, which is calculated as:
	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
	 *
	 * present_pages is physical pages existing within the zone, which
	 * is calculated as:
	 *	present_pages = spanned_pages - absent_pages(pages in holes);
	 *
	 * present_early_pages is present pages existing within the zone
	 * located on memory available since early boot, excluding hotplugged
	 * memory.
	 *
	 * managed_pages is present pages managed by the buddy system, which
	 * is calculated as (reserved_pages includes pages allocated by the
	 * bootmem allocator):
	 *	managed_pages = present_pages - reserved_pages;
	 *
	 * cma pages is present pages that are assigned for CMA use
	 * (MIGRATE_CMA).
	 *
	 * So present_pages may be used by memory hotplug or memory power
	 * management logic to figure out unmanaged pages by checking
	 * (present_pages - managed_pages). And managed_pages should be used
	 * by page allocator and vm scanner to calculate all kinds of watermarks
	 * and thresholds.
	 *
	 * Locking rules:
	 *
	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
	 * It is a seqlock because it has to be read outside of zone->lock,
	 * and it is done in the main allocator path.  But, it is written
	 * quite infrequently.
	 *
	 * The span_seq lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 *
	 * Write access to present_pages at runtime should be protected by
	 * mem_hotplug_begin/done(). Any reader who can't tolerant drift of
	 * present_pages should use get_online_mems() to get a stable value.
	 */
 atomic_long_t managed_pages;
 unsigned long spanned_pages;
 unsigned long present_pages;

 unsigned long present_early_pages;


 unsigned long cma_pages;


 const char *name;


 /*
	 * Number of isolated pageblock. It is used to solve incorrect
	 * freepage counting problem due to racy retrieving migratetype
	 * of pageblock. Protected by zone->lock.
	 */
 unsigned long nr_isolate_pageblock;



 /* see spanned/present_pages for more description */
 seqlock_t span_seqlock;


 int initialized;

 /* Write-intensive fields used from the page allocator */
 struct cacheline_padding _pad1_;

 /* free areas of different sizes */
 struct free_area free_area[11];

 /* zone flags, see below */
 unsigned long flags;

 /* Primarily protects free_area */
 spinlock_t lock;

 /* Write-intensive fields used by compaction and vmstats. */
 struct cacheline_padding _pad2_;

 /*
	 * When free pages are below this point, additional steps are taken
	 * when reading the number of free pages to avoid per-cpu counter
	 * drift allowing watermarks to be breached
	 */
 unsigned long percpu_drift_mark;


 /* pfn where compaction free scanner should start */
 unsigned long compact_cached_free_pfn;
 /* pfn where compaction migration scanner should start */
 unsigned long compact_cached_migrate_pfn[2];
 unsigned long compact_init_migrate_pfn;
 unsigned long compact_init_free_pfn;



 /*
	 * On compaction failure, 1<<compact_defer_shift compactions
	 * are skipped before trying again. The number attempted since
	 * last failure is tracked with compact_considered.
	 * compact_order_failed is the minimum compaction failed order.
	 */
 unsigned int compact_considered;
 unsigned int compact_defer_shift;
 int compact_order_failed;



 /* Set to true when the PG_migrate_skip bits should be cleared */
 bool compact_blockskip_flush;


 bool contiguous;

 struct cacheline_padding _pad3_;
 /* Zone statistics */
 atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];
 atomic_long_t vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];
} __attribute__((__aligned__(1 << ((6)))));

enum pgdat_flags {
 PGDAT_DIRTY, /* reclaim scanning has recently found
					 * many dirty file pages at the tail
					 * of the LRU.
					 */
 PGDAT_WRITEBACK, /* reclaim scanning has recently found
					 * many pages under writeback
					 */
 PGDAT_RECLAIM_LOCKED, /* prevents concurrent reclaim */
};

enum zone_flags {
 ZONE_BOOSTED_WATERMARK, /* zone recently boosted watermarks.
					 * Cleared when kswapd is woken.
					 */
 ZONE_RECLAIM_ACTIVE, /* kswapd may be scanning the zone. */
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long zone_managed_pages(struct zone *zone)
{
 return (unsigned long)atomic_long_read(&zone->managed_pages);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long zone_cma_pages(struct zone *zone)
{

 return zone->cma_pages;



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long zone_end_pfn(const struct zone *zone)
{
 return zone->zone_start_pfn + zone->spanned_pages;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)
{
 return zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zone_is_initialized(struct zone *zone)
{
 return zone->initialized;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zone_is_empty(struct zone *zone)
{
 return zone->spanned_pages == 0;
}


/*
 * The zone field is never updated after free_area_init_core()
 * sets it, so none of the operations on it need to be atomic.
 */

/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */
# 942 "./include/linux/mmzone.h"
/*
 * Define the bit shifts to access each section.  For non-existent
 * sections we define the shift as 0; that plus a 0 mask ensures
 * the compiler will optimise away reference to them.
 */






/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
# 973 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum zone_type page_zonenum(const struct page *page)
{
 do { kcsan_set_access_mask(((1UL << 2) - 1) << (((((sizeof(unsigned long)*8) - 0) - 4) - 2) * (2 != 0))); __kcsan_check_access(&(page->flags), sizeof(page->flags), (1 << 3) /* Access is an assertion. */); kcsan_set_access_mask(0); kcsan_atomic_next(1); } while (0);
 return (page->flags >> (((((sizeof(unsigned long)*8) - 0) - 4) - 2) * (2 != 0))) & ((1UL << 2) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum zone_type folio_zonenum(const struct folio *folio)
{
 return page_zonenum(&folio->page);
}
# 1011 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_zone_device_page(const struct page *page)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zone_device_pages_have_same_pgmap(const struct page *a,
           const struct page *b)
{
 return true;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_is_zone_device(const struct folio *folio)
{
 return is_zone_device_page(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_zone_movable_page(const struct page *page)
{
 return page_zonenum(page) == ZONE_MOVABLE;
}


/*
 * Return true if [start_pfn, start_pfn + nr_pages) range has a non-empty
 * intersection with the given zone
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zone_intersects(struct zone *zone,
  unsigned long start_pfn, unsigned long nr_pages)
{
 if (zone_is_empty(zone))
  return false;
 if (start_pfn >= zone_end_pfn(zone) ||
     start_pfn + nr_pages <= zone->zone_start_pfn)
  return false;

 return true;
}

/*
 * The "priority" of VM scanning is how much of the queues we will scan in one
 * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the
 * queues ("queue_length >> 12") during an aging round.
 */


/* Maximum number of zones on a zonelist */


enum {
 ZONELIST_FALLBACK, /* zonelist with fallback */

 /*
	 * The NUMA zonelists are doubled because we need zonelists that
	 * restrict the allocations to a single node for __GFP_THISNODE.
	 */
 ZONELIST_NOFALLBACK, /* zonelist without fallback (__GFP_THISNODE) */

 MAX_ZONELISTS
};

/*
 * This struct contains information about a zone in a zonelist. It is stored
 * here to avoid dereferences into large structures and lookups of tables
 */
struct zoneref {
 struct zone *zone; /* Pointer to actual zone */
 int zone_idx; /* zone_idx(zoneref->zone) */
};

/*
 * One allocation request operates on a zonelist. A zonelist
 * is a list of zones, the first one is the 'goal' of the
 * allocation, the other zones are fallback zones, in decreasing
 * priority.
 *
 * To speed the reading of the zonelist, the zonerefs contain the zone index
 * of the entry being read. Helper functions to access information given
 * a struct zoneref are
 *
 * zonelist_zone()	- Return the struct zone * for an entry in _zonerefs
 * zonelist_zone_idx()	- Return the index of the zone for an entry
 * zonelist_node_idx()	- Return the index of the node for an entry
 */
struct zonelist {
 struct zoneref _zonerefs[((1 << 4) * 4 /* __MAX_NR_ZONES */) + 1];
};

/*
 * The array of struct pages for flatmem.
 * It must be declared for SPARSEMEM as well because there are configurations
 * that rely on that.
 */
extern struct page *mem_map;


struct deferred_split {
 spinlock_t split_queue_lock;
 struct list_head split_queue;
 unsigned long split_queue_len;
};


/*
 * On NUMA machines, each NUMA node would have a pg_data_t to describe
 * it's memory layout. On UMA machines there is a single pglist_data which
 * describes the whole memory.
 *
 * Memory statistics and page replacement data structures are maintained on a
 * per-zone basis.
 */
typedef struct pglist_data {
 /*
	 * node_zones contains just the zones for THIS node. Not all of the
	 * zones may be populated, but it is the full list. It is referenced by
	 * this node's node_zonelists as well as other node's node_zonelists.
	 */
 struct zone node_zones[4 /* __MAX_NR_ZONES */];

 /*
	 * node_zonelists contains references to all zones in all nodes.
	 * Generally the first zones will be references to this node's
	 * node_zones.
	 */
 struct zonelist node_zonelists[MAX_ZONELISTS];

 int nr_zones; /* number of populated zones in this node */







 /*
	 * Must be held any time you expect node_start_pfn,
	 * node_present_pages, node_spanned_pages or nr_zones to stay constant.
	 * Also synchronizes pgdat->first_deferred_pfn during deferred page
	 * init.
	 *
	 * pgdat_resize_lock() and pgdat_resize_unlock() are provided to
	 * manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG
	 * or CONFIG_DEFERRED_STRUCT_PAGE_INIT.
	 *
	 * Nests above zone->lock and zone->span_seqlock
	 */
 spinlock_t node_size_lock;

 unsigned long node_start_pfn;
 unsigned long node_present_pages; /* total number of physical pages */
 unsigned long node_spanned_pages; /* total size of physical page
					     range, including holes */
 int node_id;
 wait_queue_head_t kswapd_wait;
 wait_queue_head_t pfmemalloc_wait;

 /* workqueues for throttling reclaim for different reasons. */
 wait_queue_head_t reclaim_wait[NR_VMSCAN_THROTTLE];

 atomic_t nr_writeback_throttled;/* nr of writeback-throttled tasks */
 unsigned long nr_reclaim_start; /* nr pages written while throttled
					 * when throttling started. */

 struct mutex kswapd_lock;

 struct task_struct *kswapd; /* Protected by kswapd_lock */
 int kswapd_order;
 enum zone_type kswapd_highest_zoneidx;

 int kswapd_failures; /* Number of 'reclaimed == 0' runs */


 int kcompactd_max_order;
 enum zone_type kcompactd_highest_zoneidx;
 wait_queue_head_t kcompactd_wait;
 struct task_struct *kcompactd;
 bool proactive_compact_trigger;

 /*
	 * This is a per-node reserve of pages that are not available
	 * to userspace allocations.
	 */
 unsigned long totalreserve_pages;


 /*
	 * node reclaim becomes active if more unmapped pages exist.
	 */
 unsigned long min_unmapped_pages;
 unsigned long min_slab_pages;


 /* Write-intensive fields used by page reclaim */
 struct cacheline_padding _pad1_;
# 1214 "./include/linux/mmzone.h"
 struct deferred_split deferred_split_queue;



 /* start time in ms of current promote rate limit period */
 unsigned int nbp_rl_start;
 /* number of promote candidate pages at start time of current rate limit period */
 unsigned long nbp_rl_nr_cand;
 /* promote threshold in ms */
 unsigned int nbp_threshold;
 /* start time in ms of current promote threshold adjustment period */
 unsigned int nbp_th_start;
 /*
	 * number of promote candidate pages at start time of current promote
	 * threshold adjustment period
	 */
 unsigned long nbp_th_nr_cand;

 /* Fields commonly accessed by the page reclaim scanner */

 /*
	 * NOTE: THIS IS UNUSED IF MEMCG IS ENABLED.
	 *
	 * Use mem_cgroup_lruvec() to look up lruvecs.
	 */
 struct lruvec __lruvec;

 unsigned long flags;






 struct cacheline_padding _pad2_;

 /* Per-node vmstats */
 struct per_cpu_nodestat /* nothing */ *per_cpu_nodestats;
 atomic_long_t vm_stat[NR_VM_NODE_STAT_ITEMS];

 struct memory_tier /* nothing */ *memtier;

} pg_data_t;







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pgdat_end_pfn(pg_data_t *pgdat)
{
 return pgdat->node_start_pfn + pgdat->node_spanned_pages;
}

# 1 "./include/linux/memory_hotplug.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/mmzone.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 6 "./include/linux/memory_hotplug.h" 2

# 1 "./include/linux/notifier.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 *	Routines to manage notifier chains for passing status changes to any
 *	interested routines. We need this instead of hard coded call lists so
 *	that modules can poke their nose into the innards. The network devices
 *	needed them so here they are for the rest of you.
 *
 *				Alan Cox <Alan.Cox@linux.org>
 */






# 1 "./include/linux/srcu.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/*
 * Sleepable Read-Copy Update mechanism for mutual exclusion
 *
 * Copyright (C) IBM Corporation, 2006
 * Copyright (C) Fujitsu, 2012
 *
 * Author: Paul McKenney <paulmck@linux.ibm.com>
 *	   Lai Jiangshan <laijs@cn.fujitsu.com>
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 *		Documentation/RCU/ *.txt
 *
 */







# 1 "./include/linux/rcu_segcblist.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/*
 * RCU segmented callback lists
 *
 * This seemingly RCU-private file must be available to SRCU users
 * because the size of the TREE SRCU srcu_struct structure depends
 * on these definitions.
 *
 * Copyright IBM Corporation, 2017
 *
 * Authors: Paul E. McKenney <paulmck@linux.net.ibm.com>
 */







/* Simple unsegmented callback lists. */
struct rcu_cblist {
 struct callback_head *head;
 struct callback_head **tail;
 long len;
};



/* Complicated segmented callback lists.  ;-) */

/*
 * Index values for segments in rcu_segcblist structure.
 *
 * The segments are as follows:
 *
 * [head, *tails[RCU_DONE_TAIL]):
 *	Callbacks whose grace period has elapsed, and thus can be invoked.
 * [*tails[RCU_DONE_TAIL], *tails[RCU_WAIT_TAIL]):
 *	Callbacks waiting for the current GP from the current CPU's viewpoint.
 * [*tails[RCU_WAIT_TAIL], *tails[RCU_NEXT_READY_TAIL]):
 *	Callbacks that arrived before the next GP started, again from
 *	the current CPU's viewpoint.  These can be handled by the next GP.
 * [*tails[RCU_NEXT_READY_TAIL], *tails[RCU_NEXT_TAIL]):
 *	Callbacks that might have arrived after the next GP started.
 *	There is some uncertainty as to when a given GP starts and
 *	ends, but a CPU knows the exact times if it is the one starting
 *	or ending the GP.  Other CPUs know that the previous GP ends
 *	before the next one starts.
 *
 * Note that RCU_WAIT_TAIL cannot be empty unless RCU_NEXT_READY_TAIL is also
 * empty.
 *
 * The ->gp_seq[] array contains the grace-period number at which the
 * corresponding segment of callbacks will be ready to invoke.  A given
 * element of this array is meaningful only when the corresponding segment
 * is non-empty, and it is never valid for RCU_DONE_TAIL (whose callbacks
 * are already ready to invoke) or for RCU_NEXT_TAIL (whose callbacks have
 * not yet been assigned a grace-period number).
 */







/*
 *                     ==NOCB Offloading state machine==
 *
 *
 *  ----------------------------------------------------------------------------
 *  |                              SEGCBLIST_RCU_CORE                          |
 *  |                                                                          |
 *  |  Callbacks processed by rcu_core() from softirqs or local                |
 *  |  rcuc kthread, without holding nocb_lock.                                |
 *  ----------------------------------------------------------------------------
 *                                         |
 *                                         v
 *  ----------------------------------------------------------------------------
 *  |       SEGCBLIST_RCU_CORE | SEGCBLIST_LOCKING | SEGCBLIST_OFFLOADED       |
 *  |                                                                          |
 *  | Callbacks processed by rcu_core() from softirqs or local                 |
 *  | rcuc kthread, while holding nocb_lock. Waking up CB and GP kthreads,     |
 *  | allowing nocb_timer to be armed.                                         |
 *  ----------------------------------------------------------------------------
 *                                         |
 *                                         v
 *                        -----------------------------------
 *                        |                                 |
 *                        v                                 v
 *  ---------------------------------------  ----------------------------------|
 *  |        SEGCBLIST_RCU_CORE   |       |  |     SEGCBLIST_RCU_CORE   |      |
 *  |        SEGCBLIST_LOCKING    |       |  |     SEGCBLIST_LOCKING    |      |
 *  |        SEGCBLIST_OFFLOADED  |       |  |     SEGCBLIST_OFFLOADED  |      |
 *  |        SEGCBLIST_KTHREAD_CB         |  |     SEGCBLIST_KTHREAD_GP        |
 *  |                                     |  |                                 |
 *  |                                     |  |                                 |
 *  | CB kthread woke up and              |  | GP kthread woke up and          |
 *  | acknowledged SEGCBLIST_OFFLOADED.   |  | acknowledged SEGCBLIST_OFFLOADED|
 *  | Processes callbacks concurrently    |  |                                 |
 *  | with rcu_core(), holding            |  |                                 |
 *  | nocb_lock.                          |  |                                 |
 *  ---------------------------------------  -----------------------------------
 *                        |                                 |
 *                        -----------------------------------
 *                                         |
 *                                         v
 *  |--------------------------------------------------------------------------|
 *  |                           SEGCBLIST_LOCKING    |                         |
 *  |                           SEGCBLIST_OFFLOADED  |                         |
 *  |                           SEGCBLIST_KTHREAD_GP |                         |
 *  |                           SEGCBLIST_KTHREAD_CB                           |
 *  |                                                                          |
 *  |   Kthreads handle callbacks holding nocb_lock, local rcu_core() stops    |
 *  |   handling callbacks. Enable bypass queueing.                            |
 *  ----------------------------------------------------------------------------
 */



/*
 *                       ==NOCB De-Offloading state machine==
 *
 *
 *  |--------------------------------------------------------------------------|
 *  |                           SEGCBLIST_LOCKING    |                         |
 *  |                           SEGCBLIST_OFFLOADED  |                         |
 *  |                           SEGCBLIST_KTHREAD_CB |                         |
 *  |                           SEGCBLIST_KTHREAD_GP                           |
 *  |                                                                          |
 *  |   CB/GP kthreads handle callbacks holding nocb_lock, local rcu_core()    |
 *  |   ignores callbacks. Bypass enqueue is enabled.                          |
 *  ----------------------------------------------------------------------------
 *                                      |
 *                                      v
 *  |--------------------------------------------------------------------------|
 *  |                           SEGCBLIST_RCU_CORE   |                         |
 *  |                           SEGCBLIST_LOCKING    |                         |
 *  |                           SEGCBLIST_OFFLOADED  |                         |
 *  |                           SEGCBLIST_KTHREAD_CB |                         |
 *  |                           SEGCBLIST_KTHREAD_GP                           |
 *  |                                                                          |
 *  |   CB/GP kthreads handle callbacks holding nocb_lock, local rcu_core()    |
 *  |   handles callbacks concurrently. Bypass enqueue is enabled.             |
 *  |   Invoke RCU core so we make sure not to preempt it in the middle with   |
 *  |   leaving some urgent work unattended within a jiffy.                    |
 *  ----------------------------------------------------------------------------
 *                                      |
 *                                      v
 *  |--------------------------------------------------------------------------|
 *  |                           SEGCBLIST_RCU_CORE   |                         |
 *  |                           SEGCBLIST_LOCKING    |                         |
 *  |                           SEGCBLIST_KTHREAD_CB |                         |
 *  |                           SEGCBLIST_KTHREAD_GP                           |
 *  |                                                                          |
 *  |   CB/GP kthreads and local rcu_core() handle callbacks concurrently      |
 *  |   holding nocb_lock. Wake up CB and GP kthreads if necessary. Disable    |
 *  |   bypass enqueue.                                                        |
 *  ----------------------------------------------------------------------------
 *                                      |
 *                                      v
 *                     -----------------------------------
 *                     |                                 |
 *                     v                                 v
 *  ---------------------------------------------------------------------------|
 *  |                                     |                                    |
 *  |        SEGCBLIST_RCU_CORE |         |       SEGCBLIST_RCU_CORE |         |
 *  |        SEGCBLIST_LOCKING  |         |       SEGCBLIST_LOCKING  |         |
 *  |        SEGCBLIST_KTHREAD_CB         |       SEGCBLIST_KTHREAD_GP         |
 *  |                                     |                                    |
 *  | GP kthread woke up and              |   CB kthread woke up and           |
 *  | acknowledged the fact that          |   acknowledged the fact that       |
 *  | SEGCBLIST_OFFLOADED got cleared.    |   SEGCBLIST_OFFLOADED got cleared. |
 *  |                                     |   The CB kthread goes to sleep     |
 *  | The callbacks from the target CPU   |   until it ever gets re-offloaded. |
 *  | will be ignored from the GP kthread |                                    |
 *  | loop.                               |                                    |
 *  ----------------------------------------------------------------------------
 *                      |                                 |
 *                      -----------------------------------
 *                                      |
 *                                      v
 *  ----------------------------------------------------------------------------
 *  |                SEGCBLIST_RCU_CORE | SEGCBLIST_LOCKING                    |
 *  |                                                                          |
 *  | Callbacks processed by rcu_core() from softirqs or local                 |
 *  | rcuc kthread, while holding nocb_lock. Forbid nocb_timer to be armed.    |
 *  | Flush pending nocb_timer. Flush nocb bypass callbacks.                   |
 *  ----------------------------------------------------------------------------
 *                                      |
 *                                      v
 *  ----------------------------------------------------------------------------
 *  |                         SEGCBLIST_RCU_CORE                               |
 *  |                                                                          |
 *  |  Callbacks processed by rcu_core() from softirqs or local                |
 *  |  rcuc kthread, without holding nocb_lock.                                |
 *  ----------------------------------------------------------------------------
 */







struct rcu_segcblist {
 struct callback_head *head;
 struct callback_head **tails[4];
 unsigned long gp_seq[4];



 long len;

 long seglen[4];
 u8 flags;
};
# 23 "./include/linux/srcu.h" 2

struct srcu_struct;
# 41 "./include/linux/srcu.h"
int init_srcu_struct(struct srcu_struct *ssp);







# 1 "./include/linux/srcutree.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/*
 * Sleepable Read-Copy Update mechanism for mutual exclusion,
 *	tree variant.
 *
 * Copyright (C) IBM Corporation, 2017
 *
 * Author: Paul McKenney <paulmck@linux.ibm.com>
 */




# 1 "./include/linux/rcu_node_tree.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/*
 * RCU node combining tree definitions.  These are used to compute
 * global attributes while avoiding common-case global contention.  A key
 * property that these computations rely on is a tournament-style approach
 * where only one of the tasks contending a lower level in the tree need
 * advance to the next higher level.  If properly configured, this allows
 * unlimited scalability while maintaining a constant level of contention
 * on the root node.
 *
 * This seemingly RCU-private file must be available to SRCU users
 * because the size of the TREE SRCU srcu_struct structure depends
 * on these definitions.
 *
 * Copyright IBM Corporation, 2017
 *
 * Author: Paul E. McKenney <paulmck@linux.ibm.com>
 */






/*
 * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and
 * CONFIG_RCU_FANOUT_LEAF.
 * In theory, it should be possible to add more levels straightforwardly.
 * In practice, this did work well going from three levels to four.
 * Of course, your mileage may vary.
 */
# 15 "./include/linux/srcutree.h" 2


struct srcu_node;
struct srcu_struct;

/*
 * Per-CPU structure feeding into leaf srcu_node, similar in function
 * to rcu_node.
 */
struct srcu_data {
 /* Read-side state. */
 atomic_long_t srcu_lock_count[2]; /* Locks per CPU. */
 atomic_long_t srcu_unlock_count[2]; /* Unlocks per CPU. */
 int srcu_nmi_safety; /* NMI-safe srcu_struct structure? */

 /* Update-side state. */
 spinlock_t lock __attribute__((__aligned__(1 << ((6)))));
 struct rcu_segcblist srcu_cblist; /* List of callbacks.*/
 unsigned long srcu_gp_seq_needed; /* Furthest future GP needed. */
 unsigned long srcu_gp_seq_needed_exp; /* Furthest future exp GP. */
 bool srcu_cblist_invoking; /* Invoking these CBs? */
 struct timer_list delay_work; /* Delay for CB invoking */
 struct work_struct work; /* Context for CB invoking. */
 struct callback_head srcu_barrier_head; /* For srcu_barrier() use. */
 struct srcu_node *mynode; /* Leaf srcu_node. */
 unsigned long grpmask; /* Mask for leaf srcu_node */
      /*  ->srcu_data_have_cbs[]. */
 int cpu;
 struct srcu_struct *ssp;
};

/*
 * Node in SRCU combining tree, similar in function to rcu_data.
 */
struct srcu_node {
 spinlock_t lock;
 unsigned long srcu_have_cbs[4]; /* GP seq for children having CBs, but only */
      /*  if greater than ->srcu_gq_seq. */
 unsigned long srcu_data_have_cbs[4]; /* Which srcu_data structs have CBs for given GP? */
 unsigned long srcu_gp_seq_needed_exp; /* Furthest future exp GP. */
 struct srcu_node *srcu_parent; /* Next up in tree. */
 int grplo; /* Least CPU for node. */
 int grphi; /* Biggest CPU for node. */
};

/*
 * Per-SRCU-domain structure, similar in function to rcu_state.
 */
struct srcu_struct {
 struct srcu_node *node; /* Combining tree. */
 struct srcu_node *level[2 + 1];
      /* First node at each level. */
 int srcu_size_state; /* Small-to-big transition state. */
 struct mutex srcu_cb_mutex; /* Serialize CB preparation. */
 spinlock_t lock; /* Protect counters and size state. */
 struct mutex srcu_gp_mutex; /* Serialize GP work. */
 unsigned int srcu_idx; /* Current rdr array element. */
 unsigned long srcu_gp_seq; /* Grace-period seq #. */
 unsigned long srcu_gp_seq_needed; /* Latest gp_seq needed. */
 unsigned long srcu_gp_seq_needed_exp; /* Furthest future exp GP. */
 unsigned long srcu_gp_start; /* Last GP start timestamp (jiffies) */
 unsigned long srcu_last_gp_end; /* Last GP end timestamp (ns) */
 unsigned long srcu_size_jiffies; /* Current contention-measurement interval. */
 unsigned long srcu_n_lock_retries; /* Contention events in current interval. */
 unsigned long srcu_n_exp_nodelay; /* # expedited no-delays in current GP phase. */
 struct srcu_data /* nothing */ *sda; /* Per-CPU srcu_data array. */
 bool sda_is_static; /* May ->sda be passed to free_percpu()? */
 unsigned long srcu_barrier_seq; /* srcu_barrier seq #. */
 struct mutex srcu_barrier_mutex; /* Serialize barrier ops. */
 struct completion srcu_barrier_completion;
      /* Awaken barrier rq at end. */
 atomic_t srcu_barrier_cpu_cnt; /* # CPUs not yet posting a */
      /*  callback for the barrier */
      /*  operation. */
 unsigned long reschedule_jiffies;
 unsigned long reschedule_count;
 struct delayed_work work;
 struct lockdep_map dep_map;
};

/* Values for size state variable (->srcu_size_state). */
# 106 "./include/linux/srcutree.h"
/* Values for state variable (bottom bits of ->srcu_gp_seq). */
# 120 "./include/linux/srcutree.h"
/*
 * Define and initialize a srcu struct at build time.
 * Do -not- call init_srcu_struct() nor cleanup_srcu_struct() on it.
 *
 * Note that although DEFINE_STATIC_SRCU() hides the name from other
 * files, the per-CPU variable rules nevertheless require that the
 * chosen name be globally unique.  These rules also prohibit use of
 * DEFINE_STATIC_SRCU() within a function.  If these rules are too
 * restrictive, declare the srcu_struct manually.  For example, in
 * each file:
 *
 *	static struct srcu_struct my_srcu;
 *
 * Then, before the first use of each my_srcu, manually initialize it:
 *
 *	init_srcu_struct(&my_srcu);
 *
 * See include/linux/percpu-defs.h for the rules on per-CPU variables.
 */
# 154 "./include/linux/srcutree.h"
void synchronize_srcu_expedited(struct srcu_struct *ssp);
void srcu_barrier(struct srcu_struct *ssp);
void srcu_torture_stats_print(struct srcu_struct *ssp, char *tt, char *tf);
# 50 "./include/linux/srcu.h" 2




void call_srcu(struct srcu_struct *ssp, struct callback_head *head,
  void (*func)(struct callback_head *head));
void cleanup_srcu_struct(struct srcu_struct *ssp);
int __srcu_read_lock(struct srcu_struct *ssp) ;
void __srcu_read_unlock(struct srcu_struct *ssp, int idx) ;
void synchronize_srcu(struct srcu_struct *ssp);
unsigned long get_state_synchronize_srcu(struct srcu_struct *ssp);
unsigned long start_poll_synchronize_srcu(struct srcu_struct *ssp);
bool poll_state_synchronize_srcu(struct srcu_struct *ssp, unsigned long cookie);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __srcu_read_lock_nmisafe(struct srcu_struct *ssp)
{
 return __srcu_read_lock(ssp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)
{
 __srcu_read_unlock(ssp, idx);
}


void srcu_init(void);
# 107 "./include/linux/srcu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int srcu_read_lock_held(const struct srcu_struct *ssp)
{
 return 1;
}
# 121 "./include/linux/srcu.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void srcu_check_nmi_safety(struct srcu_struct *ssp,
      bool nmi_safe) { }



/**
 * srcu_dereference_check - fetch SRCU-protected pointer for later dereferencing
 * @p: the pointer to fetch and protect for later dereferencing
 * @ssp: pointer to the srcu_struct, which is used to check that we
 *	really are in an SRCU read-side critical section.
 * @c: condition to check for update-side use
 *
 * If PROVE_RCU is enabled, invoking this outside of an RCU read-side
 * critical section will result in an RCU-lockdep splat, unless @c evaluates
 * to 1.  The @c argument will normally be a logical expression containing
 * lockdep_is_held() calls.
 */




/**
 * srcu_dereference - fetch SRCU-protected pointer for later dereferencing
 * @p: the pointer to fetch and protect for later dereferencing
 * @ssp: pointer to the srcu_struct, which is used to check that we
 *	really are in an SRCU read-side critical section.
 *
 * Makes rcu_dereference_check() do the dirty work.  If PROVE_RCU
 * is enabled, invoking this outside of an RCU read-side critical
 * section will result in an RCU-lockdep splat.
 */


/**
 * srcu_dereference_notrace - no tracing and no lockdep calls from here
 * @p: the pointer to fetch and protect for later dereferencing
 * @ssp: pointer to the srcu_struct, which is used to check that we
 *	really are in an SRCU read-side critical section.
 */


/**
 * srcu_read_lock - register a new reader for an SRCU-protected structure.
 * @ssp: srcu_struct in which to register the new reader.
 *
 * Enter an SRCU read-side critical section.  Note that SRCU read-side
 * critical sections may be nested.  However, it is illegal to
 * call anything that waits on an SRCU grace period for the same
 * srcu_struct, whether directly or indirectly.  Please note that
 * one way to indirectly wait on an SRCU grace period is to acquire
 * a mutex that is held elsewhere while calling synchronize_srcu() or
 * synchronize_srcu_expedited().
 *
 * Note that srcu_read_lock() and the matching srcu_read_unlock() must
 * occur in the same context, for example, it is illegal to invoke
 * srcu_read_unlock() in an irq handler if the matching srcu_read_lock()
 * was invoked in process context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int srcu_read_lock(struct srcu_struct *ssp)
{
 int retval;

 srcu_check_nmi_safety(ssp, false);
 retval = __srcu_read_lock(ssp);
 do { } while (0);
 return retval;
}

/**
 * srcu_read_lock_nmisafe - register a new reader for an SRCU-protected structure.
 * @ssp: srcu_struct in which to register the new reader.
 *
 * Enter an SRCU read-side critical section, but in an NMI-safe manner.
 * See srcu_read_lock() for more information.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int srcu_read_lock_nmisafe(struct srcu_struct *ssp)
{
 int retval;

 srcu_check_nmi_safety(ssp, true);
 retval = __srcu_read_lock_nmisafe(ssp);
 do { } while (0);
 return retval;
}

/* Used by tracing, cannot be traced and cannot invoke lockdep. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__no_instrument_function__)) int
srcu_read_lock_notrace(struct srcu_struct *ssp)
{
 int retval;

 srcu_check_nmi_safety(ssp, false);
 retval = __srcu_read_lock(ssp);
 return retval;
}

/**
 * srcu_read_unlock - unregister a old reader from an SRCU-protected structure.
 * @ssp: srcu_struct in which to unregister the old reader.
 * @idx: return value from corresponding srcu_read_lock().
 *
 * Exit an SRCU read-side critical section.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void srcu_read_unlock(struct srcu_struct *ssp, int idx)

{
 ({ int __ret_warn_on = !!(idx & ~0x1); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/srcu.h\"; .popsection; .long 14472b - .; .short 227; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 srcu_check_nmi_safety(ssp, false);
 do { } while (0);
 __srcu_read_unlock(ssp, idx);
}

/**
 * srcu_read_unlock_nmisafe - unregister a old reader from an SRCU-protected structure.
 * @ssp: srcu_struct in which to unregister the old reader.
 * @idx: return value from corresponding srcu_read_lock().
 *
 * Exit an SRCU read-side critical section, but in an NMI-safe manner.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)

{
 ({ int __ret_warn_on = !!(idx & ~0x1); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/srcu.h\"; .popsection; .long 14472b - .; .short 243; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 srcu_check_nmi_safety(ssp, true);
 do { } while (0);
 __srcu_read_unlock_nmisafe(ssp, idx);
}

/* Used by tracing, cannot be traced and cannot call lockdep. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__no_instrument_function__)) void
srcu_read_unlock_notrace(struct srcu_struct *ssp, int idx)
{
 srcu_check_nmi_safety(ssp, false);
 __srcu_read_unlock(ssp, idx);
}

/**
 * smp_mb__after_srcu_read_unlock - ensure full ordering after srcu_read_unlock
 *
 * Converts the preceding srcu_read_unlock into a two-way memory barrier.
 *
 * Call this after srcu_read_unlock, to guarantee that all memory operations
 * that occur after smp_mb__after_srcu_read_unlock will appear to happen after
 * the preceding srcu_read_unlock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void smp_mb__after_srcu_read_unlock(void)
{
 /* __srcu_read_unlock has smp_mb() internally so nothing to do here. */
}
# 17 "./include/linux/notifier.h" 2

/*
 * Notifier chains are of four types:
 *
 *	Atomic notifier chains: Chain callbacks run in interrupt/atomic
 *		context. Callouts are not allowed to block.
 *	Blocking notifier chains: Chain callbacks run in process context.
 *		Callouts are allowed to block.
 *	Raw notifier chains: There are no restrictions on callbacks,
 *		registration, or unregistration.  All locking and protection
 *		must be provided by the caller.
 *	SRCU notifier chains: A variant of blocking notifier chains, with
 *		the same restrictions.
 *
 * atomic_notifier_chain_register() may be called from an atomic context,
 * but blocking_notifier_chain_register() and srcu_notifier_chain_register()
 * must be called from a process context.  Ditto for the corresponding
 * _unregister() routines.
 *
 * atomic_notifier_chain_unregister(), blocking_notifier_chain_unregister(),
 * and srcu_notifier_chain_unregister() _must not_ be called from within
 * the call chain.
 *
 * SRCU notifier chains are an alternative form of blocking notifier chains.
 * They use SRCU (Sleepable Read-Copy Update) instead of rw-semaphores for
 * protection of the chain links.  This means there is _very_ low overhead
 * in srcu_notifier_call_chain(): no cache bounces and no memory barriers.
 * As compensation, srcu_notifier_chain_unregister() is rather expensive.
 * SRCU notifier chains should be used when the chain will be called very
 * often but notifier_blocks will seldom be removed.
 */

struct notifier_block;

typedef int (*notifier_fn_t)(struct notifier_block *nb,
   unsigned long action, void *data);

struct notifier_block {
 notifier_fn_t notifier_call;
 struct notifier_block /* nothing */ *next;
 int priority;
};

struct atomic_notifier_head {
 spinlock_t lock;
 struct notifier_block /* nothing */ *head;
};

struct blocking_notifier_head {
 struct rw_semaphore rwsem;
 struct notifier_block /* nothing */ *head;
};

struct raw_notifier_head {
 struct notifier_block /* nothing */ *head;
};

struct srcu_notifier_head {
 struct mutex mutex;
 struct srcu_struct srcu;
 struct notifier_block /* nothing */ *head;
};
# 92 "./include/linux/notifier.h"
/* srcu_notifier_heads must be cleaned up dynamically */
extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
# 144 "./include/linux/notifier.h"
extern int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
  struct notifier_block *nb);
extern int blocking_notifier_chain_register(struct blocking_notifier_head *nh,
  struct notifier_block *nb);
extern int raw_notifier_chain_register(struct raw_notifier_head *nh,
  struct notifier_block *nb);
extern int srcu_notifier_chain_register(struct srcu_notifier_head *nh,
  struct notifier_block *nb);

extern int atomic_notifier_chain_register_unique_prio(
  struct atomic_notifier_head *nh, struct notifier_block *nb);
extern int blocking_notifier_chain_register_unique_prio(
  struct blocking_notifier_head *nh, struct notifier_block *nb);

extern int atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,
  struct notifier_block *nb);
extern int blocking_notifier_chain_unregister(struct blocking_notifier_head *nh,
  struct notifier_block *nb);
extern int raw_notifier_chain_unregister(struct raw_notifier_head *nh,
  struct notifier_block *nb);
extern int srcu_notifier_chain_unregister(struct srcu_notifier_head *nh,
  struct notifier_block *nb);

extern int atomic_notifier_call_chain(struct atomic_notifier_head *nh,
  unsigned long val, void *v);
extern int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
  unsigned long val, void *v);
extern int raw_notifier_call_chain(struct raw_notifier_head *nh,
  unsigned long val, void *v);
extern int srcu_notifier_call_chain(struct srcu_notifier_head *nh,
  unsigned long val, void *v);

extern int blocking_notifier_call_chain_robust(struct blocking_notifier_head *nh,
  unsigned long val_up, unsigned long val_down, void *v);
extern int raw_notifier_call_chain_robust(struct raw_notifier_head *nh,
  unsigned long val_up, unsigned long val_down, void *v);

extern bool atomic_notifier_call_chain_is_empty(struct atomic_notifier_head *nh);





      /* Bad/Veto action */
/*
 * Clean way to return from the notifier and stop further calls.
 */


/* Encapsulate (negative) errno value (in particular, NOTIFY_BAD <=> EPERM). */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int notifier_from_errno(int err)
{
 if (err)
  return 0x8000 /* Don't call further */ | (0x0001 /* Suits me */ - err);

 return 0x0001 /* Suits me */;
}

/* Restore (negative) errno value from notify return value. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int notifier_to_errno(int ret)
{
 ret &= ~0x8000 /* Don't call further */;
 return ret > 0x0001 /* Suits me */ ? 0x0001 /* Suits me */ - ret : 0;
}

/*
 *	Declared notifiers so far. I can imagine quite a few more chains
 *	over time (eg laptop power reset chains, reboot chain (to clean
 *	device units up), device [un]mount chain, module load/unload chain,
 *	low memory chain, screenblank chain (for plug in modular screenblankers)
 *	VC switch chains (for loadable kernel svgalib VC switch helpers) etc...
 */

/* CPU notfiers are defined in include/linux/cpu.h. */

/* netdevice notifiers are defined in include/linux/netdevice.h */

/* reboot notifiers are defined in include/linux/reboot.h. */

/* Hibernation and suspend events are defined in include/linux/suspend.h. */

/* Virtual Terminal events are defined in include/linux/vt.h. */



/* Console keyboard events.
 * Note: KBD_KEYCODE is always sent before KBD_UNBOUND_KEYCODE, KBD_UNICODE and
 * KBD_KEYSYM. */






extern struct blocking_notifier_head reboot_notifier_list;
# 8 "./include/linux/memory_hotplug.h" 2


struct page;
struct zone;
struct pglist_data;
struct mem_section;
struct memory_group;
struct resource;
struct vmem_altmap;
struct dev_pagemap;
# 37 "./include/linux/memory_hotplug.h"
/*
 * XXX: node aware allocation can't work well to get new node's memory at this time.
 *	Because, pgdat for the new node is not allocated/initialized yet itself.
 *	To use new node's memory, more consideration will be necessary.
 */





extern pg_data_t *node_data[];
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
{
 node_data[nid] = pgdat;
}
# 68 "./include/linux/memory_hotplug.h"
struct page *pfn_to_online_page(unsigned long pfn);

/* Types for control the zone type of onlined and offlined memory */
enum {
 /* Offline the memory. */
 MMOP_OFFLINE = 0,
 /* Online the memory. Zone depends, see default_zone_for_pfn(). */
 MMOP_ONLINE,
 /* Online the memory to ZONE_NORMAL. */
 MMOP_ONLINE_KERNEL,
 /* Online the memory to ZONE_MOVABLE. */
 MMOP_ONLINE_MOVABLE,
};

/* Flags for add_memory() and friends to specify memory hotplug details. */
typedef int mhp_t;

/* No special request */

/*
 * Allow merging of the added System RAM resource with adjacent,
 * mergeable resources. After a successful call to add_memory_resource()
 * with this flag set, the resource pointer must no longer be used as it
 * might be stale, or the resource might have changed.
 */


/*
 * We want memmap (struct page array) to be self contained.
 * To do so, we will use the beginning of the hot-added range to build
 * the page tables for the memmap array that describes the entire range.
 * Only selected architectures support it with SPARSE_VMEMMAP.
 */

/*
 * The nid field specifies a memory group id (mgid) instead. The memory group
 * implies the node id (nid).
 */


/*
 * Extended parameters for memory hotplug:
 * altmap: alternative allocator for memmap array (optional)
 * pgprot: page protection flags to apply to newly created page tables
 *	(required)
 */
struct mhp_params {
 struct vmem_altmap *altmap;
 pgprot_t pgprot;
 struct dev_pagemap *pgmap;
};

bool mhp_range_allowed(u64 start, u64 size, bool need_mapping);
struct range mhp_get_pluggable_range(bool need_mapping);

/*
 * Zone resizing functions
 *
 * Note: any attempt to resize a zone should has pgdat_resize_lock()
 * zone_span_writelock() both held. This ensure the size of a zone
 * can't be changed while pgdat_resize_lock() held.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned zone_span_seqbegin(struct zone *zone)
{
 return read_seqbegin(&zone->span_seqlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int zone_span_seqretry(struct zone *zone, unsigned iv)
{
 return read_seqretry(&zone->span_seqlock, iv);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_span_writelock(struct zone *zone)
{
 write_seqlock(&zone->span_seqlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_span_writeunlock(struct zone *zone)
{
 write_sequnlock(&zone->span_seqlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_seqlock_init(struct zone *zone)
{
 do { do { spinlock_check(&(&zone->span_seqlock)->lock); *(&(&zone->span_seqlock)->lock) = (spinlock_t) { { .rlock = { .raw_lock = { { .val = { (0) } } }, } } }; } while (0); do { seqcount_spinlock_t *____s = (&(&zone->span_seqlock)->seqcount); __seqcount_init(&____s->seqcount, ((void *)0), ((void *)0)); ; } while (0); } while (0);
}
extern void adjust_present_page_count(struct page *page,
          struct memory_group *group,
          long nr_pages);
/* VM interface that may be used by firmware interface */
extern int mhp_init_memmap_on_memory(unsigned long pfn, unsigned long nr_pages,
         struct zone *zone);
extern void mhp_deinit_memmap_on_memory(unsigned long pfn, unsigned long nr_pages);
extern int online_pages(unsigned long pfn, unsigned long nr_pages,
   struct zone *zone, struct memory_group *group);
extern void __offline_isolated_pages(unsigned long start_pfn,
         unsigned long end_pfn);

typedef void (*online_page_callback_t)(struct page *page, unsigned int order);

extern void generic_online_page(struct page *page, unsigned int order);
extern int set_online_page_callback(online_page_callback_t callback);
extern int restore_online_page_callback(online_page_callback_t callback);

extern int try_online_node(int nid);

extern int arch_add_memory(int nid, u64 start, u64 size,
      struct mhp_params *params);
extern u64 max_mem_size;

extern int mhp_online_type_from_str(const char *str);

/* Default online_type (MMOP_*) when new memory blocks are added. */
extern int mhp_default_online_type;
/* If movable_node boot option specified */
extern bool movable_node_enabled;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool movable_node_is_enabled(void)
{
 return movable_node_enabled;
}

extern void arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap);
extern void __remove_pages(unsigned long start_pfn, unsigned long nr_pages,
      struct vmem_altmap *altmap);

/* reasonably generic interface to expand the physical pages */
extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
         struct mhp_params *params);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int add_pages(int nid, unsigned long start_pfn,
  unsigned long nr_pages, struct mhp_params *params)
{
 return __add_pages(nid, start_pfn, nr_pages, params);
}





void get_online_mems(void);
void put_online_mems(void);

void mem_hotplug_begin(void);
void mem_hotplug_done(void);

/* See kswapd_is_running() */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgdat_kswapd_lock(pg_data_t *pgdat)
{
 mutex_lock(&pgdat->kswapd_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgdat_kswapd_unlock(pg_data_t *pgdat)
{
 mutex_unlock(&pgdat->kswapd_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgdat_kswapd_lock_init(pg_data_t *pgdat)
{
 do { static struct lock_class_key __key; __mutex_init((&pgdat->kswapd_lock), "&pgdat->kswapd_lock", &__key); } while (0);
}
# 268 "./include/linux/memory_hotplug.h"
/*
 * Keep this declaration outside CONFIG_MEMORY_HOTPLUG as some
 * platforms might override and use arch_get_mappable_range()
 * for internal non memory hotplug purposes.
 */
struct range arch_get_mappable_range(void);


/*
 * pgdat resizing functions
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
void pgdat_resize_lock(struct pglist_data *pgdat, unsigned long *flags)
{
 do { do { ({ unsigned long __dummy; typeof(*flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); *flags = _raw_spin_lock_irqsave(spinlock_check(&pgdat->node_size_lock)); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
void pgdat_resize_unlock(struct pglist_data *pgdat, unsigned long *flags)
{
 spin_unlock_irqrestore(&pgdat->node_size_lock, *flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
void pgdat_resize_init(struct pglist_data *pgdat)
{
 do { spinlock_check(&pgdat->node_size_lock); *(&pgdat->node_size_lock) = (spinlock_t) { { .rlock = { .raw_lock = { { .val = { (0) } } }, } } }; } while (0);
}
# 305 "./include/linux/memory_hotplug.h"
extern void try_offline_node(int nid);
extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,
    struct zone *zone, struct memory_group *group);
extern int remove_memory(u64 start, u64 size);
extern void __remove_memory(u64 start, u64 size);
extern int offline_and_remove_memory(u64 start, u64 size);
# 329 "./include/linux/memory_hotplug.h"
extern void set_zone_contiguous(struct zone *zone);
extern void clear_zone_contiguous(struct zone *zone);


extern void __attribute__((__section__(".ref.text"))) __attribute__((__noinline__)) free_area_init_core_hotplug(struct pglist_data *pgdat);
extern int __add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags);
extern int add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags);
extern int add_memory_resource(int nid, struct resource *resource,
          mhp_t mhp_flags);
extern int add_memory_driver_managed(int nid, u64 start, u64 size,
         const char *resource_name,
         mhp_t mhp_flags);
extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
       unsigned long nr_pages,
       struct vmem_altmap *altmap, int migratetype);
extern void remove_pfn_range_from_zone(struct zone *zone,
           unsigned long start_pfn,
           unsigned long nr_pages);
extern int sparse_add_section(int nid, unsigned long pfn,
  unsigned long nr_pages, struct vmem_altmap *altmap,
  struct dev_pagemap *pgmap);
extern void sparse_remove_section(struct mem_section *ms,
  unsigned long pfn, unsigned long nr_pages,
  unsigned long map_offset, struct vmem_altmap *altmap);
extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
       unsigned long pnum);
extern struct zone *zone_for_pfn_range(int online_type, int nid,
  struct memory_group *group, unsigned long start_pfn,
  unsigned long nr_pages);
extern int arch_create_linear_mapping(int nid, u64 start, u64 size,
          struct mhp_params *params);
void arch_remove_linear_mapping(u64 start, u64 size);
extern bool mhp_supports_memmap_on_memory(unsigned long size);
# 1270 "./include/linux/mmzone.h" 2

void build_all_zonelists(pg_data_t *pgdat);
void wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,
     enum zone_type highest_zoneidx);
bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
    int highest_zoneidx, unsigned int alloc_flags,
    long free_pages);
bool zone_watermark_ok(struct zone *z, unsigned int order,
  unsigned long mark, int highest_zoneidx,
  unsigned int alloc_flags);
bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
  unsigned long mark, int highest_zoneidx);
/*
 * Memory initialization context, use to differentiate memory added by
 * the platform statically or via memory hotplug interface.
 */
enum meminit_context {
 MEMINIT_EARLY,
 MEMINIT_HOTPLUG,
};

extern void init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
         unsigned long size);

extern void lruvec_init(struct lruvec *lruvec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)
{

 return lruvec->pgdat;



}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int local_memory_node(int node_id) { return node_id; };


/*
 * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.
 */
# 1322 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool zone_is_zone_device(struct zone *zone)
{
 return false;
}


/*
 * Returns true if a zone has pages managed by the buddy allocator.
 * All the reclaim decisions have to use this function rather than
 * populated_zone(). If the whole zone is reserved then we can easily
 * end up with populated_zone() && !managed_zone().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool managed_zone(struct zone *zone)
{
 return zone_managed_pages(zone);
}

/* Returns true if a zone has memory */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool populated_zone(struct zone *zone)
{
 return zone->present_pages;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int zone_to_nid(struct zone *zone)
{
 return zone->node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_set_nid(struct zone *zone, int nid)
{
 zone->node = nid;
}
# 1364 "./include/linux/mmzone.h"
extern int movable_zone;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_highmem_idx(enum zone_type idx)
{




 return 0;

}

/**
 * is_highmem - helper function to quickly check if a struct zone is a
 *              highmem zone or not.  This is an attempt to keep references
 *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.
 * @zone: pointer to struct zone variable
 * Return: 1 for a highmem zone, 0 otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_highmem(struct zone *zone)
{
 return is_highmem_idx(((zone) - (zone)->zone_pgdat->node_zones));
}


bool has_managed_dma(void);







/* These two functions are used to setup the per zone pages min values */
struct ctl_table;

int min_free_kbytes_sysctl_handler(struct ctl_table *, int, void *, size_t *,
  loff_t *);
int watermark_scale_factor_sysctl_handler(struct ctl_table *, int, void *,
  size_t *, loff_t *);
extern int sysctl_lowmem_reserve_ratio[4 /* __MAX_NR_ZONES */];
int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int, void *,
  size_t *, loff_t *);
int percpu_pagelist_high_fraction_sysctl_handler(struct ctl_table *, int,
  void *, size_t *, loff_t *);
int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,
  void *, size_t *, loff_t *);
int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
  void *, size_t *, loff_t *);
int numa_zonelist_order_handler(struct ctl_table *, int,
  void *, size_t *, loff_t *);
extern int percpu_pagelist_high_fraction;
extern char numa_zonelist_order[];
# 1429 "./include/linux/mmzone.h"
# 1 "./arch/arm64/include/asm/mmzone.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./arch/arm64/include/asm/numa.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/asm/topology.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







struct pci_bus;
int pcibus_to_node(struct pci_bus *bus);






# 1 "./include/linux/arch_topology.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * include/linux/arch_topology.h - arch specific cpu topology information
 */






void topology_normalize_cpu_scale(void);
int topology_update_cpu_topology(void);


void topology_init_cpu_capacity_cppc(void);


struct device_node;
bool topology_parse_cpu_capacity(struct device_node *cpu_node, int cpu);

extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) cpu_scale;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long topology_get_cpu_scale(int cpu)
{
 return (*({ do { const void /* nothing */ *__vpp_verify = (typeof((&(cpu_scale)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(cpu_scale)))) *)((&(cpu_scale)))); (typeof((typeof(*((&(cpu_scale)))) *)((&(cpu_scale))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}

void topology_set_cpu_scale(unsigned int cpu, unsigned long capacity);

extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) arch_freq_scale;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long topology_get_freq_scale(int cpu)
{
 return (*({ do { const void /* nothing */ *__vpp_verify = (typeof((&(arch_freq_scale)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(arch_freq_scale)))) *)((&(arch_freq_scale)))); (typeof((typeof(*((&(arch_freq_scale)))) *)((&(arch_freq_scale))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}

void topology_set_freq_scale(const struct cpumask *cpus, unsigned long cur_freq,
        unsigned long max_freq);
bool topology_scale_freq_invariant(void);

enum scale_freq_source {
 SCALE_FREQ_SOURCE_CPUFREQ = 0,
 SCALE_FREQ_SOURCE_ARCH,
 SCALE_FREQ_SOURCE_CPPC,
};

struct scale_freq_data {
 enum scale_freq_source source;
 void (*set_freq_scale)(void);
};

void topology_scale_freq_tick(void);
void topology_set_scale_freq_source(struct scale_freq_data *data, const struct cpumask *cpus);
void topology_clear_scale_freq_source(enum scale_freq_source source, const struct cpumask *cpus);

extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) thermal_pressure;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long topology_get_thermal_pressure(int cpu)
{
 return (*({ do { const void /* nothing */ *__vpp_verify = (typeof((&(thermal_pressure)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(thermal_pressure)))) *)((&(thermal_pressure)))); (typeof((typeof(*((&(thermal_pressure)))) *)((&(thermal_pressure))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}

void topology_update_thermal_pressure(const struct cpumask *cpus,
          unsigned long capped_freq);

struct cpu_topology {
 int thread_id;
 int core_id;
 int cluster_id;
 int package_id;
 cpumask_t thread_sibling;
 cpumask_t core_sibling;
 cpumask_t cluster_sibling;
 cpumask_t llc_sibling;
};


extern struct cpu_topology cpu_topology[256];
# 87 "./include/linux/arch_topology.h"
void init_cpu_topology(void);
void store_cpu_topology(unsigned int cpuid);
const struct cpumask *cpu_coregroup_mask(int cpu);
const struct cpumask *cpu_clustergroup_mask(int cpu);
void update_siblings_masks(unsigned int cpu);
void remove_cpu_topology(unsigned int cpuid);
void reset_cpu_topology(void);
int parse_acpi_topology(void);
# 18 "./arch/arm64/include/asm/topology.h" 2

void update_freq_counters_refs(void);

/* Replace task scheduler's default frequency-invariant accounting */
# 31 "./arch/arm64/include/asm/topology.h"
/* Replace task scheduler's default cpu-invariant accounting */


/* Enable topology flag updates */


/* Replace task scheduler's default thermal pressure API */



# 1 "./include/asm-generic/topology.h" 1
/*
 * linux/include/asm-generic/topology.h
 *
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */
# 42 "./arch/arm64/include/asm/topology.h" 2
# 6 "./arch/arm64/include/asm/numa.h" 2
# 1 "./include/asm-generic/numa.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







int __node_distance(int from, int to);


extern nodemask_t numa_nodes_parsed __attribute__((__section__(".init.data")));

extern bool numa_off;

/* Mappings between node number and cpus on that node. */
extern cpumask_var_t node_to_cpumask_map[(1 << 4)];
void numa_clear_node(unsigned int cpu);




/* Returns a pointer to the cpumask of CPUs on Node 'node'. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct cpumask *cpumask_of_node(int node)
{
 if (node == (-1))
  return ((struct cpumask *)(1 ? (cpu_all_bits) : (void *)sizeof(__check_is_bitmap(cpu_all_bits))));

 return node_to_cpumask_map[node];
}


void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) arch_numa_init(void);
int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) numa_add_memblk(int nodeid, u64 start, u64 end);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) numa_set_distance(int from, int to, int distance);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) numa_free_distance(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) early_map_cpu_to_node(unsigned int cpu, int nid);
void numa_store_cpu_info(unsigned int cpu);
void numa_add_cpu(unsigned int cpu);
void numa_remove_cpu(unsigned int cpu);
# 7 "./arch/arm64/include/asm/numa.h" 2
# 8 "./arch/arm64/include/asm/mmzone.h" 2

extern struct pglist_data *node_data[];
# 1430 "./include/linux/mmzone.h" 2



extern struct pglist_data *first_online_pgdat(void);
extern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);
extern struct zone *next_zone(struct zone *zone);

/**
 * for_each_online_pgdat - helper macro to iterate over all online nodes
 * @pgdat: pointer to a pg_data_t variable
 */




/**
 * for_each_zone - helper macro to iterate over all memory zones
 * @zone: pointer to struct zone variable
 *
 * The user only needs to declare the zone variable, for_each_zone
 * fills it in.
 */
# 1465 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct zone *zonelist_zone(struct zoneref *zoneref)
{
 return zoneref->zone;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int zonelist_zone_idx(struct zoneref *zoneref)
{
 return zoneref->zone_idx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int zonelist_node_idx(struct zoneref *zoneref)
{
 return zone_to_nid(zoneref->zone);
}

struct zoneref *__next_zones_zonelist(struct zoneref *z,
     enum zone_type highest_zoneidx,
     nodemask_t *nodes);

/**
 * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point
 * @z: The cursor used as a starting point for the search
 * @highest_zoneidx: The zone index of the highest zone to return
 * @nodes: An optional nodemask to filter the zonelist with
 *
 * This function returns the next zone at or below a given zone index that is
 * within the allowed nodemask using a cursor as the starting point for the
 * search. The zoneref returned is a cursor that represents the current zone
 * being examined. It should be advanced by one before calling
 * next_zones_zonelist again.
 *
 * Return: the next zone at or below highest_zoneidx within the allowed
 * nodemask using a cursor within a zonelist as a starting point
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct zoneref *next_zones_zonelist(struct zoneref *z,
     enum zone_type highest_zoneidx,
     nodemask_t *nodes)
{
 if (__builtin_expect(!!(!nodes && zonelist_zone_idx(z) <= highest_zoneidx), 1))
  return z;
 return __next_zones_zonelist(z, highest_zoneidx, nodes);
}

/**
 * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist
 * @zonelist: The zonelist to search for a suitable zone
 * @highest_zoneidx: The zone index of the highest zone to return
 * @nodes: An optional nodemask to filter the zonelist with
 *
 * This function returns the first zone at or below a given zone index that is
 * within the allowed nodemask. The zoneref returned is a cursor that can be
 * used to iterate the zonelist with next_zones_zonelist by advancing it by
 * one before calling.
 *
 * When no eligible zone is found, zoneref->zone is NULL (zoneref itself is
 * never NULL). This may happen either genuinely, or due to concurrent nodemask
 * update due to cpuset modification.
 *
 * Return: Zoneref pointer for the first suitable zone found
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
     enum zone_type highest_zoneidx,
     nodemask_t *nodes)
{
 return next_zones_zonelist(zonelist->_zonerefs,
       highest_zoneidx, nodes);
}

/**
 * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask
 * @zone: The current zone in the iterator
 * @z: The current pointer within zonelist->_zonerefs being iterated
 * @zlist: The zonelist being iterated
 * @highidx: The zone index of the highest zone to return
 * @nodemask: Nodemask allowed by the allocator
 *
 * This iterator iterates though all zones at or below a given zone index and
 * within a given nodemask
 */
# 1557 "./include/linux/mmzone.h"
/**
 * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index
 * @zone: The current zone in the iterator
 * @z: The current pointer within zonelist->zones being iterated
 * @zlist: The zonelist being iterated
 * @highidx: The zone index of the highest zone to return
 *
 * This iterator iterates though all zones at or below a given zone index.
 */



/* Whether the 'nodes' are all movable nodes */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool movable_only_nodes(nodemask_t *nodes)
{
 struct zonelist *zonelist;
 struct zoneref *z;
 int nid;

 if (__nodes_empty(&(*nodes), (1 << 4)))
  return false;

 /*
	 * We can chose arbitrary node from the nodemask to get a
	 * zonelist as they are interlinked. We just need to find
	 * at least one zone that can satisfy kernel allocations.
	 */
 nid = __first_node(&(*nodes));
 zonelist = &(node_data[(nid)])->node_zonelists[ZONELIST_FALLBACK];
 z = first_zones_zonelist(zonelist, ZONE_NORMAL, nodes);
 return (!z->zone) ? true : false;
}
# 1601 "./include/linux/mmzone.h"
/*
 * PA_SECTION_SHIFT		physical address to/from section number
 * PFN_SECTION_SHIFT		pfn to/from section number
 */
# 1620 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pfn_to_section_nr(unsigned long pfn)
{
 return pfn >> (27 - 12);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long section_nr_to_pfn(unsigned long sec)
{
 return sec << (27 - 12);
}
# 1648 "./include/linux/mmzone.h"
struct mem_section_usage {

 unsigned long subsection_map[((((1UL << (27 - 21))) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];

 /* See declaration of similar field in struct zone */
 unsigned long pageblock_flags[0];
};

void subsection_map_init(unsigned long pfn, unsigned long nr_pages);

struct page;
struct page_ext;
struct mem_section {
 /*
	 * This is, logically, a pointer to an array of struct
	 * pages.  However, it is stored with some other magic.
	 * (see sparse.c::sparse_init_one_section())
	 *
	 * Additionally during early boot we encode node id of
	 * the location of the section here to guide allocation.
	 * (see sparse.c::memory_present())
	 *
	 * Making it a UL at least makes someone do a cast
	 * before using it wrong.
	 */
 unsigned long section_mem_map;

 struct mem_section_usage *usage;
# 1684 "./include/linux/mmzone.h"
 /*
	 * WARNING: mem_section must be a power-of-2 in size for the
	 * calculation and use of SECTION_ROOT_MASK to make sense.
	 */
};
# 1701 "./include/linux/mmzone.h"
extern struct mem_section **mem_section;




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long *section_to_usemap(struct mem_section *ms)
{
 return ms->usage->pageblock_flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct mem_section *__nr_to_section(unsigned long nr)
{
 unsigned long root = ((nr) / (((1UL) << 12) / sizeof (struct mem_section)));

 if (__builtin_expect(!!(root >= ((((1UL << (48 - 27))) + ((((1UL) << 12) / sizeof (struct mem_section))) - 1) / ((((1UL) << 12) / sizeof (struct mem_section))))), 0))
  return ((void *)0);


 if (!mem_section || !mem_section[root])
  return ((void *)0);

 return &mem_section[root][nr & ((((1UL) << 12) / sizeof (struct mem_section)) - 1)];
}
extern size_t mem_section_usage_size(void);

/*
 * We use the lower bits of the mem_map pointer to store
 * a little bit of information.  The pointer is calculated
 * as mem_map - section_nr_to_pfn(pnum).  The result is
 * aligned to the minimum alignment of the two values:
 *   1. All mem_map arrays are page-aligned.
 *   2. section_nr_to_pfn() always clears PFN_SECTION_SHIFT
 *      lowest bits.  PFN_SECTION_SHIFT is arch-specific
 *      (equal SECTION_SIZE_BITS - PAGE_SHIFT), and the
 *      worst combination is powerpc with 256k pages,
 *      which results in PFN_SECTION_SHIFT equal 6.
 * To sum it up, at least 6 bits are available on all architectures.
 * However, we can exceed 6 bits on some other architectures except
 * powerpc (e.g. 15 bits are available on x86_64, 13 bits are available
 * with the worst case of 64K pages on arm64) if we make sure the
 * exceeded bit is not applicable to powerpc.
 */
enum {
 SECTION_MARKED_PRESENT_BIT,
 SECTION_HAS_MEM_MAP_BIT,
 SECTION_IS_ONLINE_BIT,
 SECTION_IS_EARLY_BIT,



 SECTION_MAP_LAST_BIT,
};
# 1764 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *__section_mem_map_addr(struct mem_section *section)
{
 unsigned long map = section->section_mem_map;
 map &= (~(((((1UL))) << (SECTION_MAP_LAST_BIT)) - 1));
 return (struct page *)map;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int present_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_MARKED_PRESENT_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int present_section_nr(unsigned long nr)
{
 return present_section(__nr_to_section(nr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int valid_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_HAS_MEM_MAP_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int early_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_IS_EARLY_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int valid_section_nr(unsigned long nr)
{
 return valid_section(__nr_to_section(nr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int online_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_IS_ONLINE_BIT))));
}
# 1809 "./include/linux/mmzone.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int online_device_section(struct mem_section *section)
{
 return 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int online_section_nr(unsigned long nr)
{
 return online_section(__nr_to_section(nr));
}


void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn);
void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct mem_section *__pfn_to_section(unsigned long pfn)
{
 return __nr_to_section(pfn_to_section_nr(pfn));
}

extern unsigned long __highest_present_section_nr;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int subsection_map_index(unsigned long pfn)
{
 return (pfn & ~((~((1UL << (27 - 12))-1)))) / (1UL << (21 - 12));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pfn_section_valid(struct mem_section *ms, unsigned long pfn)
{
 int idx = subsection_map_index(pfn);

 return ((__builtin_constant_p(idx) && __builtin_constant_p((uintptr_t)(ms->usage->subsection_map) != (uintptr_t)((void *)0)) && (uintptr_t)(ms->usage->subsection_map) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(ms->usage->subsection_map))) ? const_test_bit(idx, ms->usage->subsection_map) : generic_test_bit(idx, ms->usage->subsection_map));
}
# 1852 "./include/linux/mmzone.h"
/**
 * pfn_valid - check if there is a valid memory map entry for a PFN
 * @pfn: the page frame number to check
 *
 * Check if there is a valid memory map entry aka struct page for the @pfn.
 * Note, that availability of the memory map entry does not imply that
 * there is actual usable memory at that @pfn. The struct page may
 * represent a hole or an unusable page frame.
 *
 * Return: 1 for PFNs that have memory map entries and 0 otherwise
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pfn_valid(unsigned long pfn)
{
 struct mem_section *ms;

 /*
	 * Ensure the upper PAGE_SHIFT bits are clear in the
	 * pfn. Else it might lead to false positives when
	 * some of the upper bits are set, but the lower bits
	 * match a valid pfn.
	 */
 if (((unsigned long)((((phys_addr_t)(pfn) << 12)) >> 12)) != pfn)
  return 0;

 if (pfn_to_section_nr(pfn) >= (1UL << (48 - 27)))
  return 0;
 ms = __pfn_to_section(pfn);
 if (!valid_section(ms))
  return 0;
 /*
	 * Traditionally early sections always returned pfn_valid() for
	 * the entire section-sized span.
	 */
 return early_section(ms) || pfn_section_valid(ms, pfn);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pfn_in_present_section(unsigned long pfn)
{
 if (pfn_to_section_nr(pfn) >= (1UL << (48 - 27)))
  return 0;
 return present_section(__pfn_to_section(pfn));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long next_present_section_nr(unsigned long section_nr)
{
 while (++section_nr <= __highest_present_section_nr) {
  if (present_section_nr(section_nr))
   return section_nr;
 }

 return -1;
}

/*
 * These are _only_ used during initialisation, therefore they
 * can use __initdata ...  They could have names to indicate
 * this restriction.
 */
# 1921 "./include/linux/mmzone.h"
void sparse_init(void);
# 8 "./include/linux/gfp.h" 2
# 1 "./include/linux/topology.h" 1
/*
 * include/linux/topology.h
 *
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */
# 46 "./include/linux/topology.h"
int topology_update_cpu_topology(void);

/* Conform to ACPI 2.0 SLIT distance definitions */







/*
 * If the distance between nodes in a system is larger than RECLAIM_DISTANCE
 * (in whatever arch specific measurement units returned by node_distance())
 * and node_reclaim_mode is enabled then the VM will only call node_reclaim()
 * on nodes within this distance.
 */



/*
 * The following tunable allows platforms to override the default node
 * reclaim distance (RECLAIM_DISTANCE) if remote memory accesses are
 * sufficiently fast that the default value actually hurts
 * performance.
 *
 * AMD EPYC machines use this because even though the 2-hop distance
 * is 32 (3.2x slower than a local memory access) performance actually
 * *improves* if allowed to reclaim memory and load balance tasks
 * between NUMA nodes 2-hops apart.
 */
extern int __attribute__((__section__(".data..read_mostly"))) node_reclaim_distance;






extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(int) numa_node;


/* Returns the number of the current Node. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int numa_node_id(void)
{
 return ({ typeof(numa_node) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(numa_node)) { case 1: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; });
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpu_to_node(int cpu)
{
 return (*({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(numa_node)))) *)((&(numa_node)))); (typeof((typeof(*((&(numa_node)))) *)((&(numa_node))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_numa_node(int node)
{
 do { do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(numa_node)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)node); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)node); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)node); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)node); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_cpu_numa_node(int cpu, int node)
{
 (*({ do { const void /* nothing */ *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(numa_node)))) *)((&(numa_node)))); (typeof((typeof(*((&(numa_node)))) *)((&(numa_node))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); })) = node;
}
# 167 "./include/linux/topology.h"
/* Returns the number of the nearest Node with memory */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int numa_mem_id(void)
{
 return numa_node_id();
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpu_to_mem(int cpu)
{
 return cpu_to_node(cpu);
}
# 237 "./include/linux/topology.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct cpumask *cpu_smt_mask(int cpu)
{
 return (&cpu_topology[cpu].thread_sibling);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct cpumask *cpu_cpu_mask(int cpu)
{
 return cpumask_of_node(cpu_to_node(cpu));
}
# 9 "./include/linux/gfp.h" 2

struct vm_area_struct;

/* Convert GFP flags to their corresponding migrate type */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int gfp_migratetype(const gfp_t gfp_flags)
{
 ((void)(sizeof(( long)((gfp_flags & ((( gfp_t)0x10u)|(( gfp_t)0x08u) /* ZONE_MOVABLE allowed */)) == ((( gfp_t)0x10u)|(( gfp_t)0x08u) /* ZONE_MOVABLE allowed */)))));
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_275(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(1UL << GFP_MOVABLE_SHIFT) != ___GFP_MOVABLE"))); if (!(!((1UL << 3) != 0x08u))) __compiletime_assert_275(); } while (0);
# 20 "./include/linux/gfp.h"
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_276(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(___GFP_MOVABLE >> GFP_MOVABLE_SHIFT) != MIGRATE_MOVABLE"))); if (!(!((0x08u >> 3) != MIGRATE_MOVABLE))) __compiletime_assert_276(); } while (0);
# 21 "./include/linux/gfp.h"
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_277(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(___GFP_RECLAIMABLE >> GFP_MOVABLE_SHIFT) != MIGRATE_RECLAIMABLE"))); if (!(!((0x10u >> 3) != MIGRATE_RECLAIMABLE))) __compiletime_assert_277(); } while (0);
# 22 "./include/linux/gfp.h"
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_278(void) __attribute__((__error__("BUILD_BUG_ON failed: " "((___GFP_MOVABLE | ___GFP_RECLAIMABLE) >> GFP_MOVABLE_SHIFT) != MIGRATE_HIGHATOMIC"))); if (!(!(((0x08u | 0x10u) >>
# 22 "./include/linux/gfp.h"
 3) != MIGRATE_HIGHATOMIC))) __compiletime_assert_278(); } while (0);


 if (__builtin_expect(!!(page_group_by_mobility_disabled), 0))
  return MIGRATE_UNMOVABLE;

 /* Group based on mobility */
 return ( unsigned long)(gfp_flags & ((( gfp_t)0x10u)|(( gfp_t)0x08u) /* ZONE_MOVABLE allowed */)) >> 3;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gfpflags_allow_blocking(const gfp_t gfp_flags)
{
 return !!(gfp_flags & (( gfp_t)0x400u) /* Caller can reclaim */);
}
# 57 "./include/linux/gfp.h"
/*
 * GFP_ZONE_TABLE is a word size bitstring that is used for looking up the
 * zone to use given the lowest 4 bits of gfp_t. Entries are GFP_ZONES_SHIFT
 * bits long and there are 16 of them to cover all possible combinations of
 * __GFP_DMA, __GFP_DMA32, __GFP_MOVABLE and __GFP_HIGHMEM.
 *
 * The zone fallback order is MOVABLE=>HIGHMEM=>NORMAL=>DMA32=>DMA.
 * But GFP_MOVABLE is not only a zone specifier but also an allocation
 * policy. Therefore __GFP_MOVABLE plus another zone selector is valid.
 * Only 1 bit of the lowest 3 bits (DMA,DMA32,HIGHMEM) can be set to "1".
 *
 *       bit       result
 *       =================
 *       0x0    => NORMAL
 *       0x1    => DMA or NORMAL
 *       0x2    => HIGHMEM or NORMAL
 *       0x3    => BAD (DMA+HIGHMEM)
 *       0x4    => DMA32 or NORMAL
 *       0x5    => BAD (DMA+DMA32)
 *       0x6    => BAD (HIGHMEM+DMA32)
 *       0x7    => BAD (HIGHMEM+DMA32+DMA)
 *       0x8    => NORMAL (MOVABLE+0)
 *       0x9    => DMA or NORMAL (MOVABLE+DMA)
 *       0xa    => MOVABLE (Movable is valid only if HIGHMEM is set too)
 *       0xb    => BAD (MOVABLE+HIGHMEM+DMA)
 *       0xc    => DMA32 or NORMAL (MOVABLE+DMA32)
 *       0xd    => BAD (MOVABLE+DMA32+DMA)
 *       0xe    => BAD (MOVABLE+DMA32+HIGHMEM)
 *       0xf    => BAD (MOVABLE+DMA32+HIGHMEM+DMA)
 *
 * GFP_ZONES_SHIFT must be <= 2 on 32 bit platforms.
 */
# 112 "./include/linux/gfp.h"
/*
 * GFP_ZONE_BAD is a bitmap for all combinations of __GFP_DMA, __GFP_DMA32
 * __GFP_HIGHMEM and __GFP_MOVABLE that are not permitted. One flag per
 * entry starting with bit 0. Bit is set if the combination is not
 * allowed.
 */
# 129 "./include/linux/gfp.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum zone_type gfp_zone(gfp_t flags)
{
 enum zone_type z;
 int bit = ( int) (flags & ((( gfp_t)0x01u)|(( gfp_t)0x02u)|(( gfp_t)0x04u)|(( gfp_t)0x08u) /* ZONE_MOVABLE allowed */));

 z = (( (ZONE_NORMAL << 0 * 2) | (ZONE_DMA << 0x01u * 2) | (ZONE_NORMAL << 0x02u * 2) | (ZONE_DMA32 << 0x04u * 2) | (ZONE_NORMAL << 0x08u * 2) | (ZONE_DMA << (0x08u | 0x01u) * 2) | (ZONE_MOVABLE << (0x08u | 0x02u) * 2) | (ZONE_DMA32 << (0x08u | 0x04u) * 2)) >> (bit * 2)) &
      ((1 << 2) - 1);
 ((void)(sizeof(( long)((( 1 << (0x01u | 0x02u) | 1 << (0x01u | 0x04u) | 1 << (0x04u | 0x02u) | 1 << (0x01u | 0x04u | 0x02u) | 1 << (0x08u | 0x02u | 0x01u) | 1 << (0x08u | 0x04u | 0x01u) | 1 << (0x08u | 0x04u | 0x02u) | 1 << (0x08u | 0x04u | 0x01u | 0x02u) ) >> bit) & 1))));
 return z;
}

/*
 * There is only one page-allocator function, and two main namespaces to
 * it. The alloc_page*() variants return 'struct page *' and as such
 * can allocate highmem pages, the *get*page*() variants return
 * virtual kernel addresses to the allocated page(s).
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int gfp_zonelist(gfp_t flags)
{

 if (__builtin_expect(!!(flags & (( gfp_t)0x200000u)), 0))
  return ZONELIST_NOFALLBACK;

 return ZONELIST_FALLBACK;
}

/*
 * We get the zone list from the current node and the gfp_mask.
 * This zone list contains a maximum of MAX_NUMNODES*MAX_NR_ZONES zones.
 * There are two zonelists per node, one for all zones with memory and
 * one containing just zones from the node the zonelist belongs to.
 *
 * For the case of non-NUMA systems the NODE_DATA() gets optimized to
 * &contig_page_data at compile-time.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct zonelist *node_zonelist(int nid, gfp_t flags)
{
 return (node_data[(nid)])->node_zonelists + gfp_zonelist(flags);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_free_page(struct page *page, int order) { }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_alloc_page(struct page *page, int order) { }


struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
  nodemask_t *nodemask);
struct folio *__folio_alloc(gfp_t gfp, unsigned int order, int preferred_nid,
  nodemask_t *nodemask);

unsigned long __alloc_pages_bulk(gfp_t gfp, int preferred_nid,
    nodemask_t *nodemask, int nr_pages,
    struct list_head *page_list,
    struct page **page_array);

unsigned long alloc_pages_bulk_array_mempolicy(gfp_t gfp,
    unsigned long nr_pages,
    struct page **page_array);

/* Bulk allocate order-0 pages */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
alloc_pages_bulk_list(gfp_t gfp, unsigned long nr_pages, struct list_head *list)
{
 return __alloc_pages_bulk(gfp, numa_mem_id(), ((void *)0), nr_pages, list, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
alloc_pages_bulk_array(gfp_t gfp, unsigned long nr_pages, struct page **page_array)
{
 return __alloc_pages_bulk(gfp, numa_mem_id(), ((void *)0), nr_pages, ((void *)0), page_array);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
alloc_pages_bulk_array_node(gfp_t gfp, int nid, unsigned long nr_pages, struct page **page_array)
{
 if (nid == (-1))
  nid = numa_mem_id();

 return __alloc_pages_bulk(gfp, nid, ((void *)0), nr_pages, ((void *)0), page_array);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void warn_if_node_offline(int this_node, gfp_t gfp_mask)
{
 gfp_t warn_gfp = gfp_mask & ((( gfp_t)0x200000u)|(( gfp_t)0x2000u));

 if (warn_gfp != ((( gfp_t)0x200000u)|(( gfp_t)0x2000u)))
  return;

 if (node_state((this_node), N_ONLINE))
  return;

 ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "4" /* warning conditions */ "%pGg allocation from offline node %d\n", &gfp_mask, this_node); });
 dump_stack();
}

/*
 * Allocate pages, preferring the node given as nid. The node must be valid and
 * online. For more general interface, see alloc_pages_node().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *
__alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)
{
 ((void)(sizeof(( long)(nid < 0 || nid >= (1 << 4)))));
 warn_if_node_offline(nid, gfp_mask);

 return __alloc_pages(gfp_mask, order, nid, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
struct folio *__folio_alloc_node(gfp_t gfp, unsigned int order, int nid)
{
 ((void)(sizeof(( long)(nid < 0 || nid >= (1 << 4)))));
 warn_if_node_offline(nid, gfp);

 return __folio_alloc(gfp, order, nid, ((void *)0));
}

/*
 * Allocate pages, preferring the node given as nid. When nid == NUMA_NO_NODE,
 * prefer the current CPU's closest node. Otherwise node must be valid and
 * online.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
      unsigned int order)
{
 if (nid == (-1))
  nid = numa_mem_id();

 return __alloc_pages_node(nid, gfp_mask, order);
}


struct page *alloc_pages(gfp_t gfp, unsigned int order);
struct folio *folio_alloc(gfp_t gfp, unsigned order);
struct folio *vma_alloc_folio(gfp_t gfp, int order, struct vm_area_struct *vma,
  unsigned long addr, bool hugepage);
# 281 "./include/linux/gfp.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *alloc_page_vma(gfp_t gfp,
  struct vm_area_struct *vma, unsigned long addr)
{
 struct folio *folio = vma_alloc_folio(gfp, 0, vma, addr, false);

 return &folio->page;
}

extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);
extern unsigned long get_zeroed_page(gfp_t gfp_mask);

void *alloc_pages_exact(size_t size, gfp_t gfp_mask) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
void free_pages_exact(void *virt, size_t size);
__attribute__((__section__(".meminit.text"))) __attribute__((__cold__)) __attribute__((__no_instrument_function__)) void *alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask) __attribute__((__alloc_size__(2))) __attribute__((__malloc__));







extern void __free_pages(struct page *page, unsigned int order);
extern void free_pages(unsigned long addr, unsigned int order);

struct page_frag_cache;
extern void __page_frag_cache_drain(struct page *page, unsigned int count);
extern void *page_frag_alloc_align(struct page_frag_cache *nc,
       unsigned int fragsz, gfp_t gfp_mask,
       unsigned int align_mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *page_frag_alloc(struct page_frag_cache *nc,
        unsigned int fragsz, gfp_t gfp_mask)
{
 return page_frag_alloc_align(nc, fragsz, gfp_mask, ~0u);
}

extern void page_frag_free(void *addr);




void page_alloc_init(void);
void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp);
void drain_all_pages(struct zone *zone);
void drain_local_pages(struct zone *zone);

void page_alloc_init_late(void);

/*
 * gfp_allowed_mask is set to GFP_BOOT_MASK during early boot to restrict what
 * GFP flags are used before interrupts are enabled. Once interrupts are
 * enabled, it is set to __GFP_BITS_MASK while the system is running. During
 * hibernation, it is used by PM to avoid I/O during memory allocation while
 * devices are suspended.
 */
extern gfp_t gfp_allowed_mask;

/* Returns true if the gfp_mask allows use of ALLOC_NO_WATERMARK */
bool gfp_pfmemalloc_allowed(gfp_t gfp_mask);

extern void pm_restrict_gfp_mask(void);
extern void pm_restore_gfp_mask(void);

extern gfp_t vma_thp_gfp_mask(struct vm_area_struct *vma);


extern bool pm_suspended_storage(void);
# 356 "./include/linux/gfp.h"
/* The below functions must be run on a range from a single zone. */
extern int alloc_contig_range(unsigned long start, unsigned long end,
         unsigned migratetype, gfp_t gfp_mask);
extern struct page *alloc_contig_pages(unsigned long nr_pages, gfp_t gfp_mask,
           int nid, nodemask_t *nodemask);

void free_contig_range(unsigned long pfn, unsigned long nr_pages);


/* CMA stuff */
extern void init_cma_reserved_pageblock(struct page *page);
# 16 "./include/linux/xarray.h" 2



# 1 "./include/linux/sched/mm.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/sched/mm.h"
# 1 "./include/linux/sync_core.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/*
 * This is a dummy sync_core_before_usermode() implementation that can be used
 * on all architectures which return to user-space through core serializing
 * instructions.
 * If your architecture returns to user-space through non-core-serializing
 * instructions, you need to write your own functions.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sync_core_before_usermode(void)
{
}
# 11 "./include/linux/sched/mm.h" 2
# 1 "./include/linux/ioasid.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







typedef unsigned int ioasid_t;
typedef ioasid_t (*ioasid_alloc_fn_t)(ioasid_t min, ioasid_t max, void *data);
typedef void (*ioasid_free_fn_t)(ioasid_t ioasid, void *data);

struct ioasid_set {
 int dummy;
};

/**
 * struct ioasid_allocator_ops - IOASID allocator helper functions and data
 *
 * @alloc:	helper function to allocate IOASID
 * @free:	helper function to free IOASID
 * @list:	for tracking ops that share helper functions but not data
 * @pdata:	data belong to the allocator, provided when calling alloc()
 */
struct ioasid_allocator_ops {
 ioasid_alloc_fn_t alloc;
 ioasid_free_fn_t free;
 struct list_head list;
 void *pdata;
};
# 49 "./include/linux/ioasid.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ioasid_t ioasid_alloc(struct ioasid_set *set, ioasid_t min,
        ioasid_t max, void *private)
{
 return ((ioasid_t)-1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ioasid_free(ioasid_t ioasid) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *ioasid_find(struct ioasid_set *set, ioasid_t ioasid,
    bool (*getter)(void *))
{
 return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ioasid_register_allocator(struct ioasid_allocator_ops *allocator)
{
 return -524 /* Operation is not supported */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ioasid_unregister_allocator(struct ioasid_allocator_ops *allocator)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ioasid_set_data(ioasid_t ioasid, void *data)
{
 return -524 /* Operation is not supported */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pasid_valid(ioasid_t ioasid)
{
 return false;
}
# 12 "./include/linux/sched/mm.h" 2

/*
 * Routines for handling mm_structs
 */
extern struct mm_struct *mm_alloc(void);

/**
 * mmgrab() - Pin a &struct mm_struct.
 * @mm: The &struct mm_struct to pin.
 *
 * Make sure that @mm will not get freed even after the owning task
 * exits. This doesn't guarantee that the associated address space
 * will still exist later on and mmget_not_zero() has to be used before
 * accessing it.
 *
 * This is a preferred way to pin @mm for a longer/unbounded amount
 * of time.
 *
 * Use mmdrop() to release the reference acquired by mmgrab().
 *
 * See also <Documentation/mm/active_mm.rst> for an in-depth explanation
 * of &mm_struct.mm_count vs &mm_struct.mm_users.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmgrab(struct mm_struct *mm)
{
 atomic_inc(&mm->mm_count);
}

extern void __mmdrop(struct mm_struct *mm);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmdrop(struct mm_struct *mm)
{
 /*
	 * The implicit full barrier implied by atomic_dec_and_test() is
	 * required by the membarrier system call before returning to
	 * user-space, after storing to rq->curr.
	 */
 if (__builtin_expect(!!(atomic_dec_and_test(&mm->mm_count)), 0))
  __mmdrop(mm);
}
# 76 "./include/linux/sched/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmdrop_sched(struct mm_struct *mm)
{
 mmdrop(mm);
}


/**
 * mmget() - Pin the address space associated with a &struct mm_struct.
 * @mm: The address space to pin.
 *
 * Make sure that the address space of the given &struct mm_struct doesn't
 * go away. This does not protect against parts of the address space being
 * modified or freed, however.
 *
 * Never use this function to pin this address space for an
 * unbounded/indefinite amount of time.
 *
 * Use mmput() to release the reference acquired by mmget().
 *
 * See also <Documentation/mm/active_mm.rst> for an in-depth explanation
 * of &mm_struct.mm_count vs &mm_struct.mm_users.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmget(struct mm_struct *mm)
{
 atomic_inc(&mm->mm_users);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mmget_not_zero(struct mm_struct *mm)
{
 return atomic_inc_not_zero(&mm->mm_users);
}

/* mmput gets rid of the mappings and all user-space */
extern void mmput(struct mm_struct *);

/* same as above but performs the slow path from the async context. Can
 * be called from the atomic context as well
 */
void mmput_async(struct mm_struct *);


/* Grab a reference to a task's mm, if it is not already going away */
extern struct mm_struct *get_task_mm(struct task_struct *task);
/*
 * Grab a reference to a task's mm, if it is not already going away
 * and ptrace_may_access with the mode parameter passed to it
 * succeeds.
 */
extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);
/* Remove the current tasks stale references to the old mm_struct on exit() */
extern void exit_mm_release(struct task_struct *, struct mm_struct *);
/* Remove the current tasks stale references to the old mm_struct on exec() */
extern void exec_mm_release(struct task_struct *, struct mm_struct *);


extern void mm_update_next_owner(struct mm_struct *mm);
# 147 "./include/linux/sched/mm.h"
extern void arch_pick_mmap_layout(struct mm_struct *mm,
      struct rlimit *rlim_stack);
extern unsigned long
arch_get_unmapped_area(struct file *, unsigned long, unsigned long,
         unsigned long, unsigned long);
extern unsigned long
arch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
     unsigned long len, unsigned long pgoff,
     unsigned long flags);

unsigned long
generic_get_unmapped_area(struct file *filp, unsigned long addr,
     unsigned long len, unsigned long pgoff,
     unsigned long flags);
unsigned long
generic_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
      unsigned long len, unsigned long pgoff,
      unsigned long flags);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool in_vfork(struct task_struct *tsk)
{
 bool ret;

 /*
	 * need RCU to access ->real_parent if CLONE_VM was used along with
	 * CLONE_PARENT.
	 *
	 * We check real_parent->mm == tsk->mm because CLONE_VFORK does not
	 * imply CLONE_VM
	 *
	 * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus
	 * ->real_parent is not necessarily the task doing vfork(), so in
	 * theory we can't rely on task_lock() if we want to dereference it.
	 *
	 * And in this case we can't trust the real_parent->mm == tsk->mm
	 * check, it can be false negative. But we do not care, if init or
	 * another oom-unkillable task does this it should blame itself.
	 */
 rcu_read_lock();
 ret = tsk->vfork_done &&
   ({ /* Dependency order vs. p above. */ typeof(*(tsk->real_parent)) *__UNIQUE_ID_rcu279 = (typeof(*(tsk->real_parent)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_280(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((tsk->real_parent)) == sizeof(char) || sizeof((tsk->real_parent)) == sizeof(short) || sizeof((tsk->real_parent)) == sizeof(int) || sizeof((tsk->real_parent)) == sizeof(long)) || sizeof((tsk->real_parent)) == sizeof(long long))) __compiletime_assert_280(); } while (0); (*(const volatile typeof( _Generic(((tsk->real_parent)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((tsk->real_parent)))) *)&((tsk->real_parent))); }); do { } while (0 && (!((0) || rcu_read_lock_held()))); ; ((typeof(*(tsk->real_parent)) *)(__UNIQUE_ID_rcu279)); })->mm == tsk->mm;
# 192 "./include/linux/sched/mm.h"
 rcu_read_unlock();

 return ret;
}

/*
 * Applies per-task gfp context to the given allocation flags.
 * PF_MEMALLOC_NOIO implies GFP_NOIO
 * PF_MEMALLOC_NOFS implies GFP_NOFS
 * PF_MEMALLOC_PIN  implies !GFP_MOVABLE
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) gfp_t current_gfp_context(gfp_t flags)
{
 unsigned int pflags = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_281(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(get_current()->flags) == sizeof(char) || sizeof(get_current()->flags) == sizeof(short) || sizeof(get_current()->flags) == sizeof(int) || sizeof(get_current()->flags) == sizeof(long)) || sizeof(get_current()->flags) == sizeof(long long))) __compiletime_assert_281(); } while (0); (*(const volatile typeof( _Generic((get_current()->flags), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (get_current()->flags))) *)&(get_current()->flags)); });
# 207 "./include/linux/sched/mm.h"
 if (__builtin_expect(!!(pflags & (0x00080000 /* All allocation requests will inherit GFP_NOIO */ | 0x00040000 /* All allocation requests will inherit GFP_NOFS */ | 0x10000000 /* Allocation context constrained to zones which allow long term pinning. */)), 0)) {
  /*
		 * NOIO implies both NOIO and NOFS and it is a weaker context
		 * so always make sure it makes precedence
		 */
  if (pflags & 0x00080000 /* All allocation requests will inherit GFP_NOIO */)
   flags &= ~((( gfp_t)0x40u) | (( gfp_t)0x80u));
  else if (pflags & 0x00040000 /* All allocation requests will inherit GFP_NOFS */)
   flags &= ~(( gfp_t)0x80u);

  if (pflags & 0x10000000 /* Allocation context constrained to zones which allow long term pinning. */)
   flags &= ~(( gfp_t)0x08u) /* ZONE_MOVABLE allowed */;
 }
 return flags;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __fs_reclaim_acquire(unsigned long ip) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __fs_reclaim_release(unsigned long ip) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void fs_reclaim_acquire(gfp_t gfp_mask) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void fs_reclaim_release(gfp_t gfp_mask) { }


/* Any memory-allocation retry loop should use
 * memalloc_retry_wait(), and pass the flags for the most
 * constrained allocation attempt that might have failed.
 * This provides useful documentation of where loops are,
 * and a central place to fine tune the waiting as the MM
 * implementation changes.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memalloc_retry_wait(gfp_t gfp_flags)
{
 /* We use io_schedule_timeout because waiting for memory
	 * typically included waiting for dirty pages to be
	 * written out, which requires IO.
	 */
 do { do { } while (0); do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_282(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(get_current()->__state) == sizeof(char) || sizeof(get_current()->__state) == sizeof(short) || sizeof(get_current()->__state) == sizeof(int) || sizeof(get_current()->__state) == sizeof(long)) || sizeof(get_current()->__state) == sizeof(long long))) __compiletime_assert_282(); } while (0); do { *(volatile typeof(get_current()->__state) *)&(get_current()->__state) = ((0x00000002)); } while (0); } while (0); } while (0);
# 249 "./include/linux/sched/mm.h"
 gfp_flags = current_gfp_context(gfp_flags);
 if (gfpflags_allow_blocking(gfp_flags) &&
     !(gfp_flags & (( gfp_t)0x10000u)))
  /* Probably waited already, no need for much more */
  io_schedule_timeout(1);
 else
  /* Probably didn't wait, and has now released a lock,
		 * so now is a good time to wait
		 */
  io_schedule_timeout(250 /* Internal kernel timer frequency *//50);
}

/**
 * might_alloc - Mark possible allocation sites
 * @gfp_mask: gfp_t flags that would be used to allocate
 *
 * Similar to might_sleep() and other annotations, this can be used in functions
 * that might allocate, but often don't. Compiles to nothing without
 * CONFIG_LOCKDEP. Includes a conditional might_sleep() if @gfp allows blocking.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void might_alloc(gfp_t gfp_mask)
{
 fs_reclaim_acquire(gfp_mask);
 fs_reclaim_release(gfp_mask);

 do { if (gfpflags_allow_blocking(gfp_mask)) do { do { } while (0); } while (0); } while (0);
}

/**
 * memalloc_noio_save - Marks implicit GFP_NOIO allocation scope.
 *
 * This functions marks the beginning of the GFP_NOIO allocation scope.
 * All further allocations will implicitly drop __GFP_IO flag and so
 * they are safe for the IO critical section from the allocation recursion
 * point of view. Use memalloc_noio_restore to end the scope with flags
 * returned by this function.
 *
 * This function is safe to be used from any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int memalloc_noio_save(void)
{
 unsigned int flags = get_current()->flags & 0x00080000 /* All allocation requests will inherit GFP_NOIO */;
 get_current()->flags |= 0x00080000 /* All allocation requests will inherit GFP_NOIO */;
 return flags;
}

/**
 * memalloc_noio_restore - Ends the implicit GFP_NOIO scope.
 * @flags: Flags to restore.
 *
 * Ends the implicit GFP_NOIO scope started by memalloc_noio_save function.
 * Always make sure that the given flags is the return value from the
 * pairing memalloc_noio_save call.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memalloc_noio_restore(unsigned int flags)
{
 get_current()->flags = (get_current()->flags & ~0x00080000 /* All allocation requests will inherit GFP_NOIO */) | flags;
}

/**
 * memalloc_nofs_save - Marks implicit GFP_NOFS allocation scope.
 *
 * This functions marks the beginning of the GFP_NOFS allocation scope.
 * All further allocations will implicitly drop __GFP_FS flag and so
 * they are safe for the FS critical section from the allocation recursion
 * point of view. Use memalloc_nofs_restore to end the scope with flags
 * returned by this function.
 *
 * This function is safe to be used from any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int memalloc_nofs_save(void)
{
 unsigned int flags = get_current()->flags & 0x00040000 /* All allocation requests will inherit GFP_NOFS */;
 get_current()->flags |= 0x00040000 /* All allocation requests will inherit GFP_NOFS */;
 return flags;
}

/**
 * memalloc_nofs_restore - Ends the implicit GFP_NOFS scope.
 * @flags: Flags to restore.
 *
 * Ends the implicit GFP_NOFS scope started by memalloc_nofs_save function.
 * Always make sure that the given flags is the return value from the
 * pairing memalloc_nofs_save call.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memalloc_nofs_restore(unsigned int flags)
{
 get_current()->flags = (get_current()->flags & ~0x00040000 /* All allocation requests will inherit GFP_NOFS */) | flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int memalloc_noreclaim_save(void)
{
 unsigned int flags = get_current()->flags & 0x00000800 /* Allocating memory */;
 get_current()->flags |= 0x00000800 /* Allocating memory */;
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memalloc_noreclaim_restore(unsigned int flags)
{
 get_current()->flags = (get_current()->flags & ~0x00000800 /* Allocating memory */) | flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int memalloc_pin_save(void)
{
 unsigned int flags = get_current()->flags & 0x10000000 /* Allocation context constrained to zones which allow long term pinning. */;

 get_current()->flags |= 0x10000000 /* Allocation context constrained to zones which allow long term pinning. */;
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memalloc_pin_restore(unsigned int flags)
{
 get_current()->flags = (get_current()->flags & ~0x10000000 /* Allocation context constrained to zones which allow long term pinning. */) | flags;
}


extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct mem_cgroup *) int_active_memcg;
/**
 * set_active_memcg - Starts the remote memcg charging scope.
 * @memcg: memcg to charge.
 *
 * This function marks the beginning of the remote memcg charging scope. All the
 * __GFP_ACCOUNT allocations till the end of the scope will be charged to the
 * given memcg.
 *
 * NOTE: This function can nest. Users must save the return value and
 * reset the previous value after their own charging scope is over.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct mem_cgroup *
set_active_memcg(struct mem_cgroup *memcg)
{
 struct mem_cgroup *old;

 if (!(!(((preempt_count() & (((1UL << (4))-1) << (((0 + 8) + 8) + 4)))) | ((preempt_count() & (((1UL << (4))-1) << ((0 + 8) + 8)))) | ((preempt_count() & (((1UL << (8))-1) << (0 + 8))) & (1UL << (0 + 8)))))) {
  old = ({ typeof(int_active_memcg) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(int_active_memcg)) { case 1: pscr_ret__ = ({ typeof(int_active_memcg) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(int_active_memcg))__percpu_read_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; case 2: pscr_ret__ = ({ typeof(int_active_memcg) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(int_active_memcg))__percpu_read_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; case 4: pscr_ret__ = ({ typeof(int_active_memcg) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(int_active_memcg))__percpu_read_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; case 8: pscr_ret__ = ({ typeof(int_active_memcg) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(int_active_memcg))__percpu_read_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; });
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(int_active_memcg)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)memcg); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)memcg); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)memcg); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_write_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(int_active_memcg)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg))); (typeof((typeof(*(&(int_active_memcg))) *)(&(int_active_memcg)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), (unsigned long)memcg); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
 } else {
  old = get_current()->active_memcg;
  get_current()->active_memcg = memcg;
 }

 return old;
}
# 401 "./include/linux/sched/mm.h"
enum {
 MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY = (1U << 0),
 MEMBARRIER_STATE_PRIVATE_EXPEDITED = (1U << 1),
 MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY = (1U << 2),
 MEMBARRIER_STATE_GLOBAL_EXPEDITED = (1U << 3),
 MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY = (1U << 4),
 MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE = (1U << 5),
 MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY = (1U << 6),
 MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ = (1U << 7),
};

enum {
 MEMBARRIER_FLAG_SYNC_CORE = (1U << 0),
 MEMBARRIER_FLAG_RSEQ = (1U << 1),
};





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)
{
 if (get_current()->mm != mm)
  return;
 if (__builtin_expect(!!(!(atomic_read(&mm->membarrier_state) & MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE)), 1))

  return;
 sync_core_before_usermode();
}

extern void membarrier_exec_mmap(struct mm_struct *mm);

extern void membarrier_update_current_mm(struct mm_struct *next_mm);
# 474 "./include/linux/sched/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_pasid_init(struct mm_struct *mm) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_pasid_set(struct mm_struct *mm, u32 pasid) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_pasid_drop(struct mm_struct *mm) {}
# 20 "./include/linux/xarray.h" 2



/*
 * The bottom two bits of the entry determine how the XArray interprets
 * the contents:
 *
 * 00: Pointer entry
 * 10: Internal entry
 * x1: Value entry or tagged pointer
 *
 * Attempting to store internal entries in the XArray is a bug.
 *
 * Most internal entries are pointers to the next node in the tree.
 * The following internal entries have a special meaning:
 *
 * 0-62: Sibling entries
 * 256: Retry entry
 * 257: Zero entry
 *
 * Errors are also represented as internal entries, but use the negative
 * space (-4094 to -2).  They're never stored in the slots array; only
 * returned by the normal API.
 */



/**
 * xa_mk_value() - Create an XArray entry from an integer.
 * @v: Value to store in XArray.
 *
 * Context: Any context.
 * Return: An entry suitable for storing in the XArray.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_mk_value(unsigned long v)
{
 ({ int __ret_warn_on = !!((long)v < 0); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/xarray.h\"; .popsection; .long 14472b - .; .short 56; .short (1 << 0)|(((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 return (void *)((v << 1) | 1);
}

/**
 * xa_to_value() - Get value stored in an XArray entry.
 * @entry: XArray entry.
 *
 * Context: Any context.
 * Return: The value stored in the XArray entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long xa_to_value(const void *entry)
{
 return (unsigned long)entry >> 1;
}

/**
 * xa_is_value() - Determine if an entry is a value.
 * @entry: XArray entry.
 *
 * Context: Any context.
 * Return: True if the entry is a value, false if it is a pointer.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_value(const void *entry)
{
 return (unsigned long)entry & 1;
}

/**
 * xa_tag_pointer() - Create an XArray entry for a tagged pointer.
 * @p: Plain pointer.
 * @tag: Tag value (0, 1 or 3).
 *
 * If the user of the XArray prefers, they can tag their pointers instead
 * of storing value entries.  Three tags are available (0, 1 and 3).
 * These are distinct from the xa_mark_t as they are not replicated up
 * through the array and cannot be searched for.
 *
 * Context: Any context.
 * Return: An XArray entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_tag_pointer(void *p, unsigned long tag)
{
 return (void *)((unsigned long)p | tag);
}

/**
 * xa_untag_pointer() - Turn an XArray entry into a plain pointer.
 * @entry: XArray entry.
 *
 * If you have stored a tagged pointer in the XArray, call this function
 * to get the untagged version of the pointer.
 *
 * Context: Any context.
 * Return: A pointer.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_untag_pointer(void *entry)
{
 return (void *)((unsigned long)entry & ~3UL);
}

/**
 * xa_pointer_tag() - Get the tag stored in an XArray entry.
 * @entry: XArray entry.
 *
 * If you have stored a tagged pointer in the XArray, call this function
 * to get the tag of that pointer.
 *
 * Context: Any context.
 * Return: A tag.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int xa_pointer_tag(void *entry)
{
 return (unsigned long)entry & 3UL;
}

/*
 * xa_mk_internal() - Create an internal entry.
 * @v: Value to turn into an internal entry.
 *
 * Internal entries are used for a number of purposes.  Entries 0-255 are
 * used for sibling entries (only 0-62 are used by the current code).  256
 * is used for the retry entry.  257 is used for the reserved / zero entry.
 * Negative internal entries are used to represent errnos.  Node pointers
 * are also tagged as internal entries in some situations.
 *
 * Context: Any context.
 * Return: An XArray internal entry corresponding to this value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_mk_internal(unsigned long v)
{
 return (void *)((v << 2) | 2);
}

/*
 * xa_to_internal() - Extract the value from an internal entry.
 * @entry: XArray entry.
 *
 * Context: Any context.
 * Return: The value which was stored in the internal entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long xa_to_internal(const void *entry)
{
 return (unsigned long)entry >> 2;
}

/*
 * xa_is_internal() - Is the entry an internal entry?
 * @entry: XArray entry.
 *
 * Context: Any context.
 * Return: %true if the entry is an internal entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_internal(const void *entry)
{
 return ((unsigned long)entry & 3) == 2;
}



/**
 * xa_is_zero() - Is the entry a zero entry?
 * @entry: Entry retrieved from the XArray
 *
 * The normal API will return NULL as the contents of a slot containing
 * a zero entry.  You can only see zero entries by using the advanced API.
 *
 * Return: %true if the entry is a zero entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_zero(const void *entry)
{
 return __builtin_expect(!!(entry == xa_mk_internal(257)), 0);
}

/**
 * xa_is_err() - Report whether an XArray operation returned an error
 * @entry: Result from calling an XArray function
 *
 * If an XArray operation cannot complete an operation, it will return
 * a special value indicating an error.  This function tells you
 * whether an error occurred; xa_err() tells you which error occurred.
 *
 * Context: Any context.
 * Return: %true if the entry indicates an error.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_err(const void *entry)
{
 return __builtin_expect(!!(xa_is_internal(entry) && entry >= xa_mk_internal(-4095)), 0);

}

/**
 * xa_err() - Turn an XArray result into an errno.
 * @entry: Result from calling an XArray function.
 *
 * If an XArray operation cannot complete an operation, it will return
 * a special pointer value which encodes an errno.  This function extracts
 * the errno from the pointer value, or returns 0 if the pointer does not
 * represent an errno.
 *
 * Context: Any context.
 * Return: A negative errno or 0.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int xa_err(void *entry)
{
 /* xa_to_internal() would not do sign extension. */
 if (xa_is_err(entry))
  return (long)entry >> 2;
 return 0;
}

/**
 * struct xa_limit - Represents a range of IDs.
 * @min: The lowest ID to allocate (inclusive).
 * @max: The maximum ID to allocate (inclusive).
 *
 * This structure is used either directly or via the XA_LIMIT() macro
 * to communicate the range of IDs that are valid for allocation.
 * Three common ranges are predefined for you:
 * * xa_limit_32b	- [0 - UINT_MAX]
 * * xa_limit_31b	- [0 - INT_MAX]
 * * xa_limit_16b	- [0 - USHRT_MAX]
 */
struct xa_limit {
 u32 max;
 u32 min;
};







typedef unsigned xa_mark_t;







enum xa_lock_type {
 XA_LOCK_IRQ = 1,
 XA_LOCK_BH = 2,
};

/*
 * Values for xa_flags.  The radix tree stores its GFP flags in the xa_flags,
 * and we remain compatible with that.
 */
# 276 "./include/linux/xarray.h"
/* ALLOC is for a normal 0-based alloc.  ALLOC1 is for an 1-based alloc */



/**
 * struct xarray - The anchor of the XArray.
 * @xa_lock: Lock that protects the contents of the XArray.
 *
 * To use the xarray, define it statically or embed it in your data structure.
 * It is a very small data structure, so it does not usually make sense to
 * allocate it separately and keep a pointer to it in your data structure.
 *
 * You may use the xa_lock to protect your own data structures as well.
 */
/*
 * If all of the entries in the array are NULL, @xa_head is a NULL pointer.
 * If the only non-NULL entry in the array is at index 0, @xa_head is that
 * entry.  If any other entry in the array is non-NULL, @xa_head points
 * to an @xa_node.
 */
struct xarray {
 spinlock_t xa_lock;
/* private: The rest of the data structure is not to be used directly. */
 gfp_t xa_flags;
 void /* nothing */ * xa_head;
};







/**
 * DEFINE_XARRAY_FLAGS() - Define an XArray with custom flags.
 * @name: A string that names your XArray.
 * @flags: XA_FLAG values.
 *
 * This is intended for file scope definitions of XArrays.  It declares
 * and initialises an empty XArray with the chosen name and flags.  It is
 * equivalent to calling xa_init_flags() on the array, but it does the
 * initialisation at compiletime instead of runtime.
 */



/**
 * DEFINE_XARRAY() - Define an XArray.
 * @name: A string that names your XArray.
 *
 * This is intended for file scope definitions of XArrays.  It declares
 * and initialises an empty XArray with the chosen name.  It is equivalent
 * to calling xa_init() on the array, but it does the initialisation at
 * compiletime instead of runtime.
 */


/**
 * DEFINE_XARRAY_ALLOC() - Define an XArray which allocates IDs starting at 0.
 * @name: A string that names your XArray.
 *
 * This is intended for file scope definitions of allocating XArrays.
 * See also DEFINE_XARRAY().
 */


/**
 * DEFINE_XARRAY_ALLOC1() - Define an XArray which allocates IDs starting at 1.
 * @name: A string that names your XArray.
 *
 * This is intended for file scope definitions of allocating XArrays.
 * See also DEFINE_XARRAY().
 */


void *xa_load(struct xarray *, unsigned long index);
void *xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);
void *xa_erase(struct xarray *, unsigned long index);
void *xa_store_range(struct xarray *, unsigned long first, unsigned long last,
   void *entry, gfp_t);
bool xa_get_mark(struct xarray *, unsigned long index, xa_mark_t);
void xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);
void xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);
void *xa_find(struct xarray *xa, unsigned long *index,
  unsigned long max, xa_mark_t) __attribute__((nonnull(2)));
void *xa_find_after(struct xarray *xa, unsigned long *index,
  unsigned long max, xa_mark_t) __attribute__((nonnull(2)));
unsigned int xa_extract(struct xarray *, void **dst, unsigned long start,
  unsigned long max, unsigned int n, xa_mark_t);
void xa_destroy(struct xarray *);

/**
 * xa_init_flags() - Initialise an empty XArray with flags.
 * @xa: XArray.
 * @flags: XA_FLAG values.
 *
 * If you need to initialise an XArray with special flags (eg you need
 * to take the lock from interrupt context), use this function instead
 * of xa_init().
 *
 * Context: Any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xa_init_flags(struct xarray *xa, gfp_t flags)
{
 do { spinlock_check(&xa->xa_lock); *(&xa->xa_lock) = (spinlock_t) { { .rlock = { .raw_lock = { { .val = { (0) } } }, } } }; } while (0);
 xa->xa_flags = flags;
 xa->xa_head = ((void *)0);
}

/**
 * xa_init() - Initialise an empty XArray.
 * @xa: XArray.
 *
 * An empty XArray is full of NULL entries.
 *
 * Context: Any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xa_init(struct xarray *xa)
{
 xa_init_flags(xa, 0);
}

/**
 * xa_empty() - Determine if an array has any present entries.
 * @xa: XArray.
 *
 * Context: Any context.
 * Return: %true if the array contains only NULL pointers.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_empty(const struct xarray *xa)
{
 return xa->xa_head == ((void *)0);
}

/**
 * xa_marked() - Inquire whether any entry in this array has a mark set
 * @xa: Array
 * @mark: Mark value
 *
 * Context: Any context.
 * Return: %true if any entry has this mark set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_marked(const struct xarray *xa, xa_mark_t mark)
{
 return xa->xa_flags & (( gfp_t)((1U << (27 + 0)) << ( unsigned)(mark)));
}

/**
 * xa_for_each_range() - Iterate over a portion of an XArray.
 * @xa: XArray.
 * @index: Index of @entry.
 * @entry: Entry retrieved from array.
 * @start: First index to retrieve from array.
 * @last: Last index to retrieve from array.
 *
 * During the iteration, @entry will have the value of the entry stored
 * in @xa at @index.  You may modify @index during the iteration if you
 * want to skip or reprocess indices.  It is safe to modify the array
 * during the iteration.  At the end of the iteration, @entry will be set
 * to NULL and @index will have a value less than or equal to max.
 *
 * xa_for_each_range() is O(n.log(n)) while xas_for_each() is O(n).  You have
 * to handle your own locking with xas_for_each(), and if you have to unlock
 * after each iteration, it will also end up being O(n.log(n)).
 * xa_for_each_range() will spin if it hits a retry entry; if you intend to
 * see retry entries, you should use the xas_for_each() iterator instead.
 * The xas_for_each() iterator will expand into more inline code than
 * xa_for_each_range().
 *
 * Context: Any context.  Takes and releases the RCU lock.
 */






/**
 * xa_for_each_start() - Iterate over a portion of an XArray.
 * @xa: XArray.
 * @index: Index of @entry.
 * @entry: Entry retrieved from array.
 * @start: First index to retrieve from array.
 *
 * During the iteration, @entry will have the value of the entry stored
 * in @xa at @index.  You may modify @index during the iteration if you
 * want to skip or reprocess indices.  It is safe to modify the array
 * during the iteration.  At the end of the iteration, @entry will be set
 * to NULL and @index will have a value less than or equal to max.
 *
 * xa_for_each_start() is O(n.log(n)) while xas_for_each() is O(n).  You have
 * to handle your own locking with xas_for_each(), and if you have to unlock
 * after each iteration, it will also end up being O(n.log(n)).
 * xa_for_each_start() will spin if it hits a retry entry; if you intend to
 * see retry entries, you should use the xas_for_each() iterator instead.
 * The xas_for_each() iterator will expand into more inline code than
 * xa_for_each_start().
 *
 * Context: Any context.  Takes and releases the RCU lock.
 */



/**
 * xa_for_each() - Iterate over present entries in an XArray.
 * @xa: XArray.
 * @index: Index of @entry.
 * @entry: Entry retrieved from array.
 *
 * During the iteration, @entry will have the value of the entry stored
 * in @xa at @index.  You may modify @index during the iteration if you want
 * to skip or reprocess indices.  It is safe to modify the array during the
 * iteration.  At the end of the iteration, @entry will be set to NULL and
 * @index will have a value less than or equal to max.
 *
 * xa_for_each() is O(n.log(n)) while xas_for_each() is O(n).  You have
 * to handle your own locking with xas_for_each(), and if you have to unlock
 * after each iteration, it will also end up being O(n.log(n)).  xa_for_each()
 * will spin if it hits a retry entry; if you intend to see retry entries,
 * you should use the xas_for_each() iterator instead.  The xas_for_each()
 * iterator will expand into more inline code than xa_for_each().
 *
 * Context: Any context.  Takes and releases the RCU lock.
 */



/**
 * xa_for_each_marked() - Iterate over marked entries in an XArray.
 * @xa: XArray.
 * @index: Index of @entry.
 * @entry: Entry retrieved from array.
 * @filter: Selection criterion.
 *
 * During the iteration, @entry will have the value of the entry stored
 * in @xa at @index.  The iteration will skip all entries in the array
 * which do not match @filter.  You may modify @index during the iteration
 * if you want to skip or reprocess indices.  It is safe to modify the array
 * during the iteration.  At the end of the iteration, @entry will be set to
 * NULL and @index will have a value less than or equal to max.
 *
 * xa_for_each_marked() is O(n.log(n)) while xas_for_each_marked() is O(n).
 * You have to handle your own locking with xas_for_each(), and if you have
 * to unlock after each iteration, it will also end up being O(n.log(n)).
 * xa_for_each_marked() will spin if it hits a retry entry; if you intend to
 * see retry entries, you should use the xas_for_each_marked() iterator
 * instead.  The xas_for_each_marked() iterator will expand into more inline
 * code than xa_for_each_marked().
 *
 * Context: Any context.  Takes and releases the RCU lock.
 */
# 551 "./include/linux/xarray.h"
/*
 * Versions of the normal API which require the caller to hold the
 * xa_lock.  If the GFP flags allow it, they will drop the lock to
 * allocate memory, then reacquire it afterwards.  These functions
 * may also re-enable interrupts if the XArray flags indicate the
 * locking should be interrupt safe.
 */
void *__xa_erase(struct xarray *, unsigned long index);
void *__xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);
void *__xa_cmpxchg(struct xarray *, unsigned long index, void *old,
  void *entry, gfp_t);
int __attribute__((__warn_unused_result__)) __xa_insert(struct xarray *, unsigned long index,
  void *entry, gfp_t);
int __attribute__((__warn_unused_result__)) __xa_alloc(struct xarray *, u32 *id, void *entry,
  struct xa_limit, gfp_t);
int __attribute__((__warn_unused_result__)) __xa_alloc_cyclic(struct xarray *, u32 *id, void *entry,
  struct xa_limit, u32 *next, gfp_t);
void __xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);
void __xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);

/**
 * xa_store_bh() - Store this entry in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @entry: New entry.
 * @gfp: Memory allocation flags.
 *
 * This function is like calling xa_store() except it disables softirqs
 * while holding the array lock.
 *
 * Context: Any context.  Takes and releases the xa_lock while
 * disabling softirqs.
 * Return: The old entry at this index or xa_err() if an error happened.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_store_bh(struct xarray *xa, unsigned long index,
  void *entry, gfp_t gfp)
{
 void *curr;

 might_alloc(gfp);
 spin_lock_bh(&(xa)->xa_lock);
 curr = __xa_store(xa, index, entry, gfp);
 spin_unlock_bh(&(xa)->xa_lock);

 return curr;
}

/**
 * xa_store_irq() - Store this entry in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @entry: New entry.
 * @gfp: Memory allocation flags.
 *
 * This function is like calling xa_store() except it disables interrupts
 * while holding the array lock.
 *
 * Context: Process context.  Takes and releases the xa_lock while
 * disabling interrupts.
 * Return: The old entry at this index or xa_err() if an error happened.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_store_irq(struct xarray *xa, unsigned long index,
  void *entry, gfp_t gfp)
{
 void *curr;

 might_alloc(gfp);
 spin_lock_irq(&(xa)->xa_lock);
 curr = __xa_store(xa, index, entry, gfp);
 spin_unlock_irq(&(xa)->xa_lock);

 return curr;
}

/**
 * xa_erase_bh() - Erase this entry from the XArray.
 * @xa: XArray.
 * @index: Index of entry.
 *
 * After this function returns, loading from @index will return %NULL.
 * If the index is part of a multi-index entry, all indices will be erased
 * and none of the entries will be part of a multi-index entry.
 *
 * Context: Any context.  Takes and releases the xa_lock while
 * disabling softirqs.
 * Return: The entry which used to be at this index.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_erase_bh(struct xarray *xa, unsigned long index)
{
 void *entry;

 spin_lock_bh(&(xa)->xa_lock);
 entry = __xa_erase(xa, index);
 spin_unlock_bh(&(xa)->xa_lock);

 return entry;
}

/**
 * xa_erase_irq() - Erase this entry from the XArray.
 * @xa: XArray.
 * @index: Index of entry.
 *
 * After this function returns, loading from @index will return %NULL.
 * If the index is part of a multi-index entry, all indices will be erased
 * and none of the entries will be part of a multi-index entry.
 *
 * Context: Process context.  Takes and releases the xa_lock while
 * disabling interrupts.
 * Return: The entry which used to be at this index.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_erase_irq(struct xarray *xa, unsigned long index)
{
 void *entry;

 spin_lock_irq(&(xa)->xa_lock);
 entry = __xa_erase(xa, index);
 spin_unlock_irq(&(xa)->xa_lock);

 return entry;
}

/**
 * xa_cmpxchg() - Conditionally replace an entry in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @old: Old value to test against.
 * @entry: New value to place in array.
 * @gfp: Memory allocation flags.
 *
 * If the entry at @index is the same as @old, replace it with @entry.
 * If the return value is equal to @old, then the exchange was successful.
 *
 * Context: Any context.  Takes and releases the xa_lock.  May sleep
 * if the @gfp flags permit.
 * Return: The old value at this index or xa_err() if an error happened.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_cmpxchg(struct xarray *xa, unsigned long index,
   void *old, void *entry, gfp_t gfp)
{
 void *curr;

 might_alloc(gfp);
 spin_lock(&(xa)->xa_lock);
 curr = __xa_cmpxchg(xa, index, old, entry, gfp);
 spin_unlock(&(xa)->xa_lock);

 return curr;
}

/**
 * xa_cmpxchg_bh() - Conditionally replace an entry in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @old: Old value to test against.
 * @entry: New value to place in array.
 * @gfp: Memory allocation flags.
 *
 * This function is like calling xa_cmpxchg() except it disables softirqs
 * while holding the array lock.
 *
 * Context: Any context.  Takes and releases the xa_lock while
 * disabling softirqs.  May sleep if the @gfp flags permit.
 * Return: The old value at this index or xa_err() if an error happened.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_cmpxchg_bh(struct xarray *xa, unsigned long index,
   void *old, void *entry, gfp_t gfp)
{
 void *curr;

 might_alloc(gfp);
 spin_lock_bh(&(xa)->xa_lock);
 curr = __xa_cmpxchg(xa, index, old, entry, gfp);
 spin_unlock_bh(&(xa)->xa_lock);

 return curr;
}

/**
 * xa_cmpxchg_irq() - Conditionally replace an entry in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @old: Old value to test against.
 * @entry: New value to place in array.
 * @gfp: Memory allocation flags.
 *
 * This function is like calling xa_cmpxchg() except it disables interrupts
 * while holding the array lock.
 *
 * Context: Process context.  Takes and releases the xa_lock while
 * disabling interrupts.  May sleep if the @gfp flags permit.
 * Return: The old value at this index or xa_err() if an error happened.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_cmpxchg_irq(struct xarray *xa, unsigned long index,
   void *old, void *entry, gfp_t gfp)
{
 void *curr;

 might_alloc(gfp);
 spin_lock_irq(&(xa)->xa_lock);
 curr = __xa_cmpxchg(xa, index, old, entry, gfp);
 spin_unlock_irq(&(xa)->xa_lock);

 return curr;
}

/**
 * xa_insert() - Store this entry in the XArray unless another entry is
 *			already present.
 * @xa: XArray.
 * @index: Index into array.
 * @entry: New entry.
 * @gfp: Memory allocation flags.
 *
 * Inserting a NULL entry will store a reserved entry (like xa_reserve())
 * if no entry is present.  Inserting will fail if a reserved entry is
 * present, even though loading from this index will return NULL.
 *
 * Context: Any context.  Takes and releases the xa_lock.  May sleep if
 * the @gfp flags permit.
 * Return: 0 if the store succeeded.  -EBUSY if another entry was present.
 * -ENOMEM if memory could not be allocated.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) xa_insert(struct xarray *xa,
  unsigned long index, void *entry, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock(&(xa)->xa_lock);
 err = __xa_insert(xa, index, entry, gfp);
 spin_unlock(&(xa)->xa_lock);

 return err;
}

/**
 * xa_insert_bh() - Store this entry in the XArray unless another entry is
 *			already present.
 * @xa: XArray.
 * @index: Index into array.
 * @entry: New entry.
 * @gfp: Memory allocation flags.
 *
 * Inserting a NULL entry will store a reserved entry (like xa_reserve())
 * if no entry is present.  Inserting will fail if a reserved entry is
 * present, even though loading from this index will return NULL.
 *
 * Context: Any context.  Takes and releases the xa_lock while
 * disabling softirqs.  May sleep if the @gfp flags permit.
 * Return: 0 if the store succeeded.  -EBUSY if another entry was present.
 * -ENOMEM if memory could not be allocated.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) xa_insert_bh(struct xarray *xa,
  unsigned long index, void *entry, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock_bh(&(xa)->xa_lock);
 err = __xa_insert(xa, index, entry, gfp);
 spin_unlock_bh(&(xa)->xa_lock);

 return err;
}

/**
 * xa_insert_irq() - Store this entry in the XArray unless another entry is
 *			already present.
 * @xa: XArray.
 * @index: Index into array.
 * @entry: New entry.
 * @gfp: Memory allocation flags.
 *
 * Inserting a NULL entry will store a reserved entry (like xa_reserve())
 * if no entry is present.  Inserting will fail if a reserved entry is
 * present, even though loading from this index will return NULL.
 *
 * Context: Process context.  Takes and releases the xa_lock while
 * disabling interrupts.  May sleep if the @gfp flags permit.
 * Return: 0 if the store succeeded.  -EBUSY if another entry was present.
 * -ENOMEM if memory could not be allocated.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) xa_insert_irq(struct xarray *xa,
  unsigned long index, void *entry, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock_irq(&(xa)->xa_lock);
 err = __xa_insert(xa, index, entry, gfp);
 spin_unlock_irq(&(xa)->xa_lock);

 return err;
}

/**
 * xa_alloc() - Find somewhere to store this entry in the XArray.
 * @xa: XArray.
 * @id: Pointer to ID.
 * @entry: New entry.
 * @limit: Range of ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Finds an empty entry in @xa between @limit.min and @limit.max,
 * stores the index into the @id pointer, then stores the entry at
 * that index.  A concurrent lookup will not see an uninitialised @id.
 *
 * Context: Any context.  Takes and releases the xa_lock.  May sleep if
 * the @gfp flags permit.
 * Return: 0 on success, -ENOMEM if memory could not be allocated or
 * -EBUSY if there are no free entries in @limit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) int xa_alloc(struct xarray *xa, u32 *id,
  void *entry, struct xa_limit limit, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock(&(xa)->xa_lock);
 err = __xa_alloc(xa, id, entry, limit, gfp);
 spin_unlock(&(xa)->xa_lock);

 return err;
}

/**
 * xa_alloc_bh() - Find somewhere to store this entry in the XArray.
 * @xa: XArray.
 * @id: Pointer to ID.
 * @entry: New entry.
 * @limit: Range of ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Finds an empty entry in @xa between @limit.min and @limit.max,
 * stores the index into the @id pointer, then stores the entry at
 * that index.  A concurrent lookup will not see an uninitialised @id.
 *
 * Context: Any context.  Takes and releases the xa_lock while
 * disabling softirqs.  May sleep if the @gfp flags permit.
 * Return: 0 on success, -ENOMEM if memory could not be allocated or
 * -EBUSY if there are no free entries in @limit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) xa_alloc_bh(struct xarray *xa, u32 *id,
  void *entry, struct xa_limit limit, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock_bh(&(xa)->xa_lock);
 err = __xa_alloc(xa, id, entry, limit, gfp);
 spin_unlock_bh(&(xa)->xa_lock);

 return err;
}

/**
 * xa_alloc_irq() - Find somewhere to store this entry in the XArray.
 * @xa: XArray.
 * @id: Pointer to ID.
 * @entry: New entry.
 * @limit: Range of ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Finds an empty entry in @xa between @limit.min and @limit.max,
 * stores the index into the @id pointer, then stores the entry at
 * that index.  A concurrent lookup will not see an uninitialised @id.
 *
 * Context: Process context.  Takes and releases the xa_lock while
 * disabling interrupts.  May sleep if the @gfp flags permit.
 * Return: 0 on success, -ENOMEM if memory could not be allocated or
 * -EBUSY if there are no free entries in @limit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) xa_alloc_irq(struct xarray *xa, u32 *id,
  void *entry, struct xa_limit limit, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock_irq(&(xa)->xa_lock);
 err = __xa_alloc(xa, id, entry, limit, gfp);
 spin_unlock_irq(&(xa)->xa_lock);

 return err;
}

/**
 * xa_alloc_cyclic() - Find somewhere to store this entry in the XArray.
 * @xa: XArray.
 * @id: Pointer to ID.
 * @entry: New entry.
 * @limit: Range of allocated ID.
 * @next: Pointer to next ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Finds an empty entry in @xa between @limit.min and @limit.max,
 * stores the index into the @id pointer, then stores the entry at
 * that index.  A concurrent lookup will not see an uninitialised @id.
 * The search for an empty entry will start at @next and will wrap
 * around if necessary.
 *
 * Context: Any context.  Takes and releases the xa_lock.  May sleep if
 * the @gfp flags permit.
 * Return: 0 if the allocation succeeded without wrapping.  1 if the
 * allocation succeeded after wrapping, -ENOMEM if memory could not be
 * allocated or -EBUSY if there are no free entries in @limit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int xa_alloc_cyclic(struct xarray *xa, u32 *id, void *entry,
  struct xa_limit limit, u32 *next, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock(&(xa)->xa_lock);
 err = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);
 spin_unlock(&(xa)->xa_lock);

 return err;
}

/**
 * xa_alloc_cyclic_bh() - Find somewhere to store this entry in the XArray.
 * @xa: XArray.
 * @id: Pointer to ID.
 * @entry: New entry.
 * @limit: Range of allocated ID.
 * @next: Pointer to next ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Finds an empty entry in @xa between @limit.min and @limit.max,
 * stores the index into the @id pointer, then stores the entry at
 * that index.  A concurrent lookup will not see an uninitialised @id.
 * The search for an empty entry will start at @next and will wrap
 * around if necessary.
 *
 * Context: Any context.  Takes and releases the xa_lock while
 * disabling softirqs.  May sleep if the @gfp flags permit.
 * Return: 0 if the allocation succeeded without wrapping.  1 if the
 * allocation succeeded after wrapping, -ENOMEM if memory could not be
 * allocated or -EBUSY if there are no free entries in @limit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int xa_alloc_cyclic_bh(struct xarray *xa, u32 *id, void *entry,
  struct xa_limit limit, u32 *next, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock_bh(&(xa)->xa_lock);
 err = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);
 spin_unlock_bh(&(xa)->xa_lock);

 return err;
}

/**
 * xa_alloc_cyclic_irq() - Find somewhere to store this entry in the XArray.
 * @xa: XArray.
 * @id: Pointer to ID.
 * @entry: New entry.
 * @limit: Range of allocated ID.
 * @next: Pointer to next ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Finds an empty entry in @xa between @limit.min and @limit.max,
 * stores the index into the @id pointer, then stores the entry at
 * that index.  A concurrent lookup will not see an uninitialised @id.
 * The search for an empty entry will start at @next and will wrap
 * around if necessary.
 *
 * Context: Process context.  Takes and releases the xa_lock while
 * disabling interrupts.  May sleep if the @gfp flags permit.
 * Return: 0 if the allocation succeeded without wrapping.  1 if the
 * allocation succeeded after wrapping, -ENOMEM if memory could not be
 * allocated or -EBUSY if there are no free entries in @limit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int xa_alloc_cyclic_irq(struct xarray *xa, u32 *id, void *entry,
  struct xa_limit limit, u32 *next, gfp_t gfp)
{
 int err;

 might_alloc(gfp);
 spin_lock_irq(&(xa)->xa_lock);
 err = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);
 spin_unlock_irq(&(xa)->xa_lock);

 return err;
}

/**
 * xa_reserve() - Reserve this index in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @gfp: Memory allocation flags.
 *
 * Ensures there is somewhere to store an entry at @index in the array.
 * If there is already something stored at @index, this function does
 * nothing.  If there was nothing there, the entry is marked as reserved.
 * Loading from a reserved entry returns a %NULL pointer.
 *
 * If you do not use the entry that you have reserved, call xa_release()
 * or xa_erase() to free any unnecessary memory.
 *
 * Context: Any context.  Takes and releases the xa_lock.
 * May sleep if the @gfp flags permit.
 * Return: 0 if the reservation succeeded or -ENOMEM if it failed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__))
int xa_reserve(struct xarray *xa, unsigned long index, gfp_t gfp)
{
 return xa_err(xa_cmpxchg(xa, index, ((void *)0), xa_mk_internal(257), gfp));
}

/**
 * xa_reserve_bh() - Reserve this index in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @gfp: Memory allocation flags.
 *
 * A softirq-disabling version of xa_reserve().
 *
 * Context: Any context.  Takes and releases the xa_lock while
 * disabling softirqs.
 * Return: 0 if the reservation succeeded or -ENOMEM if it failed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__))
int xa_reserve_bh(struct xarray *xa, unsigned long index, gfp_t gfp)
{
 return xa_err(xa_cmpxchg_bh(xa, index, ((void *)0), xa_mk_internal(257), gfp));
}

/**
 * xa_reserve_irq() - Reserve this index in the XArray.
 * @xa: XArray.
 * @index: Index into array.
 * @gfp: Memory allocation flags.
 *
 * An interrupt-disabling version of xa_reserve().
 *
 * Context: Process context.  Takes and releases the xa_lock while
 * disabling interrupts.
 * Return: 0 if the reservation succeeded or -ENOMEM if it failed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__))
int xa_reserve_irq(struct xarray *xa, unsigned long index, gfp_t gfp)
{
 return xa_err(xa_cmpxchg_irq(xa, index, ((void *)0), xa_mk_internal(257), gfp));
}

/**
 * xa_release() - Release a reserved entry.
 * @xa: XArray.
 * @index: Index of entry.
 *
 * After calling xa_reserve(), you can call this function to release the
 * reservation.  If the entry at @index has been stored to, this function
 * will do nothing.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xa_release(struct xarray *xa, unsigned long index)
{
 xa_cmpxchg(xa, index, xa_mk_internal(257), ((void *)0), 0);
}

/* Everything below here is the Advanced API.  Proceed with caution. */

/*
 * The xarray is constructed out of a set of 'chunks' of pointers.  Choosing
 * the best chunk size requires some tradeoffs.  A power of two recommends
 * itself so that we can walk the tree based purely on shifts and masks.
 * Generally, the larger the better; as the number of slots per level of the
 * tree increases, the less tall the tree needs to be.  But that needs to be
 * balanced against the memory consumption of each node.  On a 64-bit system,
 * xa_node is currently 576 bytes, and we get 7 of them per 4kB page.  If we
 * doubled the number of slots per node, we'd get only 3 nodes per 4kB page.
 */
# 1133 "./include/linux/xarray.h"
/*
 * @count is the count of every non-NULL element in the ->slots array
 * whether that is a value entry, a retry entry, a user pointer,
 * a sibling entry or a pointer to the next level of the tree.
 * @nr_values is the count of every element in ->slots which is
 * either a value entry or a sibling of a value entry.
 */
struct xa_node {
 unsigned char shift; /* Bits remaining in each slot */
 unsigned char offset; /* Slot offset in parent */
 unsigned char count; /* Total entry count */
 unsigned char nr_values; /* Value entry count */
 struct xa_node /* nothing */ *parent; /* NULL at top of tree */
 struct xarray *array; /* The array we belong to */
 union {
  struct list_head private_list; /* For tree user */
  struct callback_head callback_head; /* Used when freeing node */
 };
 void /* nothing */ *slots[(1UL << (0 ? 4 : 6))];
 union {
  unsigned long tags[3][((((1UL << (0 ? 4 : 6))) + (64) - 1) / (64))];
  unsigned long marks[3][((((1UL << (0 ? 4 : 6))) + (64) - 1) / (64))];
 };
};

void xa_dump(const struct xarray *);
void xa_dump_node(const struct xa_node *);
# 1179 "./include/linux/xarray.h"
/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_head(const struct xarray *xa)
{
 return ({ /* Dependency order vs. p above. */ typeof(*(xa->xa_head)) *__UNIQUE_ID_rcu283 = (typeof(*(xa->xa_head)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_284(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((xa->xa_head)) == sizeof(char) || sizeof((xa->xa_head)) == sizeof(short) || sizeof((xa->xa_head)) == sizeof(int) || sizeof((xa->xa_head)) == sizeof(long)) || sizeof((xa->xa_head)) == sizeof(long long))) __compiletime_assert_284(); } while (0); (*(const volatile typeof( _Generic(((xa->xa_head)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((xa->xa_head)))) *)&((xa->xa_head))); }); do { } while (0 && (!((lockdep_is_held(&xa->xa_lock)) || rcu_read_lock_held()))); ; ((typeof(*(xa->xa_head)) *)(__UNIQUE_ID_rcu283)); });
# 1184 "./include/linux/xarray.h"
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_head_locked(const struct xarray *xa)
{
 return ({ do { } while (0 && (!((lockdep_is_held(&xa->xa_lock))))); ; ((typeof(*(xa->xa_head)) *)((xa->xa_head))); });

}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_entry(const struct xarray *xa,
    const struct xa_node *node, unsigned int offset)
{
 do { } while (0);
 return ({ /* Dependency order vs. p above. */ typeof(*(node->slots[offset])) *__UNIQUE_ID_rcu285 = (typeof(*(node->slots[offset])) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_286(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((node->slots[offset])) == sizeof(char) || sizeof((node->slots[offset])) == sizeof(short) || sizeof((node->slots[offset])) == sizeof(int) || sizeof((node->slots[offset])) == sizeof(long)) || sizeof((node->slots[offset])) == sizeof(long long))) __compiletime_assert_286(); } while (0); (*(const volatile typeof( _Generic(((node->slots[offset])), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((node->slots[offset])))) *)&((node->slots[offset]))); }); do { } while (0 && (!((lockdep_is_held(&xa->xa_lock)) || rcu_read_lock_held()))); ; ((typeof(*(node->slots[offset])) *)(__UNIQUE_ID_rcu285)); });
# 1200 "./include/linux/xarray.h"
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_entry_locked(const struct xarray *xa,
    const struct xa_node *node, unsigned int offset)
{
 do { } while (0);
 return ({ do { } while (0 && (!((lockdep_is_held(&xa->xa_lock))))); ; ((typeof(*(node->slots[offset])) *)((node->slots[offset]))); });

}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct xa_node *xa_parent(const struct xarray *xa,
     const struct xa_node *node)
{
 return ({ /* Dependency order vs. p above. */ typeof(*(node->parent)) *__UNIQUE_ID_rcu287 = (typeof(*(node->parent)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_288(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((node->parent)) == sizeof(char) || sizeof((node->parent)) == sizeof(short) || sizeof((node->parent)) == sizeof(int) || sizeof((node->parent)) == sizeof(long)) || sizeof((node->parent)) == sizeof(long long))) __compiletime_assert_288(); } while (0); (*(const volatile typeof( _Generic(((node->parent)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((node->parent)))) *)&((node->parent))); }); do { } while (0 && (!((lockdep_is_held(&xa->xa_lock)) || rcu_read_lock_held()))); ; ((typeof(*(node->parent)) *)(__UNIQUE_ID_rcu287)); });
# 1217 "./include/linux/xarray.h"
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct xa_node *xa_parent_locked(const struct xarray *xa,
     const struct xa_node *node)
{
 return ({ do { } while (0 && (!((lockdep_is_held(&xa->xa_lock))))); ; ((typeof(*(node->parent)) *)((node->parent))); });

}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_mk_node(const struct xa_node *node)
{
 return (void *)((unsigned long)node | 2);
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct xa_node *xa_to_node(const void *entry)
{
 return (struct xa_node *)((unsigned long)entry - 2);
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_node(const void *entry)
{
 return xa_is_internal(entry) && (unsigned long)entry > 4096;
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xa_mk_sibling(unsigned int offset)
{
 return xa_mk_internal(offset);
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long xa_to_sibling(const void *entry)
{
 return xa_to_internal(entry);
}

/**
 * xa_is_sibling() - Is the entry a sibling entry?
 * @entry: Entry retrieved from the XArray
 *
 * Return: %true if the entry is a sibling entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_sibling(const void *entry)
{
 return 1 && xa_is_internal(entry) &&
  (entry < xa_mk_sibling((1UL << (0 ? 4 : 6)) - 1));
}



/**
 * xa_is_retry() - Is the entry a retry entry?
 * @entry: Entry retrieved from the XArray
 *
 * Return: %true if the entry is a retry entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_retry(const void *entry)
{
 return __builtin_expect(!!(entry == xa_mk_internal(256)), 0);
}

/**
 * xa_is_advanced() - Is the entry only permitted for the advanced API?
 * @entry: Entry to be stored in the XArray.
 *
 * Return: %true if the entry cannot be stored by the normal API.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xa_is_advanced(const void *entry)
{
 return xa_is_internal(entry) && (entry <= xa_mk_internal(256));
}

/**
 * typedef xa_update_node_t - A callback function from the XArray.
 * @node: The node which is being processed
 *
 * This function is called every time the XArray updates the count of
 * present and value entries in a node.  It allows advanced users to
 * maintain the private_list in the node.
 *
 * Context: The xa_lock is held and interrupts may be disabled.
 *	    Implementations should not drop the xa_lock, nor re-enable
 *	    interrupts.
 */
typedef void (*xa_update_node_t)(struct xa_node *node);

void xa_delete_node(struct xa_node *, xa_update_node_t);

/*
 * The xa_state is opaque to its users.  It contains various different pieces
 * of state involved in the current operation on the XArray.  It should be
 * declared on the stack and passed between the various internal routines.
 * The various elements in it should not be accessed directly, but only
 * through the provided accessor functions.  The below documentation is for
 * the benefit of those working on the code, not for users of the XArray.
 *
 * @xa_node usually points to the xa_node containing the slot we're operating
 * on (and @xa_offset is the offset in the slots array).  If there is a
 * single entry in the array at index 0, there are no allocated xa_nodes to
 * point to, and so we store %NULL in @xa_node.  @xa_node is set to
 * the value %XAS_RESTART if the xa_state is not walked to the correct
 * position in the tree of nodes for this operation.  If an error occurs
 * during an operation, it is set to an %XAS_ERROR value.  If we run off the
 * end of the allocated nodes, it is set to %XAS_BOUNDS.
 */
struct xa_state {
 struct xarray *xa;
 unsigned long xa_index;
 unsigned char xa_shift;
 unsigned char xa_sibs;
 unsigned char xa_offset;
 unsigned char xa_pad; /* Helps gcc generate better code */
 struct xa_node *xa_node;
 struct xa_node *xa_alloc;
 xa_update_node_t xa_update;
 struct list_lru *xa_lru;
};

/*
 * We encode errnos in the xas->xa_node.  If an error has happened, we need to
 * drop the lock to fix it, and once we've done so the xa_state is invalid.
 */
# 1360 "./include/linux/xarray.h"
/**
 * XA_STATE() - Declare an XArray operation state.
 * @name: Name of this operation state (usually xas).
 * @array: Array to operate on.
 * @index: Initial index of interest.
 *
 * Declare and initialise an xa_state on the stack.
 */



/**
 * XA_STATE_ORDER() - Declare an XArray operation state.
 * @name: Name of this operation state (usually xas).
 * @array: Array to operate on.
 * @index: Initial index of interest.
 * @order: Order of entry.
 *
 * Declare and initialise an xa_state on the stack.  This variant of
 * XA_STATE() allows you to specify the 'order' of the element you
 * want to operate on.`
 */
# 1401 "./include/linux/xarray.h"
/**
 * xas_error() - Return an errno stored in the xa_state.
 * @xas: XArray operation state.
 *
 * Return: 0 if no error has been noted.  A negative errno if one has.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int xas_error(const struct xa_state *xas)
{
 return xa_err(xas->xa_node);
}

/**
 * xas_set_err() - Note an error in the xa_state.
 * @xas: XArray operation state.
 * @err: Negative error number.
 *
 * Only call this function with a negative @err; zero or positive errors
 * will probably not behave the way you think they should.  If you want
 * to clear the error from an xa_state, use xas_reset().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xas_set_err(struct xa_state *xas, long err)
{
 xas->xa_node = ((struct xa_node *)(((unsigned long)err << 2) | 2UL));
}

/**
 * xas_invalid() - Is the xas in a retry or error state?
 * @xas: XArray operation state.
 *
 * Return: %true if the xas cannot be used for operations.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xas_invalid(const struct xa_state *xas)
{
 return (unsigned long)xas->xa_node & 3;
}

/**
 * xas_valid() - Is the xas a valid cursor into the array?
 * @xas: XArray operation state.
 *
 * Return: %true if the xas can be used for operations.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xas_valid(const struct xa_state *xas)
{
 return !xas_invalid(xas);
}

/**
 * xas_is_node() - Does the xas point to a node?
 * @xas: XArray operation state.
 *
 * Return: %true if the xas currently references a node.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xas_is_node(const struct xa_state *xas)
{
 return xas_valid(xas) && xas->xa_node;
}

/* True if the pointer is something other than a node */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xas_not_node(struct xa_node *node)
{
 return ((unsigned long)node & 3) || !node;
}

/* True if the node represents RESTART or an error */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xas_frozen(struct xa_node *node)
{
 return (unsigned long)node & 2;
}

/* True if the node represents head-of-tree, RESTART or BOUNDS */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xas_top(struct xa_node *node)
{
 return node <= ((struct xa_node *)3UL);
}

/**
 * xas_reset() - Reset an XArray operation state.
 * @xas: XArray operation state.
 *
 * Resets the error or walk state of the @xas so future walks of the
 * array will start from the root.  Use this if you have dropped the
 * xarray lock and want to reuse the xa_state.
 *
 * Context: Any context.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xas_reset(struct xa_state *xas)
{
 xas->xa_node = ((struct xa_node *)3UL);
}

/**
 * xas_retry() - Retry the operation if appropriate.
 * @xas: XArray operation state.
 * @entry: Entry from xarray.
 *
 * The advanced functions may sometimes return an internal entry, such as
 * a retry entry or a zero entry.  This function sets up the @xas to restart
 * the walk from the head of the array if needed.
 *
 * Context: Any context.
 * Return: true if the operation needs to be retried.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool xas_retry(struct xa_state *xas, const void *entry)
{
 if (xa_is_zero(entry))
  return true;
 if (!xa_is_retry(entry))
  return false;
 xas_reset(xas);
 return true;
}

void *xas_load(struct xa_state *);
void *xas_store(struct xa_state *, void *entry);
void *xas_find(struct xa_state *, unsigned long max);
void *xas_find_conflict(struct xa_state *);

bool xas_get_mark(const struct xa_state *, xa_mark_t);
void xas_set_mark(const struct xa_state *, xa_mark_t);
void xas_clear_mark(const struct xa_state *, xa_mark_t);
void *xas_find_marked(struct xa_state *, unsigned long max, xa_mark_t);
void xas_init_marks(const struct xa_state *);

bool xas_nomem(struct xa_state *, gfp_t);
void xas_destroy(struct xa_state *);
void xas_pause(struct xa_state *);

void xas_create_range(struct xa_state *);


int xa_get_order(struct xarray *, unsigned long index);
void xas_split(struct xa_state *, void *entry, unsigned int order);
void xas_split_alloc(struct xa_state *, void *entry, unsigned int order, gfp_t);
# 1553 "./include/linux/xarray.h"
/**
 * xas_reload() - Refetch an entry from the xarray.
 * @xas: XArray operation state.
 *
 * Use this function to check that a previously loaded entry still has
 * the same value.  This is useful for the lockless pagecache lookup where
 * we walk the array with only the RCU lock to protect us, lock the page,
 * then check that the page hasn't moved since we looked it up.
 *
 * The caller guarantees that @xas is still valid.  If it may be in an
 * error or restart state, call xas_load() instead.
 *
 * Return: The entry at this location in the xarray.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xas_reload(struct xa_state *xas)
{
 struct xa_node *node = xas->xa_node;
 void *entry;
 char offset;

 if (!node)
  return xa_head(xas->xa);
 if (1) {
  offset = (xas->xa_index >> node->shift) & ((1UL << (0 ? 4 : 6)) - 1);
  entry = xa_entry(xas->xa, node, offset);
  if (!xa_is_sibling(entry))
   return entry;
  offset = xa_to_sibling(entry);
 } else {
  offset = xas->xa_offset;
 }
 return xa_entry(xas->xa, node, offset);
}

/**
 * xas_set() - Set up XArray operation state for a different index.
 * @xas: XArray operation state.
 * @index: New index into the XArray.
 *
 * Move the operation state to refer to a different index.  This will
 * have the effect of starting a walk from the top; see xas_next()
 * to move to an adjacent index.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xas_set(struct xa_state *xas, unsigned long index)
{
 xas->xa_index = index;
 xas->xa_node = ((struct xa_node *)3UL);
}

/**
 * xas_advance() - Skip over sibling entries.
 * @xas: XArray operation state.
 * @index: Index of last sibling entry.
 *
 * Move the operation state to refer to the last sibling entry.
 * This is useful for loops that normally want to see sibling
 * entries but sometimes want to skip them.  Use xas_set() if you
 * want to move to an index which is not part of this entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xas_advance(struct xa_state *xas, unsigned long index)
{
 unsigned char shift = xas_is_node(xas) ? xas->xa_node->shift : 0;

 xas->xa_index = index;
 xas->xa_offset = (index >> shift) & ((1UL << (0 ? 4 : 6)) - 1);
}

/**
 * xas_set_order() - Set up XArray operation state for a multislot entry.
 * @xas: XArray operation state.
 * @index: Target of the operation.
 * @order: Entry occupies 2^@order indices.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xas_set_order(struct xa_state *xas, unsigned long index,
     unsigned int order)
{

 xas->xa_index = order < 64 ? (index >> order) << order : 0;
 xas->xa_shift = order - (order % (0 ? 4 : 6));
 xas->xa_sibs = (1 << (order % (0 ? 4 : 6))) - 1;
 xas->xa_node = ((struct xa_node *)3UL);




}

/**
 * xas_set_update() - Set up XArray operation state for a callback.
 * @xas: XArray operation state.
 * @update: Function to call when updating a node.
 *
 * The XArray can notify a caller after it has updated an xa_node.
 * This is advanced functionality and is only needed by the page cache.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xas_set_update(struct xa_state *xas, xa_update_node_t update)
{
 xas->xa_update = update;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void xas_set_lru(struct xa_state *xas, struct list_lru *lru)
{
 xas->xa_lru = lru;
}

/**
 * xas_next_entry() - Advance iterator to next present entry.
 * @xas: XArray operation state.
 * @max: Highest index to return.
 *
 * xas_next_entry() is an inline function to optimise xarray traversal for
 * speed.  It is equivalent to calling xas_find(), and will call xas_find()
 * for all the hard cases.
 *
 * Return: The next present entry after the one currently referred to by @xas.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xas_next_entry(struct xa_state *xas, unsigned long max)
{
 struct xa_node *node = xas->xa_node;
 void *entry;

 if (__builtin_expect(!!(xas_not_node(node) || node->shift || xas->xa_offset != (xas->xa_index & ((1UL << (0 ? 4 : 6)) - 1))), 0))

  return xas_find(xas, max);

 do {
  if (__builtin_expect(!!(xas->xa_index >= max), 0))
   return xas_find(xas, max);
  if (__builtin_expect(!!(xas->xa_offset == ((1UL << (0 ? 4 : 6)) - 1)), 0))
   return xas_find(xas, max);
  entry = xa_entry(xas->xa, node, xas->xa_offset + 1);
  if (__builtin_expect(!!(xa_is_internal(entry)), 0))
   return xas_find(xas, max);
  xas->xa_offset++;
  xas->xa_index++;
 } while (!entry);

 return entry;
}

/* Private */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int xas_find_chunk(struct xa_state *xas, bool advance,
  xa_mark_t mark)
{
 unsigned long *addr = xas->xa_node->marks[( unsigned)mark];
 unsigned int offset = xas->xa_offset;

 if (advance)
  offset++;
 if ((1UL << (0 ? 4 : 6)) == 64) {
  if (offset < (1UL << (0 ? 4 : 6))) {
   unsigned long data = *addr & (~0UL << offset);
   if (data)
    return __ffs(data);
  }
  return (1UL << (0 ? 4 : 6));
 }

 return find_next_bit(addr, (1UL << (0 ? 4 : 6)), offset);
}

/**
 * xas_next_marked() - Advance iterator to next marked entry.
 * @xas: XArray operation state.
 * @max: Highest index to return.
 * @mark: Mark to search for.
 *
 * xas_next_marked() is an inline function to optimise xarray traversal for
 * speed.  It is equivalent to calling xas_find_marked(), and will call
 * xas_find_marked() for all the hard cases.
 *
 * Return: The next marked entry after the one currently referred to by @xas.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xas_next_marked(struct xa_state *xas, unsigned long max,
        xa_mark_t mark)
{
 struct xa_node *node = xas->xa_node;
 void *entry;
 unsigned int offset;

 if (__builtin_expect(!!(xas_not_node(node) || node->shift), 0))
  return xas_find_marked(xas, max, mark);
 offset = xas_find_chunk(xas, true, mark);
 xas->xa_offset = offset;
 xas->xa_index = (xas->xa_index & ~((1UL << (0 ? 4 : 6)) - 1)) + offset;
 if (xas->xa_index > max)
  return ((void *)0);
 if (offset == (1UL << (0 ? 4 : 6)))
  return xas_find_marked(xas, max, mark);
 entry = xa_entry(xas->xa, node, offset);
 if (!entry)
  return xas_find_marked(xas, max, mark);
 return entry;
}

/*
 * If iterating while holding a lock, drop the lock and reschedule
 * every %XA_CHECK_SCHED loops.
 */
enum {
 XA_CHECK_SCHED = 4096,
};

/**
 * xas_for_each() - Iterate over a range of an XArray.
 * @xas: XArray operation state.
 * @entry: Entry retrieved from the array.
 * @max: Maximum index to retrieve from array.
 *
 * The loop body will be executed for each entry present in the xarray
 * between the current xas position and @max.  @entry will be set to
 * the entry retrieved from the xarray.  It is safe to delete entries
 * from the array in the loop body.  You should hold either the RCU lock
 * or the xa_lock while iterating.  If you need to drop the lock, call
 * xas_pause() first.
 */




/**
 * xas_for_each_marked() - Iterate over a range of an XArray.
 * @xas: XArray operation state.
 * @entry: Entry retrieved from the array.
 * @max: Maximum index to retrieve from array.
 * @mark: Mark to search for.
 *
 * The loop body will be executed for each marked entry in the xarray
 * between the current xas position and @max.  @entry will be set to
 * the entry retrieved from the xarray.  It is safe to delete entries
 * from the array in the loop body.  You should hold either the RCU lock
 * or the xa_lock while iterating.  If you need to drop the lock, call
 * xas_pause() first.
 */




/**
 * xas_for_each_conflict() - Iterate over a range of an XArray.
 * @xas: XArray operation state.
 * @entry: Entry retrieved from the array.
 *
 * The loop body will be executed for each entry in the XArray that
 * lies within the range specified by @xas.  If the loop terminates
 * normally, @entry will be %NULL.  The user may break out of the loop,
 * which will leave @entry set to the conflicting entry.  The caller
 * may also call xa_set_err() to exit the loop while setting an error
 * to record the reason.
 */



void *__xas_next(struct xa_state *);
void *__xas_prev(struct xa_state *);

/**
 * xas_prev() - Move iterator to previous index.
 * @xas: XArray operation state.
 *
 * If the @xas was in an error state, it will remain in an error state
 * and this function will return %NULL.  If the @xas has never been walked,
 * it will have the effect of calling xas_load().  Otherwise one will be
 * subtracted from the index and the state will be walked to the correct
 * location in the array for the next operation.
 *
 * If the iterator was referencing index 0, this function wraps
 * around to %ULONG_MAX.
 *
 * Return: The entry at the new index.  This may be %NULL or an internal
 * entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xas_prev(struct xa_state *xas)
{
 struct xa_node *node = xas->xa_node;

 if (__builtin_expect(!!(xas_not_node(node) || node->shift || xas->xa_offset == 0), 0))

  return __xas_prev(xas);

 xas->xa_index--;
 xas->xa_offset--;
 return xa_entry(xas->xa, node, xas->xa_offset);
}

/**
 * xas_next() - Move state to next index.
 * @xas: XArray operation state.
 *
 * If the @xas was in an error state, it will remain in an error state
 * and this function will return %NULL.  If the @xas has never been walked,
 * it will have the effect of calling xas_load().  Otherwise one will be
 * added to the index and the state will be walked to the correct
 * location in the array for the next operation.
 *
 * If the iterator was referencing index %ULONG_MAX, this function wraps
 * around to 0.
 *
 * Return: The entry at the new index.  This may be %NULL or an internal
 * entry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xas_next(struct xa_state *xas)
{
 struct xa_node *node = xas->xa_node;

 if (__builtin_expect(!!(xas_not_node(node) || node->shift || xas->xa_offset == ((1UL << (0 ? 4 : 6)) - 1)), 0))

  return __xas_next(xas);

 xas->xa_index++;
 xas->xa_offset++;
 return xa_entry(xas->xa, node, xas->xa_offset);
}
# 15 "./include/linux/list_lru.h" 2

struct mem_cgroup;

/* list_lru_walk_cb has to always return one of those */
enum lru_status {
 LRU_REMOVED, /* item removed from list */
 LRU_REMOVED_RETRY, /* item removed, but lock has been
				   dropped and reacquired */
 LRU_ROTATE, /* item referenced, give another pass */
 LRU_SKIP, /* item cannot be locked, skip */
 LRU_RETRY, /* item not freeable. May drop the lock
				   internally, but has to return locked. */
};

struct list_lru_one {
 struct list_head list;
 /* may become negative during memcg reparenting */
 long nr_items;
};

struct list_lru_memcg {
 struct callback_head rcu;
 /* array of per cgroup per node lists, indexed by node id */
 struct list_lru_one node[];
};

struct list_lru_node {
 /* protects all lists on the node, including per cgroup */
 spinlock_t lock;
 /* global list, used for the root cgroup in cgroup aware lrus */
 struct list_lru_one lru;
 long nr_items;
} __attribute__((__aligned__((1 << (6)))));

struct list_lru {
 struct list_lru_node *node;

 struct list_head list;
 int shrinker_id;
 bool memcg_aware;
 struct xarray xa;

};

void list_lru_destroy(struct list_lru *lru);
int __list_lru_init(struct list_lru *lru, bool memcg_aware,
      struct lock_class_key *key, struct shrinker *shrinker);
# 70 "./include/linux/list_lru.h"
int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,
    gfp_t gfp);
void memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent);

/**
 * list_lru_add: add an element to the lru list's tail
 * @list_lru: the lru pointer
 * @item: the item to be added.
 *
 * If the element is already part of a list, this function returns doing
 * nothing. Therefore the caller does not need to keep state about whether or
 * not the element already belongs in the list and is allowed to lazy update
 * it. Note however that this is valid for *a* list, not *this* list. If
 * the caller organize itself in a way that elements can be in more than
 * one type of list, it is up to the caller to fully remove the item from
 * the previous list (with list_lru_del() for instance) before moving it
 * to @list_lru
 *
 * Return value: true if the list was updated, false otherwise
 */
bool list_lru_add(struct list_lru *lru, struct list_head *item);

/**
 * list_lru_del: delete an element to the lru list
 * @list_lru: the lru pointer
 * @item: the item to be deleted.
 *
 * This function works analogously as list_lru_add in terms of list
 * manipulation. The comments about an element already pertaining to
 * a list are also valid for list_lru_del.
 *
 * Return value: true if the list was updated, false otherwise
 */
bool list_lru_del(struct list_lru *lru, struct list_head *item);

/**
 * list_lru_count_one: return the number of objects currently held by @lru
 * @lru: the lru pointer.
 * @nid: the node id to count from.
 * @memcg: the cgroup to count from.
 *
 * Always return a non-negative number, 0 for empty lists. There is no
 * guarantee that the list is not updated while the count is being computed.
 * Callers that want such a guarantee need to provide an outer lock.
 */
unsigned long list_lru_count_one(struct list_lru *lru,
     int nid, struct mem_cgroup *memcg);
unsigned long list_lru_count_node(struct list_lru *lru, int nid);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long list_lru_shrink_count(struct list_lru *lru,
        struct shrink_control *sc)
{
 return list_lru_count_one(lru, sc->nid, sc->memcg);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long list_lru_count(struct list_lru *lru)
{
 long count = 0;
 int nid;

 for (((nid)) = __first_node(&(node_states[N_NORMAL_MEMORY])); ((nid) >= 0) && ((nid)) < (1 << 4); ((nid)) = __next_node((((nid))), &((node_states[N_NORMAL_MEMORY]))))
  count += list_lru_count_node(lru, nid);

 return count;
}

void list_lru_isolate(struct list_lru_one *list, struct list_head *item);
void list_lru_isolate_move(struct list_lru_one *list, struct list_head *item,
      struct list_head *head);

typedef enum lru_status (*list_lru_walk_cb)(struct list_head *item,
  struct list_lru_one *list, spinlock_t *lock, void *cb_arg);

/**
 * list_lru_walk_one: walk a list_lru, isolating and disposing freeable items.
 * @lru: the lru pointer.
 * @nid: the node id to scan from.
 * @memcg: the cgroup to scan from.
 * @isolate: callback function that is responsible for deciding what to do with
 *  the item currently being scanned
 * @cb_arg: opaque type that will be passed to @isolate
 * @nr_to_walk: how many items to scan.
 *
 * This function will scan all elements in a particular list_lru, calling the
 * @isolate callback for each of those items, along with the current list
 * spinlock and a caller-provided opaque. The @isolate callback can choose to
 * drop the lock internally, but *must* return with the lock held. The callback
 * will return an enum lru_status telling the list_lru infrastructure what to
 * do with the object being scanned.
 *
 * Please note that nr_to_walk does not mean how many objects will be freed,
 * just how many objects will be scanned.
 *
 * Return value: the number of objects effectively removed from the LRU.
 */
unsigned long list_lru_walk_one(struct list_lru *lru,
    int nid, struct mem_cgroup *memcg,
    list_lru_walk_cb isolate, void *cb_arg,
    unsigned long *nr_to_walk);
/**
 * list_lru_walk_one_irq: walk a list_lru, isolating and disposing freeable items.
 * @lru: the lru pointer.
 * @nid: the node id to scan from.
 * @memcg: the cgroup to scan from.
 * @isolate: callback function that is responsible for deciding what to do with
 *  the item currently being scanned
 * @cb_arg: opaque type that will be passed to @isolate
 * @nr_to_walk: how many items to scan.
 *
 * Same as @list_lru_walk_one except that the spinlock is acquired with
 * spin_lock_irq().
 */
unsigned long list_lru_walk_one_irq(struct list_lru *lru,
        int nid, struct mem_cgroup *memcg,
        list_lru_walk_cb isolate, void *cb_arg,
        unsigned long *nr_to_walk);
unsigned long list_lru_walk_node(struct list_lru *lru, int nid,
     list_lru_walk_cb isolate, void *cb_arg,
     unsigned long *nr_to_walk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
list_lru_shrink_walk(struct list_lru *lru, struct shrink_control *sc,
       list_lru_walk_cb isolate, void *cb_arg)
{
 return list_lru_walk_one(lru, sc->nid, sc->memcg, isolate, cb_arg,
     &sc->nr_to_scan);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
list_lru_shrink_walk_irq(struct list_lru *lru, struct shrink_control *sc,
    list_lru_walk_cb isolate, void *cb_arg)
{
 return list_lru_walk_one_irq(lru, sc->nid, sc->memcg, isolate, cb_arg,
         &sc->nr_to_scan);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
list_lru_walk(struct list_lru *lru, list_lru_walk_cb isolate,
       void *cb_arg, unsigned long nr_to_walk)
{
 long isolated = 0;
 int nid;

 for (((nid)) = __first_node(&(node_states[N_NORMAL_MEMORY])); ((nid) >= 0) && ((nid)) < (1 << 4); ((nid)) = __next_node((((nid))), &((node_states[N_NORMAL_MEMORY])))) {
  isolated += list_lru_walk_node(lru, nid, isolate,
            cb_arg, &nr_to_walk);
  if (nr_to_walk <= 0)
   break;
 }
 return isolated;
}
# 14 "./include/linux/fs.h" 2

# 1 "./include/linux/radix-tree.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * Copyright (C) 2001 Momchil Velikov
 * Portions Copyright (C) 2001 Christoph Hellwig
 * Copyright (C) 2006 Nick Piggin
 * Copyright (C) 2012 Konstantin Khlebnikov
 */
# 24 "./include/linux/radix-tree.h"
/* Keep unconverted code working */



struct radix_tree_preload {
 local_lock_t lock;
 unsigned nr;
 /* nodes->parent points to next preallocated node */
 struct xa_node *nodes;
};
extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct radix_tree_preload) radix_tree_preloads;

/*
 * The bottom two bits of the slot determine how the remaining bits in the
 * slot are interpreted:
 *
 * 00 - data pointer
 * 10 - internal entry
 * x1 - value entry
 *
 * The internal entry may be a pointer to the next level in the tree, a
 * sibling entry, or an indicator that the entry in this slot has been moved
 * to another location in the tree and the lookup should be restarted.  While
 * NULL fits the 'data pointer' pattern, it means that there is no entry in
 * the tree for this index (no matter what level of the tree it is found at).
 * This means that storing a NULL entry in the tree is the same as deleting
 * the entry from the tree.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool radix_tree_is_internal_node(void *ptr)
{
 return ((unsigned long)ptr & 3UL) ==
    2UL;
}

/*** radix-tree API starts here ***/
# 74 "./include/linux/radix-tree.h"
/* The IDR tag is stored in the low bits of xa_flags */

/* The top bits of xa_flags are used to store the root tags */
# 86 "./include/linux/radix-tree.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool radix_tree_empty(const struct xarray *root)
{
 return root->xa_head == ((void *)0);
}

/**
 * struct radix_tree_iter - radix tree iterator state
 *
 * @index:	index of current slot
 * @next_index:	one beyond the last index for this chunk
 * @tags:	bit-mask for tag-iterating
 * @node:	node that contains current slot
 *
 * This radix tree iterator works in terms of "chunks" of slots.  A chunk is a
 * subinterval of slots contained within one radix tree leaf node.  It is
 * described by a pointer to its first slot and a struct radix_tree_iter
 * which holds the chunk's position in the tree and its size.  For tagged
 * iteration radix_tree_iter also holds the slots' bit-mask for one chosen
 * radix tree tag.
 */
struct radix_tree_iter {
 unsigned long index;
 unsigned long next_index;
 unsigned long tags;
 struct xa_node *node;
};

/**
 * Radix-tree synchronization
 *
 * The radix-tree API requires that users provide all synchronisation (with
 * specific exceptions, noted below).
 *
 * Synchronization of access to the data items being stored in the tree, and
 * management of their lifetimes must be completely managed by API users.
 *
 * For API usage, in general,
 * - any function _modifying_ the tree or tags (inserting or deleting
 *   items, setting or clearing tags) must exclude other modifications, and
 *   exclude any functions reading the tree.
 * - any function _reading_ the tree or tags (looking up items or tags,
 *   gang lookups) must exclude modifications to the tree, but may occur
 *   concurrently with other readers.
 *
 * The notable exceptions to this rule are the following functions:
 * __radix_tree_lookup
 * radix_tree_lookup
 * radix_tree_lookup_slot
 * radix_tree_tag_get
 * radix_tree_gang_lookup
 * radix_tree_gang_lookup_tag
 * radix_tree_gang_lookup_tag_slot
 * radix_tree_tagged
 *
 * The first 7 functions are able to be called locklessly, using RCU. The
 * caller must ensure calls to these functions are made within rcu_read_lock()
 * regions. Other readers (lock-free or otherwise) and modifications may be
 * running concurrently.
 *
 * It is still required that the caller manage the synchronization and lifetimes
 * of the items. So if RCU lock-free lookups are used, typically this would mean
 * that the items have their own locks, or are amenable to lock-free access; and
 * that the items are freed by RCU (or only freed after having been deleted from
 * the radix tree *and* a synchronize_rcu() grace period).
 *
 * (Note, rcu_assign_pointer and rcu_dereference are not needed to control
 * access to data items when inserting into or looking up from the radix tree)
 *
 * Note that the value returned by radix_tree_tag_get() may not be relied upon
 * if only the RCU read lock is held.  Functions to set/clear tags and to
 * delete nodes running concurrently with it may affect its result such that
 * two consecutive reads in the same locked section may return different
 * values.  If reliability is required, modification functions must also be
 * excluded from concurrency.
 *
 * radix_tree_tagged is able to be called without locking or RCU.
 */

/**
 * radix_tree_deref_slot - dereference a slot
 * @slot: slot pointer, returned by radix_tree_lookup_slot
 *
 * For use with radix_tree_lookup_slot().  Caller must hold tree at least read
 * locked across slot lookup and dereference. Not required if write lock is
 * held (ie. items cannot be concurrently inserted).
 *
 * radix_tree_deref_retry must be used to confirm validity of the pointer if
 * only the read lock is held.
 *
 * Return: entry stored in that slot.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *radix_tree_deref_slot(void /* nothing */ **slot)
{
 return ({ /* Dependency order vs. p above. */ typeof(*(*slot)) *__UNIQUE_ID_rcu289 = (typeof(*(*slot)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_290(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((*slot)) == sizeof(char) || sizeof((*slot)) == sizeof(short) || sizeof((*slot)) == sizeof(int) || sizeof((*slot)) == sizeof(long)) || sizeof((*slot)) == sizeof(long long))) __compiletime_assert_290(); } while (0); (*(const volatile typeof( _Generic(((*slot)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((*slot)))) *)&((*slot))); }); do { } while (0 && (!((0) || rcu_read_lock_held()))); ; ((typeof(*(*slot)) *)(__UNIQUE_ID_rcu289)); });
# 180 "./include/linux/radix-tree.h"
}

/**
 * radix_tree_deref_slot_protected - dereference a slot with tree lock held
 * @slot: slot pointer, returned by radix_tree_lookup_slot
 *
 * Similar to radix_tree_deref_slot.  The caller does not hold the RCU read
 * lock but it must hold the tree lock to prevent parallel updates.
 *
 * Return: entry stored in that slot.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *radix_tree_deref_slot_protected(void /* nothing */ **slot,
       spinlock_t *treelock)
{
 return ({ do { } while (0 && (!((lockdep_is_held(treelock))))); ; ((typeof(*(*slot)) *)((*slot))); });
}

/**
 * radix_tree_deref_retry	- check radix_tree_deref_slot
 * @arg:	pointer returned by radix_tree_deref_slot
 * Returns:	0 if retry is not required, otherwise retry is required
 *
 * radix_tree_deref_retry must be used with radix_tree_deref_slot.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int radix_tree_deref_retry(void *arg)
{
 return __builtin_expect(!!(radix_tree_is_internal_node(arg)), 0);
}

/**
 * radix_tree_exception	- radix_tree_deref_slot returned either exception?
 * @arg:	value returned by radix_tree_deref_slot
 * Returns:	0 if well-aligned pointer, non-0 if either kind of exception.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int radix_tree_exception(void *arg)
{
 return __builtin_expect(!!((unsigned long)arg & 3UL), 0);
}

int radix_tree_insert(struct xarray *, unsigned long index,
   void *);
void *__radix_tree_lookup(const struct xarray *, unsigned long index,
     struct xa_node **nodep, void /* nothing */ ***slotp);
void *radix_tree_lookup(const struct xarray *, unsigned long);
void /* nothing */ **radix_tree_lookup_slot(const struct xarray *,
     unsigned long index);
void __radix_tree_replace(struct xarray *, struct xa_node *,
     void /* nothing */ **slot, void *entry);
void radix_tree_iter_replace(struct xarray *,
  const struct radix_tree_iter *, void /* nothing */ **slot, void *entry);
void radix_tree_replace_slot(struct xarray *,
        void /* nothing */ **slot, void *entry);
void radix_tree_iter_delete(struct xarray *,
   struct radix_tree_iter *iter, void /* nothing */ **slot);
void *radix_tree_delete_item(struct xarray *, unsigned long, void *);
void *radix_tree_delete(struct xarray *, unsigned long);
unsigned int radix_tree_gang_lookup(const struct xarray *,
   void **results, unsigned long first_index,
   unsigned int max_items);
int radix_tree_preload(gfp_t gfp_mask);
int radix_tree_maybe_preload(gfp_t gfp_mask);
void radix_tree_init(void);
void *radix_tree_tag_set(struct xarray *,
   unsigned long index, unsigned int tag);
void *radix_tree_tag_clear(struct xarray *,
   unsigned long index, unsigned int tag);
int radix_tree_tag_get(const struct xarray *,
   unsigned long index, unsigned int tag);
void radix_tree_iter_tag_clear(struct xarray *,
  const struct radix_tree_iter *iter, unsigned int tag);
unsigned int radix_tree_gang_lookup_tag(const struct xarray *,
  void **results, unsigned long first_index,
  unsigned int max_items, unsigned int tag);
unsigned int radix_tree_gang_lookup_tag_slot(const struct xarray *,
  void /* nothing */ ***results, unsigned long first_index,
  unsigned int max_items, unsigned int tag);
int radix_tree_tagged(const struct xarray *, unsigned int tag);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void radix_tree_preload_end(void)
{
 do { local_lock_release(({ do { const void /* nothing */ *__vpp_verify = (typeof((&radix_tree_preloads.lock) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&radix_tree_preloads.lock)) *)(&radix_tree_preloads.lock)); (typeof((typeof(*(&radix_tree_preloads.lock)) *)(&radix_tree_preloads.lock))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0); } while (0);
}

void /* nothing */ **idr_get_free(struct xarray *root,
         struct radix_tree_iter *iter, gfp_t gfp,
         unsigned long max);

enum {
 RADIX_TREE_ITER_TAG_MASK = 0x0f, /* tag index in lower nybble */
 RADIX_TREE_ITER_TAGGED = 0x10, /* lookup tagged slots */
 RADIX_TREE_ITER_CONTIG = 0x20, /* stop at first hole */
};

/**
 * radix_tree_iter_init - initialize radix tree iterator
 *
 * @iter:	pointer to iterator state
 * @start:	iteration starting index
 * Returns:	NULL
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void /* nothing */ **
radix_tree_iter_init(struct radix_tree_iter *iter, unsigned long start)
{
 /*
	 * Leave iter->tags uninitialized. radix_tree_next_chunk() will fill it
	 * in the case of a successful tagged chunk lookup.  If the lookup was
	 * unsuccessful or non-tagged then nobody cares about ->tags.
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
 iter->index = 0;
 iter->next_index = start;
 return ((void *)0);
}

/**
 * radix_tree_next_chunk - find next chunk of slots for iteration
 *
 * @root:	radix tree root
 * @iter:	iterator state
 * @flags:	RADIX_TREE_ITER_* flags and tag index
 * Returns:	pointer to chunk first slot, or NULL if there no more left
 *
 * This function looks up the next chunk in the radix tree starting from
 * @iter->next_index.  It returns a pointer to the chunk's first slot.
 * Also it fills @iter with data about chunk: position in the tree (index),
 * its end (next_index), and constructs a bit mask for tagged iterating (tags).
 */
void /* nothing */ **radix_tree_next_chunk(const struct xarray *,
        struct radix_tree_iter *iter, unsigned flags);

/**
 * radix_tree_iter_lookup - look up an index in the radix tree
 * @root: radix tree root
 * @iter: iterator state
 * @index: key to look up
 *
 * If @index is present in the radix tree, this function returns the slot
 * containing it and updates @iter to describe the entry.  If @index is not
 * present, it returns NULL.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void /* nothing */ **
radix_tree_iter_lookup(const struct xarray *root,
   struct radix_tree_iter *iter, unsigned long index)
{
 radix_tree_iter_init(iter, index);
 return radix_tree_next_chunk(root, iter, RADIX_TREE_ITER_CONTIG);
}

/**
 * radix_tree_iter_retry - retry this chunk of the iteration
 * @iter:	iterator state
 *
 * If we iterate over a tree protected only by the RCU lock, a race
 * against deletion or creation may result in seeing a slot for which
 * radix_tree_deref_retry() returns true.  If so, call this function
 * and continue the iteration.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__))
void /* nothing */ **radix_tree_iter_retry(struct radix_tree_iter *iter)
{
 iter->next_index = iter->index;
 iter->tags = 0;
 return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
__radix_tree_iter_add(struct radix_tree_iter *iter, unsigned long slots)
{
 return iter->index + slots;
}

/**
 * radix_tree_iter_resume - resume iterating when the chunk may be invalid
 * @slot: pointer to current slot
 * @iter: iterator state
 * Returns: New slot pointer
 *
 * If the iterator needs to release then reacquire a lock, the chunk may
 * have been invalidated by an insertion or deletion.  Call this function
 * before releasing the lock to continue the iteration from the next index.
 */
void /* nothing */ **__attribute__((__warn_unused_result__)) radix_tree_iter_resume(void /* nothing */ **slot,
     struct radix_tree_iter *iter);

/**
 * radix_tree_chunk_size - get current chunk size
 *
 * @iter:	pointer to radix tree iterator
 * Returns:	current chunk size
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long
radix_tree_chunk_size(struct radix_tree_iter *iter)
{
 return iter->next_index - iter->index;
}

/**
 * radix_tree_next_slot - find next slot in chunk
 *
 * @slot:	pointer to current slot
 * @iter:	pointer to iterator state
 * @flags:	RADIX_TREE_ITER_*, should be constant
 * Returns:	pointer to next slot, or NULL if there no more left
 *
 * This function updates @iter->index in the case of a successful lookup.
 * For tagged lookup it also eats @iter->tags.
 *
 * There are several cases where 'slot' can be passed in as NULL to this
 * function.  These cases result from the use of radix_tree_iter_resume() or
 * radix_tree_iter_retry().  In these cases we don't end up dereferencing
 * 'slot' because either:
 * a) we are doing tagged iteration and iter->tags has been set to 0, or
 * b) we are doing non-tagged iteration, and iter->index and iter->next_index
 *    have been set up so that radix_tree_chunk_size() returns 1 or 0.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void /* nothing */ **radix_tree_next_slot(void /* nothing */ **slot,
    struct radix_tree_iter *iter, unsigned flags)
{
 if (flags & RADIX_TREE_ITER_TAGGED) {
  iter->tags >>= 1;
  if (__builtin_expect(!!(!iter->tags), 0))
   return ((void *)0);
  if (__builtin_expect(!!(iter->tags & 1ul), 1)) {
   iter->index = __radix_tree_iter_add(iter, 1);
   slot++;
   goto found;
  }
  if (!(flags & RADIX_TREE_ITER_CONTIG)) {
   unsigned offset = __ffs(iter->tags);

   iter->tags >>= offset++;
   iter->index = __radix_tree_iter_add(iter, offset);
   slot += offset;
   goto found;
  }
 } else {
  long count = radix_tree_chunk_size(iter);

  while (--count > 0) {
   slot++;
   iter->index = __radix_tree_iter_add(iter, 1);

   if (__builtin_expect(!!(*slot), 1))
    goto found;
   if (flags & RADIX_TREE_ITER_CONTIG) {
    /* forbid switching to the next chunk */
    iter->next_index = 0;
    break;
   }
  }
 }
 return ((void *)0);

 found:
 return slot;
}

/**
 * radix_tree_for_each_slot - iterate over non-empty slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */





/**
 * radix_tree_for_each_tagged - iterate over tagged slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 * @tag:	tag index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */
# 16 "./include/linux/fs.h" 2








# 1 "./include/linux/capability.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * This is <linux/capability.h>
 *
 * Andrew G. Morgan <morgan@kernel.org>
 * Alexander Kjeldaas <astor@guardian.no>
 * with help from Aleph1, Roland Buresund and Andrew Main.
 *
 * See here for the libcap library ("POSIX draft" compliance):
 *
 * ftp://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.6/
 */



# 1 "./include/uapi/linux/capability.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * This is <linux/capability.h>
 *
 * Andrew G. Morgan <morgan@kernel.org>
 * Alexander Kjeldaas <astor@guardian.no>
 * with help from Aleph1, Roland Buresund and Andrew Main.
 *
 * See here for the libcap library ("POSIX draft" compliance):
 *
 * ftp://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.6/
 */






/* User-level do most of the mapping between kernel and user
   capabilities based on the version tag given by the kernel. The
   kernel might be somewhat backwards compatible, but don't bet on
   it. */

/* Note, cap_t, is defined by POSIX (draft) to be an "opaque" pointer to
   a set of three capability sets.  The transposition of 3*the
   following structure to such a composite is better handled in a user
   library since the draft standard requires the use of malloc/free
   etc.. */
# 39 "./include/uapi/linux/capability.h"
typedef struct __user_cap_header_struct {
 __u32 version;
 int pid;
} /* nothing */ *cap_user_header_t;

typedef struct __user_cap_data_struct {
        __u32 effective;
        __u32 permitted;
        __u32 inheritable;
} /* nothing */ *cap_user_data_t;
# 72 "./include/uapi/linux/capability.h"
struct vfs_cap_data {
 __le32 magic_etc; /* Little endian */
 struct {
  __le32 permitted; /* Little endian */
  __le32 inheritable; /* Little endian */
 } data[2];
};

/*
 * same as vfs_cap_data but with a rootid at the end
 */
struct vfs_ns_cap_data {
 __le32 magic_etc;
 struct {
  __le32 permitted; /* Little endian */
  __le32 inheritable; /* Little endian */
 } data[2];
 __le32 rootid;
};
# 105 "./include/uapi/linux/capability.h"
/**
 ** POSIX-draft defined capabilities.
 **/

/* In a system with the [_POSIX_CHOWN_RESTRICTED] option defined, this
   overrides the restriction of changing file ownership and group
   ownership. */



/* Override all DAC access, including ACL execute access if
   [_POSIX_ACL] is defined. Excluding DAC access covered by
   CAP_LINUX_IMMUTABLE. */



/* Overrides all DAC restrictions regarding read and search on files
   and directories, including ACL restrictions if [_POSIX_ACL] is
   defined. Excluding DAC access covered by CAP_LINUX_IMMUTABLE. */



/* Overrides all restrictions about allowed operations on files, where
   file owner ID must be equal to the user ID, except where CAP_FSETID
   is applicable. It doesn't override MAC and DAC restrictions. */



/* Overrides the following restrictions that the effective user ID
   shall match the file owner ID when setting the S_ISUID and S_ISGID
   bits on that file; that the effective group ID (or one of the
   supplementary group IDs) shall match the file owner ID when setting
   the S_ISGID bit on that file; that the S_ISUID and S_ISGID bits are
   cleared on successful return from chown(2) (not implemented). */



/* Overrides the restriction that the real or effective user ID of a
   process sending a signal must match the real or effective user ID
   of the process receiving the signal. */



/* Allows setgid(2) manipulation */
/* Allows setgroups(2) */
/* Allows forged gids on socket credentials passing. */



/* Allows set*uid(2) manipulation (including fsuid). */
/* Allows forged pids on socket credentials passing. */




/**
 ** Linux-specific capabilities
 **/

/* Without VFS support for capabilities:
 *   Transfer any capability in your permitted set to any pid,
 *   remove any capability in your permitted set from any pid
 * With VFS support for capabilities (neither of above, but)
 *   Add any capability from current's capability bounding set
 *       to the current process' inheritable set
 *   Allow taking bits out of capability bounding set
 *   Allow modification of the securebits for a process
 */



/* Allow modification of S_IMMUTABLE and S_APPEND file attributes */



/* Allows binding to TCP/UDP sockets below 1024 */
/* Allows binding to ATM VCIs below 32 */



/* Allow broadcasting, listen to multicast */



/* Allow interface configuration */
/* Allow administration of IP firewall, masquerading and accounting */
/* Allow setting debug option on sockets */
/* Allow modification of routing tables */
/* Allow setting arbitrary process / process group ownership on
   sockets */
/* Allow binding to any address for transparent proxying (also via NET_RAW) */
/* Allow setting TOS (type of service) */
/* Allow setting promiscuous mode */
/* Allow clearing driver statistics */
/* Allow multicasting */
/* Allow read/write of device-specific registers */
/* Allow activation of ATM control sockets */



/* Allow use of RAW sockets */
/* Allow use of PACKET sockets */
/* Allow binding to any address for transparent proxying (also via NET_ADMIN) */



/* Allow locking of shared memory segments */
/* Allow mlock and mlockall (which doesn't really have anything to do
   with IPC) */



/* Override IPC ownership checks */



/* Insert and remove kernel modules - modify kernel without limit */


/* Allow ioperm/iopl access */
/* Allow sending USB messages to any device via /dev/bus/usb */



/* Allow use of chroot() */



/* Allow ptrace() of any process */



/* Allow configuration of process accounting */



/* Allow configuration of the secure attention key */
/* Allow administration of the random device */
/* Allow examination and configuration of disk quotas */
/* Allow setting the domainname */
/* Allow setting the hostname */
/* Allow mount() and umount(), setting up new smb connection */
/* Allow some autofs root ioctls */
/* Allow nfsservctl */
/* Allow VM86_REQUEST_IRQ */
/* Allow to read/write pci config on alpha */
/* Allow irix_prctl on mips (setstacksize) */
/* Allow flushing all cache on m68k (sys_cacheflush) */
/* Allow removing semaphores */
/* Used instead of CAP_CHOWN to "chown" IPC message queues, semaphores
   and shared memory */
/* Allow locking/unlocking of shared memory segment */
/* Allow turning swap on/off */
/* Allow forged pids on socket credentials passing */
/* Allow setting readahead and flushing buffers on block devices */
/* Allow setting geometry in floppy driver */
/* Allow turning DMA on/off in xd driver */
/* Allow administration of md devices (mostly the above, but some
   extra ioctls) */
/* Allow tuning the ide driver */
/* Allow access to the nvram device */
/* Allow administration of apm_bios, serial and bttv (TV) device */
/* Allow manufacturer commands in isdn CAPI support driver */
/* Allow reading non-standardized portions of pci configuration space */
/* Allow DDI debug ioctl on sbpcd driver */
/* Allow setting up serial ports */
/* Allow sending raw qic-117 commands */
/* Allow enabling/disabling tagged queuing on SCSI controllers and sending
   arbitrary SCSI commands */
/* Allow setting encryption key on loopback filesystem */
/* Allow setting zone reclaim policy */
/* Allow everything under CAP_BPF and CAP_PERFMON for backward compatibility */



/* Allow use of reboot() */



/* Allow raising priority and setting priority on other (different
   UID) processes */
/* Allow use of FIFO and round-robin (realtime) scheduling on own
   processes and setting the scheduling algorithm used by another
   process. */
/* Allow setting cpu affinity on other processes */
/* Allow setting realtime ioprio class */
/* Allow setting ioprio class on other processes */



/* Override resource limits. Set resource limits. */
/* Override quota limits. */
/* Override reserved space on ext2 filesystem */
/* Modify data journaling mode on ext3 filesystem (uses journaling
   resources) */
/* NOTE: ext2 honors fsuid when checking for resource overrides, so
   you can override using fsuid too */
/* Override size restrictions on IPC message queues */
/* Allow more than 64hz interrupts from the real-time clock */
/* Override max number of consoles on console allocation */
/* Override max number of keymaps */
/* Control memory reclaim behavior */



/* Allow manipulation of system clock */
/* Allow irix_stime on mips */
/* Allow setting the real-time clock */



/* Allow configuration of tty devices */
/* Allow vhangup() of tty */



/* Allow the privileged aspects of mknod() */



/* Allow taking of leases on files */



/* Allow writing the audit log via unicast netlink socket */



/* Allow configuration of audit via unicast netlink socket */



/* Set or remove capabilities on files.
   Map uid=0 into a child user namespace. */



/* Override MAC access.
   The base kernel enforces no MAC policy.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based overrides of that policy, this is
   the capability it should use to do so. */



/* Allow MAC configuration or state changes.
   The base kernel requires no MAC configuration.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based checks on modifications to that
   policy or the data required to maintain it, this is the
   capability it should use to do so. */



/* Allow configuring the kernel's syslog (printk behaviour) */



/* Allow triggering something that will wake the system */



/* Allow preventing system suspends */



/* Allow reading the audit log via multicast netlink socket */



/*
 * Allow system performance and observability privileged operations
 * using perf_events, i915_perf and other kernel subsystems
 */



/*
 * CAP_BPF allows the following BPF operations:
 * - Creating all types of BPF maps
 * - Advanced verifier features
 *   - Indirect variable access
 *   - Bounded loops
 *   - BPF to BPF function calls
 *   - Scalar precision tracking
 *   - Larger complexity limits
 *   - Dead code elimination
 *   - And potentially other features
 * - Loading BPF Type Format (BTF) data
 * - Retrieve xlated and JITed code of BPF programs
 * - Use bpf_spin_lock() helper
 *
 * CAP_PERFMON relaxes the verifier checks further:
 * - BPF progs can use of pointer-to-integer conversions
 * - speculation attack hardening measures are bypassed
 * - bpf_probe_read to read arbitrary kernel memory is allowed
 * - bpf_trace_printk to print kernel memory is allowed
 *
 * CAP_SYS_ADMIN is required to use bpf_probe_write_user.
 *
 * CAP_SYS_ADMIN is required to iterate system wide loaded
 * programs, maps, links, BTFs and convert their IDs to file descriptors.
 *
 * CAP_PERFMON and CAP_BPF are required to load tracing programs.
 * CAP_NET_ADMIN and CAP_BPF are required to load networking programs.
 */



/* Allow checkpoint/restore related operations */
/* Allow PID selection during clone3() */
/* Allow writing to ns_last_pid */







/*
 * Bit location of each capability (used by user-space library and kernel)
 */
# 17 "./include/linux/capability.h" 2





extern int file_caps_enabled;

typedef struct kernel_cap_struct {
 __u32 cap[2];
} kernel_cap_t;

/* same as vfs_ns_cap_data but in cpu endian and always filled completely */
struct cpu_vfs_cap_data {
 __u32 magic_etc;
 kernel_cap_t permitted;
 kernel_cap_t inheritable;
 kuid_t rootid;
};





struct file;
struct inode;
struct dentry;
struct task_struct;
struct user_namespace;

extern const kernel_cap_t __cap_empty_set;
extern const kernel_cap_t __cap_init_eff_set;

/*
 * Internal kernel functions only
 */




/*
 * CAP_FS_MASK and CAP_NFSD_MASKS:
 *
 * The fs mask is all the privileges that fsuid==0 historically meant.
 * At one time in the past, that included CAP_MKNOD and CAP_LINUX_IMMUTABLE.
 *
 * It has never meant setting security.* and trusted.* xattrs.
 *
 * We could also define fsmask as follows:
 *   1. CAP_FS_MASK is the privilege to bypass all fs-related DAC permissions
 *   2. The security.* and trusted.* xattrs are fs-related MAC permissions
 */
# 118 "./include/linux/capability.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_combine(const kernel_cap_t a,
           const kernel_cap_t b)
{
 kernel_cap_t dest;
 do { unsigned __capi; for (__capi = 0; __capi < 2; ++__capi) { dest.cap[__capi] = a.cap[__capi] | b.cap[__capi]; } } while (0);
 return dest;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_intersect(const kernel_cap_t a,
      const kernel_cap_t b)
{
 kernel_cap_t dest;
 do { unsigned __capi; for (__capi = 0; __capi < 2; ++__capi) { dest.cap[__capi] = a.cap[__capi] & b.cap[__capi]; } } while (0);
 return dest;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_drop(const kernel_cap_t a,
        const kernel_cap_t drop)
{
 kernel_cap_t dest;
 do { unsigned __capi; for (__capi = 0; __capi < 2; ++__capi) { dest.cap[__capi] = a.cap[__capi] &~ drop.cap[__capi]; } } while (0);
 return dest;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_invert(const kernel_cap_t c)
{
 kernel_cap_t dest;
 do { unsigned __capi; for (__capi = 0; __capi < 2; ++__capi) { dest.cap[__capi] = ~ c.cap[__capi]; } } while (0);
 return dest;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cap_isclear(const kernel_cap_t a)
{
 unsigned __capi;
 for (__capi = 0; __capi < 2; ++__capi) {
  if (a.cap[__capi] != 0)
   return false;
 }
 return true;
}

/*
 * Check if "a" is a subset of "set".
 * return true if ALL of the capabilities in "a" are also in "set"
 *	cap_issubset(0101, 1111) will return true
 * return false if ANY of the capabilities in "a" are not in "set"
 *	cap_issubset(1111, 0101) will return false
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cap_issubset(const kernel_cap_t a, const kernel_cap_t set)
{
 kernel_cap_t dest;
 dest = cap_drop(a, set);
 return cap_isclear(dest);
}

/* Used to decide between falling back on the old suser() or fsuser(). */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_drop_fs_set(const kernel_cap_t a)
{
 const kernel_cap_t __cap_fs_set = ((kernel_cap_t){{ ((1U << ((0) & 31)) /* mask for indexed __u32 */ | (1U << ((27) & 31)) /* mask for indexed __u32 */ | (1U << ((1) & 31)) /* mask for indexed __u32 */ | (1U << ((2) & 31)) /* mask for indexed __u32 */ | (1U << ((3) & 31)) /* mask for indexed __u32 */ | (1U << ((4) & 31)) /* mask for indexed __u32 */) | (1U << ((9) & 31)) /* mask for indexed __u32 */, ((1U << ((32) & 31)) /* mask for indexed __u32 */) } });
 return cap_drop(a, __cap_fs_set);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_raise_fs_set(const kernel_cap_t a,
         const kernel_cap_t permitted)
{
 const kernel_cap_t __cap_fs_set = ((kernel_cap_t){{ ((1U << ((0) & 31)) /* mask for indexed __u32 */ | (1U << ((27) & 31)) /* mask for indexed __u32 */ | (1U << ((1) & 31)) /* mask for indexed __u32 */ | (1U << ((2) & 31)) /* mask for indexed __u32 */ | (1U << ((3) & 31)) /* mask for indexed __u32 */ | (1U << ((4) & 31)) /* mask for indexed __u32 */) | (1U << ((9) & 31)) /* mask for indexed __u32 */, ((1U << ((32) & 31)) /* mask for indexed __u32 */) } });
 return cap_combine(a,
      cap_intersect(permitted, __cap_fs_set));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_drop_nfsd_set(const kernel_cap_t a)
{
 const kernel_cap_t __cap_fs_set = ((kernel_cap_t){{ ((1U << ((0) & 31)) /* mask for indexed __u32 */ | (1U << ((27) & 31)) /* mask for indexed __u32 */ | (1U << ((1) & 31)) /* mask for indexed __u32 */ | (1U << ((2) & 31)) /* mask for indexed __u32 */ | (1U << ((3) & 31)) /* mask for indexed __u32 */ | (1U << ((4) & 31)) /* mask for indexed __u32 */) | (1U << ((24) & 31)) /* mask for indexed __u32 */, ((1U << ((32) & 31)) /* mask for indexed __u32 */) } });
 return cap_drop(a, __cap_fs_set);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kernel_cap_t cap_raise_nfsd_set(const kernel_cap_t a,
           const kernel_cap_t permitted)
{
 const kernel_cap_t __cap_nfsd_set = ((kernel_cap_t){{ ((1U << ((0) & 31)) /* mask for indexed __u32 */ | (1U << ((27) & 31)) /* mask for indexed __u32 */ | (1U << ((1) & 31)) /* mask for indexed __u32 */ | (1U << ((2) & 31)) /* mask for indexed __u32 */ | (1U << ((3) & 31)) /* mask for indexed __u32 */ | (1U << ((4) & 31)) /* mask for indexed __u32 */) | (1U << ((24) & 31)) /* mask for indexed __u32 */, ((1U << ((32) & 31)) /* mask for indexed __u32 */) } });
 return cap_combine(a,
      cap_intersect(permitted, __cap_nfsd_set));
}


extern bool has_capability(struct task_struct *t, int cap);
extern bool has_ns_capability(struct task_struct *t,
         struct user_namespace *ns, int cap);
extern bool has_capability_noaudit(struct task_struct *t, int cap);
extern bool has_ns_capability_noaudit(struct task_struct *t,
          struct user_namespace *ns, int cap);
extern bool capable(int cap);
extern bool ns_capable(struct user_namespace *ns, int cap);
extern bool ns_capable_noaudit(struct user_namespace *ns, int cap);
extern bool ns_capable_setid(struct user_namespace *ns, int cap);
# 250 "./include/linux/capability.h"
bool privileged_wrt_inode_uidgid(struct user_namespace *ns,
     struct user_namespace *mnt_userns,
     const struct inode *inode);
bool capable_wrt_inode_uidgid(struct user_namespace *mnt_userns,
         const struct inode *inode, int cap);
extern bool file_ns_capable(const struct file *file, struct user_namespace *ns, int cap);
extern bool ptracer_capable(struct task_struct *tsk, struct user_namespace *ns);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool perfmon_capable(void)
{
 return capable(38) || capable(21);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bpf_capable(void)
{
 return capable(39) || capable(21);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool checkpoint_restore_ns_capable(struct user_namespace *ns)
{
 return ns_capable(ns, 40) ||
  ns_capable(ns, 21);
}

/* audit system wants to get cap info from files as well */
int get_vfs_caps_from_disk(struct user_namespace *mnt_userns,
      const struct dentry *dentry,
      struct cpu_vfs_cap_data *cpu_caps);

int cap_convert_nscap(struct user_namespace *mnt_userns, struct dentry *dentry,
        const void **ivalue, size_t size);
# 25 "./include/linux/fs.h" 2
# 1 "./include/linux/semaphore.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (c) 2008 Intel Corporation
 * Author: Matthew Wilcox <willy@linux.intel.com>
 *
 * Please see kernel/locking/semaphore.c for documentation of these functions
 */






/* Please don't access any members of this structure directly */
struct semaphore {
 raw_spinlock_t lock;
 unsigned int count;
 struct list_head wait_list;
};
# 31 "./include/linux/semaphore.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sema_init(struct semaphore *sem, int val)
{
 static struct lock_class_key __key;
 *sem = (struct semaphore) { .lock = (raw_spinlock_t) { .raw_lock = { { .val = { (0) } } }, }, .count = val, .wait_list = { &((*sem).wait_list), &((*sem).wait_list) }, };
 do { (void)("semaphore->lock"); (void)(&__key); } while (0);
}

extern void down(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_interruptible(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_killable(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_trylock(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_timeout(struct semaphore *sem, long jiffies);
extern void up(struct semaphore *sem);
# 26 "./include/linux/fs.h" 2
# 1 "./include/linux/fcntl.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/uapi/linux/fcntl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



# 1 "./arch/arm64/include/uapi/asm/fcntl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Copyright (C) 2012 ARM Ltd.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */



/*
 * Using our own definitions for AArch32 (compat) support.
 */





# 1 "./include/uapi/asm-generic/fcntl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





/*
 * FMODE_EXEC is 0x20
 * FMODE_NONOTIFY is 0x4000000
 * These cannot be used by userspace O_* until internal and external open
 * flags are split.
 * -Eric Paris
 */

/*
 * When introducing new O_* bits, please check its uniqueness in fcntl_init().
 */
# 66 "./include/uapi/asm-generic/fcntl.h"
/*
 * Before Linux 2.6.33 only O_DSYNC semantics were implemented, but using
 * the O_SYNC flag.  We continue to use the existing numerical value
 * for O_DSYNC semantics now, but using the correct symbolic name for it.
 * This new value is used to request true Posix O_SYNC semantics.  It is
 * defined in this strange way to make sure applications compiled against
 * new headers get at least O_DSYNC semantics on older kernels.
 *
 * This has the nice side-effect that we can simply test for O_DSYNC
 * wherever we do not care if O_DSYNC or O_SYNC is used.
 *
 * Note: __O_SYNC must never be used directly.
 */
# 92 "./include/uapi/asm-generic/fcntl.h"
/* a horrid kludge trying to make sure that this will fail on old kernels */
# 136 "./include/uapi/asm-generic/fcntl.h"
/*
 * Open File Description Locks
 *
 * Usually record locks held by a process are released on *any* close and are
 * not inherited across a fork().
 *
 * These cmd values will set locks that conflict with process-associated
 * record  locks, but are "owned" by the open file description, not the
 * process. This means that they are inherited across fork() like BSD (flock)
 * locks, and they are only released automatically when the last reference to
 * the the open file against which they were acquired is put.
 */
# 156 "./include/uapi/asm-generic/fcntl.h"
struct f_owner_ex {
 int type;
 __kernel_pid_t pid;
};

/* for F_[GET|SET]FL */


/* for posix fcntl() and lockf() */






/* for old implementation of bsd flock () */





/* operations for bsd flock(), also used by the kernel implementation */






/*
 * LOCK_MAND support has been removed from the kernel. We leave the symbols
 * here to not break legacy builds, but these should not be used in new code.
 */
# 196 "./include/uapi/asm-generic/fcntl.h"
struct flock {
 short l_type;
 short l_whence;
 __kernel_off_t l_start;
 __kernel_off_t l_len;
 __kernel_pid_t l_pid;






};

struct flock64 {
 short l_type;
 short l_whence;
 __kernel_loff_t l_start;
 __kernel_loff_t l_len;
 __kernel_pid_t l_pid;



};
# 29 "./arch/arm64/include/uapi/asm/fcntl.h" 2
# 6 "./include/uapi/linux/fcntl.h" 2
# 1 "./include/uapi/linux/openat2.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */





/*
 * Arguments for how openat2(2) should open the target path. If only @flags and
 * @mode are non-zero, then openat2(2) operates very similarly to openat(2).
 *
 * However, unlike openat(2), unknown or invalid bits in @flags result in
 * -EINVAL rather than being silently ignored. @mode must be zero unless one of
 * {O_CREAT, O_TMPFILE} are set.
 *
 * @flags: O_* flags.
 * @mode: O_CREAT/O_TMPFILE file mode.
 * @resolve: RESOLVE_* flags.
 */
struct open_how {
 __u64 flags;
 __u64 mode;
 __u64 resolve;
};

/* how->resolve flags for openat2(2). */
# 7 "./include/uapi/linux/fcntl.h" 2




/*
 * Cancel a blocking posix lock; internal use only until we expose an
 * asynchronous lock api to userspace:
 */


/* Create a file descriptor with FD_CLOEXEC set. */


/*
 * Request nofications on a directory.
 * See below for events that may be notified.
 */


/*
 * Set and get of pipe page size array
 */



/*
 * Set/Get seals
 */



/*
 * Types of seals
 */





/* (1U << 31) is reserved for signed error codes */

/*
 * Set/Get write life time hints. {GET,SET}_RW_HINT operate on the
 * underlying inode, while {GET,SET}_FILE_RW_HINT operate only on
 * the specific file.
 */





/*
 * Valid hint values for F_{GET,SET}_RW_HINT. 0 is "not set", or can be
 * used to clear any hints previously set.
 */







/*
 * The originally introduced spelling is remained from the first
 * versions of the patch set that introduced the feature, see commit
 * v4.13-rc1~212^2~51.
 */


/*
 * Types of directory notifications that may be requested.
 */
# 87 "./include/uapi/linux/fcntl.h"
/*
 * The constants AT_REMOVEDIR and AT_EACCESS have the same value.  AT_EACCESS is
 * meaningful only to faccessat, while AT_REMOVEDIR is meaningful only to
 * unlinkat.  The two functions do completely different things and therefore,
 * the flags can be allowed to overlap.  For example, passing AT_REMOVEDIR to
 * faccessat would be undefined behavior and thus treating it equivalent to
 * AT_EACCESS is valid undefined behavior.
 */
# 7 "./include/linux/fcntl.h" 2

/* List of all valid flags for the open/openat flags argument: */






/* List of all valid flags for the how->resolve argument: */




/* List of all open_how "versions". */
# 27 "./include/linux/fs.h" 2



# 1 "./include/linux/migrate_mode.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/*
 * MIGRATE_ASYNC means never block
 * MIGRATE_SYNC_LIGHT in the current implementation means to allow blocking
 *	on most operations but not ->writepage as the potential stall time
 *	is too significant
 * MIGRATE_SYNC will block when migrating pages
 * MIGRATE_SYNC_NO_COPY will block when migrating pages but will not copy pages
 *	with the CPU. Instead, page copy happens outside the migratepage()
 *	callback and is likely using a DMA engine. See migrate_vma() and HMM
 *	(mm/hmm.c) for users of this mode.
 */
enum migrate_mode {
 MIGRATE_ASYNC,
 MIGRATE_SYNC_LIGHT,
 MIGRATE_SYNC,
 MIGRATE_SYNC_NO_COPY,
};

enum migrate_reason {
 MR_COMPACTION,
 MR_MEMORY_FAILURE,
 MR_MEMORY_HOTPLUG,
 MR_SYSCALL, /* also applies to cpusets */
 MR_MEMPOLICY_MBIND,
 MR_NUMA_MISPLACED,
 MR_CONTIG_RANGE,
 MR_LONGTERM_PIN,
 MR_DEMOTION,
 MR_TYPES
};
# 31 "./include/linux/fs.h" 2


# 1 "./include/linux/percpu-rwsem.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/rcuwait.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/sched/signal.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/signal.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







struct task_struct;

/* for sysctl */
extern int print_fatal_signals;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void copy_siginfo(kernel_siginfo_t *to,
    const kernel_siginfo_t *from)
{
 memcpy(to, from, sizeof(*to));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_siginfo(kernel_siginfo_t *info)
{
 memset(info, 0, sizeof(*info));
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void copy_siginfo_to_external(siginfo_t *to,
         const kernel_siginfo_t *from)
{
 memcpy(to, from, sizeof(*from));
 memset(((char *)to) + sizeof(struct kernel_siginfo), 0,
  (sizeof(struct siginfo) - sizeof(struct kernel_siginfo)));
}

int copy_siginfo_to_user(siginfo_t /* nothing */ *to, const kernel_siginfo_t *from);
int copy_siginfo_from_user(kernel_siginfo_t *to, const siginfo_t /* nothing */ *from);

enum siginfo_layout {
 SIL_KILL,
 SIL_TIMER,
 SIL_POLL,
 SIL_FAULT,
 SIL_FAULT_TRAPNO,
 SIL_FAULT_MCEERR,
 SIL_FAULT_BNDERR,
 SIL_FAULT_PKUERR,
 SIL_FAULT_PERF_EVENT,
 SIL_CHLD,
 SIL_RT,
 SIL_SYS,
};

enum siginfo_layout siginfo_layout(unsigned sig, int si_code);

/*
 * Define some primitives to manipulate sigset_t.
 */




/* We don't use <linux/bitops.h> for these because there is no need to
   be atomic.  */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigaddset(sigset_t *set, int _sig)
{
 unsigned long sig = _sig - 1;
 if ((64 / 64) == 1)
  set->sig[0] |= 1UL << sig;
 else
  set->sig[sig / 64] |= 1UL << (sig % 64);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigdelset(sigset_t *set, int _sig)
{
 unsigned long sig = _sig - 1;
 if ((64 / 64) == 1)
  set->sig[0] &= ~(1UL << sig);
 else
  set->sig[sig / 64] &= ~(1UL << (sig % 64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sigismember(sigset_t *set, int _sig)
{
 unsigned long sig = _sig - 1;
 if ((64 / 64) == 1)
  return 1 & (set->sig[0] >> sig);
 else
  return 1 & (set->sig[sig / 64] >> (sig % 64));
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sigisemptyset(sigset_t *set)
{
 switch ((64 / 64)) {
 case 4:
  return (set->sig[3] | set->sig[2] |
   set->sig[1] | set->sig[0]) == 0;
 case 2:
  return (set->sig[1] | set->sig[0]) == 0;
 case 1:
  return set->sig[0] == 0;
 default:
  do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_291(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_291(); } while (0);
# 105 "./include/linux/signal.h"
  return 0;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sigequalsets(const sigset_t *set1, const sigset_t *set2)
{
 switch ((64 / 64)) {
 case 4:
  return (set1->sig[3] == set2->sig[3]) &&
   (set1->sig[2] == set2->sig[2]) &&
   (set1->sig[1] == set2->sig[1]) &&
   (set1->sig[0] == set2->sig[0]);
 case 2:
  return (set1->sig[1] == set2->sig[1]) &&
   (set1->sig[0] == set2->sig[0]);
 case 1:
  return set1->sig[0] == set2->sig[0];
 }
 return 0;
}
# 156 "./include/linux/signal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigorsets(sigset_t *r, const sigset_t *a, const sigset_t *b) { unsigned long a0, a1, a2, a3, b0, b1, b2, b3; switch ((64 / 64)) { case 4: a3 = a->sig[3]; a2 = a->sig[2]; b3 = b->sig[3]; b2 = b->sig[2]; r->sig[3] = ((a3) | (b3)); r->sig[2] = ((a2) | (b2)); __attribute__((__fallthrough__)); case 2: a1 = a->sig[1]; b1 = b->sig[1]; r->sig[1] = ((a1) | (b1)); __attribute__((__fallthrough__)); case 1: a0 = a->sig[0]; b0 = b->sig[0]; r->sig[0] = ((a0) | (b0)); break; default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_292(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_292(); } while (0); } }
# 159 "./include/linux/signal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigandsets(sigset_t *r, const sigset_t *a, const sigset_t *b) { unsigned long a0, a1, a2, a3, b0, b1, b2, b3; switch ((64 / 64)) { case 4: a3 = a->sig[3]; a2 = a->sig[2]; b3 = b->sig[3]; b2 = b->sig[2]; r->sig[3] = ((a3) & (b3)); r->sig[2] = ((a2) & (b2)); __attribute__((__fallthrough__)); case 2: a1 = a->sig[1]; b1 = b->sig[1]; r->sig[1] = ((a1) & (b1)); __attribute__((__fallthrough__)); case 1: a0 = a->sig[0]; b0 = b->sig[0]; r->sig[0] = ((a0) & (b0)); break; default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_293(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_293(); } while (0); } }
# 162 "./include/linux/signal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigandnsets(sigset_t *r, const sigset_t *a, const sigset_t *b) { unsigned long a0, a1, a2, a3, b0, b1, b2, b3; switch ((64 / 64)) { case 4: a3 = a->sig[3]; a2 = a->sig[2]; b3 = b->sig[3]; b2 = b->sig[2]; r->sig[3] = ((a3) & ~(b3)); r->sig[2] = ((a2) & ~(b2)); __attribute__((__fallthrough__)); case 2: a1 = a->sig[1]; b1 = b->sig[1]; r->sig[1] = ((a1) & ~(b1)); __attribute__((__fallthrough__)); case 1: a0 = a->sig[0]; b0 = b->sig[0]; r->sig[0] = ((a0) & ~(b0)); break; default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_294(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_294(); } while (0); } }
# 186 "./include/linux/signal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void signotset(sigset_t *set) { switch ((64 / 64)) { case 4: set->sig[3] = (~(set->sig[3])); set->sig[2] = (~(set->sig[2])); __attribute__((__fallthrough__)); case 2: set->sig[1] = (~(set->sig[1])); __attribute__((__fallthrough__)); case 1: set->sig[0] = (~(set->sig[0])); break; default: do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_295(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_295(); } while (0); } }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigemptyset(sigset_t *set)
{
 switch ((64 / 64)) {
 default:
  memset(set, 0, sizeof(sigset_t));
  break;
 case 2: set->sig[1] = 0;
  __attribute__((__fallthrough__));
 case 1: set->sig[0] = 0;
  break;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigfillset(sigset_t *set)
{
 switch ((64 / 64)) {
 default:
  memset(set, -1, sizeof(sigset_t));
  break;
 case 2: set->sig[1] = -1;
  __attribute__((__fallthrough__));
 case 1: set->sig[0] = -1;
  break;
 }
}

/* Some extensions for manipulating the low 32 signals in particular.  */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigaddsetmask(sigset_t *set, unsigned long mask)
{
 set->sig[0] |= mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sigdelsetmask(sigset_t *set, unsigned long mask)
{
 set->sig[0] &= ~mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sigtestsetmask(sigset_t *set, unsigned long mask)
{
 return (set->sig[0] & mask) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void siginitset(sigset_t *set, unsigned long mask)
{
 set->sig[0] = mask;
 switch ((64 / 64)) {
 default:
  memset(&set->sig[1], 0, sizeof(long)*((64 / 64)-1));
  break;
 case 2: set->sig[1] = 0;
  break;
 case 1: ;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void siginitsetinv(sigset_t *set, unsigned long mask)
{
 set->sig[0] = ~mask;
 switch ((64 / 64)) {
 default:
  memset(&set->sig[1], -1, sizeof(long)*((64 / 64)-1));
  break;
 case 2: set->sig[1] = -1;
  break;
 case 1: ;
 }
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_sigpending(struct sigpending *sig)
{
 sigemptyset(&sig->signal);
 INIT_LIST_HEAD(&sig->list);
}

extern void flush_sigqueue(struct sigpending *queue);

/* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int valid_signal(unsigned long sig)
{
 return sig <= 64 ? 1 : 0;
}

struct timespec;
struct pt_regs;
enum pid_type;

extern int next_signal(struct sigpending *pending, sigset_t *mask);
extern int do_send_sig_info(int sig, struct kernel_siginfo *info,
    struct task_struct *p, enum pid_type type);
extern int group_send_sig_info(int sig, struct kernel_siginfo *info,
          struct task_struct *p, enum pid_type type);
extern int send_signal_locked(int sig, struct kernel_siginfo *info,
         struct task_struct *p, enum pid_type type);
extern int sigprocmask(int, sigset_t *, sigset_t *);
extern void set_current_blocked(sigset_t *);
extern void __set_current_blocked(const sigset_t *);
extern int show_unhandled_signals;

extern bool get_signal(struct ksignal *ksig);
extern void signal_setup_done(int failed, struct ksignal *ksig, int stepping);
extern void exit_signals(struct task_struct *tsk);
extern void kernel_sigaction(int, __sighandler_t);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void allow_signal(int sig)
{
 /*
	 * Kernel threads handle their own signals. Let the signal code
	 * know it'll be handled, so that they don't get converted to
	 * SIGKILL or just silently dropped.
	 */
 kernel_sigaction(sig, (( __sighandler_t)2));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void allow_kernel_signal(int sig)
{
 /*
	 * Kernel threads handle their own signals. Let the signal code
	 * know signals sent by the kernel will be handled, so that they
	 * don't get silently dropped.
	 */
 kernel_sigaction(sig, (( __sighandler_t)3));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void disallow_signal(int sig)
{
 kernel_sigaction(sig, (( __sighandler_t)1) /* ignore signal */);
}

extern struct kmem_cache *sighand_cachep;

extern bool unhandled_signal(struct task_struct *tsk, int sig);

/*
 * In POSIX a signal is sent either to a specific thread (Linux task)
 * or to the process as a whole (Linux thread group).  How the signal
 * is sent determines whether it's to one thread or the whole group,
 * which determines which signal mask(s) are involved in blocking it
 * from being delivered until later.  When the signal is delivered,
 * either it's caught or ignored by a user handler or it has a default
 * effect that applies to the whole thread group (POSIX process).
 *
 * The possible effects an unblocked signal set to SIG_DFL can have are:
 *   ignore	- Nothing Happens
 *   terminate	- kill the process, i.e. all threads in the group,
 * 		  similar to exit_group.  The group leader (only) reports
 *		  WIFSIGNALED status to its parent.
 *   coredump	- write a core dump file describing all threads using
 *		  the same mm and then kill all those threads
 *   stop 	- stop all the threads in the group, i.e. TASK_STOPPED state
 *
 * SIGKILL and SIGSTOP cannot be caught, blocked, or ignored.
 * Other signals when not blocked and set to SIG_DFL behaves as follows.
 * The job control signals also have other special effects.
 *
 *	+--------------------+------------------+
 *	|  POSIX signal      |  default action  |
 *	+--------------------+------------------+
 *	|  SIGHUP            |  terminate	|
 *	|  SIGINT            |	terminate	|
 *	|  SIGQUIT           |	coredump 	|
 *	|  SIGILL            |	coredump 	|
 *	|  SIGTRAP           |	coredump 	|
 *	|  SIGABRT/SIGIOT    |	coredump 	|
 *	|  SIGBUS            |	coredump 	|
 *	|  SIGFPE            |	coredump 	|
 *	|  SIGKILL           |	terminate(+)	|
 *	|  SIGUSR1           |	terminate	|
 *	|  SIGSEGV           |	coredump 	|
 *	|  SIGUSR2           |	terminate	|
 *	|  SIGPIPE           |	terminate	|
 *	|  SIGALRM           |	terminate	|
 *	|  SIGTERM           |	terminate	|
 *	|  SIGCHLD           |	ignore   	|
 *	|  SIGCONT           |	ignore(*)	|
 *	|  SIGSTOP           |	stop(*)(+)  	|
 *	|  SIGTSTP           |	stop(*)  	|
 *	|  SIGTTIN           |	stop(*)  	|
 *	|  SIGTTOU           |	stop(*)  	|
 *	|  SIGURG            |	ignore   	|
 *	|  SIGXCPU           |	coredump 	|
 *	|  SIGXFSZ           |	coredump 	|
 *	|  SIGVTALRM         |	terminate	|
 *	|  SIGPROF           |	terminate	|
 *	|  SIGPOLL/SIGIO     |	terminate	|
 *	|  SIGSYS/SIGUNUSED  |	coredump 	|
 *	|  SIGSTKFLT         |	terminate	|
 *	|  SIGWINCH          |	ignore   	|
 *	|  SIGPWR            |	terminate	|
 *	|  SIGRTMIN-SIGRTMAX |	terminate       |
 *	+--------------------+------------------+
 *	|  non-POSIX signal  |  default action  |
 *	+--------------------+------------------+
 *	|  SIGEMT            |  coredump	|
 *	+--------------------+------------------+
 *
 * (+) For SIGKILL and SIGSTOP the action is "always", not just "default".
 * (*) Special job control effects:
 * When SIGCONT is sent, it resumes the process (all threads in the group)
 * from TASK_STOPPED state and also clears any pending/queued stop signals
 * (any of those marked with "stop(*)").  This happens regardless of blocking,
 * catching, or ignoring SIGCONT.  When any stop signal is sent, it clears
 * any pending/queued SIGCONT signals; this happens regardless of blocking,
 * catching, or ignored the stop signal, though (except for SIGSTOP) the
 * default action of stopping the process may happen later or never.
 */
# 454 "./include/linux/signal.h"
void signals_init(void);

int restore_altstack(const stack_t /* nothing */ *);
int __save_altstack(stack_t /* nothing */ *, unsigned long);
# 470 "./include/linux/signal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sigaltstack_size_valid(size_t size) { return true; }



struct seq_file;
extern void render_sigset_t(struct seq_file *, const char *, sigset_t *);
# 7 "./include/linux/sched/signal.h" 2

# 1 "./include/linux/sched/jobctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





struct task_struct;

/*
 * task->jobctl flags
 */
# 43 "./include/linux/sched/jobctl.h"
extern bool task_set_jobctl_pending(struct task_struct *task, unsigned long mask);
extern void task_clear_jobctl_trapping(struct task_struct *task);
extern void task_clear_jobctl_pending(struct task_struct *task, unsigned long mask);
# 9 "./include/linux/sched/signal.h" 2
# 1 "./include/linux/sched/task.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Interface between the scheduler and various task lifetime (fork()/exit())
 * functionality:
 */


# 1 "./include/linux/uaccess.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/fault-inject-usercopy.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * This header provides a wrapper for injecting failures to user space memory
 * access functions.
 */
# 18 "./include/linux/fault-inject-usercopy.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool should_fail_usercopy(void) { return false; }
# 6 "./include/linux/uaccess.h" 2





# 1 "./arch/arm64/include/asm/uaccess.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/uaccess.h
 *
 * Copyright (C) 2012 ARM Ltd.
 */




# 1 "./arch/arm64/include/asm/kernel-pgtable.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Kernel page table mapping
 *
 * Copyright (C) 2015 ARM Ltd.
 */




# 1 "./arch/arm64/include/asm/boot.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






/*
 * arm64 requires the DTB to be 8 byte aligned and
 * not exceed 2MB in size.
 */



/*
 * arm64 requires the kernel image to placed at a 2 MB aligned base address
 */
# 12 "./arch/arm64/include/asm/kernel-pgtable.h" 2



/*
 * The linear mapping and the start of memory are both 2M aligned (per
 * the arm64 booting.txt requirements). Hence we can use section mapping
 * with 4K (section size = 2M) but not with 16K (section size = 32M) or
 * 64K (section size = 512M).
 */

/*
 * The idmap and swapper page tables need some space reserved in the kernel
 * image. Both require pgd, pud (4 levels only) and pmd tables to (section)
 * map the kernel. With the 64K page configuration, swapper and idmap need to
 * map to pte level. The swapper also maps the FDT (see __create_page_tables
 * for more information). Note that the number of ID map translation levels
 * could be increased on the fly if system RAM is out of reach for the default
 * VA range, so pages required to map highest possible PA are reserved in all
 * cases.
 */







/*
 * If KASLR is enabled, then an offset K is added to the kernel address
 * space. The bottom 21 bits of this offset are zero to guarantee 2MB
 * alignment for PA and VA.
 *
 * For each pagetable level of the swapper, we know that the shift will
 * be larger than 21 (for the 4KB granule case we use section maps thus
 * the smallest shift is actually 30) thus there is the possibility that
 * KASLR can increase the number of pagetable entries by 1, so we make
 * room for this extra entry.
 *
 * Note KASLR cannot increase the number of required entries for a level
 * by more than one because it increments both the virtual start and end
 * addresses equally (the extra entry comes from the case where the end
 * address is just pushed over a boundary and the start address isn't).
 */
# 85 "./arch/arm64/include/asm/kernel-pgtable.h"
/* the initial ID map may need two extra pages if it needs to be extended */







/* Initial memory map size */
# 104 "./arch/arm64/include/asm/kernel-pgtable.h"
/*
 * Initial memory map attributes.
 */
# 118 "./arch/arm64/include/asm/kernel-pgtable.h"
/*
 * To make optimal use of block mappings when laying out the linear
 * mapping, round down the base of physical memory to a size that can
 * be mapped efficiently, i.e., either PUD_SIZE (4k granule) or PMD_SIZE
 * (64k granule), or a multiple that can be mapped using contiguous bits
 * in the page tables: 32 * PMD_SIZE (16k granule)
 */
# 133 "./arch/arm64/include/asm/kernel-pgtable.h"
/*
 * sparsemem vmemmap imposes an additional requirement on the alignment of
 * memstart_addr, due to the fact that the base of the vmemmap region
 * has a direct correspondence, and needs to appear sufficiently aligned
 * in the virtual address space.
 */
# 12 "./arch/arm64/include/asm/uaccess.h" 2


/*
 * User space memory access functions
 */




# 1 "./arch/arm64/include/asm/asm-extable.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
# 14 "./arch/arm64/include/asm/asm-extable.h"
/* Data fields for EX_TYPE_UACCESS_ERR_ZERO */





/* Data fields for EX_TYPE_LOAD_UNALIGNED_ZEROPAD */
# 22 "./arch/arm64/include/asm/uaccess.h" 2


# 1 "./arch/arm64/include/asm/mte.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright (C) 2020 ARM Ltd.
 */
# 21 "./arch/arm64/include/asm/mte.h"
void mte_clear_page_tags(void *addr);
unsigned long mte_copy_tags_from_user(void *to, const void /* nothing */ *from,
          unsigned long n);
unsigned long mte_copy_tags_to_user(void /* nothing */ *to, void *from,
        unsigned long n);
int mte_save_tags(struct page *page);
void mte_save_page_tags(const void *page_addr, void *tag_storage);
bool mte_restore_tags(swp_entry_t entry, struct page *page);
void mte_restore_page_tags(void *page_addr, const void *tag_storage);
void mte_invalidate_tags(int type, unsigned long offset);
void mte_invalidate_tags_area(int type);
void *mte_allocate_tag_storage(void);
void mte_free_tag_storage(char *storage);



/* track which pages have valid allocation tags */


void mte_zero_clear_page_tags(void *addr);
void mte_sync_tags(pte_t old_pte, pte_t pte);
void mte_copy_page_tags(void *kto, const void *kfrom);
void mte_thread_init_user(void);
void mte_thread_switch(struct task_struct *next);
void mte_cpu_setup(void);
void mte_suspend_enter(void);
void mte_suspend_exit(void);
long set_mte_ctrl(struct task_struct *task, unsigned long arg);
long get_mte_ctrl(struct task_struct *task);
int mte_ptrace_copy_tags(struct task_struct *child, long request,
    unsigned long addr, unsigned long data);
size_t mte_probe_user_range(const char /* nothing */ *uaddr, size_t size);
# 97 "./arch/arm64/include/asm/mte.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mte_disable_tco_entry(struct task_struct *task)
{
 if (!system_supports_mte())
  return;

 /*
	 * Re-enable tag checking (TCO set on exception entry). This is only
	 * necessary if MTE is enabled in either the kernel or the userspace
	 * task in synchronous or asymmetric mode (SCTLR_EL1.TCF0 bit 0 is set
	 * for both). With MTE disabled in the kernel and disabled or
	 * asynchronous in userspace, tag check faults (including in uaccesses)
	 * are not reported, therefore there is no need to re-enable checking.
	 * This is beneficial on microarchitectures where re-enabling TCO is
	 * expensive.
	 */
 if (kasan_hw_tags_enabled() ||
     (task->thread.sctlr_user & (1UL << 38)))
  asm volatile(".inst " "(0xd500401f | ((3) << 16 | (4) << 5) | ((!!(0)) << 8))" "\n\t");
}
# 152 "./arch/arm64/include/asm/mte.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool system_uses_mte_async_or_asymm_mode(void)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mte_check_tfsr_el1(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mte_check_tfsr_entry(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mte_check_tfsr_exit(void)
{
}
# 25 "./arch/arm64/include/asm/uaccess.h" 2


# 1 "./arch/arm64/include/asm/extable.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * The exception table consists of pairs of relative offsets: the first
 * is the relative offset to an instruction that is allowed to fault,
 * and the second is the relative offset at which the program should
 * continue. No registers are modified, so it is entirely up to the
 * continuation code to figure out what to do.
 *
 * All the routines below use bits of fixup code that are out of line
 * with the main instruction path.  This means when everything is well,
 * we don't even have to jump over them.  Further, they do not intrude
 * on our cache or tlb entries.
 */

struct exception_table_entry
{
 int insn, fixup;
 short type, data;
};
# 37 "./arch/arm64/include/asm/extable.h"
bool ex_handler_bpf(const struct exception_table_entry *ex,
      struct pt_regs *regs);
# 48 "./arch/arm64/include/asm/extable.h"
bool fixup_exception(struct pt_regs *regs);
# 28 "./arch/arm64/include/asm/uaccess.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __access_ok(const void /* nothing */ *ptr, unsigned long size);

/*
 * Test whether a block of memory is a valid user space address.
 * Returns 1 if the range is valid, 0 otherwise.
 *
 * This is equivalent to the following test:
 * (u65)addr + (u65)size <= (u65)TASK_SIZE_MAX
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int access_ok(const void /* nothing */ *addr, unsigned long size)
{
 /*
	 * Asynchronous I/O running in a kernel thread does not have the
	 * TIF_TAGGED_ADDR flag of the process owning the mm, so always untag
	 * the user address before checking.
	 */
 if (1 &&
     (get_current()->flags & 0x00200000 /* I am a kernel thread */ || test_ti_thread_flag(((struct thread_info *)get_current()), 26 /* Allow tagged user addresses */)))
  addr = ({ u64 __addr = ( u64)(addr); __addr &= (( __typeof__(__addr))sign_extend64(( u64)(__addr), 55)); ( __typeof__(addr))__addr; });

 return __builtin_expect(!!(__access_ok(addr, size)), 1);
}


# 1 "./include/asm-generic/access_ok.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Checking whether a pointer is valid for user space access.
 * These definitions work on most architectures, but overrides can
 * be used where necessary.
 */

/*
 * architectures with compat tasks have a variable TASK_SIZE and should
 * override this to a constant.
 */





/*
 * 'size' is a compile-time constant for most callers, so optimize for
 * this case to turn the check into a single comparison against a constant
 * limit and catch all possible overflows.
 * On architectures with separate user address space (m68k, s390, parisc,
 * sparc64) or those without an MMU, this should always return true.
 *
 * This version was originally contributed by Jonas Bonn for the
 * OpenRISC architecture, and was found to be the most efficient
 * for constant 'size' and 'limit' values.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __access_ok(const void /* nothing */ *ptr, unsigned long size)
{
 unsigned long limit = ((((1UL))) << (48));
 unsigned long addr = (unsigned long)ptr;

 if (0 ||
     !1)
  return true;

 return (size <= limit) && (addr <= (limit - size));
}
# 54 "./arch/arm64/include/asm/uaccess.h" 2

/*
 * User access enabling/disabling.
 */
# 116 "./arch/arm64/include/asm/uaccess.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uaccess_ttbr0_disable(void)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uaccess_ttbr0_enable(void)
{
 return false;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __uaccess_disable_hw_pan(void)
{
 asm(".if ""1"" == 1\n" "661:\n\t" "nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "30" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".inst " "(0xd500401f | ((0) << 16 | (4) << 5) | ((!!(0)) << 8))" "\n\t" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n");

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __uaccess_enable_hw_pan(void)
{
 asm(".if ""1"" == 1\n" "661:\n\t" "nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "30" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".inst " "(0xd500401f | ((0) << 16 | (4) << 5) | ((!!(1)) << 8))" "\n\t" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n");

}

/*
 * The Tag Check Flag (TCF) mode for MTE is per EL, hence TCF0
 * affects EL0 and TCF affects EL1 irrespective of which TTBR is
 * used.
 * The kernel accesses TTBR0 usually with LDTR/STTR instructions
 * when UAO is available, so these would act as EL0 accesses using
 * TCF0.
 * However futex.h code uses exclusives which would be executed as
 * EL1, this can potentially cause a tag check fault even if the
 * user disables TCF0.
 *
 * To address the problem we set the PSTATE.TCO bit in uaccess_enable()
 * and reset it in uaccess_disable().
 *
 * The Tag check override (TCO) bit disables temporarily the tag checking
 * preventing the issue.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __uaccess_disable_tco(void)
{
 asm volatile(".if ""0"" == 1\n" "661:\n\t" "nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "43" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".inst " "(0xd500401f | ((3) << 16 | (4) << 5) | ((!!(0)) << 8))" "\n\t" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n");

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __uaccess_enable_tco(void)
{
 asm volatile(".if ""0"" == 1\n" "661:\n\t" "nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "43" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" ".inst " "(0xd500401f | ((3) << 16 | (4) << 5) | ((!!(1)) << 8))" "\n\t" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n");

}

/*
 * These functions disable tag checking only if in MTE async mode
 * since the sync mode generates exceptions synchronously and the
 * nofault or load_unaligned_zeropad can handle them.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __uaccess_disable_tco_async(void)
{
 if (system_uses_mte_async_or_asymm_mode())
   __uaccess_disable_tco();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __uaccess_enable_tco_async(void)
{
 if (system_uses_mte_async_or_asymm_mode())
  __uaccess_enable_tco();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uaccess_disable_privileged(void)
{
 __uaccess_disable_tco();

 if (uaccess_ttbr0_disable())
  return;

 __uaccess_enable_hw_pan();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uaccess_enable_privileged(void)
{
 __uaccess_enable_tco();

 if (uaccess_ttbr0_enable())
  return;

 __uaccess_disable_hw_pan();
}

/*
 * Sanitize a uaccess pointer such that it cannot reach any kernel address.
 *
 * Clearing bit 55 ensures the pointer cannot address any portion of the TTBR1
 * address range (i.e. any kernel address), and either the pointer falls within
 * the TTBR0 address range or must cause a fault.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void /* nothing */ *__uaccess_mask_ptr(const void /* nothing */ *ptr)
{
 void /* nothing */ *safe_ptr;

 asm volatile(
 "	bic	%0, %1, %2\n"
 : "=r" (safe_ptr)
 : "r" (ptr),
   "i" (((((1UL))) << (55)))
 );

 return safe_ptr;
}

/*
 * The "__xxx" versions of the user access functions do not verify the address
 * space - it must have been done previously with a separate "access_ok()"
 * call.
 *
 * The "__xxx_error" versions set the third argument to -EFAULT if an error
 * occurs, and leave it unchanged on success.
 */
# 265 "./arch/arm64/include/asm/uaccess.h"
/*
 * We must not call into the scheduler between uaccess_ttbr0_enable() and
 * uaccess_ttbr0_disable(). As `x` and `ptr` could contain blocking functions,
 * we must evaluate these outside of the critical section.
 */
# 304 "./arch/arm64/include/asm/uaccess.h"
/*
 * We must not call into the scheduler between __uaccess_enable_tco_async() and
 * __uaccess_disable_tco_async(). As `dst` and `src` may contain blocking
 * functions, we must evaluate these outside of the critical section.
 */
# 353 "./arch/arm64/include/asm/uaccess.h"
/*
 * We must not call into the scheduler between uaccess_ttbr0_enable() and
 * uaccess_ttbr0_disable(). As `x` and `ptr` could contain blocking functions,
 * we must evaluate these outside of the critical section.
 */
# 390 "./arch/arm64/include/asm/uaccess.h"
/*
 * We must not call into the scheduler between __uaccess_enable_tco_async() and
 * __uaccess_disable_tco_async(). As `dst` and `src` may contain blocking
 * functions, we must evaluate these outside of the critical section.
 */
# 410 "./arch/arm64/include/asm/uaccess.h"
extern unsigned long __attribute__((__warn_unused_result__)) __arch_copy_from_user(void *to, const void /* nothing */ *from, unsigned long n);
# 421 "./arch/arm64/include/asm/uaccess.h"
extern unsigned long __attribute__((__warn_unused_result__)) __arch_copy_to_user(void /* nothing */ *to, const void *from, unsigned long n);
# 435 "./arch/arm64/include/asm/uaccess.h"
extern unsigned long __attribute__((__warn_unused_result__)) __arch_clear_user(void /* nothing */ *to, unsigned long n);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __attribute__((__warn_unused_result__)) __clear_user(void /* nothing */ *to, unsigned long n)
{
 if (access_ok(to, n)) {
  uaccess_ttbr0_enable();
  n = __arch_clear_user(__uaccess_mask_ptr(to), n);
  uaccess_ttbr0_disable();
 }
 return n;
}


extern long strncpy_from_user(char *dest, const char /* nothing */ *src, long count);

extern __attribute__((__warn_unused_result__)) long strnlen_user(const char /* nothing */ *str, long n);
# 465 "./arch/arm64/include/asm/uaccess.h"
/*
 * Return 0 on success, the number of bytes not probed otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t probe_subpage_writeable(const char /* nothing */ *uaddr,
          size_t size)
{
 if (!system_supports_mte())
  return 0;
 return mte_probe_user_range(uaddr, size);
}
# 12 "./include/linux/uaccess.h" 2

/*
 * Architectures should provide two primitives (raw_copy_{to,from}_user())
 * and get rid of their private instances of copy_{to,from}_user() and
 * __copy_{to,from}_user{,_inatomic}().
 *
 * raw_copy_{to,from}_user(to, from, size) should copy up to size bytes and
 * return the amount left to copy.  They should assume that access_ok() has
 * already been checked (and succeeded); they should *not* zero-pad anything.
 * No KASAN or object size checks either - those belong here.
 *
 * Both of these functions should attempt to copy size bytes starting at from
 * into the area starting at to.  They must not fetch or store anything
 * outside of those areas.  Return value must be between 0 (everything
 * copied successfully) and size (nothing copied).
 *
 * If raw_copy_{to,from}_user(to, from, size) returns N, size - N bytes starting
 * at to must become equal to the bytes fetched from the corresponding area
 * starting at from.  All data past to + size - N must be left unmodified.
 *
 * If copying succeeds, the return value must be 0.  If some data cannot be
 * fetched, it is permitted to copy less than had been fetched; the only
 * hard requirement is that not storing anything at all (i.e. returning size)
 * should happen only when nothing could be copied.  In other words, you don't
 * have to squeeze as much as possible - it is allowed, but not necessary.
 *
 * For raw_copy_from_user() to always points to kernel memory and no faults
 * on store should happen.  Interpretation of from is affected by set_fs().
 * For raw_copy_to_user() it's the other way round.
 *
 * Both can be inlined - it's up to architectures whether it wants to bother
 * with that.  They should not be used directly; they are used to implement
 * the 6 functions (copy_{to,from}_user(), __copy_{to,from}_user_inatomic())
 * that are used instead.  Out of those, __... ones are inlined.  Plain
 * copy_{to,from}_user() might or might not be inlined.  If you want them
 * inlined, have asm/uaccess.h define INLINE_COPY_{TO,FROM}_USER.
 *
 * NOTE: only copy_from_user() zero-pads the destination in case of short copy.
 * Neither __copy_from_user() nor __copy_from_user_inatomic() zero anything
 * at all; their callers absolutely must check the return value.
 *
 * Biarch ones should also provide raw_copy_in_user() - similar to the above,
 * but both source and destination are __user pointers (affected by set_fs()
 * as usual) and both source and destination can trigger faults.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__)) unsigned long
__copy_from_user_inatomic(void *to, const void /* nothing */ *from, unsigned long n)
{
 unsigned long res;

 instrument_copy_from_user_before(to, from, n);
 check_object_size(to, n, false);
 res = ({ unsigned long __acfu_ret; uaccess_ttbr0_enable(); __acfu_ret = __arch_copy_from_user((to), __uaccess_mask_ptr(from), (n)); uaccess_ttbr0_disable(); __acfu_ret; });
 instrument_copy_from_user_after(to, from, n, res);
 return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__)) unsigned long
__copy_from_user(void *to, const void /* nothing */ *from, unsigned long n)
{
 unsigned long res;

 might_fault();
 instrument_copy_from_user_before(to, from, n);
 if (should_fail_usercopy())
  return n;
 check_object_size(to, n, false);
 res = ({ unsigned long __acfu_ret; uaccess_ttbr0_enable(); __acfu_ret = __arch_copy_from_user((to), __uaccess_mask_ptr(from), (n)); uaccess_ttbr0_disable(); __acfu_ret; });
 instrument_copy_from_user_after(to, from, n, res);
 return res;
}

/**
 * __copy_to_user_inatomic: - Copy a block of data into user space, with less checking.
 * @to:   Destination address, in user space.
 * @from: Source address, in kernel space.
 * @n:    Number of bytes to copy.
 *
 * Context: User context only.
 *
 * Copy data from kernel space to user space.  Caller must check
 * the specified block with access_ok() before calling this function.
 * The caller should also make sure he pins the user space address
 * so that we don't result in page fault and sleep.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__)) unsigned long
__copy_to_user_inatomic(void /* nothing */ *to, const void *from, unsigned long n)
{
 if (should_fail_usercopy())
  return n;
 instrument_copy_to_user(to, from, n);
 check_object_size(from, n, true);
 return ({ unsigned long __actu_ret; uaccess_ttbr0_enable(); __actu_ret = __arch_copy_to_user(__uaccess_mask_ptr(to), (from), (n)); uaccess_ttbr0_disable(); __actu_ret; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__)) unsigned long
__copy_to_user(void /* nothing */ *to, const void *from, unsigned long n)
{
 might_fault();
 if (should_fail_usercopy())
  return n;
 instrument_copy_to_user(to, from, n);
 check_object_size(from, n, true);
 return ({ unsigned long __actu_ret; uaccess_ttbr0_enable(); __actu_ret = __arch_copy_to_user(__uaccess_mask_ptr(to), (from), (n)); uaccess_ttbr0_disable(); __actu_ret; });
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) unsigned long
_copy_from_user(void *to, const void /* nothing */ *from, unsigned long n)
{
 unsigned long res = n;
 might_fault();
 if (!should_fail_usercopy() && __builtin_expect(!!(access_ok(from, n)), 1)) {
  instrument_copy_from_user_before(to, from, n);
  res = ({ unsigned long __acfu_ret; uaccess_ttbr0_enable(); __acfu_ret = __arch_copy_from_user((to), __uaccess_mask_ptr(from), (n)); uaccess_ttbr0_disable(); __acfu_ret; });
  instrument_copy_from_user_after(to, from, n, res);
 }
 if (__builtin_expect(!!(res), 0))
  memset(to + (n - res), 0, res);
 return res;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) unsigned long
_copy_to_user(void /* nothing */ *to, const void *from, unsigned long n)
{
 might_fault();
 if (should_fail_usercopy())
  return n;
 if (access_ok(to, n)) {
  instrument_copy_to_user(to, from, n);
  n = ({ unsigned long __actu_ret; uaccess_ttbr0_enable(); __actu_ret = __arch_copy_to_user(__uaccess_mask_ptr(to), (from), (n)); uaccess_ttbr0_disable(); __actu_ret; });
 }
 return n;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __attribute__((__warn_unused_result__))
copy_from_user(void *to, const void /* nothing */ *from, unsigned long n)
{
 if (check_copy_size(to, n, false))
  n = _copy_from_user(to, from, n);
 return n;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long __attribute__((__warn_unused_result__))
copy_to_user(void /* nothing */ *to, const void *from, unsigned long n)
{
 if (check_copy_size(from, n, true))
  n = _copy_to_user(to, from, n);
 return n;
}


/*
 * Without arch opt-in this generic copy_mc_to_kernel() will not handle
 * #MC (or arch equivalent) during source read.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long __attribute__((__warn_unused_result__))
copy_mc_to_kernel(void *dst, const void *src, size_t cnt)
{
 memcpy(dst, src, cnt);
 return 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void pagefault_disabled_inc(void)
{
 get_current()->pagefault_disabled++;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void pagefault_disabled_dec(void)
{
 get_current()->pagefault_disabled--;
}

/*
 * These routines enable/disable the pagefault handler. If disabled, it will
 * not take any locks and go straight to the fixup table.
 *
 * User access methods will not sleep when called from a pagefault_disabled()
 * environment.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pagefault_disable(void)
{
 pagefault_disabled_inc();
 /*
	 * make sure to have issued the store before a pagefault
	 * can hit.
	 */
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pagefault_enable(void)
{
 /*
	 * make sure to issue those last loads/stores before enabling
	 * the pagefault handler again.
	 */
 __asm__ __volatile__("": : :"memory");
 pagefault_disabled_dec();
}

/*
 * Is the pagefault handler disabled? If so, user access methods will not sleep.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pagefault_disabled(void)
{
 return get_current()->pagefault_disabled != 0;
}

/*
 * The pagefault handler is in general disabled by pagefault_disable() or
 * when in irq context (via in_atomic()).
 *
 * This function should only be used by the fault handlers. Other users should
 * stick to pagefault_disabled().
 * Please NEVER use preempt_disable() to disable the fault handler. With
 * !CONFIG_PREEMPT_COUNT, this is like a NOP. So the handler won't be disabled.
 * in_atomic() will report different values based on !CONFIG_PREEMPT_COUNT.
 */
# 267 "./include/linux/uaccess.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) unsigned long
__copy_from_user_inatomic_nocache(void *to, const void /* nothing */ *from,
      unsigned long n)
{
 return __copy_from_user_inatomic(to, from, n);
}



extern __attribute__((__warn_unused_result__)) int check_zeroed_user(const void /* nothing */ *from, size_t size);

/**
 * copy_struct_from_user: copy a struct from userspace
 * @dst:   Destination address, in kernel space. This buffer must be @ksize
 *         bytes long.
 * @ksize: Size of @dst struct.
 * @src:   Source address, in userspace.
 * @usize: (Alleged) size of @src struct.
 *
 * Copies a struct from userspace to kernel space, in a way that guarantees
 * backwards-compatibility for struct syscall arguments (as long as future
 * struct extensions are made such that all new fields are *appended* to the
 * old struct, and zeroed-out new fields have the same meaning as the old
 * struct).
 *
 * @ksize is just sizeof(*dst), and @usize should've been passed by userspace.
 * The recommended usage is something like the following:
 *
 *   SYSCALL_DEFINE2(foobar, const struct foo __user *, uarg, size_t, usize)
 *   {
 *      int err;
 *      struct foo karg = {};
 *
 *      if (usize > PAGE_SIZE)
 *        return -E2BIG;
 *      if (usize < FOO_SIZE_VER0)
 *        return -EINVAL;
 *
 *      err = copy_struct_from_user(&karg, sizeof(karg), uarg, usize);
 *      if (err)
 *        return err;
 *
 *      // ...
 *   }
 *
 * There are three cases to consider:
 *  * If @usize == @ksize, then it's copied verbatim.
 *  * If @usize < @ksize, then the userspace has passed an old struct to a
 *    newer kernel. The rest of the trailing bytes in @dst (@ksize - @usize)
 *    are to be zero-filled.
 *  * If @usize > @ksize, then the userspace has passed a new struct to an
 *    older kernel. The trailing bytes unknown to the kernel (@usize - @ksize)
 *    are checked to ensure they are zeroed, otherwise -E2BIG is returned.
 *
 * Returns (in all cases, some data may have been copied):
 *  * -E2BIG:  (@usize > @ksize) and there are non-zero trailing bytes in @src.
 *  * -EFAULT: access to userspace failed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__)) int
copy_struct_from_user(void *dst, size_t ksize, const void /* nothing */ *src,
        size_t usize)
{
 size_t size = __builtin_choose_expr(((!!(sizeof((typeof(ksize) *)1 == (typeof(usize) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(ksize) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(usize) * 0l)) : (int *)8))))), ((ksize) < (usize) ? (ksize) : (usize)), ({ typeof(ksize) __UNIQUE_ID___x296 = (ksize); typeof(usize) __UNIQUE_ID___y297 = (usize); ((__UNIQUE_ID___x296) < (__UNIQUE_ID___y297) ? (__UNIQUE_ID___x296) : (__UNIQUE_ID___y297)); }));
 size_t rest = __builtin_choose_expr(((!!(sizeof((typeof(ksize) *)1 == (typeof(usize) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(ksize) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(usize) * 0l)) : (int *)8))))), ((ksize) > (usize) ? (ksize) : (usize)), ({ typeof(ksize) __UNIQUE_ID___x298 = (ksize); typeof(usize) __UNIQUE_ID___y299 = (usize); ((__UNIQUE_ID___x298) > (__UNIQUE_ID___y299) ? (__UNIQUE_ID___x298) : (__UNIQUE_ID___y299)); })) - size;

 /* Deal with trailing bytes. */
 if (usize < ksize) {
  memset(dst + size, 0, rest);
 } else if (usize > ksize) {
  int ret = check_zeroed_user(src + size, rest);
  if (ret <= 0)
   return ret ?: -7 /* Argument list too long */;
 }
 /* Copy the interoperable parts of the struct. */
 if (copy_from_user(dst, src, size))
  return -14 /* Bad address */;
 return 0;
}

bool copy_from_kernel_nofault_allowed(const void *unsafe_src, size_t size);

long copy_from_kernel_nofault(void *dst, const void *src, size_t size);
long __attribute__((__no_instrument_function__)) copy_to_kernel_nofault(void *dst, const void *src, size_t size);

long copy_from_user_nofault(void *dst, const void /* nothing */ *src, size_t size);
long __attribute__((__no_instrument_function__)) copy_to_user_nofault(void /* nothing */ *dst, const void *src,
  size_t size);

long strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr,
  long count);

long strncpy_from_user_nofault(char *dst, const void /* nothing */ *unsafe_addr,
  long count);
long strnlen_user_nofault(const void /* nothing */ *unsafe_addr, long count);
# 381 "./include/linux/uaccess.h"
/**
 * get_kernel_nofault(): safely attempt to read from a location
 * @val: read into this variable
 * @ptr: address to read from
 *
 * Returns 0 on success, or -EFAULT.
 */
# 401 "./include/linux/uaccess.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long user_access_save(void) { return 0UL; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void user_access_restore(unsigned long flags) { }
# 12 "./include/linux/sched/task.h" 2

struct task_struct;
struct rusage;
union thread_union;
struct css_set;

/* All the bits taken by the old clone syscall. */


struct kernel_clone_args {
 u64 flags;
 int /* nothing */ *pidfd;
 int /* nothing */ *child_tid;
 int /* nothing */ *parent_tid;
 int exit_signal;
 unsigned long stack;
 unsigned long stack_size;
 unsigned long tls;
 pid_t *set_tid;
 /* Number of elements in *set_tid */
 size_t set_tid_size;
 int cgroup;
 int io_thread;
 int kthread;
 int idle;
 int (*fn)(void *);
 void *fn_arg;
 struct cgroup *cgrp;
 struct css_set *cset;
};

/*
 * This serializes "schedule()" and also protects
 * the run-queue from deletions/modifications (but
 * _adding_ to the beginning of the run-queue has
 * a separate lock).
 */
extern rwlock_t tasklist_lock;
extern spinlock_t mmlist_lock;

extern union thread_union init_thread_union;
extern struct task_struct init_task;

extern int lockdep_tasklist_lock_is_held(void);

extern void schedule_tail(struct task_struct *prev);
extern void init_idle(struct task_struct *idle, int cpu);

extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
extern void sched_cgroup_fork(struct task_struct *p, struct kernel_clone_args *kargs);
extern void sched_post_fork(struct task_struct *p);
extern void sched_dead(struct task_struct *p);

void __attribute__((__noreturn__)) do_task_dead(void);
void __attribute__((__noreturn__)) make_task_dead(int signr);

extern void proc_caches_init(void);

extern void fork_init(void);

extern void release_task(struct task_struct * p);

extern int copy_thread(struct task_struct *, const struct kernel_clone_args *);

extern void flush_thread(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void exit_thread(struct task_struct *tsk)
{
}

extern __attribute__((__noreturn__)) void do_group_exit(int);

extern void exit_files(struct task_struct *);
extern void exit_itimers(struct task_struct *);

extern pid_t kernel_clone(struct kernel_clone_args *kargs);
struct task_struct *create_io_thread(int (*fn)(void *), void *arg, int node);
struct task_struct *fork_idle(int);
struct mm_struct *copy_init_mm(void);
extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
extern pid_t user_mode_thread(int (*fn)(void *), void *arg, unsigned long flags);
extern long kernel_wait4(pid_t, int /* nothing */ *, int, struct rusage *);
int kernel_wait(pid_t pid, int *stat);

extern void free_task(struct task_struct *tsk);

/* sched_exec is called by processes performing an exec */

extern void sched_exec(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct task_struct *get_task_struct(struct task_struct *t)
{
 refcount_inc(&t->usage);
 return t;
}

extern void __put_task_struct(struct task_struct *t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void put_task_struct(struct task_struct *t)
{
 if (refcount_dec_and_test(&t->usage))
  __put_task_struct(t);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void put_task_struct_many(struct task_struct *t, int nr)
{
 if (refcount_sub_and_test(nr, &t->usage))
  __put_task_struct(t);
}

void put_task_struct_rcu_user(struct task_struct *task);

/* Free all architecture-specific resources held by a thread. */
void release_thread(struct task_struct *dead_task);
# 154 "./include/linux/sched/task.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct vm_struct *task_stack_vm_area(const struct task_struct *t)
{
 return t->stack_vm_area;
}







/*
 * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
 * subscriptions and synchronises with wait4().  Also used in procfs.  Also
 * pins the final release of task.io_context.  Also protects ->cpuset and
 * ->cgroup.subsys[]. And ->vfork_done. And ->sysvshm.shm_clist.
 *
 * Nests both inside and outside of read_lock(&tasklist_lock).
 * It must not be nested with write_lock_irq(&tasklist_lock),
 * neither inside nor outside.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_lock(struct task_struct *p)
{
 spin_lock(&p->alloc_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void task_unlock(struct task_struct *p)
{
 spin_unlock(&p->alloc_lock);
}
# 10 "./include/linux/sched/signal.h" 2
# 1 "./include/linux/cred.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/* Credentials management - see Documentation/security/credentials.rst
 *
 * Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 */






# 1 "./include/linux/key.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/* Authentication token and access key management
 *
 * Copyright (C) 2004, 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * See Documentation/security/keys/core.rst for information on keys/keyrings.
 */
# 17 "./include/linux/key.h"
# 1 "./include/linux/sysctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * sysctl.h: General linux system control interface
 *
 * Begun 24 March 1995, Stephen Tweedie
 *
 ****************************************************************
 ****************************************************************
 **
 **  WARNING:
 **  The values in this file are exported to user space via
 **  the sysctl() binary interface.  Do *NOT* change the
 **  numbering of any existing values here, and do not change
 **  any numbers within any one set of values.  If you have to
 **  redefine an existing interface, use a new number for it.
 **  The kernel will then return -ENOTDIR to any application using
 **  the old binary interface.
 **
 ****************************************************************
 ****************************************************************
 */
# 30 "./include/linux/sysctl.h"
# 1 "./include/uapi/linux/sysctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * sysctl.h: General linux system control interface
 *
 * Begun 24 March 1995, Stephen Tweedie
 *
 ****************************************************************
 ****************************************************************
 **
 **  WARNING:
 **  The values in this file are exported to user space via
 **  the sysctl() binary interface.  Do *NOT* change the
 **  numbering of any existing values here, and do not change
 **  any numbers within any one set of values.  If you have to
 **  redefine an existing interface, use a new number for it.
 **  The kernel will then return -ENOTDIR to any application using
 **  the old binary interface.
 **
 ****************************************************************
 ****************************************************************
 */
# 35 "./include/uapi/linux/sysctl.h"
struct __sysctl_args {
 int /* nothing */ *name;
 int nlen;
 void /* nothing */ *oldval;
 size_t /* nothing */ *oldlenp;
 void /* nothing */ *newval;
 size_t newlen;
 unsigned long __unused[4];
};

/* Define sysctl names first */

/* Top-level names: */

enum
{
 CTL_KERN=1, /* General kernel info and control */
 CTL_VM=2, /* VM management */
 CTL_NET=3, /* Networking */
 CTL_PROC=4, /* removal breaks strace(1) compilation */
 CTL_FS=5, /* Filesystems */
 CTL_DEBUG=6, /* Debugging */
 CTL_DEV=7, /* Devices */
 CTL_BUS=8, /* Busses */
 CTL_ABI=9, /* Binary emulation */
 CTL_CPU=10, /* CPU stuff (speed scaling, etc) */
 CTL_ARLAN=254, /* arlan wireless driver */
 CTL_S390DBF=5677, /* s390 debug */
 CTL_SUNRPC=7249, /* sunrpc debug */
 CTL_PM=9899, /* frv power management */
 CTL_FRV=9898, /* frv specific sysctls */
};

/* CTL_BUS names: */
enum
{
 CTL_BUS_ISA=1 /* ISA */
};

/* /proc/sys/fs/inotify/ */
enum
{
 INOTIFY_MAX_USER_INSTANCES=1, /* max instances per user */
 INOTIFY_MAX_USER_WATCHES=2, /* max watches per user */
 INOTIFY_MAX_QUEUED_EVENTS=3 /* max queued events per instance */
};

/* CTL_KERN names: */
enum
{
 KERN_OSTYPE=1, /* string: system version */
 KERN_OSRELEASE=2, /* string: system release */
 KERN_OSREV=3, /* int: system revision */
 KERN_VERSION=4, /* string: compile time info */
 KERN_SECUREMASK=5, /* struct: maximum rights mask */
 KERN_PROF=6, /* table: profiling information */
 KERN_NODENAME=7, /* string: hostname */
 KERN_DOMAINNAME=8, /* string: domainname */

 KERN_PANIC=15, /* int: panic timeout */
 KERN_REALROOTDEV=16, /* real root device to mount after initrd */

 KERN_SPARC_REBOOT=21, /* reboot command on Sparc */
 KERN_CTLALTDEL=22, /* int: allow ctl-alt-del to reboot */
 KERN_PRINTK=23, /* struct: control printk logging parameters */
 KERN_NAMETRANS=24, /* Name translation */
 KERN_PPC_HTABRECLAIM=25, /* turn htab reclaimation on/off on PPC */
 KERN_PPC_ZEROPAGED=26, /* turn idle page zeroing on/off on PPC */
 KERN_PPC_POWERSAVE_NAP=27, /* use nap mode for power saving */
 KERN_MODPROBE=28, /* string: modprobe path */
 KERN_SG_BIG_BUFF=29, /* int: sg driver reserved buffer size */
 KERN_ACCT=30, /* BSD process accounting parameters */
 KERN_PPC_L2CR=31, /* l2cr register on PPC */

 KERN_RTSIGNR=32, /* Number of rt sigs queued */
 KERN_RTSIGMAX=33, /* Max queuable */

 KERN_SHMMAX=34, /* long: Maximum shared memory segment */
 KERN_MSGMAX=35, /* int: Maximum size of a messege */
 KERN_MSGMNB=36, /* int: Maximum message queue size */
 KERN_MSGPOOL=37, /* int: Maximum system message pool size */
 KERN_SYSRQ=38, /* int: Sysreq enable */
 KERN_MAX_THREADS=39, /* int: Maximum nr of threads in the system */
  KERN_RANDOM=40, /* Random driver */
  KERN_SHMALL=41, /* int: Maximum size of shared memory */
  KERN_MSGMNI=42, /* int: msg queue identifiers */
  KERN_SEM=43, /* struct: sysv semaphore limits */
  KERN_SPARC_STOP_A=44, /* int: Sparc Stop-A enable */
  KERN_SHMMNI=45, /* int: shm array identifiers */
 KERN_OVERFLOWUID=46, /* int: overflow UID */
 KERN_OVERFLOWGID=47, /* int: overflow GID */
 KERN_SHMPATH=48, /* string: path to shm fs */
 KERN_HOTPLUG=49, /* string: path to uevent helper (deprecated) */
 KERN_IEEE_EMULATION_WARNINGS=50, /* int: unimplemented ieee instructions */
 KERN_S390_USER_DEBUG_LOGGING=51, /* int: dumps of user faults */
 KERN_CORE_USES_PID=52, /* int: use core or core.%pid */
 KERN_TAINTED=53, /* int: various kernel tainted flags */
 KERN_CADPID=54, /* int: PID of the process to notify on CAD */
 KERN_PIDMAX=55, /* int: PID # limit */
   KERN_CORE_PATTERN=56, /* string: pattern for core-file names */
 KERN_PANIC_ON_OOPS=57, /* int: whether we will panic on an oops */
 KERN_HPPA_PWRSW=58, /* int: hppa soft-power enable */
 KERN_HPPA_UNALIGNED=59, /* int: hppa unaligned-trap enable */
 KERN_PRINTK_RATELIMIT=60, /* int: tune printk ratelimiting */
 KERN_PRINTK_RATELIMIT_BURST=61, /* int: tune printk ratelimiting */
 KERN_PTY=62, /* dir: pty driver */
 KERN_NGROUPS_MAX=63, /* int: NGROUPS_MAX */
 KERN_SPARC_SCONS_PWROFF=64, /* int: serial console power-off halt */
 KERN_HZ_TIMER=65, /* int: hz timer on or off */
 KERN_UNKNOWN_NMI_PANIC=66, /* int: unknown nmi panic flag */
 KERN_BOOTLOADER_TYPE=67, /* int: boot loader type */
 KERN_RANDOMIZE=68, /* int: randomize virtual address space */
 KERN_SETUID_DUMPABLE=69, /* int: behaviour of dumps for setuid core */
 KERN_SPIN_RETRY=70, /* int: number of spinlock retries */
 KERN_ACPI_VIDEO_FLAGS=71, /* int: flags for setting up video after ACPI sleep */
 KERN_IA64_UNALIGNED=72, /* int: ia64 unaligned userland trap enable */
 KERN_COMPAT_LOG=73, /* int: print compat layer  messages */
 KERN_MAX_LOCK_DEPTH=74, /* int: rtmutex's maximum lock depth */
 KERN_NMI_WATCHDOG=75, /* int: enable/disable nmi watchdog */
 KERN_PANIC_ON_NMI=76, /* int: whether we will panic on an unrecovered */
 KERN_PANIC_ON_WARN=77, /* int: call panic() in WARN() functions */
 KERN_PANIC_PRINT=78, /* ulong: bitmask to print system info on panic */
};



/* CTL_VM names: */
enum
{
 VM_UNUSED1=1, /* was: struct: Set vm swapping control */
 VM_UNUSED2=2, /* was; int: Linear or sqrt() swapout for hogs */
 VM_UNUSED3=3, /* was: struct: Set free page thresholds */
 VM_UNUSED4=4, /* Spare */
 VM_OVERCOMMIT_MEMORY=5, /* Turn off the virtual memory safety limit */
 VM_UNUSED5=6, /* was: struct: Set buffer memory thresholds */
 VM_UNUSED7=7, /* was: struct: Set cache memory thresholds */
 VM_UNUSED8=8, /* was: struct: Control kswapd behaviour */
 VM_UNUSED9=9, /* was: struct: Set page table cache parameters */
 VM_PAGE_CLUSTER=10, /* int: set number of pages to swap together */
 VM_DIRTY_BACKGROUND=11, /* dirty_background_ratio */
 VM_DIRTY_RATIO=12, /* dirty_ratio */
 VM_DIRTY_WB_CS=13, /* dirty_writeback_centisecs */
 VM_DIRTY_EXPIRE_CS=14, /* dirty_expire_centisecs */
 VM_NR_PDFLUSH_THREADS=15, /* nr_pdflush_threads */
 VM_OVERCOMMIT_RATIO=16, /* percent of RAM to allow overcommit in */
 VM_PAGEBUF=17, /* struct: Control pagebuf parameters */
 VM_HUGETLB_PAGES=18, /* int: Number of available Huge Pages */
 VM_SWAPPINESS=19, /* Tendency to steal mapped memory */
 VM_LOWMEM_RESERVE_RATIO=20,/* reservation ratio for lower memory zones */
 VM_MIN_FREE_KBYTES=21, /* Minimum free kilobytes to maintain */
 VM_MAX_MAP_COUNT=22, /* int: Maximum number of mmaps/address-space */
 VM_LAPTOP_MODE=23, /* vm laptop mode */
 VM_BLOCK_DUMP=24, /* block dump mode */
 VM_HUGETLB_GROUP=25, /* permitted hugetlb group */
 VM_VFS_CACHE_PRESSURE=26, /* dcache/icache reclaim pressure */
 VM_LEGACY_VA_LAYOUT=27, /* legacy/compatibility virtual address space layout */
 VM_SWAP_TOKEN_TIMEOUT=28, /* default time for token time out */
 VM_DROP_PAGECACHE=29, /* int: nuke lots of pagecache */
 VM_PERCPU_PAGELIST_FRACTION=30,/* int: fraction of pages in each percpu_pagelist */
 VM_ZONE_RECLAIM_MODE=31, /* reclaim local zone memory before going off node */
 VM_MIN_UNMAPPED=32, /* Set min percent of unmapped pages */
 VM_PANIC_ON_OOM=33, /* panic at out-of-memory */
 VM_VDSO_ENABLED=34, /* map VDSO into new processes? */
 VM_MIN_SLAB=35, /* Percent pages ignored by node reclaim */
};


/* CTL_NET names: */
enum
{
 NET_CORE=1,
 NET_ETHER=2,
 NET_802=3,
 NET_UNIX=4,
 NET_IPV4=5,
 NET_IPX=6,
 NET_ATALK=7,
 NET_NETROM=8,
 NET_AX25=9,
 NET_BRIDGE=10,
 NET_ROSE=11,
 NET_IPV6=12,
 NET_X25=13,
 NET_TR=14,
 NET_DECNET=15,
 NET_ECONET=16,
 NET_SCTP=17,
 NET_LLC=18,
 NET_NETFILTER=19,
 NET_DCCP=20,
 NET_IRDA=412,
};

/* /proc/sys/kernel/random */
enum
{
 RANDOM_POOLSIZE=1,
 RANDOM_ENTROPY_COUNT=2,
 RANDOM_READ_THRESH=3,
 RANDOM_WRITE_THRESH=4,
 RANDOM_BOOT_ID=5,
 RANDOM_UUID=6
};

/* /proc/sys/kernel/pty */
enum
{
 PTY_MAX=1,
 PTY_NR=2
};

/* /proc/sys/bus/isa */
enum
{
 BUS_ISA_MEM_BASE=1,
 BUS_ISA_PORT_BASE=2,
 BUS_ISA_PORT_SHIFT=3
};

/* /proc/sys/net/core */
enum
{
 NET_CORE_WMEM_MAX=1,
 NET_CORE_RMEM_MAX=2,
 NET_CORE_WMEM_DEFAULT=3,
 NET_CORE_RMEM_DEFAULT=4,
/* was	NET_CORE_DESTROY_DELAY */
 NET_CORE_MAX_BACKLOG=6,
 NET_CORE_FASTROUTE=7,
 NET_CORE_MSG_COST=8,
 NET_CORE_MSG_BURST=9,
 NET_CORE_OPTMEM_MAX=10,
 NET_CORE_HOT_LIST_LENGTH=11,
 NET_CORE_DIVERT_VERSION=12,
 NET_CORE_NO_CONG_THRESH=13,
 NET_CORE_NO_CONG=14,
 NET_CORE_LO_CONG=15,
 NET_CORE_MOD_CONG=16,
 NET_CORE_DEV_WEIGHT=17,
 NET_CORE_SOMAXCONN=18,
 NET_CORE_BUDGET=19,
 NET_CORE_AEVENT_ETIME=20,
 NET_CORE_AEVENT_RSEQTH=21,
 NET_CORE_WARNINGS=22,
};

/* /proc/sys/net/ethernet */

/* /proc/sys/net/802 */

/* /proc/sys/net/unix */

enum
{
 NET_UNIX_DESTROY_DELAY=1,
 NET_UNIX_DELETE_DELAY=2,
 NET_UNIX_MAX_DGRAM_QLEN=3,
};

/* /proc/sys/net/netfilter */
enum
{
 NET_NF_CONNTRACK_MAX=1,
 NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT=2,
 NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV=3,
 NET_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED=4,
 NET_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT=5,
 NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT=6,
 NET_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK=7,
 NET_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT=8,
 NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE=9,
 NET_NF_CONNTRACK_UDP_TIMEOUT=10,
 NET_NF_CONNTRACK_UDP_TIMEOUT_STREAM=11,
 NET_NF_CONNTRACK_ICMP_TIMEOUT=12,
 NET_NF_CONNTRACK_GENERIC_TIMEOUT=13,
 NET_NF_CONNTRACK_BUCKETS=14,
 NET_NF_CONNTRACK_LOG_INVALID=15,
 NET_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS=16,
 NET_NF_CONNTRACK_TCP_LOOSE=17,
 NET_NF_CONNTRACK_TCP_BE_LIBERAL=18,
 NET_NF_CONNTRACK_TCP_MAX_RETRANS=19,
 NET_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED=20,
 NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT=21,
 NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED=22,
 NET_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED=23,
 NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT=24,
 NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD=25,
 NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT=26,
 NET_NF_CONNTRACK_COUNT=27,
 NET_NF_CONNTRACK_ICMPV6_TIMEOUT=28,
 NET_NF_CONNTRACK_FRAG6_TIMEOUT=29,
 NET_NF_CONNTRACK_FRAG6_LOW_THRESH=30,
 NET_NF_CONNTRACK_FRAG6_HIGH_THRESH=31,
 NET_NF_CONNTRACK_CHECKSUM=32,
};

/* /proc/sys/net/ipv4 */
enum
{
 /* v2.0 compatibile variables */
 NET_IPV4_FORWARD=8,
 NET_IPV4_DYNADDR=9,

 NET_IPV4_CONF=16,
 NET_IPV4_NEIGH=17,
 NET_IPV4_ROUTE=18,
 NET_IPV4_FIB_HASH=19,
 NET_IPV4_NETFILTER=20,

 NET_IPV4_TCP_TIMESTAMPS=33,
 NET_IPV4_TCP_WINDOW_SCALING=34,
 NET_IPV4_TCP_SACK=35,
 NET_IPV4_TCP_RETRANS_COLLAPSE=36,
 NET_IPV4_DEFAULT_TTL=37,
 NET_IPV4_AUTOCONFIG=38,
 NET_IPV4_NO_PMTU_DISC=39,
 NET_IPV4_TCP_SYN_RETRIES=40,
 NET_IPV4_IPFRAG_HIGH_THRESH=41,
 NET_IPV4_IPFRAG_LOW_THRESH=42,
 NET_IPV4_IPFRAG_TIME=43,
 NET_IPV4_TCP_MAX_KA_PROBES=44,
 NET_IPV4_TCP_KEEPALIVE_TIME=45,
 NET_IPV4_TCP_KEEPALIVE_PROBES=46,
 NET_IPV4_TCP_RETRIES1=47,
 NET_IPV4_TCP_RETRIES2=48,
 NET_IPV4_TCP_FIN_TIMEOUT=49,
 NET_IPV4_IP_MASQ_DEBUG=50,
 NET_TCP_SYNCOOKIES=51,
 NET_TCP_STDURG=52,
 NET_TCP_RFC1337=53,
 NET_TCP_SYN_TAILDROP=54,
 NET_TCP_MAX_SYN_BACKLOG=55,
 NET_IPV4_LOCAL_PORT_RANGE=56,
 NET_IPV4_ICMP_ECHO_IGNORE_ALL=57,
 NET_IPV4_ICMP_ECHO_IGNORE_BROADCASTS=58,
 NET_IPV4_ICMP_SOURCEQUENCH_RATE=59,
 NET_IPV4_ICMP_DESTUNREACH_RATE=60,
 NET_IPV4_ICMP_TIMEEXCEED_RATE=61,
 NET_IPV4_ICMP_PARAMPROB_RATE=62,
 NET_IPV4_ICMP_ECHOREPLY_RATE=63,
 NET_IPV4_ICMP_IGNORE_BOGUS_ERROR_RESPONSES=64,
 NET_IPV4_IGMP_MAX_MEMBERSHIPS=65,
 NET_TCP_TW_RECYCLE=66,
 NET_IPV4_ALWAYS_DEFRAG=67,
 NET_IPV4_TCP_KEEPALIVE_INTVL=68,
 NET_IPV4_INET_PEER_THRESHOLD=69,
 NET_IPV4_INET_PEER_MINTTL=70,
 NET_IPV4_INET_PEER_MAXTTL=71,
 NET_IPV4_INET_PEER_GC_MINTIME=72,
 NET_IPV4_INET_PEER_GC_MAXTIME=73,
 NET_TCP_ORPHAN_RETRIES=74,
 NET_TCP_ABORT_ON_OVERFLOW=75,
 NET_TCP_SYNACK_RETRIES=76,
 NET_TCP_MAX_ORPHANS=77,
 NET_TCP_MAX_TW_BUCKETS=78,
 NET_TCP_FACK=79,
 NET_TCP_REORDERING=80,
 NET_TCP_ECN=81,
 NET_TCP_DSACK=82,
 NET_TCP_MEM=83,
 NET_TCP_WMEM=84,
 NET_TCP_RMEM=85,
 NET_TCP_APP_WIN=86,
 NET_TCP_ADV_WIN_SCALE=87,
 NET_IPV4_NONLOCAL_BIND=88,
 NET_IPV4_ICMP_RATELIMIT=89,
 NET_IPV4_ICMP_RATEMASK=90,
 NET_TCP_TW_REUSE=91,
 NET_TCP_FRTO=92,
 NET_TCP_LOW_LATENCY=93,
 NET_IPV4_IPFRAG_SECRET_INTERVAL=94,
 NET_IPV4_IGMP_MAX_MSF=96,
 NET_TCP_NO_METRICS_SAVE=97,
 NET_TCP_DEFAULT_WIN_SCALE=105,
 NET_TCP_MODERATE_RCVBUF=106,
 NET_TCP_TSO_WIN_DIVISOR=107,
 NET_TCP_BIC_BETA=108,
 NET_IPV4_ICMP_ERRORS_USE_INBOUND_IFADDR=109,
 NET_TCP_CONG_CONTROL=110,
 NET_TCP_ABC=111,
 NET_IPV4_IPFRAG_MAX_DIST=112,
  NET_TCP_MTU_PROBING=113,
 NET_TCP_BASE_MSS=114,
 NET_IPV4_TCP_WORKAROUND_SIGNED_WINDOWS=115,
 NET_TCP_DMA_COPYBREAK=116,
 NET_TCP_SLOW_START_AFTER_IDLE=117,
 NET_CIPSOV4_CACHE_ENABLE=118,
 NET_CIPSOV4_CACHE_BUCKET_SIZE=119,
 NET_CIPSOV4_RBM_OPTFMT=120,
 NET_CIPSOV4_RBM_STRICTVALID=121,
 NET_TCP_AVAIL_CONG_CONTROL=122,
 NET_TCP_ALLOWED_CONG_CONTROL=123,
 NET_TCP_MAX_SSTHRESH=124,
 NET_TCP_FRTO_RESPONSE=125,
};

enum {
 NET_IPV4_ROUTE_FLUSH=1,
 NET_IPV4_ROUTE_MIN_DELAY=2, /* obsolete since 2.6.25 */
 NET_IPV4_ROUTE_MAX_DELAY=3, /* obsolete since 2.6.25 */
 NET_IPV4_ROUTE_GC_THRESH=4,
 NET_IPV4_ROUTE_MAX_SIZE=5,
 NET_IPV4_ROUTE_GC_MIN_INTERVAL=6,
 NET_IPV4_ROUTE_GC_TIMEOUT=7,
 NET_IPV4_ROUTE_GC_INTERVAL=8, /* obsolete since 2.6.38 */
 NET_IPV4_ROUTE_REDIRECT_LOAD=9,
 NET_IPV4_ROUTE_REDIRECT_NUMBER=10,
 NET_IPV4_ROUTE_REDIRECT_SILENCE=11,
 NET_IPV4_ROUTE_ERROR_COST=12,
 NET_IPV4_ROUTE_ERROR_BURST=13,
 NET_IPV4_ROUTE_GC_ELASTICITY=14,
 NET_IPV4_ROUTE_MTU_EXPIRES=15,
 NET_IPV4_ROUTE_MIN_PMTU=16,
 NET_IPV4_ROUTE_MIN_ADVMSS=17,
 NET_IPV4_ROUTE_SECRET_INTERVAL=18,
 NET_IPV4_ROUTE_GC_MIN_INTERVAL_MS=19,
};

enum
{
 NET_PROTO_CONF_ALL=-2,
 NET_PROTO_CONF_DEFAULT=-3

 /* And device ifindices ... */
};

enum
{
 NET_IPV4_CONF_FORWARDING=1,
 NET_IPV4_CONF_MC_FORWARDING=2,
 NET_IPV4_CONF_PROXY_ARP=3,
 NET_IPV4_CONF_ACCEPT_REDIRECTS=4,
 NET_IPV4_CONF_SECURE_REDIRECTS=5,
 NET_IPV4_CONF_SEND_REDIRECTS=6,
 NET_IPV4_CONF_SHARED_MEDIA=7,
 NET_IPV4_CONF_RP_FILTER=8,
 NET_IPV4_CONF_ACCEPT_SOURCE_ROUTE=9,
 NET_IPV4_CONF_BOOTP_RELAY=10,
 NET_IPV4_CONF_LOG_MARTIANS=11,
 NET_IPV4_CONF_TAG=12,
 NET_IPV4_CONF_ARPFILTER=13,
 NET_IPV4_CONF_MEDIUM_ID=14,
 NET_IPV4_CONF_NOXFRM=15,
 NET_IPV4_CONF_NOPOLICY=16,
 NET_IPV4_CONF_FORCE_IGMP_VERSION=17,
 NET_IPV4_CONF_ARP_ANNOUNCE=18,
 NET_IPV4_CONF_ARP_IGNORE=19,
 NET_IPV4_CONF_PROMOTE_SECONDARIES=20,
 NET_IPV4_CONF_ARP_ACCEPT=21,
 NET_IPV4_CONF_ARP_NOTIFY=22,
 NET_IPV4_CONF_ARP_EVICT_NOCARRIER=23,
};

/* /proc/sys/net/ipv4/netfilter */
enum
{
 NET_IPV4_NF_CONNTRACK_MAX=1,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT=2,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV=3,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED=4,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT=5,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT=6,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK=7,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT=8,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE=9,
 NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT=10,
 NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT_STREAM=11,
 NET_IPV4_NF_CONNTRACK_ICMP_TIMEOUT=12,
 NET_IPV4_NF_CONNTRACK_GENERIC_TIMEOUT=13,
 NET_IPV4_NF_CONNTRACK_BUCKETS=14,
 NET_IPV4_NF_CONNTRACK_LOG_INVALID=15,
 NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS=16,
 NET_IPV4_NF_CONNTRACK_TCP_LOOSE=17,
 NET_IPV4_NF_CONNTRACK_TCP_BE_LIBERAL=18,
 NET_IPV4_NF_CONNTRACK_TCP_MAX_RETRANS=19,
  NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED=20,
  NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT=21,
  NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED=22,
  NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED=23,
  NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT=24,
  NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD=25,
  NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT=26,
 NET_IPV4_NF_CONNTRACK_COUNT=27,
 NET_IPV4_NF_CONNTRACK_CHECKSUM=28,
};

/* /proc/sys/net/ipv6 */
enum {
 NET_IPV6_CONF=16,
 NET_IPV6_NEIGH=17,
 NET_IPV6_ROUTE=18,
 NET_IPV6_ICMP=19,
 NET_IPV6_BINDV6ONLY=20,
 NET_IPV6_IP6FRAG_HIGH_THRESH=21,
 NET_IPV6_IP6FRAG_LOW_THRESH=22,
 NET_IPV6_IP6FRAG_TIME=23,
 NET_IPV6_IP6FRAG_SECRET_INTERVAL=24,
 NET_IPV6_MLD_MAX_MSF=25,
};

enum {
 NET_IPV6_ROUTE_FLUSH=1,
 NET_IPV6_ROUTE_GC_THRESH=2,
 NET_IPV6_ROUTE_MAX_SIZE=3,
 NET_IPV6_ROUTE_GC_MIN_INTERVAL=4,
 NET_IPV6_ROUTE_GC_TIMEOUT=5,
 NET_IPV6_ROUTE_GC_INTERVAL=6,
 NET_IPV6_ROUTE_GC_ELASTICITY=7,
 NET_IPV6_ROUTE_MTU_EXPIRES=8,
 NET_IPV6_ROUTE_MIN_ADVMSS=9,
 NET_IPV6_ROUTE_GC_MIN_INTERVAL_MS=10
};

enum {
 NET_IPV6_FORWARDING=1,
 NET_IPV6_HOP_LIMIT=2,
 NET_IPV6_MTU=3,
 NET_IPV6_ACCEPT_RA=4,
 NET_IPV6_ACCEPT_REDIRECTS=5,
 NET_IPV6_AUTOCONF=6,
 NET_IPV6_DAD_TRANSMITS=7,
 NET_IPV6_RTR_SOLICITS=8,
 NET_IPV6_RTR_SOLICIT_INTERVAL=9,
 NET_IPV6_RTR_SOLICIT_DELAY=10,
 NET_IPV6_USE_TEMPADDR=11,
 NET_IPV6_TEMP_VALID_LFT=12,
 NET_IPV6_TEMP_PREFERED_LFT=13,
 NET_IPV6_REGEN_MAX_RETRY=14,
 NET_IPV6_MAX_DESYNC_FACTOR=15,
 NET_IPV6_MAX_ADDRESSES=16,
 NET_IPV6_FORCE_MLD_VERSION=17,
 NET_IPV6_ACCEPT_RA_DEFRTR=18,
 NET_IPV6_ACCEPT_RA_PINFO=19,
 NET_IPV6_ACCEPT_RA_RTR_PREF=20,
 NET_IPV6_RTR_PROBE_INTERVAL=21,
 NET_IPV6_ACCEPT_RA_RT_INFO_MAX_PLEN=22,
 NET_IPV6_PROXY_NDP=23,
 NET_IPV6_ACCEPT_SOURCE_ROUTE=25,
 NET_IPV6_ACCEPT_RA_FROM_LOCAL=26,
 NET_IPV6_ACCEPT_RA_RT_INFO_MIN_PLEN=27,
 NET_IPV6_RA_DEFRTR_METRIC=28,
 __NET_IPV6_MAX
};

/* /proc/sys/net/ipv6/icmp */
enum {
 NET_IPV6_ICMP_RATELIMIT = 1,
 NET_IPV6_ICMP_ECHO_IGNORE_ALL = 2
};

/* /proc/sys/net/<protocol>/neigh/<dev> */
enum {
 NET_NEIGH_MCAST_SOLICIT = 1,
 NET_NEIGH_UCAST_SOLICIT = 2,
 NET_NEIGH_APP_SOLICIT = 3,
 NET_NEIGH_RETRANS_TIME = 4,
 NET_NEIGH_REACHABLE_TIME = 5,
 NET_NEIGH_DELAY_PROBE_TIME = 6,
 NET_NEIGH_GC_STALE_TIME = 7,
 NET_NEIGH_UNRES_QLEN = 8,
 NET_NEIGH_PROXY_QLEN = 9,
 NET_NEIGH_ANYCAST_DELAY = 10,
 NET_NEIGH_PROXY_DELAY = 11,
 NET_NEIGH_LOCKTIME = 12,
 NET_NEIGH_GC_INTERVAL = 13,
 NET_NEIGH_GC_THRESH1 = 14,
 NET_NEIGH_GC_THRESH2 = 15,
 NET_NEIGH_GC_THRESH3 = 16,
 NET_NEIGH_RETRANS_TIME_MS = 17,
 NET_NEIGH_REACHABLE_TIME_MS = 18,
 NET_NEIGH_INTERVAL_PROBE_TIME_MS = 19,
};

/* /proc/sys/net/dccp */
enum {
 NET_DCCP_DEFAULT=1,
};

/* /proc/sys/net/ipx */
enum {
 NET_IPX_PPROP_BROADCASTING=1,
 NET_IPX_FORWARDING=2
};

/* /proc/sys/net/llc */
enum {
 NET_LLC2=1,
 NET_LLC_STATION=2,
};

/* /proc/sys/net/llc/llc2 */
enum {
 NET_LLC2_TIMEOUT=1,
};

/* /proc/sys/net/llc/station */
enum {
 NET_LLC_STATION_ACK_TIMEOUT=1,
};

/* /proc/sys/net/llc/llc2/timeout */
enum {
 NET_LLC2_ACK_TIMEOUT=1,
 NET_LLC2_P_TIMEOUT=2,
 NET_LLC2_REJ_TIMEOUT=3,
 NET_LLC2_BUSY_TIMEOUT=4,
};

/* /proc/sys/net/appletalk */
enum {
 NET_ATALK_AARP_EXPIRY_TIME=1,
 NET_ATALK_AARP_TICK_TIME=2,
 NET_ATALK_AARP_RETRANSMIT_LIMIT=3,
 NET_ATALK_AARP_RESOLVE_TIME=4
};


/* /proc/sys/net/netrom */
enum {
 NET_NETROM_DEFAULT_PATH_QUALITY=1,
 NET_NETROM_OBSOLESCENCE_COUNT_INITIALISER=2,
 NET_NETROM_NETWORK_TTL_INITIALISER=3,
 NET_NETROM_TRANSPORT_TIMEOUT=4,
 NET_NETROM_TRANSPORT_MAXIMUM_TRIES=5,
 NET_NETROM_TRANSPORT_ACKNOWLEDGE_DELAY=6,
 NET_NETROM_TRANSPORT_BUSY_DELAY=7,
 NET_NETROM_TRANSPORT_REQUESTED_WINDOW_SIZE=8,
 NET_NETROM_TRANSPORT_NO_ACTIVITY_TIMEOUT=9,
 NET_NETROM_ROUTING_CONTROL=10,
 NET_NETROM_LINK_FAILS_COUNT=11,
 NET_NETROM_RESET=12
};

/* /proc/sys/net/ax25 */
enum {
 NET_AX25_IP_DEFAULT_MODE=1,
 NET_AX25_DEFAULT_MODE=2,
 NET_AX25_BACKOFF_TYPE=3,
 NET_AX25_CONNECT_MODE=4,
 NET_AX25_STANDARD_WINDOW=5,
 NET_AX25_EXTENDED_WINDOW=6,
 NET_AX25_T1_TIMEOUT=7,
 NET_AX25_T2_TIMEOUT=8,
 NET_AX25_T3_TIMEOUT=9,
 NET_AX25_IDLE_TIMEOUT=10,
 NET_AX25_N2=11,
 NET_AX25_PACLEN=12,
 NET_AX25_PROTOCOL=13,
 NET_AX25_DAMA_SLAVE_TIMEOUT=14
};

/* /proc/sys/net/rose */
enum {
 NET_ROSE_RESTART_REQUEST_TIMEOUT=1,
 NET_ROSE_CALL_REQUEST_TIMEOUT=2,
 NET_ROSE_RESET_REQUEST_TIMEOUT=3,
 NET_ROSE_CLEAR_REQUEST_TIMEOUT=4,
 NET_ROSE_ACK_HOLD_BACK_TIMEOUT=5,
 NET_ROSE_ROUTING_CONTROL=6,
 NET_ROSE_LINK_FAIL_TIMEOUT=7,
 NET_ROSE_MAX_VCS=8,
 NET_ROSE_WINDOW_SIZE=9,
 NET_ROSE_NO_ACTIVITY_TIMEOUT=10
};

/* /proc/sys/net/x25 */
enum {
 NET_X25_RESTART_REQUEST_TIMEOUT=1,
 NET_X25_CALL_REQUEST_TIMEOUT=2,
 NET_X25_RESET_REQUEST_TIMEOUT=3,
 NET_X25_CLEAR_REQUEST_TIMEOUT=4,
 NET_X25_ACK_HOLD_BACK_TIMEOUT=5,
 NET_X25_FORWARD=6
};

/* /proc/sys/net/token-ring */
enum
{
 NET_TR_RIF_TIMEOUT=1
};

/* /proc/sys/net/decnet/ */
enum {
 NET_DECNET_NODE_TYPE = 1,
 NET_DECNET_NODE_ADDRESS = 2,
 NET_DECNET_NODE_NAME = 3,
 NET_DECNET_DEFAULT_DEVICE = 4,
 NET_DECNET_TIME_WAIT = 5,
 NET_DECNET_DN_COUNT = 6,
 NET_DECNET_DI_COUNT = 7,
 NET_DECNET_DR_COUNT = 8,
 NET_DECNET_DST_GC_INTERVAL = 9,
 NET_DECNET_CONF = 10,
 NET_DECNET_NO_FC_MAX_CWND = 11,
 NET_DECNET_MEM = 12,
 NET_DECNET_RMEM = 13,
 NET_DECNET_WMEM = 14,
 NET_DECNET_DEBUG_LEVEL = 255
};

/* /proc/sys/net/decnet/conf/<dev> */
enum {
 NET_DECNET_CONF_LOOPBACK = -2,
 NET_DECNET_CONF_DDCMP = -3,
 NET_DECNET_CONF_PPP = -4,
 NET_DECNET_CONF_X25 = -5,
 NET_DECNET_CONF_GRE = -6,
 NET_DECNET_CONF_ETHER = -7

 /* ... and ifindex of devices */
};

/* /proc/sys/net/decnet/conf/<dev>/ */
enum {
 NET_DECNET_CONF_DEV_PRIORITY = 1,
 NET_DECNET_CONF_DEV_T1 = 2,
 NET_DECNET_CONF_DEV_T2 = 3,
 NET_DECNET_CONF_DEV_T3 = 4,
 NET_DECNET_CONF_DEV_FORWARDING = 5,
 NET_DECNET_CONF_DEV_BLKSIZE = 6,
 NET_DECNET_CONF_DEV_STATE = 7
};

/* /proc/sys/net/sctp */
enum {
 NET_SCTP_RTO_INITIAL = 1,
 NET_SCTP_RTO_MIN = 2,
 NET_SCTP_RTO_MAX = 3,
 NET_SCTP_RTO_ALPHA = 4,
 NET_SCTP_RTO_BETA = 5,
 NET_SCTP_VALID_COOKIE_LIFE = 6,
 NET_SCTP_ASSOCIATION_MAX_RETRANS = 7,
 NET_SCTP_PATH_MAX_RETRANS = 8,
 NET_SCTP_MAX_INIT_RETRANSMITS = 9,
 NET_SCTP_HB_INTERVAL = 10,
 NET_SCTP_PRESERVE_ENABLE = 11,
 NET_SCTP_MAX_BURST = 12,
 NET_SCTP_ADDIP_ENABLE = 13,
 NET_SCTP_PRSCTP_ENABLE = 14,
 NET_SCTP_SNDBUF_POLICY = 15,
 NET_SCTP_SACK_TIMEOUT = 16,
 NET_SCTP_RCVBUF_POLICY = 17,
};

/* /proc/sys/net/bridge */
enum {
 NET_BRIDGE_NF_CALL_ARPTABLES = 1,
 NET_BRIDGE_NF_CALL_IPTABLES = 2,
 NET_BRIDGE_NF_CALL_IP6TABLES = 3,
 NET_BRIDGE_NF_FILTER_VLAN_TAGGED = 4,
 NET_BRIDGE_NF_FILTER_PPPOE_TAGGED = 5,
};


/* CTL_FS names: */
enum
{
 FS_NRINODE=1, /* int:current number of allocated inodes */
 FS_STATINODE=2,
 FS_MAXINODE=3, /* int:maximum number of inodes that can be allocated */
 FS_NRDQUOT=4, /* int:current number of allocated dquots */
 FS_MAXDQUOT=5, /* int:maximum number of dquots that can be allocated */
 FS_NRFILE=6, /* int:current number of allocated filedescriptors */
 FS_MAXFILE=7, /* int:maximum number of filedescriptors that can be allocated */
 FS_DENTRY=8,
 FS_NRSUPER=9, /* int:current number of allocated super_blocks */
 FS_MAXSUPER=10, /* int:maximum number of super_blocks that can be allocated */
 FS_OVERFLOWUID=11, /* int: overflow UID */
 FS_OVERFLOWGID=12, /* int: overflow GID */
 FS_LEASES=13, /* int: leases enabled */
 FS_DIR_NOTIFY=14, /* int: directory notification enabled */
 FS_LEASE_TIME=15, /* int: maximum time to wait for a lease break */
 FS_DQSTATS=16, /* disc quota usage statistics and control */
 FS_XFS=17, /* struct: control xfs parameters */
 FS_AIO_NR=18, /* current system-wide number of aio requests */
 FS_AIO_MAX_NR=19, /* system-wide maximum number of aio requests */
 FS_INOTIFY=20, /* inotify submenu */
 FS_OCFS2=988, /* ocfs2 */
};

/* /proc/sys/fs/quota/ */
enum {
 FS_DQ_LOOKUPS = 1,
 FS_DQ_DROPS = 2,
 FS_DQ_READS = 3,
 FS_DQ_WRITES = 4,
 FS_DQ_CACHE_HITS = 5,
 FS_DQ_ALLOCATED = 6,
 FS_DQ_FREE = 7,
 FS_DQ_SYNCS = 8,
 FS_DQ_WARNINGS = 9,
};

/* CTL_DEBUG names: */

/* CTL_DEV names: */
enum {
 DEV_CDROM=1,
 DEV_HWMON=2,
 DEV_PARPORT=3,
 DEV_RAID=4,
 DEV_MAC_HID=5,
 DEV_SCSI=6,
 DEV_IPMI=7,
};

/* /proc/sys/dev/cdrom */
enum {
 DEV_CDROM_INFO=1,
 DEV_CDROM_AUTOCLOSE=2,
 DEV_CDROM_AUTOEJECT=3,
 DEV_CDROM_DEBUG=4,
 DEV_CDROM_LOCK=5,
 DEV_CDROM_CHECK_MEDIA=6
};

/* /proc/sys/dev/parport */
enum {
 DEV_PARPORT_DEFAULT=-3
};

/* /proc/sys/dev/raid */
enum {
 DEV_RAID_SPEED_LIMIT_MIN=1,
 DEV_RAID_SPEED_LIMIT_MAX=2
};

/* /proc/sys/dev/parport/default */
enum {
 DEV_PARPORT_DEFAULT_TIMESLICE=1,
 DEV_PARPORT_DEFAULT_SPINTIME=2
};

/* /proc/sys/dev/parport/parport n */
enum {
 DEV_PARPORT_SPINTIME=1,
 DEV_PARPORT_BASE_ADDR=2,
 DEV_PARPORT_IRQ=3,
 DEV_PARPORT_DMA=4,
 DEV_PARPORT_MODES=5,
 DEV_PARPORT_DEVICES=6,
 DEV_PARPORT_AUTOPROBE=16
};

/* /proc/sys/dev/parport/parport n/devices/ */
enum {
 DEV_PARPORT_DEVICES_ACTIVE=-3,
};

/* /proc/sys/dev/parport/parport n/devices/device n */
enum {
 DEV_PARPORT_DEVICE_TIMESLICE=1,
};

/* /proc/sys/dev/mac_hid */
enum {
 DEV_MAC_HID_KEYBOARD_SENDS_LINUX_KEYCODES=1,
 DEV_MAC_HID_KEYBOARD_LOCK_KEYCODES=2,
 DEV_MAC_HID_MOUSE_BUTTON_EMULATION=3,
 DEV_MAC_HID_MOUSE_BUTTON2_KEYCODE=4,
 DEV_MAC_HID_MOUSE_BUTTON3_KEYCODE=5,
 DEV_MAC_HID_ADB_MOUSE_SENDS_KEYCODES=6
};

/* /proc/sys/dev/scsi */
enum {
 DEV_SCSI_LOGGING_LEVEL=1,
};

/* /proc/sys/dev/ipmi */
enum {
 DEV_IPMI_POWEROFF_POWERCYCLE=1,
};

/* /proc/sys/abi */
enum
{
 ABI_DEFHANDLER_COFF=1, /* default handler for coff binaries */
 ABI_DEFHANDLER_ELF=2, /* default handler for ELF binaries */
 ABI_DEFHANDLER_LCALL7=3,/* default handler for procs using lcall7 */
 ABI_DEFHANDLER_LIBCSO=4,/* default handler for an libc.so ELF interp */
 ABI_TRACE=5, /* tracing flags */
 ABI_FAKE_UTSNAME=6, /* fake target utsname information */
};
# 31 "./include/linux/sysctl.h" 2

/* For the /proc/sys support */
struct completion;
struct ctl_table;
struct nsproxy;
struct ctl_table_root;
struct ctl_table_header;
struct ctl_dir;

/* Keep the same order as in fs/proc/proc_sysctl.c */
# 52 "./include/linux/sysctl.h"
/* this is needed for the proc_dointvec_minmax for [fs_]overflow UID and GID */



extern const int sysctl_vals[];





extern const unsigned long sysctl_long_vals[];

typedef int proc_handler(struct ctl_table *ctl, int write, void *buffer,
  size_t *lenp, loff_t *ppos);

int proc_dostring(struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_dobool(struct ctl_table *table, int write, void *buffer,
  size_t *lenp, loff_t *ppos);
int proc_dointvec(struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_douintvec(struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_dointvec_minmax(struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_douintvec_minmax(struct ctl_table *table, int write, void *buffer,
  size_t *lenp, loff_t *ppos);
int proc_dou8vec_minmax(struct ctl_table *table, int write, void *buffer,
   size_t *lenp, loff_t *ppos);
int proc_dointvec_jiffies(struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_dointvec_ms_jiffies_minmax(struct ctl_table *table, int write,
  void *buffer, size_t *lenp, loff_t *ppos);
int proc_dointvec_userhz_jiffies(struct ctl_table *, int, void *, size_t *,
  loff_t *);
int proc_dointvec_ms_jiffies(struct ctl_table *, int, void *, size_t *,
  loff_t *);
int proc_doulongvec_minmax(struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_doulongvec_ms_jiffies_minmax(struct ctl_table *table, int, void *,
  size_t *, loff_t *);
int proc_do_large_bitmap(struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_do_static_key(struct ctl_table *table, int write, void *buffer,
  size_t *lenp, loff_t *ppos);

/*
 * Register a set of sysctl names by calling register_sysctl_table
 * with an initialised array of struct ctl_table's.  An entry with
 * NULL procname terminates the table.  table->de will be
 * set up by the registration and need not be initialised in advance.
 *
 * sysctl names can be mirrored automatically under /proc/sys.  The
 * procname supplied controls /proc naming.
 *
 * The table's mode will be honoured for proc-fs access.
 *
 * Leaf nodes in the sysctl tree will be represented by a single file
 * under /proc; non-leaf nodes will be represented by directories.  A
 * null procname disables /proc mirroring at this node.
 *
 * The data and maxlen fields of the ctl_table
 * struct enable minimal validation of the values being written to be
 * performed, and the mode field allows minimal authentication.
 *
 * There must be a proc_handler routine for any terminal nodes
 * mirrored under /proc/sys (non-terminals are handled by a built-in
 * directory handler).  Several default handlers are available to
 * cover common cases.
 */

/* Support for userspace poll() to watch for changes */
struct ctl_table_poll {
 atomic_t event;
 wait_queue_head_t wait;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *proc_sys_poll_event(struct ctl_table_poll *poll)
{
 return (void *)(unsigned long)atomic_read(&poll->event);
}
# 134 "./include/linux/sysctl.h"
/* A sysctl table is an array of struct ctl_table: */
struct ctl_table {
 const char *procname; /* Text ID for /proc/sys, or zero */
 void *data;
 int maxlen;
 umode_t mode;
 struct ctl_table *child; /* Deprecated */
 proc_handler *proc_handler; /* Callback for text formatting */
 struct ctl_table_poll *poll;
 void *extra1;
 void *extra2;
} ;

struct ctl_node {
 struct rb_node node;
 struct ctl_table_header *header;
};

/* struct ctl_table_header is used to maintain dynamic lists of
   struct ctl_table trees. */
struct ctl_table_header {
 union {
  struct {
   struct ctl_table *ctl_table;
   int used;
   int count;
   int nreg;
  };
  struct callback_head rcu;
 };
 struct completion *unregistering;
 struct ctl_table *ctl_table_arg;
 struct ctl_table_root *root;
 struct ctl_table_set *set;
 struct ctl_dir *parent;
 struct ctl_node *node;
 struct hlist_head inodes; /* head for proc_inode->sysctl_inodes */
};

struct ctl_dir {
 /* Header must be at the start of ctl_dir */
 struct ctl_table_header header;
 struct rb_root root;
};

struct ctl_table_set {
 int (*is_seen)(struct ctl_table_set *);
 struct ctl_dir dir;
};

struct ctl_table_root {
 struct ctl_table_set default_set;
 struct ctl_table_set *(*lookup)(struct ctl_table_root *root);
 void (*set_ownership)(struct ctl_table_header *head,
         struct ctl_table *table,
         kuid_t *uid, kgid_t *gid);
 int (*permissions)(struct ctl_table_header *head, struct ctl_table *table);
};

/* struct ctl_path describes where in the hierarchy a table is added */
struct ctl_path {
 const char *procname;
};
# 210 "./include/linux/sysctl.h"
extern int __register_sysctl_base(struct ctl_table *base_table);



void proc_sys_poll_notify(struct ctl_table_poll *poll);

extern void setup_sysctl_set(struct ctl_table_set *p,
 struct ctl_table_root *root,
 int (*is_seen)(struct ctl_table_set *));
extern void retire_sysctl_set(struct ctl_table_set *set);

struct ctl_table_header *__register_sysctl_table(
 struct ctl_table_set *set,
 const char *path, struct ctl_table *table);
struct ctl_table_header *__register_sysctl_paths(
 struct ctl_table_set *set,
 const struct ctl_path *path, struct ctl_table *table);
struct ctl_table_header *register_sysctl(const char *path, struct ctl_table *table);
struct ctl_table_header *register_sysctl_table(struct ctl_table * table);
struct ctl_table_header *register_sysctl_paths(const struct ctl_path *path,
      struct ctl_table *table);

void unregister_sysctl_table(struct ctl_table_header * table);

extern int sysctl_init_bases(void);
extern void __register_sysctl_init(const char *path, struct ctl_table *table,
     const char *table_name);

extern struct ctl_table_header *register_sysctl_mount_point(const char *path);

void do_sysctl_args(void);
int do_proc_douintvec(struct ctl_table *table, int write,
        void *buffer, size_t *lenp, loff_t *ppos,
        int (*conv)(unsigned long *lvalp,
      unsigned int *valp,
      int write, void *data),
        void *data);

extern int pwrsw_enabled;
extern int unaligned_enabled;
extern int unaligned_dump_stack;
extern int no_unaligned_warning;

extern struct ctl_table sysctl_mount_point[];
# 306 "./include/linux/sysctl.h"
int sysctl_max_threads(struct ctl_table *table, int write, void *buffer,
  size_t *lenp, loff_t *ppos);
# 18 "./include/linux/key.h" 2


# 1 "./include/linux/assoc_array.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/* Generic associative array implementation.
 *
 * See Documentation/core-api/assoc_array.rst for information.
 *
 * Copyright (C) 2013 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 */
# 19 "./include/linux/assoc_array.h"
/*
 * Generic associative array.
 */
struct assoc_array {
 struct assoc_array_ptr *root; /* The node at the root of the tree */
 unsigned long nr_leaves_on_tree;
};

/*
 * Operations on objects and index keys for use by array manipulation routines.
 */
struct assoc_array_ops {
 /* Method to get a chunk of an index key from caller-supplied data */
 unsigned long (*get_key_chunk)(const void *index_key, int level);

 /* Method to get a piece of an object's index key */
 unsigned long (*get_object_key_chunk)(const void *object, int level);

 /* Is this the object we're looking for? */
 bool (*compare_object)(const void *object, const void *index_key);

 /* How different is an object from an index key, to a bit position in
	 * their keys? (or -1 if they're the same)
	 */
 int (*diff_objects)(const void *object, const void *index_key);

 /* Method to free an object. */
 void (*free_object)(void *object);
};

/*
 * Access and manipulation functions.
 */
struct assoc_array_edit;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void assoc_array_init(struct assoc_array *array)
{
 array->root = ((void *)0);
 array->nr_leaves_on_tree = 0;
}

extern int assoc_array_iterate(const struct assoc_array *array,
          int (*iterator)(const void *object,
            void *iterator_data),
          void *iterator_data);
extern void *assoc_array_find(const struct assoc_array *array,
         const struct assoc_array_ops *ops,
         const void *index_key);
extern void assoc_array_destroy(struct assoc_array *array,
    const struct assoc_array_ops *ops);
extern struct assoc_array_edit *assoc_array_insert(struct assoc_array *array,
         const struct assoc_array_ops *ops,
         const void *index_key,
         void *object);
extern void assoc_array_insert_set_object(struct assoc_array_edit *edit,
       void *object);
extern struct assoc_array_edit *assoc_array_delete(struct assoc_array *array,
         const struct assoc_array_ops *ops,
         const void *index_key);
extern struct assoc_array_edit *assoc_array_clear(struct assoc_array *array,
        const struct assoc_array_ops *ops);
extern void assoc_array_apply_edit(struct assoc_array_edit *edit);
extern void assoc_array_cancel_edit(struct assoc_array_edit *edit);
extern int assoc_array_gc(struct assoc_array *array,
     const struct assoc_array_ops *ops,
     bool (*iterator)(void *object, void *iterator_data),
     void *iterator_data);
# 21 "./include/linux/key.h" 2






/* key handle serial number */
typedef int32_t key_serial_t;

/* key handle permissions mask */
typedef uint32_t key_perm_t;

struct key;
struct net;
# 74 "./include/linux/key.h"
/*
 * The permissions required on a key that we're looking up.
 */
enum key_need_perm {
 KEY_NEED_UNSPECIFIED, /* Needed permission unspecified */
 KEY_NEED_VIEW, /* Require permission to view attributes */
 KEY_NEED_READ, /* Require permission to read content */
 KEY_NEED_WRITE, /* Require permission to update / modify */
 KEY_NEED_SEARCH, /* Require permission to search (keyring) or find (key) */
 KEY_NEED_LINK, /* Require permission to link */
 KEY_NEED_SETATTR, /* Require permission to change attributes */
 KEY_NEED_UNLINK, /* Require permission to unlink key */
 KEY_SYSADMIN_OVERRIDE, /* Special: override by CAP_SYS_ADMIN */
 KEY_AUTHTOKEN_OVERRIDE, /* Special: override by possession of auth token */
 KEY_DEFER_PERM_CHECK, /* Special: permission check is deferred */
};

enum key_lookup_flag {
 KEY_LOOKUP_CREATE = 0x01,
 KEY_LOOKUP_PARTIAL = 0x02,
 KEY_LOOKUP_ALL = (KEY_LOOKUP_CREATE | KEY_LOOKUP_PARTIAL),
};

struct seq_file;
struct user_struct;
struct signal_struct;
struct cred;

struct key_type;
struct key_owner;
struct key_tag;
struct keyring_list;
struct keyring_name;

struct key_tag {
 struct callback_head rcu;
 refcount_t usage;
 bool removed; /* T when subject removed */
};

struct keyring_index_key {
 /* [!] If this structure is altered, the union in struct key must change too! */
 unsigned long hash; /* Hash value */
 union {
  struct {

   u16 desc_len;
   char desc[sizeof(long) - 2]; /* First few chars of description */




  };
  unsigned long x;
 };
 struct key_type *type;
 struct key_tag *domain_tag; /* Domain of operation */
 const char *description;
};

union key_payload {
 void /* nothing */ *rcu_data0;
 void *data[4];
};

/*****************************************************************************/
/*
 * key reference with possession attribute handling
 *
 * NOTE! key_ref_t is a typedef'd pointer to a type that is not actually
 * defined. This is because we abuse the bottom bit of the reference to carry a
 * flag to indicate whether the calling process possesses that key in one of
 * its keyrings.
 *
 * the key_ref_t has been made a separate type so that the compiler can reject
 * attempts to dereference it without proper conversion.
 *
 * the three functions are used to assemble and disassemble references
 */
typedef struct __key_reference_with_attributes *key_ref_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) key_ref_t make_key_ref(const struct key *key,
         bool possession)
{
 return (key_ref_t) ((unsigned long) key | possession);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct key *key_ref_to_ptr(const key_ref_t key_ref)
{
 return (struct key *) ((unsigned long) key_ref & ~1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_key_possessed(const key_ref_t key_ref)
{
 return (unsigned long) key_ref & 1UL;
}

typedef int (*key_restrict_link_func_t)(struct key *dest_keyring,
     const struct key_type *type,
     const union key_payload *payload,
     struct key *restriction_key);

struct key_restriction {
 key_restrict_link_func_t check;
 struct key *key;
 struct key_type *keytype;
};

enum key_state {
 KEY_IS_UNINSTANTIATED,
 KEY_IS_POSITIVE, /* Positively instantiated */
};

/*****************************************************************************/
/*
 * authentication token / access credential / keyring
 * - types of key include:
 *   - keyrings
 *   - disk encryption IDs
 *   - Kerberos TGTs and tickets
 */
struct key {
 refcount_t usage; /* number of references */
 key_serial_t serial; /* key serial number */
 union {
  struct list_head graveyard_link;
  struct rb_node serial_node;
 };



 struct rw_semaphore sem; /* change vs change sem */
 struct key_user *user; /* owner of this key */
 void *security; /* security data for this key */
 union {
  time64_t expiry; /* time at which key expires (or 0) */
  time64_t revoked_at; /* time at which key was revoked */
 };
 time64_t last_used_at; /* last time used for LRU keyring discard */
 kuid_t uid;
 kgid_t gid;
 key_perm_t perm; /* access permissions */
 unsigned short quotalen; /* length added to quota */
 unsigned short datalen; /* payload data length
						 * - may not match RCU dereferenced payload
						 * - payload should contain own length
						 */
 short state; /* Key state (+) or rejection error (-) */






 unsigned long flags; /* status flags (change with bitops) */
# 240 "./include/linux/key.h"
 /* the key type and key description string
	 * - the desc is used to match a key against search criteria
	 * - it should be a printable string
	 * - eg: for krb5 AFS, this might be "afs@REDHAT.COM"
	 */
 union {
  struct keyring_index_key index_key;
  struct {
   unsigned long hash;
   unsigned long len_desc;
   struct key_type *type; /* type of key */
   struct key_tag *domain_tag; /* Domain of operation */
   char *description;
  };
 };

 /* key data
	 * - this is used to hold the data actually used in cryptography or
	 *   whatever
	 */
 union {
  union key_payload payload;
  struct {
   /* Keyring bits */
   struct list_head name_link;
   struct assoc_array keys;
  };
 };

 /* This is set on a keyring to restrict the addition of a link to a key
	 * to it.  If this structure isn't provided then it is assumed that the
	 * keyring is open to any addition.  It is ignored for non-keyring
	 * keys. Only set this value using keyring_restrict(), keyring_alloc(),
	 * or key_alloc().
	 *
	 * This is intended for use with rings of trusted keys whereby addition
	 * to the keyring needs to be controlled.  KEY_ALLOC_BYPASS_RESTRICTION
	 * overrides this, allowing the kernel to add extra keys without
	 * restriction.
	 */
 struct key_restriction *restrict_link;
};

extern struct key *key_alloc(struct key_type *type,
        const char *desc,
        kuid_t uid, kgid_t gid,
        const struct cred *cred,
        key_perm_t perm,
        unsigned long flags,
        struct key_restriction *restrict_link);
# 300 "./include/linux/key.h"
extern void key_revoke(struct key *key);
extern void key_invalidate(struct key *key);
extern void key_put(struct key *key);
extern bool key_put_tag(struct key_tag *tag);
extern void key_remove_domain(struct key_tag *domain_tag);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct key *__key_get(struct key *key)
{
 refcount_inc(&key->usage);
 return key;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct key *key_get(struct key *key)
{
 return key ? __key_get(key) : key;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void key_ref_put(key_ref_t key_ref)
{
 key_put(key_ref_to_ptr(key_ref));
}

extern struct key *request_key_tag(struct key_type *type,
       const char *description,
       struct key_tag *domain_tag,
       const char *callout_info);

extern struct key *request_key_rcu(struct key_type *type,
       const char *description,
       struct key_tag *domain_tag);

extern struct key *request_key_with_auxdata(struct key_type *type,
         const char *description,
         struct key_tag *domain_tag,
         const void *callout_info,
         size_t callout_len,
         void *aux);

/**
 * request_key - Request a key and wait for construction
 * @type: Type of key.
 * @description: The searchable description of the key.
 * @callout_info: The data to pass to the instantiation upcall (or NULL).
 *
 * As for request_key_tag(), but with the default global domain tag.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct key *request_key(struct key_type *type,
          const char *description,
          const char *callout_info)
{
 return request_key_tag(type, description, ((void *)0), callout_info);
}


/**
 * request_key_net - Request a key for a net namespace and wait for construction
 * @type: Type of key.
 * @description: The searchable description of the key.
 * @net: The network namespace that is the key's domain of operation.
 * @callout_info: The data to pass to the instantiation upcall (or NULL).
 *
 * As for request_key() except that it does not add the returned key to a
 * keyring if found, new keys are always allocated in the user's quota, the
 * callout_info must be a NUL-terminated string and no auxiliary data can be
 * passed.  Only keys that operate the specified network namespace are used.
 *
 * Furthermore, it then works as wait_for_key_construction() to wait for the
 * completion of keys undergoing construction with a non-interruptible wait.
 */



/**
 * request_key_net_rcu - Request a key for a net namespace under RCU conditions
 * @type: Type of key.
 * @description: The searchable description of the key.
 * @net: The network namespace that is the key's domain of operation.
 *
 * As for request_key_rcu() except that only keys that operate the specified
 * network namespace are used.
 */




extern int wait_for_key_construction(struct key *key, bool intr);

extern int key_validate(const struct key *key);

extern key_ref_t key_create_or_update(key_ref_t keyring,
          const char *type,
          const char *description,
          const void *payload,
          size_t plen,
          key_perm_t perm,
          unsigned long flags);

extern int key_update(key_ref_t key,
        const void *payload,
        size_t plen);

extern int key_link(struct key *keyring,
      struct key *key);

extern int key_move(struct key *key,
      struct key *from_keyring,
      struct key *to_keyring,
      unsigned int flags);

extern int key_unlink(struct key *keyring,
        struct key *key);

extern struct key *keyring_alloc(const char *description, kuid_t uid, kgid_t gid,
     const struct cred *cred,
     key_perm_t perm,
     unsigned long flags,
     struct key_restriction *restrict_link,
     struct key *dest);

extern int restrict_link_reject(struct key *keyring,
    const struct key_type *type,
    const union key_payload *payload,
    struct key *restriction_key);

extern int keyring_clear(struct key *keyring);

extern key_ref_t keyring_search(key_ref_t keyring,
    struct key_type *type,
    const char *description,
    bool recurse);

extern int keyring_add_key(struct key *keyring,
      struct key *key);

extern int keyring_restrict(key_ref_t keyring, const char *type,
       const char *restriction);

extern struct key *key_lookup(key_serial_t id);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) key_serial_t key_serial(const struct key *key)
{
 return key ? key->serial : 0;
}

extern void key_set_timeout(struct key *, unsigned);

extern key_ref_t lookup_user_key(key_serial_t id, unsigned long flags,
     enum key_need_perm need_perm);
extern void key_free_user_ns(struct user_namespace *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) short key_read_state(const struct key *key)
{
 /* Barrier versus mark_key_instantiated(). */
 return ({ union { typeof( _Generic((*&key->state), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&key->state))) __val; char __c[1]; } __u; typeof(&key->state) __p = (&key->state); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_300(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&key->state) == sizeof(char) || sizeof(*&key->state) == sizeof(short) || sizeof(*&key->state) == sizeof(int) || sizeof(*&key->state) == sizeof(long)))) __compiletime_assert_300(); } while (0); kasan_check_read(__p, sizeof(*&key->state)); switch (sizeof(*&key->state)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&key->state))__u.__val; });
# 454 "./include/linux/key.h"
}

/**
 * key_is_positive - Determine if a key has been positively instantiated
 * @key: The key to check.
 *
 * Return true if the specified key has been positively instantiated, false
 * otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool key_is_positive(const struct key *key)
{
 return key_read_state(key) == KEY_IS_POSITIVE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool key_is_negative(const struct key *key)
{
 return key_read_state(key) < 0;
}
# 486 "./include/linux/key.h"
extern struct ctl_table key_sysctls[];

/*
 * the userspace interface
 */
extern int install_thread_keyring_to_cred(struct cred *cred);
extern void key_fsuid_changed(struct cred *new_cred);
extern void key_fsgid_changed(struct cred *new_cred);
extern void key_init(void);
# 14 "./include/linux/cred.h" 2



# 1 "./include/linux/sched/user.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







# 1 "./include/linux/ratelimit.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ratelimit_state_init(struct ratelimit_state *rs,
     int interval, int burst)
{
 memset(rs, 0, sizeof(*rs));

 do { *(&rs->lock) = (raw_spinlock_t) { .raw_lock = { { .val = { (0) } } }, }; } while (0);
 rs->interval = interval;
 rs->burst = burst;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ratelimit_default_init(struct ratelimit_state *rs)
{
 return ratelimit_state_init(rs, (5 * 250 /* Internal kernel timer frequency */),
     10);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ratelimit_state_exit(struct ratelimit_state *rs)
{
 if (!(rs->flags & ((((1UL))) << (0))))
  return;

 if (rs->missed) {
  ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "4" /* warning conditions */ "%s: %d output lines suppressed due to ratelimiting\n", get_current()->comm, rs->missed); });

  rs->missed = 0;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
ratelimit_set_flags(struct ratelimit_state *rs, unsigned long flags)
{
 rs->flags = flags;
}

extern struct ratelimit_state printk_ratelimit_state;
# 10 "./include/linux/sched/user.h" 2

/*
 * Some day this will be a full-fledged user tracking system..
 */
struct user_struct {
 refcount_t __count; /* reference count */

 struct percpu_counter epoll_watches; /* The number of file descriptors currently watched */

 unsigned long unix_inflight; /* How many files in flight in unix sockets */
 atomic_long_t pipe_bufs; /* how many pages are allocated in pipe buffers */

 /* Hash table maintenance information */
 struct hlist_node uidhash_node;
 kuid_t uid;




 atomic_long_t locked_vm;





 /* Miscellaneous per-user rate limit */
 struct ratelimit_state ratelimit;
};

extern int uids_sysfs_init(void);

extern struct user_struct *find_user(kuid_t);

extern struct user_struct root_user;



/* per-UID process charging. */
extern struct user_struct * alloc_uid(kuid_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct user_struct *get_uid(struct user_struct *u)
{
 refcount_inc(&u->__count);
 return u;
}
extern void free_uid(struct user_struct *);
# 18 "./include/linux/cred.h" 2

struct cred;
struct inode;

/*
 * COW Supplementary groups list
 */
struct group_info {
 atomic_t usage;
 int ngroups;
 kgid_t gid[];
} ;

/**
 * get_group_info - Get a reference to a group info structure
 * @group_info: The group info to reference
 *
 * This gets a reference to a set of supplementary groups.
 *
 * If the caller is accessing a task's credentials, they must hold the RCU read
 * lock when reading.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct group_info *get_group_info(struct group_info *gi)
{
 atomic_inc(&gi->usage);
 return gi;
}

/**
 * put_group_info - Release a reference to a group info structure
 * @group_info: The group info to release
 */







extern struct group_info *groups_alloc(int);
extern void groups_free(struct group_info *);

extern int in_group_p(kgid_t);
extern int in_egroup_p(kgid_t);
extern int groups_search(const struct group_info *, kgid_t);

extern int set_current_groups(struct group_info *);
extern void set_groups(struct cred *, struct group_info *);
extern bool may_setgroups(void);
extern void groups_sort(struct group_info *);
# 87 "./include/linux/cred.h"
/*
 * The security context of a task
 *
 * The parts of the context break down into two categories:
 *
 *  (1) The objective context of a task.  These parts are used when some other
 *	task is attempting to affect this one.
 *
 *  (2) The subjective context.  These details are used when the task is acting
 *	upon another object, be that a file, a task, a key or whatever.
 *
 * Note that some members of this structure belong to both categories - the
 * LSM security pointer for instance.
 *
 * A task has two security pointers.  task->real_cred points to the objective
 * context that defines that task's actual details.  The objective part of this
 * context is used whenever that task is acted upon.
 *
 * task->cred points to the subjective context that defines the details of how
 * that task is going to act upon another object.  This may be overridden
 * temporarily to point to another security context, but normally points to the
 * same context as task->real_cred.
 */
struct cred {
 atomic_t usage;







 kuid_t uid; /* real UID of the task */
 kgid_t gid; /* real GID of the task */
 kuid_t suid; /* saved UID of the task */
 kgid_t sgid; /* saved GID of the task */
 kuid_t euid; /* effective UID of the task */
 kgid_t egid; /* effective GID of the task */
 kuid_t fsuid; /* UID for VFS ops */
 kgid_t fsgid; /* GID for VFS ops */
 unsigned securebits; /* SUID-less security management */
 kernel_cap_t cap_inheritable; /* caps our children can inherit */
 kernel_cap_t cap_permitted; /* caps we're permitted */
 kernel_cap_t cap_effective; /* caps we can actually use */
 kernel_cap_t cap_bset; /* capability bounding set */
 kernel_cap_t cap_ambient; /* Ambient capability set */

 unsigned char jit_keyring; /* default keyring to attach requested
					 * keys to */
 struct key *session_keyring; /* keyring inherited over fork */
 struct key *process_keyring; /* keyring private to this process */
 struct key *thread_keyring; /* keyring private to this thread */
 struct key *request_key_auth; /* assumed request_key authority */


 void *security; /* LSM security */

 struct user_struct *user; /* real user ID subscription */
 struct user_namespace *user_ns; /* user_ns the caps and keyrings are relative to. */
 struct ucounts *ucounts;
 struct group_info *group_info; /* supplementary groups for euid/fsgid */
 /* RCU deletion */
 union {
  int non_rcu; /* Can we skip RCU deletion? */
  struct callback_head rcu; /* RCU deletion hook */
 };
} ;

extern void __put_cred(struct cred *);
extern void exit_creds(struct task_struct *);
extern int copy_creds(struct task_struct *, unsigned long);
extern const struct cred *get_task_cred(struct task_struct *);
extern struct cred *cred_alloc_blank(void);
extern struct cred *prepare_creds(void);
extern struct cred *prepare_exec_creds(void);
extern int commit_creds(struct cred *);
extern void abort_creds(struct cred *);
extern const struct cred *override_creds(const struct cred *);
extern void revert_creds(const struct cred *);
extern struct cred *prepare_kernel_cred(struct task_struct *);
extern int change_create_files_as(struct cred *, struct inode *);
extern int set_security_override(struct cred *, u32);
extern int set_security_override_from_ctx(struct cred *, const char *);
extern int set_create_files_as(struct cred *, struct inode *);
extern int cred_fscmp(const struct cred *, const struct cred *);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) cred_init(void);
extern int set_cred_ucounts(struct cred *);

/*
 * check for validity of credentials
 */
# 204 "./include/linux/cred.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void validate_creds(const struct cred *cred)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void validate_creds_for_do_exit(struct task_struct *tsk)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void validate_process_creds(void)
{
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cap_ambient_invariant_ok(const struct cred *cred)
{
 return cap_issubset(cred->cap_ambient,
       cap_intersect(cred->cap_permitted,
       cred->cap_inheritable));
}

/**
 * get_new_cred - Get a reference on a new set of credentials
 * @cred: The new credentials to reference
 *
 * Get a reference on the specified set of new credentials.  The caller must
 * release the reference.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct cred *get_new_cred(struct cred *cred)
{
 atomic_inc(&cred->usage);
 return cred;
}

/**
 * get_cred - Get a reference on a set of credentials
 * @cred: The credentials to reference
 *
 * Get a reference on the specified set of credentials.  The caller must
 * release the reference.  If %NULL is passed, it is returned with no action.
 *
 * This is used to deal with a committed set of credentials.  Although the
 * pointer is const, this will temporarily discard the const and increment the
 * usage count.  The purpose of this is to attempt to catch at compile time the
 * accidental alteration of a set of credentials that should be considered
 * immutable.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct cred *get_cred(const struct cred *cred)
{
 struct cred *nonconst_cred = (struct cred *) cred;
 if (!cred)
  return cred;
 validate_creds(cred);
 nonconst_cred->non_rcu = 0;
 return get_new_cred(nonconst_cred);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct cred *get_cred_rcu(const struct cred *cred)
{
 struct cred *nonconst_cred = (struct cred *) cred;
 if (!cred)
  return ((void *)0);
 if (!atomic_inc_not_zero(&nonconst_cred->usage))
  return ((void *)0);
 validate_creds(cred);
 nonconst_cred->non_rcu = 0;
 return cred;
}

/**
 * put_cred - Release a reference to a set of credentials
 * @cred: The credentials to release
 *
 * Release a reference to a set of credentials, deleting them when the last ref
 * is released.  If %NULL is passed, nothing is done.
 *
 * This takes a const pointer to a set of credentials because the credentials
 * on task_struct are attached by const pointers to prevent accidental
 * alteration of otherwise immutable credential sets.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void put_cred(const struct cred *_cred)
{
 struct cred *cred = (struct cred *) _cred;

 if (cred) {
  validate_creds(cred);
  if (atomic_dec_and_test(&(cred)->usage))
   __put_cred(cred);
 }
}

/**
 * current_cred - Access the current task's subjective credentials
 *
 * Access the subjective credentials of the current task.  RCU-safe,
 * since nobody else can modify it.
 */



/**
 * current_real_cred - Access the current task's objective credentials
 *
 * Access the objective credentials of the current task.  RCU-safe,
 * since nobody else can modify it.
 */



/**
 * __task_cred - Access a task's objective credentials
 * @task: The task to query
 *
 * Access the objective credentials of a task.  The caller must hold the RCU
 * readlock.
 *
 * The result of this function should not be passed directly to get_cred();
 * rather get_task_cred() should be used instead.
 */



/**
 * get_current_cred - Get the current task's subjective credentials
 *
 * Get the subjective credentials of the current task, pinning them so that
 * they can't go away.  Accessing the current task's credentials directly is
 * not permitted.
 */



/**
 * get_current_user - Get the current task's user_struct
 *
 * Get the user record of the current task, pinning it so that it can't go
 * away.
 */
# 348 "./include/linux/cred.h"
/**
 * get_current_groups - Get the current task's supplementary group list
 *
 * Get the supplementary group list of the current task, pinning it so that it
 * can't go away.
 */
# 393 "./include/linux/cred.h"
extern struct user_namespace init_user_ns;
# 11 "./include/linux/sched/signal.h" 2





/*
 * Types defining task->signal and task->sighand and APIs using them:
 */

struct sighand_struct {
 spinlock_t siglock;
 refcount_t count;
 wait_queue_head_t signalfd_wqh;
 struct k_sigaction action[64];
};

/*
 * Per-process accounting stats:
 */
struct pacct_struct {
 int ac_flag;
 long ac_exitcode;
 unsigned long ac_mem;
 u64 ac_utime, ac_stime;
 unsigned long ac_minflt, ac_majflt;
};

struct cpu_itimer {
 u64 expires;
 u64 incr;
};

/*
 * This is the atomic variant of task_cputime, which can be used for
 * storing and updating task_cputime statistics without locking.
 */
struct task_cputime_atomic {
 atomic64_t utime;
 atomic64_t stime;
 atomic64_t sum_exec_runtime;
};







/**
 * struct thread_group_cputimer - thread group interval timer counts
 * @cputime_atomic:	atomic thread group interval timers.
 *
 * This structure contains the version of task_cputime, above, that is
 * used for thread group CPU timer calculations.
 */
struct thread_group_cputimer {
 struct task_cputime_atomic cputime_atomic;
};

struct multiprocess_signals {
 sigset_t signal;
 struct hlist_node node;
};

struct core_thread {
 struct task_struct *task;
 struct core_thread *next;
};

struct core_state {
 atomic_t nr_threads;
 struct core_thread dumper;
 struct completion startup;
};

/*
 * NOTE! "signal_struct" does not have its own
 * locking, because a shared signal_struct always
 * implies a shared sighand_struct, so locking
 * sighand_struct is always a proper superset of
 * the locking of signal_struct.
 */
struct signal_struct {
 refcount_t sigcnt;
 atomic_t live;
 int nr_threads;
 int quick_threads;
 struct list_head thread_head;

 wait_queue_head_t wait_chldexit; /* for wait4() */

 /* current thread group signal load-balancing target: */
 struct task_struct *curr_target;

 /* shared signal handling: */
 struct sigpending shared_pending;

 /* For collecting multiprocess signals during fork */
 struct hlist_head multiprocess;

 /* thread group exit support */
 int group_exit_code;
 /* notify group_exec_task when notify_count is less or equal to 0 */
 int notify_count;
 struct task_struct *group_exec_task;

 /* thread group stop support, overloads group_exit_code too */
 int group_stop_count;
 unsigned int flags; /* see SIGNAL_* flags below */

 struct core_state *core_state; /* coredumping support */

 /*
	 * PR_SET_CHILD_SUBREAPER marks a process, like a service
	 * manager, to re-parent orphan (double-forking) child processes
	 * to this process instead of 'init'. The service manager is
	 * able to receive SIGCHLD signals and is able to investigate
	 * the process until it calls wait(). All children of this
	 * process will inherit a flag if they should look for a
	 * child_subreaper process at exit.
	 */
 unsigned int is_child_subreaper:1;
 unsigned int has_child_subreaper:1;



 /* POSIX.1b Interval Timers */
 int posix_timer_id;
 struct list_head posix_timers;

 /* ITIMER_REAL timer for the process */
 struct hrtimer real_timer;
 ktime_t it_real_incr;

 /*
	 * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use
	 * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these
	 * values are defined to 0 and 1 respectively
	 */
 struct cpu_itimer it[2];

 /*
	 * Thread group totals for process CPU timers.
	 * See thread_group_cputimer(), et al, for details.
	 */
 struct thread_group_cputimer cputimer;


 /* Empty if CONFIG_POSIX_TIMERS=n */
 struct posix_cputimers posix_cputimers;

 /* PID/PID hash table linkage. */
 struct pid *pids[PIDTYPE_MAX];





 struct pid *tty_old_pgrp;

 /* boolean value for session group leader */
 int leader;

 struct tty_struct *tty; /* NULL if no tty */


 struct autogroup *autogroup;

 /*
	 * Cumulative resource counters for dead threads in the group,
	 * and for reaped dead child processes forked by this group.
	 * Live threads maintain their own counters and add to these
	 * in __exit_signal, except for the group leader.
	 */
 seqlock_t stats_lock;
 u64 utime, stime, cutime, cstime;
 u64 gtime;
 u64 cgtime;
 struct prev_cputime prev_cputime;
 unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
 unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
 unsigned long inblock, oublock, cinblock, coublock;
 unsigned long maxrss, cmaxrss;
 struct task_io_accounting ioac;

 /*
	 * Cumulative ns of schedule CPU time fo dead threads in the
	 * group, not including a zombie group leader, (This only differs
	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
	 * other than jiffies.)
	 */
 unsigned long long sum_sched_runtime;

 /*
	 * We don't bother to synchronize most readers of this at all,
	 * because there is no reader checking a limit that actually needs
	 * to get both rlim_cur and rlim_max atomically, and either one
	 * alone is a single word that can safely be read normally.
	 * getrlimit/setrlimit use task_lock(current->group_leader) to
	 * protect this instead of the siglock, because they really
	 * have no need to disable irqs.
	 */
 struct rlimit rlim[16];


 struct pacct_struct pacct; /* per-process accounting information */


 struct taskstats *stats;


 unsigned audit_tty;
 struct tty_audit_buf *tty_audit_buf;


 /*
	 * Thread is the potential origin of an oom condition; kill first on
	 * oom
	 */
 bool oom_flag_origin;
 short oom_score_adj; /* OOM kill score adjustment */
 short oom_score_adj_min; /* OOM kill score adjustment min value.
					 * Only settable by CAP_SYS_RESOURCE. */
 struct mm_struct *oom_mm; /* recorded mm when the thread group got
					 * killed by the oom killer */

 struct mutex cred_guard_mutex; /* guard against foreign influences on
					 * credential calculations
					 * (notably. ptrace)
					 * Deprecated do not use in new code.
					 * Use exec_update_lock instead.
					 */
 struct rw_semaphore exec_update_lock; /* Held while task_struct is
						 * being updated during exec,
						 * and may have inconsistent
						 * permissions.
						 */
} ;

/*
 * Bits in flags field of signal_struct.
 */



/*
 * Pending notifications to parent.
 */
# 268 "./include/linux/sched/signal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void signal_set_stop_flags(struct signal_struct *sig,
      unsigned int flags)
{
 ({ int __ret_warn_on = !!(sig->flags & 0x00000004 /* group exit in progress */); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/sched/signal.h\"; .popsection; .long 14472b - .; .short 271; .short (1 << 0)|(((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 sig->flags = (sig->flags & ~((0x00000010|0x00000020) | 0x00000001 /* job control stop in effect */ | 0x00000002 /* SIGCONT since WCONTINUED reap */)) | flags;
}

extern void flush_signals(struct task_struct *);
extern void ignore_signals(struct task_struct *);
extern void flush_signal_handlers(struct task_struct *, int force_default);
extern int dequeue_signal(struct task_struct *task, sigset_t *mask,
     kernel_siginfo_t *info, enum pid_type *type);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kernel_dequeue_signal(void)
{
 struct task_struct *task = get_current();
 kernel_siginfo_t __info;
 enum pid_type __type;
 int ret;

 spin_lock_irq(&task->sighand->siglock);
 ret = dequeue_signal(task, &task->blocked, &__info, &__type);
 spin_unlock_irq(&task->sighand->siglock);

 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kernel_signal_stop(void)
{
 spin_lock_irq(&get_current()->sighand->siglock);
 if (get_current()->jobctl & (1UL << 16 /* stop signal dequeued */)) {
  get_current()->jobctl |= (1UL << 26 /* do_signal_stop() */);
  do { unsigned long flags; /* may shadow */ do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = _raw_spin_lock_irqsave(&get_current()->pi_lock); } while (0); do { } while (0); do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_301(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(get_current()->__state) == sizeof(char) || sizeof(get_current()->__state) == sizeof(short) || sizeof(get_current()->__state) == sizeof(int) || sizeof(get_current()->__state) == sizeof(long)) || sizeof(get_current()->__state) == sizeof(long long))) __compiletime_assert_301(); } while (0); do { *(volatile typeof(get_current()->__state) *)&(get_current()->__state) = (((0x00000100 | 0x00000004))); } while (0); } while (0); do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _raw_spin_unlock_irqrestore(&get_current()->pi_lock, flags); } while (0); } while (0);
# 301 "./include/linux/sched/signal.h"
 }
 spin_unlock_irq(&get_current()->sighand->siglock);

 schedule();
}






int force_sig_fault_to_task(int sig, int code, void /* nothing */ *addr

 , struct task_struct *t);
int force_sig_fault(int sig, int code, void /* nothing */ *addr
                                                                );
int send_sig_fault(int sig, int code, void /* nothing */ *addr

 , struct task_struct *t);

int force_sig_mceerr(int code, void /* nothing */ *, short);
int send_sig_mceerr(int code, void /* nothing */ *, short, struct task_struct *);

int force_sig_bnderr(void /* nothing */ *addr, void /* nothing */ *lower, void /* nothing */ *upper);
int force_sig_pkuerr(void /* nothing */ *addr, u32 pkey);
int send_sig_perf(void /* nothing */ *addr, u32 type, u64 sig_data);

int force_sig_ptrace_errno_trap(int errno, void /* nothing */ *addr);
int force_sig_fault_trapno(int sig, int code, void /* nothing */ *addr, int trapno);
int send_sig_fault_trapno(int sig, int code, void /* nothing */ *addr, int trapno,
   struct task_struct *t);
int force_sig_seccomp(int syscall, int reason, bool force_coredump);

extern int send_sig_info(int, struct kernel_siginfo *, struct task_struct *);
extern void force_sigsegv(int sig);
extern int force_sig_info(struct kernel_siginfo *);
extern int __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp);
extern int kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid);
extern int kill_pid_usb_asyncio(int sig, int errno, sigval_t addr, struct pid *,
    const struct cred *);
extern int kill_pgrp(struct pid *pid, int sig, int priv);
extern int kill_pid(struct pid *pid, int sig, int priv);
extern __attribute__((__warn_unused_result__)) bool do_notify_parent(struct task_struct *, int);
extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
extern void force_sig(int);
extern void force_fatal_sig(int);
extern void force_exit_sig(int);
extern int send_sig(int, struct task_struct *, int);
extern int zap_other_threads(struct task_struct *p);
extern struct sigqueue *sigqueue_alloc(void);
extern void sigqueue_free(struct sigqueue *);
extern int send_sigqueue(struct sigqueue *, struct pid *, enum pid_type);
extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_notify_signal(void)
{
 clear_ti_thread_flag(((struct thread_info *)get_current()), 6 /* signal notifications exist */);
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
}

/*
 * Returns 'true' if kick_process() is needed to force a transition from
 * user -> kernel to guarantee expedient run of TWA_SIGNAL based task_work.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __set_notify_signal(struct task_struct *task)
{
 return !test_and_set_tsk_thread_flag(task, 6 /* signal notifications exist */) &&
        !wake_up_state(task, 0x00000001);
}

/*
 * Called to break out of interruptible wait loops, and enter the
 * exit_to_user_mode_loop().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_notify_signal(struct task_struct *task)
{
 if (__set_notify_signal(task))
  kick_process(task);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int restart_syscall(void)
{
 set_tsk_thread_flag(get_current(), 0 /* signal pending */);
 return -513;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int task_sigpending(struct task_struct *p)
{
 return __builtin_expect(!!(test_tsk_thread_flag(p,0 /* signal pending */)), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int signal_pending(struct task_struct *p)
{
 /*
	 * TIF_NOTIFY_SIGNAL isn't really a signal, but it requires the same
	 * behavior in terms of ensuring that we break out of wait loops
	 * so that notify signal callbacks can be processed.
	 */
 if (__builtin_expect(!!(test_tsk_thread_flag(p, 6 /* signal notifications exist */)), 0))
  return 1;
 return task_sigpending(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __fatal_signal_pending(struct task_struct *p)
{
 return __builtin_expect(!!(sigismember(&p->pending.signal, 9)), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int fatal_signal_pending(struct task_struct *p)
{
 return task_sigpending(p) && __fatal_signal_pending(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int signal_pending_state(unsigned int state, struct task_struct *p)
{
 if (!(state & (0x00000001 | 0x00000100)))
  return 0;
 if (!signal_pending(p))
  return 0;

 return (state & 0x00000001) || __fatal_signal_pending(p);
}

/*
 * This should only be used in fault handlers to decide whether we
 * should stop the current fault routine to handle the signals
 * instead, especially with the case where we've got interrupted with
 * a VM_FAULT_RETRY.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool fault_signal_pending(vm_fault_t fault_flags,
     struct pt_regs *regs)
{
 return __builtin_expect(!!((fault_flags & VM_FAULT_RETRY) && (fatal_signal_pending(get_current()) || ((((regs)->pstate & 0x0000000f) == 0x00000000) && signal_pending(get_current())))), 0);


}

/*
 * Reevaluate whether the task has signals pending delivery.
 * Wake the task if so.
 * This is required every time the blocked sigset_t changes.
 * callers must hold sighand->siglock.
 */
extern void recalc_sigpending_and_wake(struct task_struct *t);
extern void recalc_sigpending(void);
extern void calculate_sigpending(void);

extern void signal_wake_up_state(struct task_struct *t, unsigned int state);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void signal_wake_up(struct task_struct *t, bool fatal)
{
 unsigned int state = 0;
 if (fatal && !(t->jobctl & (1UL << 24 /* frozen for ptrace */))) {
  t->jobctl &= ~((1UL << 26 /* do_signal_stop() */) | (1UL << 27 /* ptrace_stop() */));
  state = 0x00000100 | 0x00000008;
 }
 signal_wake_up_state(t, state);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrace_signal_wake_up(struct task_struct *t, bool resume)
{
 unsigned int state = 0;
 if (resume) {
  t->jobctl &= ~(1UL << 27 /* ptrace_stop() */);
  state = 0x00000008;
 }
 signal_wake_up_state(t, state);
}

void task_join_group_stop(struct task_struct *task);


/*
 * Legacy restore_sigmask accessors.  These are inefficient on
 * SMP architectures because they require atomic operations.
 */

/**
 * set_restore_sigmask() - make sure saved_sigmask processing gets done
 *
 * This sets TIF_RESTORE_SIGMASK and ensures that the arch signal code
 * will run before returning to user mode, to process the flag.  For
 * all callers, TIF_SIGPENDING is already set or it's no harm to set
 * it.  TIF_RESTORE_SIGMASK need not be in the set of bits that the
 * arch code will notice on return to user mode, in case those bits
 * are scarce.  We set TIF_SIGPENDING here to ensure that the arch
 * signal code always gets run when TIF_RESTORE_SIGMASK is set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_restore_sigmask(void)
{
 set_ti_thread_flag(((struct thread_info *)get_current()), 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_tsk_restore_sigmask(struct task_struct *task)
{
 clear_tsk_thread_flag(task, 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_restore_sigmask(void)
{
 clear_ti_thread_flag(((struct thread_info *)get_current()), 20);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool test_tsk_restore_sigmask(struct task_struct *task)
{
 return test_tsk_thread_flag(task, 20);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool test_restore_sigmask(void)
{
 return test_ti_thread_flag(((struct thread_info *)get_current()), 20);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool test_and_clear_restore_sigmask(void)
{
 return test_and_clear_ti_thread_flag(((struct thread_info *)get_current()), 20);
}
# 547 "./include/linux/sched/signal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void restore_saved_sigmask(void)
{
 if (test_and_clear_restore_sigmask())
  __set_current_blocked(&get_current()->saved_sigmask);
}

extern int set_user_sigmask(const sigset_t /* nothing */ *umask, size_t sigsetsize);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void restore_saved_sigmask_unless(bool interrupted)
{
 if (interrupted)
  ({ int __ret_warn_on = !!(!signal_pending(get_current())); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/sched/signal.h\"; .popsection; .long 14472b - .; .short 558; .short (1 << 0)|(((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 else
  restore_saved_sigmask();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sigset_t *sigmask_to_save(void)
{
 sigset_t *res = &get_current()->blocked;
 if (__builtin_expect(!!(test_restore_sigmask()), 0))
  res = &get_current()->saved_sigmask;
 return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kill_cad_pid(int sig, int priv)
{
 return kill_pid(cad_pid, sig, priv);
}

/* These can be the second arg to send_sig_info/send_group_sig_info.  */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __on_sig_stack(unsigned long sp)
{




 return sp > get_current()->sas_ss_sp &&
  sp - get_current()->sas_ss_sp <= get_current()->sas_ss_size;

}

/*
 * True if we are on the alternate signal stack.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int on_sig_stack(unsigned long sp)
{
 /*
	 * If the signal stack is SS_AUTODISARM then, by construction, we
	 * can't be on the signal stack unless user code deliberately set
	 * SS_AUTODISARM when we were already on it.
	 *
	 * This improves reliability: if user state gets corrupted such that
	 * the stack pointer points very close to the end of the signal stack,
	 * then this check will enable the signal to be handled anyway.
	 */
 if (get_current()->sas_ss_flags & (1U << 31) /* disable sas during sighandling */)
  return 0;

 return __on_sig_stack(sp);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sas_ss_flags(unsigned long sp)
{
 if (!get_current()->sas_ss_size)
  return 2;

 return on_sig_stack(sp) ? 1 : 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sas_ss_reset(struct task_struct *p)
{
 p->sas_ss_sp = 0;
 p->sas_ss_size = 0;
 p->sas_ss_flags = 2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long sigsp(unsigned long sp, struct ksignal *ksig)
{
 if (__builtin_expect(!!((ksig->ka.sa.sa_flags & 0x08000000)), 0) && ! sas_ss_flags(sp))



  return get_current()->sas_ss_sp + get_current()->sas_ss_size;

 return sp;
}

extern void __cleanup_sighand(struct sighand_struct *);
extern void flush_itimer_signals(void);
# 649 "./include/linux/sched/signal.h"
extern bool current_is_single_threaded(void);

/*
 * Careful: do_each_thread/while_each_thread is a double loop so
 *          'break' will not work as expected - use goto instead.
 */
# 667 "./include/linux/sched/signal.h"
/* Careful: this is a double loop, 'break' won't work as expected. */



typedef int (*proc_visitor)(struct task_struct *p, void *data);
void walk_process_tree(struct task_struct *top, proc_visitor, void *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
{
 struct pid *pid;
 if (type == PIDTYPE_PID)
  pid = task_pid(task);
 else
  pid = task->signal->pids[type];
 return pid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid *task_tgid(struct task_struct *task)
{
 return task->signal->pids[PIDTYPE_TGID];
}

/*
 * Without tasklist or RCU lock it is not safe to dereference
 * the result of task_pgrp/task_session even if task == current,
 * we can race with another thread doing sys_setsid/sys_setpgid.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid *task_pgrp(struct task_struct *task)
{
 return task->signal->pids[PIDTYPE_PGID];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid *task_session(struct task_struct *task)
{
 return task->signal->pids[PIDTYPE_SID];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_nr_threads(struct task_struct *task)
{
 return task->signal->nr_threads;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool thread_group_leader(struct task_struct *p)
{
 return p->exit_signal >= 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
bool same_thread_group(struct task_struct *p1, struct task_struct *p2)
{
 return p1->signal == p2->signal;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct task_struct *next_thread(const struct task_struct *p)
{
 return ({ void *__mptr = (void *)(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_302(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(p->thread_group.next) == sizeof(char) || sizeof(p->thread_group.next) == sizeof(short) || sizeof(p->thread_group.next) == sizeof(int) || sizeof(p->thread_group.next) == sizeof(long)) || sizeof(p->thread_group.next) == sizeof(long long))) __compiletime_assert_302(); } while (0); (*(const volatile typeof( _Generic((p->thread_group.next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (p->thread_group.next))) *)&(p->thread_group.next)); })); _Static_assert(__builtin_types_compatible_p(typeof(*(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_302(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(p->thread_group.next) == sizeof(char) || sizeof(p->thread_group.next) == sizeof(short) || sizeof(p->thread_group.next) == sizeof(int) || sizeof(p->thread_group.next) == sizeof(long)) || sizeof(p->thread_group.next) == sizeof(long long))) __compiletime_assert_302(); } while (0); (*(const volatile typeof( _Generic((p->thread_group.next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (p->thread_group.next))) *)&(p->thread_group.next)); }))), typeof(((struct task_struct *)0)->thread_group)) || __builtin_types_compatible_p(typeof(*(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_302(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(p->thread_group.next) == sizeof(char) || sizeof(p->thread_group.next) == sizeof(short) || sizeof(p->thread_group.next) == sizeof(int) || sizeof(p->thread_group.next) == sizeof(long)) || sizeof(p->thread_group.next) == sizeof(long long))) __compiletime_assert_302(); } while (0); (*(const volatile typeof( _Generic((p->thread_group.next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (p->thread_group.next))) *)&(p->thread_group.next)); }))), typeof(void)), "pointer type mismatch in container_of()"); ((struct task_struct *)(__mptr - __builtin_offsetof(struct task_struct, thread_group))); });
# 725 "./include/linux/sched/signal.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int thread_group_empty(struct task_struct *p)
{
 return list_empty(&p->thread_group);
}




extern bool thread_group_exited(struct pid *pid);

extern struct sighand_struct *__lock_task_sighand(struct task_struct *task,
       unsigned long *flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct sighand_struct *lock_task_sighand(struct task_struct *task,
             unsigned long *flags)
{
 struct sighand_struct *ret;

 ret = __lock_task_sighand(task, flags);
 (void)(ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unlock_task_sighand(struct task_struct *task,
      unsigned long *flags)
{
 spin_unlock_irqrestore(&task->sighand->siglock, *flags);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_assert_task_sighand_held(struct task_struct *task) { }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long task_rlimit(const struct task_struct *task,
  unsigned int limit)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_303(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(task->signal->rlim[limit].rlim_cur) == sizeof(char) || sizeof(task->signal->rlim[limit].rlim_cur) == sizeof(short) || sizeof(task->signal->rlim[limit].rlim_cur) == sizeof(int) || sizeof(task->signal->rlim[limit].rlim_cur) == sizeof(long)) || sizeof(task->signal->rlim[limit].rlim_cur) == sizeof(long long))) __compiletime_assert_303(); } while (0); (*(const volatile typeof( _Generic((task->signal->rlim[limit].rlim_cur), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (task->signal->rlim[limit].rlim_cur))) *)&(task->signal->rlim[limit].rlim_cur)); });
# 766 "./include/linux/sched/signal.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long task_rlimit_max(const struct task_struct *task,
  unsigned int limit)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_304(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(task->signal->rlim[limit].rlim_max) == sizeof(char) || sizeof(task->signal->rlim[limit].rlim_max) == sizeof(short) || sizeof(task->signal->rlim[limit].rlim_max) == sizeof(int) || sizeof(task->signal->rlim[limit].rlim_max) == sizeof(long)) || sizeof(task->signal->rlim[limit].rlim_max) == sizeof(long long))) __compiletime_assert_304(); } while (0); (*(const volatile typeof( _Generic((task->signal->rlim[limit].rlim_max), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (task->signal->rlim[limit].rlim_max))) *)&(task->signal->rlim[limit].rlim_max)); });
# 772 "./include/linux/sched/signal.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long rlimit(unsigned int limit)
{
 return task_rlimit(get_current(), limit);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long rlimit_max(unsigned int limit)
{
 return task_rlimit_max(get_current(), limit);
}
# 7 "./include/linux/rcuwait.h" 2

/*
 * rcuwait provides a way of blocking and waking up a single
 * task in an rcu-safe manner.
 *
 * The only time @task is non-nil is when a user is blocked (or
 * checking if it needs to) on a condition, and reset as soon as we
 * know that the condition has succeeded and are awoken.
 */
struct rcuwait {
 struct task_struct /* nothing */ *task;
};




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rcuwait_init(struct rcuwait *w)
{
 w->task = ((void *)0);
}

/*
 * Note: this provides no serialization and, just as with waitqueues,
 * requires care to estimate as to whether or not the wait is active.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rcuwait_active(struct rcuwait *w)
{
 return !!({ typeof(*(w->task)) *__UNIQUE_ID_rcu305 = (typeof(*(w->task)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_306(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((w->task)) == sizeof(char) || sizeof((w->task)) == sizeof(short) || sizeof((w->task)) == sizeof(int) || sizeof((w->task)) == sizeof(long)) || sizeof((w->task)) == sizeof(long long))) __compiletime_assert_306(); } while (0); (*(const volatile typeof( _Generic(((w->task)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((w->task)))) *)&((w->task))); }); ; ((typeof(*(w->task)) *)(__UNIQUE_ID_rcu305)); });
# 35 "./include/linux/rcuwait.h"
}

extern int rcuwait_wake_up(struct rcuwait *w);

/*
 * The caller is responsible for locking around rcuwait_wait_event(),
 * and [prepare_to/finish]_rcuwait() such that writes to @task are
 * properly serialized.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void prepare_to_rcuwait(struct rcuwait *w)
{
 do { uintptr_t _r_a_p__v = (uintptr_t)(get_current()); ; if (__builtin_constant_p(get_current()) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_307(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((w->task)) == sizeof(char) || sizeof((w->task)) == sizeof(short) || sizeof((w->task)) == sizeof(int) || sizeof((w->task)) == sizeof(long)) || sizeof((w->task)) == sizeof(long long))) __compiletime_assert_307(); } while (0); do { *(volatile typeof((w->task)) *)&((w->task)) = ((typeof(w->task))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { typeof(&w->task) __p = (&w->task); union { typeof( _Generic((*&w->task), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&w->task))) __val; char __c[1]; } __u = { .__val = ( typeof( _Generic((*&w->task), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&w->task)))) ((typeof(*((typeof(w->task))_r_a_p__v)) /* nothing */ *)((typeof(w->task))_r_a_p__v)) }; do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_308(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&w->task) == sizeof(char) || sizeof(*&w->task) == sizeof(short) || sizeof(*&w->task) == sizeof(int) || sizeof(*&w->task) == sizeof(long)))) __compiletime_assert_308(); } while (0); kasan_check_write(__p, sizeof(*&w->task)); switch (sizeof(*&w->task)) { case 1: asm volatile ("stlrb %w1, %0" : "=Q" (*__p) : "r" (*(__u8 *)__u.__c) : "memory"); break; case 2: asm volatile ("stlrh %w1, %0" : "=Q" (*__p) : "r" (*(__u16 *)__u.__c) : "memory"); break; case 4: asm volatile ("stlr %w1, %0" : "=Q" (*__p) : "r" (*(__u32 *)__u.__c) : "memory"); break; case 8: asm volatile ("stlr %1, %0" : "=Q" (*__p) : "r" (*(__u64 *)__u.__c) : "memory"); break; } } while (0); } while (0); } while (0);
# 48 "./include/linux/rcuwait.h"
}

extern void finish_rcuwait(struct rcuwait *w);
# 8 "./include/linux/percpu-rwsem.h" 2

# 1 "./include/linux/rcu_sync.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/*
 * RCU-based infrastructure for lightweight reader-writer locking
 *
 * Copyright (c) 2015, Red Hat, Inc.
 *
 * Author: Oleg Nesterov <oleg@redhat.com>
 */







/* Structure to mediate between updaters and fastpath-using readers.  */
struct rcu_sync {
 int gp_state;
 int gp_count;
 wait_queue_head_t gp_wait;

 struct callback_head cb_head;
};

/**
 * rcu_sync_is_idle() - Are readers permitted to use their fastpaths?
 * @rsp: Pointer to rcu_sync structure to use for synchronization
 *
 * Returns true if readers are permitted to use their fastpaths.  Must be
 * invoked within some flavor of RCU read-side critical section.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool rcu_sync_is_idle(struct rcu_sync *rsp)
{
 do { } while (0 && (!rcu_read_lock_any_held()));

 return !({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_309(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(rsp->gp_state) == sizeof(char) || sizeof(rsp->gp_state) == sizeof(short) || sizeof(rsp->gp_state) == sizeof(int) || sizeof(rsp->gp_state) == sizeof(long)) || sizeof(rsp->gp_state) == sizeof(long long))) __compiletime_assert_309(); } while (0); (*(const volatile typeof( _Generic((rsp->gp_state), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (rsp->gp_state))) *)&(rsp->gp_state)); }); /* GP_IDLE */
# 37 "./include/linux/rcu_sync.h"
}

extern void rcu_sync_init(struct rcu_sync *);
extern void rcu_sync_enter_start(struct rcu_sync *);
extern void rcu_sync_enter(struct rcu_sync *);
extern void rcu_sync_exit(struct rcu_sync *);
extern void rcu_sync_dtor(struct rcu_sync *);
# 10 "./include/linux/percpu-rwsem.h" 2


struct percpu_rw_semaphore {
 struct rcu_sync rss;
 unsigned int /* nothing */ *read_count;
 struct rcuwait writer;
 wait_queue_head_t waiters;
 atomic_t block;



};
# 45 "./include/linux/percpu-rwsem.h"
extern bool __percpu_down_read(struct percpu_rw_semaphore *, bool);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_down_read(struct percpu_rw_semaphore *sem)
{
 do { do { } while (0); } while (0);

 do { } while (0);

 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 /*
	 * We are in an RCU-sched read-side critical section, so the writer
	 * cannot both change sem->state from readers_fast and start checking
	 * counters while we are here. So if we see !sem->state, we know that
	 * the writer won't be checking until we're past the preempt_enable()
	 * and that once the synchronize_rcu() is done, the writer will see
	 * anything we did within this RCU-sched read-size critical section.
	 */
 if (__builtin_expect(!!(rcu_sync_is_idle(&sem->rss)), 1))
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*sem->read_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
 else
  __percpu_down_read(sem, false); /* Unconditional memory barrier */
 /*
	 * The preempt_enable() prevents the compiler from
	 * bleeding the critical section out.
	 */
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_down_read_trylock(struct percpu_rw_semaphore *sem)
{
 bool ret = true;

 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 /*
	 * Same as in percpu_down_read().
	 */
 if (__builtin_expect(!!(rcu_sync_is_idle(&sem->rss)), 1))
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*sem->read_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
 else
  ret = __percpu_down_read(sem, true); /* Unconditional memory barrier */
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 /*
	 * The barrier() from preempt_enable() prevents the compiler from
	 * bleeding the critical section out.
	 */

 if (ret)
  do { } while (0);

 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_up_read(struct percpu_rw_semaphore *sem)
{
 do { } while (0);

 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 /*
	 * Same as in percpu_down_read().
	 */
 if (__builtin_expect(!!(rcu_sync_is_idle(&sem->rss)), 1)) {
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*sem->read_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
 } else {
  /*
		 * slowpath; reader will only ever wake a single blocked
		 * writer.
		 */
  do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0); /* B matches C */
  /*
		 * In other words, if they see our decrement (presumably to
		 * aggregate zero, as that is the only time it matters) they
		 * will also see our critical section.
		 */
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*sem->read_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*sem->read_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count))); (typeof((typeof(*(&(*sem->read_count))) *)(&(*sem->read_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*sem->read_count))(1)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
  rcuwait_wake_up(&sem->writer);
 }
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

extern bool percpu_is_read_locked(struct percpu_rw_semaphore *);
extern void percpu_down_write(struct percpu_rw_semaphore *);
extern void percpu_up_write(struct percpu_rw_semaphore *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_is_write_locked(struct percpu_rw_semaphore *sem)
{
 return atomic_read(&sem->block);
}

extern int __percpu_init_rwsem(struct percpu_rw_semaphore *,
    const char *, struct lock_class_key *);

extern void percpu_free_rwsem(struct percpu_rw_semaphore *);
# 147 "./include/linux/percpu-rwsem.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_rwsem_release(struct percpu_rw_semaphore *sem,
     bool read, unsigned long ip)
{
 do { } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_rwsem_acquire(struct percpu_rw_semaphore *sem,
     bool read, unsigned long ip)
{
 do { } while (0);
}
# 34 "./include/linux/fs.h" 2

# 1 "./include/linux/delayed_call.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Poor man's closures; I wish we could've done them sanely polymorphic,
 * but...
 */

struct delayed_call {
 void (*fn)(void *);
 void *arg;
};



/* I really wish we had closures with sane typechecking... */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_delayed_call(struct delayed_call *call,
  void (*fn)(void *), void *arg)
{
 call->fn = fn;
 call->arg = arg;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_delayed_call(struct delayed_call *call)
{
 if (call->fn)
  call->fn(call->arg);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_delayed_call(struct delayed_call *call)
{
 call->fn = ((void *)0);
}
# 36 "./include/linux/fs.h" 2
# 1 "./include/linux/uuid.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * UUID/GUID definition
 *
 * Copyright (C) 2010, 2016 Intel Corp.
 *	Huang Ying <ying.huang@intel.com>
 */



# 1 "./include/uapi/linux/uuid.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/* DO NOT USE in new code! This is solely for MEI due to legacy reasons */
/*
 * UUID/GUID definition
 *
 * Copyright (C) 2010, Intel Corp.
 *	Huang Ying <ying.huang@intel.com>
 */






typedef struct {
 __u8 b[16];
} guid_t;
# 26 "./include/uapi/linux/uuid.h"
/* backwards compatibility, don't use in new code */
typedef guid_t uuid_le;
# 12 "./include/linux/uuid.h" 2




typedef struct {
 __u8 b[16];
} uuid_t;
# 27 "./include/linux/uuid.h"
/*
 * The length of a UUID string ("aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee")
 * not including trailing NUL.
 */


extern const guid_t guid_null;
extern const uuid_t uuid_null;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool guid_equal(const guid_t *u1, const guid_t *u2)
{
 return memcmp(u1, u2, sizeof(guid_t)) == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void guid_copy(guid_t *dst, const guid_t *src)
{
 memcpy(dst, src, sizeof(guid_t));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void import_guid(guid_t *dst, const __u8 *src)
{
 memcpy(dst, src, sizeof(guid_t));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void export_guid(__u8 *dst, const guid_t *src)
{
 memcpy(dst, src, sizeof(guid_t));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool guid_is_null(const guid_t *guid)
{
 return guid_equal(guid, &guid_null);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uuid_equal(const uuid_t *u1, const uuid_t *u2)
{
 return memcmp(u1, u2, sizeof(uuid_t)) == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void uuid_copy(uuid_t *dst, const uuid_t *src)
{
 memcpy(dst, src, sizeof(uuid_t));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void import_uuid(uuid_t *dst, const __u8 *src)
{
 memcpy(dst, src, sizeof(uuid_t));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void export_uuid(__u8 *dst, const uuid_t *src)
{
 memcpy(dst, src, sizeof(uuid_t));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool uuid_is_null(const uuid_t *uuid)
{
 return uuid_equal(uuid, &uuid_null);
}

void generate_random_uuid(unsigned char uuid[16]);
void generate_random_guid(unsigned char guid[16]);

extern void guid_gen(guid_t *u);
extern void uuid_gen(uuid_t *u);

bool __attribute__((__warn_unused_result__)) uuid_is_valid(const char *uuid);

extern const u8 guid_index[16];
extern const u8 uuid_index[16];

int guid_parse(const char *uuid, guid_t *u);
int uuid_parse(const char *uuid, uuid_t *u);

/* backwards compatibility, don't use in new code */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int uuid_le_cmp(const guid_t u1, const guid_t u2)
{
 return memcmp(&u1, &u2, sizeof(guid_t));
}
# 37 "./include/linux/fs.h" 2
# 1 "./include/linux/errseq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * See Documentation/core-api/errseq.rst and lib/errseq.c
 */



typedef u32 errseq_t;

errseq_t errseq_set(errseq_t *eseq, int err);
errseq_t errseq_sample(errseq_t *eseq);
int errseq_check(errseq_t *eseq, errseq_t since);
int errseq_check_and_advance(errseq_t *eseq, errseq_t *since);
# 38 "./include/linux/fs.h" 2
# 1 "./include/linux/ioprio.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/sched/rt.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





struct task_struct;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rt_prio(int prio)
{
 if (__builtin_expect(!!(prio < 100), 0))
  return 1;
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int rt_task(struct task_struct *p)
{
 return rt_prio(p->prio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_is_realtime(struct task_struct *tsk)
{
 int policy = tsk->policy;

 if (policy == 1 || policy == 2)
  return true;
 if (policy == 6)
  return true;
 return false;
}


/*
 * Must hold either p->pi_lock or task_rq(p)->lock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct task_struct *rt_mutex_get_top_task(struct task_struct *p)
{
 return p->pi_top_task;
}
extern void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task);
extern void rt_mutex_adjust_pi(struct task_struct *p);
# 50 "./include/linux/sched/rt.h"
extern void normalize_rt_tasks(void);


/*
 * default timeslice is 100 msecs (used only for SCHED_RR tasks).
 * Timeslices get refilled after they expire.
 */
# 7 "./include/linux/ioprio.h" 2
# 1 "./include/linux/iocontext.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







enum {
 ICQ_EXITED = 1 << 2,
 ICQ_DESTROYED = 1 << 3,
};

/*
 * An io_cq (icq) is association between an io_context (ioc) and a
 * request_queue (q).  This is used by elevators which need to track
 * information per ioc - q pair.
 *
 * Elevator can request use of icq by setting elevator_type->icq_size and
 * ->icq_align.  Both size and align must be larger than that of struct
 * io_cq and elevator can use the tail area for private information.  The
 * recommended way to do this is defining a struct which contains io_cq as
 * the first member followed by private members and using its size and
 * align.  For example,
 *
 *	struct snail_io_cq {
 *		struct io_cq	icq;
 *		int		poke_snail;
 *		int		feed_snail;
 *	};
 *
 *	struct elevator_type snail_elv_type {
 *		.ops =		{ ... },
 *		.icq_size =	sizeof(struct snail_io_cq),
 *		.icq_align =	__alignof__(struct snail_io_cq),
 *		...
 *	};
 *
 * If icq_size is set, block core will manage icq's.  All requests will
 * have its ->elv.icq field set before elevator_ops->elevator_set_req_fn()
 * is called and be holding a reference to the associated io_context.
 *
 * Whenever a new icq is created, elevator_ops->elevator_init_icq_fn() is
 * called and, on destruction, ->elevator_exit_icq_fn().  Both functions
 * are called with both the associated io_context and queue locks held.
 *
 * Elevator is allowed to lookup icq using ioc_lookup_icq() while holding
 * queue lock but the returned icq is valid only until the queue lock is
 * released.  Elevators can not and should not try to create or destroy
 * icq's.
 *
 * As icq's are linked from both ioc and q, the locking rules are a bit
 * complex.
 *
 * - ioc lock nests inside q lock.
 *
 * - ioc->icq_list and icq->ioc_node are protected by ioc lock.
 *   q->icq_list and icq->q_node by q lock.
 *
 * - ioc->icq_tree and ioc->icq_hint are protected by ioc lock, while icq
 *   itself is protected by q lock.  However, both the indexes and icq
 *   itself are also RCU managed and lookup can be performed holding only
 *   the q lock.
 *
 * - icq's are not reference counted.  They are destroyed when either the
 *   ioc or q goes away.  Each request with icq set holds an extra
 *   reference to ioc to ensure it stays until the request is completed.
 *
 * - Linking and unlinking icq's are performed while holding both ioc and q
 *   locks.  Due to the lock ordering, q exit is simple but ioc exit
 *   requires reverse-order double lock dance.
 */
struct io_cq {
 struct request_queue *q;
 struct io_context *ioc;

 /*
	 * q_node and ioc_node link io_cq through icq_list of q and ioc
	 * respectively.  Both fields are unused once ioc_exit_icq() is
	 * called and shared with __rcu_icq_cache and __rcu_head which are
	 * used for RCU free of io_cq.
	 */
 union {
  struct list_head q_node;
  struct kmem_cache *__rcu_icq_cache;
 };
 union {
  struct hlist_node ioc_node;
  struct callback_head __rcu_head;
 };

 unsigned int flags;
};

/*
 * I/O subsystem state of the associated processes.  It is refcounted
 * and kmalloc'ed. These could be shared between processes.
 */
struct io_context {
 atomic_long_t refcount;
 atomic_t active_ref;

 unsigned short ioprio;
# 115 "./include/linux/iocontext.h"
};

struct task_struct;

void put_io_context(struct io_context *ioc);
void exit_io_context(struct task_struct *task);
int __copy_io(unsigned long clone_flags, struct task_struct *tsk);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int copy_io(unsigned long clone_flags, struct task_struct *tsk)
{
 if (!get_current()->io_context)
  return 0;
 return __copy_io(clone_flags, tsk);
}
# 8 "./include/linux/ioprio.h" 2

# 1 "./include/uapi/linux/ioprio.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/*
 * Gives us 8 prio classes with 13-bits of data for each class
 */
# 19 "./include/uapi/linux/ioprio.h"
/*
 * These are the io priority groups as implemented by the BFQ and mq-deadline
 * schedulers. RT is the realtime class, it always gets premium service. For
 * ATA disks supporting NCQ IO priority, RT class IOs will be processed using
 * high priority NCQ commands. BE is the best-effort scheduling class, the
 * default for any process. IDLE is the idle scheduling class, it is only
 * served when no one else is using the disk.
 */
enum {
 IOPRIO_CLASS_NONE,
 IOPRIO_CLASS_RT,
 IOPRIO_CLASS_BE,
 IOPRIO_CLASS_IDLE,
};

/*
 * The RT and BE priority classes both support up to 8 priority levels.
 */



enum {
 IOPRIO_WHO_PROCESS = 1,
 IOPRIO_WHO_PGRP,
 IOPRIO_WHO_USER,
};

/*
 * Fallback BE priority level.
 */
# 10 "./include/linux/ioprio.h" 2

/*
 * Default IO priority.
 */


/*
 * Check that a priority value has a valid class.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool ioprio_valid(unsigned short ioprio)
{
 unsigned short class = (((ioprio) >> 13) & 0x07);

 return class > IOPRIO_CLASS_NONE && class <= IOPRIO_CLASS_IDLE;
}

/*
 * if process has set io priority explicitly, use that. if not, convert
 * the cpu scheduler nice value to an io priority
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int task_nice_ioprio(struct task_struct *task)
{
 return (task_nice(task) + 20) / 5;
}

/*
 * This is for the case where the task hasn't asked for a specific IO class.
 * Check for idle and rt task process, and return appropriate IO class.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int task_nice_ioclass(struct task_struct *task)
{
 if (task->policy == 5)
  return IOPRIO_CLASS_IDLE;
 else if (task_is_realtime(task))
  return IOPRIO_CLASS_RT;
 else
  return IOPRIO_CLASS_BE;
}


int __get_task_ioprio(struct task_struct *p);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_current_ioprio(void)
{
 return __get_task_ioprio(get_current());
}

extern int set_task_ioprio(struct task_struct *task, int ioprio);


extern int ioprio_check_cap(int ioprio);
# 39 "./include/linux/fs.h" 2
# 1 "./include/linux/fs_types.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * This is a header for the common implementation of dirent
 * to fs on-disk file type conversion.  Although the fs on-disk
 * bits are specific to every file system, in practice, many
 * file systems use the exact same on-disk format to describe
 * the lower 3 file type bits that represent the 7 POSIX file
 * types.
 *
 * It is important to note that the definitions in this
 * header MUST NOT change. This would break both the
 * userspace ABI and the on-disk format of filesystems
 * using this code.
 *
 * All those file systems can use this generic code for the
 * conversions.
 */

/*
 * struct dirent file types
 * exposed to user via getdents(2), readdir(3)
 *
 * These match bits 12..15 of stat.st_mode
 * (ie "(i_mode >> 12) & 15").
 */




/* these are defined by POSIX and also present in glibc's dirent.h */
# 46 "./include/linux/fs_types.h"
/*
 * fs on-disk file types.
 * Only the low 3 bits are used for the POSIX file types.
 * Other bits are reserved for fs private use.
 * These definitions are shared and used by multiple filesystems,
 * and MUST NOT change under any circumstances.
 *
 * Note that no fs currently stores the whiteout type on-disk,
 * so whiteout dirents are exposed to user as DT_CHR.
 */
# 67 "./include/linux/fs_types.h"
/*
 * declarations for helper functions, accompanying implementation
 * is in fs/fs_types.c
 */
extern unsigned char fs_ftype_to_dtype(unsigned int filetype);
extern unsigned char fs_umode_to_ftype(umode_t mode);
extern unsigned char fs_umode_to_dtype(umode_t mode);
# 40 "./include/linux/fs.h" 2


# 1 "./include/linux/mount.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 *
 * Definitions for mount interface. This describes the in the kernel build
 * linkedlist with mounted filesystems.
 *
 * Author:  Marco van Wieringen <mvw@planets.elm.net>
 *
 */






struct super_block;
struct dentry;
struct user_namespace;
struct mnt_idmap;
struct file_system_type;
struct fs_context;
struct file;
struct path;
# 39 "./include/linux/mount.h"
/*
 * MNT_SHARED_MASK is the set of flags that should be cleared when a
 * mount becomes shared.  Currently, this is only the flag that says a
 * mount cannot be bind mounted, since this is how we create a mount
 * that shares events with another mount.  If you add a new MNT_*
 * flag, consider how it interacts with shared mounts.
 */
# 70 "./include/linux/mount.h"
struct vfsmount {
 struct dentry *mnt_root; /* root of the mounted tree */
 struct super_block *mnt_sb; /* pointer to superblock */
 int mnt_flags;
 struct mnt_idmap *mnt_idmap;
} ;

struct user_namespace *mnt_user_ns(const struct vfsmount *mnt);
struct user_namespace *mnt_idmap_owner(const struct mnt_idmap *idmap);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct mnt_idmap *mnt_idmap(const struct vfsmount *mnt)
{
 /* Pairs with smp_store_release() in do_idmap_mount(). */
 return ({ union { typeof( _Generic((*&mnt->mnt_idmap), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&mnt->mnt_idmap))) __val; char __c[1]; } __u; typeof(&mnt->mnt_idmap) __p = (&mnt->mnt_idmap); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_310(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&mnt->mnt_idmap) == sizeof(char) || sizeof(*&mnt->mnt_idmap) == sizeof(short) || sizeof(*&mnt->mnt_idmap) == sizeof(int) || sizeof(*&mnt->mnt_idmap) == sizeof(long)))) __compiletime_assert_310(); } while (0); kasan_check_read(__p, sizeof(*&mnt->mnt_idmap)); switch (sizeof(*&mnt->mnt_idmap)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&mnt->mnt_idmap))__u.__val; });
# 83 "./include/linux/mount.h"
}

extern int mnt_want_write(struct vfsmount *mnt);
extern int mnt_want_write_file(struct file *file);
extern void mnt_drop_write(struct vfsmount *mnt);
extern void mnt_drop_write_file(struct file *file);
extern void mntput(struct vfsmount *mnt);
extern struct vfsmount *mntget(struct vfsmount *mnt);
extern struct vfsmount *mnt_clone_internal(const struct path *path);
extern bool __mnt_is_readonly(struct vfsmount *mnt);
extern bool mnt_may_suid(struct vfsmount *mnt);

extern struct vfsmount *clone_private_mount(const struct path *path);
extern int __mnt_want_write(struct vfsmount *);
extern void __mnt_drop_write(struct vfsmount *);

extern struct vfsmount *fc_mount(struct fs_context *fc);
extern struct vfsmount *vfs_create_mount(struct fs_context *fc);
extern struct vfsmount *vfs_kern_mount(struct file_system_type *type,
          int flags, const char *name,
          void *data);
extern struct vfsmount *vfs_submount(const struct dentry *mountpoint,
         struct file_system_type *type,
         const char *name, void *data);

extern void mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list);
extern void mark_mounts_for_expiry(struct list_head *mounts);

extern dev_t name_to_dev_t(const char *name);
extern bool path_is_mountpoint(const struct path *path);

extern bool our_mnt(struct vfsmount *mnt);

extern struct vfsmount *kern_mount(struct file_system_type *);
extern void kern_unmount(struct vfsmount *mnt);
extern int may_umount_tree(struct vfsmount *);
extern int may_umount(struct vfsmount *);
extern long do_mount(const char *, const char /* nothing */ *,
       const char *, unsigned long, void *);
extern struct vfsmount *collect_mounts(const struct path *);
extern void drop_collected_mounts(struct vfsmount *);
extern int iterate_mounts(int (*)(struct vfsmount *, void *), void *,
     struct vfsmount *);
extern void kern_unmount_array(struct vfsmount *mnt[], unsigned int num);
# 43 "./include/linux/fs.h" 2

# 1 "./include/linux/mnt_idmapping.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






struct mnt_idmap;
struct user_namespace;

extern struct mnt_idmap nop_mnt_idmap;
extern struct user_namespace init_user_ns;

typedef struct {
 uid_t val;
} vfsuid_t;

typedef struct {
 gid_t val;
} vfsgid_t;

_Static_assert(sizeof(vfsuid_t) == sizeof(kuid_t), "sizeof(vfsuid_t) == sizeof(kuid_t)");
_Static_assert(sizeof(vfsgid_t) == sizeof(kgid_t), "sizeof(vfsgid_t) == sizeof(kgid_t)");
_Static_assert(__builtin_offsetof(vfsuid_t, val) == __builtin_offsetof(kuid_t, val), "offsetof(vfsuid_t, val) == offsetof(kuid_t, val)");
_Static_assert(__builtin_offsetof(vfsgid_t, val) == __builtin_offsetof(kgid_t, val), "offsetof(vfsgid_t, val) == offsetof(kgid_t, val)");


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) uid_t __vfsuid_val(vfsuid_t uid)
{
 return uid.val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) gid_t __vfsgid_val(vfsgid_t gid)
{
 return gid.val;
}
# 49 "./include/linux/mnt_idmapping.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsuid_valid(vfsuid_t uid)
{
 return __vfsuid_val(uid) != (uid_t)-1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsgid_valid(vfsgid_t gid)
{
 return __vfsgid_val(gid) != (gid_t)-1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsuid_eq(vfsuid_t left, vfsuid_t right)
{
 return vfsuid_valid(left) && __vfsuid_val(left) == __vfsuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsgid_eq(vfsgid_t left, vfsgid_t right)
{
 return vfsgid_valid(left) && __vfsgid_val(left) == __vfsgid_val(right);
}

/**
 * vfsuid_eq_kuid - check whether kuid and vfsuid have the same value
 * @vfsuid: the vfsuid to compare
 * @kuid: the kuid to compare
 *
 * Check whether @vfsuid and @kuid have the same values.
 *
 * Return: true if @vfsuid and @kuid have the same value, false if not.
 * Comparison between two invalid uids returns false.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsuid_eq_kuid(vfsuid_t vfsuid, kuid_t kuid)
{
 return vfsuid_valid(vfsuid) && __vfsuid_val(vfsuid) == __kuid_val(kuid);
}

/**
 * vfsgid_eq_kgid - check whether kgid and vfsgid have the same value
 * @vfsgid: the vfsgid to compare
 * @kgid: the kgid to compare
 *
 * Check whether @vfsgid and @kgid have the same values.
 *
 * Return: true if @vfsgid and @kgid have the same value, false if not.
 * Comparison between two invalid gids returns false.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsgid_eq_kgid(vfsgid_t vfsgid, kgid_t kgid)
{
 return vfsgid_valid(vfsgid) && __vfsgid_val(vfsgid) == __kgid_val(kgid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsuid_gt_kuid(vfsuid_t vfsuid, kuid_t kuid)
{
 return __vfsuid_val(vfsuid) > __kuid_val(kuid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsgid_gt_kgid(vfsgid_t vfsgid, kgid_t kgid)
{
 return __vfsgid_val(vfsgid) > __kgid_val(kgid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsuid_lt_kuid(vfsuid_t vfsuid, kuid_t kuid)
{
 return __vfsuid_val(vfsuid) < __kuid_val(kuid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsgid_lt_kgid(vfsgid_t vfsgid, kgid_t kgid)
{
 return __vfsgid_val(vfsgid) < __kgid_val(kgid);
}

/*
 * vfs{g,u}ids are created from k{g,u}ids.
 * We don't allow them to be created from regular {u,g}id.
 */






/*
 * Allow a vfs{g,u}id to be used as a k{g,u}id where we want to compare
 * whether the mapped value is identical to value of a k{g,u}id.
 */




/**
 * vfsgid_in_group_p() - check whether a vfsuid matches the caller's groups
 * @vfsgid: the mnt gid to match
 *
 * This function can be used to determine whether @vfsuid matches any of the
 * caller's groups.
 *
 * Return: 1 if vfsuid matches caller's groups, 0 if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vfsgid_in_group_p(vfsgid_t vfsgid)
{
 return in_group_p((kgid_t){ __vfsgid_val(vfsgid) });
}







/**
 * initial_idmapping - check whether this is the initial mapping
 * @ns: idmapping to check
 *
 * Check whether this is the initial mapping, mapping 0 to 0, 1 to 1,
 * [...], 1000 to 1000 [...].
 *
 * Return: true if this is the initial mapping, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool initial_idmapping(const struct user_namespace *ns)
{
 return ns == &init_user_ns;
}

/**
 * no_idmapping - check whether we can skip remapping a kuid/gid
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 *
 * This function can be used to check whether a remapping between two
 * idmappings is required.
 * An idmapped mount is a mount that has an idmapping attached to it that
 * is different from the filsystem's idmapping and the initial idmapping.
 * If the initial mapping is used or the idmapping of the mount and the
 * filesystem are identical no remapping is required.
 *
 * Return: true if remapping can be skipped, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool no_idmapping(const struct user_namespace *mnt_userns,
    const struct user_namespace *fs_userns)
{
 return initial_idmapping(mnt_userns) || mnt_userns == fs_userns;
}

/**
 * make_vfsuid - map a filesystem kuid into a mnt_userns
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 * @kuid : kuid to be mapped
 *
 * Take a @kuid and remap it from @fs_userns into @mnt_userns. Use this
 * function when preparing a @kuid to be reported to userspace.
 *
 * If no_idmapping() determines that this is not an idmapped mount we can
 * simply return @kuid unchanged.
 * If initial_idmapping() tells us that the filesystem is not mounted with an
 * idmapping we know the value of @kuid won't change when calling
 * from_kuid() so we can simply retrieve the value via __kuid_val()
 * directly.
 *
 * Return: @kuid mapped according to @mnt_userns.
 * If @kuid has no mapping in either @mnt_userns or @fs_userns INVALID_UID is
 * returned.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vfsuid_t make_vfsuid(struct user_namespace *mnt_userns,
       struct user_namespace *fs_userns,
       kuid_t kuid)
{
 uid_t uid;

 if (no_idmapping(mnt_userns, fs_userns))
  return (vfsuid_t){ __kuid_val(kuid) };
 if (initial_idmapping(fs_userns))
  uid = __kuid_val(kuid);
 else
  uid = from_kuid(fs_userns, kuid);
 if (uid == (uid_t)-1)
  return (vfsuid_t){ __kuid_val((kuid_t){ -1 }) };
 return (vfsuid_t){ __kuid_val(make_kuid(mnt_userns, uid)) };
}

/**
 * make_vfsgid - map a filesystem kgid into a mnt_userns
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 * @kgid : kgid to be mapped
 *
 * Take a @kgid and remap it from @fs_userns into @mnt_userns. Use this
 * function when preparing a @kgid to be reported to userspace.
 *
 * If no_idmapping() determines that this is not an idmapped mount we can
 * simply return @kgid unchanged.
 * If initial_idmapping() tells us that the filesystem is not mounted with an
 * idmapping we know the value of @kgid won't change when calling
 * from_kgid() so we can simply retrieve the value via __kgid_val()
 * directly.
 *
 * Return: @kgid mapped according to @mnt_userns.
 * If @kgid has no mapping in either @mnt_userns or @fs_userns INVALID_GID is
 * returned.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vfsgid_t make_vfsgid(struct user_namespace *mnt_userns,
       struct user_namespace *fs_userns,
       kgid_t kgid)
{
 gid_t gid;

 if (no_idmapping(mnt_userns, fs_userns))
  return (vfsgid_t){ __kgid_val(kgid) };
 if (initial_idmapping(fs_userns))
  gid = __kgid_val(kgid);
 else
  gid = from_kgid(fs_userns, kgid);
 if (gid == (gid_t)-1)
  return (vfsgid_t){ __kgid_val((kgid_t){ -1 }) };
 return (vfsgid_t){ __kgid_val(make_kgid(mnt_userns, gid)) };
}

/**
 * from_vfsuid - map a vfsuid into the filesystem idmapping
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 * @vfsuid : vfsuid to be mapped
 *
 * Map @vfsuid into the filesystem idmapping. This function has to be used in
 * order to e.g. write @vfsuid to inode->i_uid.
 *
 * Return: @vfsuid mapped into the filesystem idmapping
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kuid_t from_vfsuid(struct user_namespace *mnt_userns,
     struct user_namespace *fs_userns,
     vfsuid_t vfsuid)
{
 uid_t uid;

 if (no_idmapping(mnt_userns, fs_userns))
  return (kuid_t){ __vfsuid_val(vfsuid) };
 uid = from_kuid(mnt_userns, (kuid_t){ __vfsuid_val(vfsuid) });
 if (uid == (uid_t)-1)
  return (kuid_t){ -1 };
 if (initial_idmapping(fs_userns))
  return (kuid_t){ uid };
 return make_kuid(fs_userns, uid);
}

/**
 * vfsuid_has_fsmapping - check whether a vfsuid maps into the filesystem
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 * @vfsuid: vfsuid to be mapped
 *
 * Check whether @vfsuid has a mapping in the filesystem idmapping. Use this
 * function to check whether the filesystem idmapping has a mapping for
 * @vfsuid.
 *
 * Return: true if @vfsuid has a mapping in the filesystem, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsuid_has_fsmapping(struct user_namespace *mnt_userns,
     struct user_namespace *fs_userns,
     vfsuid_t vfsuid)
{
 return uid_valid(from_vfsuid(mnt_userns, fs_userns, vfsuid));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsuid_has_mapping(struct user_namespace *userns,
          vfsuid_t vfsuid)
{
 return from_kuid(userns, (kuid_t){ __vfsuid_val(vfsuid) }) != (uid_t)-1;
}

/**
 * vfsuid_into_kuid - convert vfsuid into kuid
 * @vfsuid: the vfsuid to convert
 *
 * This can be used when a vfsuid is committed as a kuid.
 *
 * Return: a kuid with the value of @vfsuid
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kuid_t vfsuid_into_kuid(vfsuid_t vfsuid)
{
 return (kuid_t){ __vfsuid_val(vfsuid) };
}

/**
 * from_vfsgid - map a vfsgid into the filesystem idmapping
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 * @vfsgid : vfsgid to be mapped
 *
 * Map @vfsgid into the filesystem idmapping. This function has to be used in
 * order to e.g. write @vfsgid to inode->i_gid.
 *
 * Return: @vfsgid mapped into the filesystem idmapping
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kgid_t from_vfsgid(struct user_namespace *mnt_userns,
     struct user_namespace *fs_userns,
     vfsgid_t vfsgid)
{
 gid_t gid;

 if (no_idmapping(mnt_userns, fs_userns))
  return (kgid_t){ __vfsgid_val(vfsgid) };
 gid = from_kgid(mnt_userns, (kgid_t){ __vfsgid_val(vfsgid) });
 if (gid == (gid_t)-1)
  return (kgid_t){ -1 };
 if (initial_idmapping(fs_userns))
  return (kgid_t){ gid };
 return make_kgid(fs_userns, gid);
}

/**
 * vfsgid_has_fsmapping - check whether a vfsgid maps into the filesystem
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 * @vfsgid: vfsgid to be mapped
 *
 * Check whether @vfsgid has a mapping in the filesystem idmapping. Use this
 * function to check whether the filesystem idmapping has a mapping for
 * @vfsgid.
 *
 * Return: true if @vfsgid has a mapping in the filesystem, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsgid_has_fsmapping(struct user_namespace *mnt_userns,
     struct user_namespace *fs_userns,
     vfsgid_t vfsgid)
{
 return gid_valid(from_vfsgid(mnt_userns, fs_userns, vfsgid));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vfsgid_has_mapping(struct user_namespace *userns,
          vfsgid_t vfsgid)
{
 return from_kgid(userns, (kgid_t){ __vfsgid_val(vfsgid) }) != (gid_t)-1;
}

/**
 * vfsgid_into_kgid - convert vfsgid into kgid
 * @vfsgid: the vfsgid to convert
 *
 * This can be used when a vfsgid is committed as a kgid.
 *
 * Return: a kgid with the value of @vfsgid
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kgid_t vfsgid_into_kgid(vfsgid_t vfsgid)
{
 return (kgid_t){ __vfsgid_val(vfsgid) };
}

/**
 * mapped_fsuid - return caller's fsuid mapped up into a mnt_userns
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 *
 * Use this helper to initialize a new vfs or filesystem object based on
 * the caller's fsuid. A common example is initializing the i_uid field of
 * a newly allocated inode triggered by a creation event such as mkdir or
 * O_CREAT. Other examples include the allocation of quotas for a specific
 * user.
 *
 * Return: the caller's current fsuid mapped up according to @mnt_userns.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kuid_t mapped_fsuid(struct user_namespace *mnt_userns,
      struct user_namespace *fs_userns)
{
 return from_vfsuid(mnt_userns, fs_userns,
      (vfsuid_t){ __kuid_val((({ ({ do { } while (0 && (!((1)))); ; ((typeof(*(get_current()->cred)) *)((get_current()->cred))); })->fsuid; }))) });
}

/**
 * mapped_fsgid - return caller's fsgid mapped up into a mnt_userns
 * @mnt_userns: the mount's idmapping
 * @fs_userns: the filesystem's idmapping
 *
 * Use this helper to initialize a new vfs or filesystem object based on
 * the caller's fsgid. A common example is initializing the i_gid field of
 * a newly allocated inode triggered by a creation event such as mkdir or
 * O_CREAT. Other examples include the allocation of quotas for a specific
 * user.
 *
 * Return: the caller's current fsgid mapped up according to @mnt_userns.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) kgid_t mapped_fsgid(struct user_namespace *mnt_userns,
      struct user_namespace *fs_userns)
{
 return from_vfsgid(mnt_userns, fs_userns,
      (vfsgid_t){ __kgid_val((({ ({ do { } while (0 && (!((1)))); ; ((typeof(*(get_current()->cred)) *)((get_current()->cred))); })->fsgid; }))) });
}
# 45 "./include/linux/fs.h" 2
# 1 "./include/linux/slab.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
 *
 * (C) SGI 2006, Christoph Lameter
 * 	Cleaned up and restructured to ease the addition of alternative
 * 	implementations of SLAB allocators.
 * (C) Linux Foundation 2008-2013
 *      Unified interface for all slab allocators
 */





# 1 "./include/linux/overflow.h" 1
/* SPDX-License-Identifier: GPL-2.0 OR MIT */







/*
 * We need to compute the minimum and maximum values representable in a given
 * type. These macros may also be useful elsewhere. It would seem more obvious
 * to do something like:
 *
 * #define type_min(T) (T)(is_signed_type(T) ? (T)1 << (8*sizeof(T)-1) : 0)
 * #define type_max(T) (T)(is_signed_type(T) ? ((T)1 << (8*sizeof(T)-1)) - 1 : ~(T)0)
 *
 * Unfortunately, the middle expressions, strictly speaking, have
 * undefined behaviour, and at least some versions of gcc warn about
 * the type_max expression (but not if -fsanitize=undefined is in
 * effect; in that case, the warning is deferred to runtime...).
 *
 * The slightly excessive casting in type_min is to make sure the
 * macros also produce sensible values for the exotic type _Bool. [The
 * overflow checkers only almost work for _Bool, but that's
 * a-feature-not-a-bug, since people shouldn't be doing arithmetic on
 * _Bools. Besides, the gcc builtins don't allow _Bool* as third
 * argument.]
 *
 * Idea stolen from
 * https://mail-index.netbsd.org/tech-misc/2007/02/05/0000.html -
 * credit to Christian Biere.
 */




/*
 * Avoids triggering -Wtype-limits compilation warning,
 * while using unsigned data types to check a < 0.
 */



/*
 * Allows for effectively applying __must_check to a macro so we can have
 * both the type-agnostic benefits of the macros while also being able to
 * enforce that the return value is, in fact, checked.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __attribute__((__warn_unused_result__)) __must_check_overflow(bool overflow)
{
 return __builtin_expect(!!(overflow), 0);
}

/**
 * check_add_overflow() - Calculate addition with overflow checking
 * @a: first addend
 * @b: second addend
 * @d: pointer to store sum
 *
 * Returns 0 on success.
 *
 * *@d holds the results of the attempted addition, but is not considered
 * "safe for use" on a non-zero return value, which indicates that the
 * sum has overflowed or been truncated.
 */



/**
 * check_sub_overflow() - Calculate subtraction with overflow checking
 * @a: minuend; value to subtract from
 * @b: subtrahend; value to subtract from @a
 * @d: pointer to store difference
 *
 * Returns 0 on success.
 *
 * *@d holds the results of the attempted subtraction, but is not considered
 * "safe for use" on a non-zero return value, which indicates that the
 * difference has underflowed or been truncated.
 */



/**
 * check_mul_overflow() - Calculate multiplication with overflow checking
 * @a: first factor
 * @b: second factor
 * @d: pointer to store product
 *
 * Returns 0 on success.
 *
 * *@d holds the results of the attempted multiplication, but is not
 * considered "safe for use" on a non-zero return value, which indicates
 * that the product has overflowed or been truncated.
 */



/**
 * check_shl_overflow() - Calculate a left-shifted value and check overflow
 * @a: Value to be shifted
 * @s: How many bits left to shift
 * @d: Pointer to where to store the result
 *
 * Computes *@d = (@a << @s)
 *
 * Returns true if '*@d' cannot hold the result or when '@a << @s' doesn't
 * make sense. Example conditions:
 *
 * - '@a << @s' causes bits to be lost when stored in *@d.
 * - '@s' is garbage (e.g. negative) or so large that the result of
 *   '@a << @s' is guaranteed to be 0.
 * - '@a' is negative.
 * - '@a << @s' sets the sign bit, if any, in '*@d'.
 *
 * '*@d' will hold the results of the attempted shift, but is not
 * considered "safe for use" if true is returned.
 */
# 143 "./include/linux/overflow.h"
/**
 * overflows_type - helper for checking the overflows between value, variables,
 *		    or data type
 *
 * @n: source constant value or variable to be checked
 * @T: destination variable or data type proposed to store @x
 *
 * Compares the @x expression for whether or not it can safely fit in
 * the storage of the type in @T. @x and @T can have different types.
 * If @x is a constant expression, this will also resolve to a constant
 * expression.
 *
 * Returns: true if overflow can occur, false otherwise.
 */





/**
 * castable_to_type - like __same_type(), but also allows for casted literals
 *
 * @n: variable or constant value
 * @T: variable or data type
 *
 * Unlike the __same_type() macro, this allows a constant value as the
 * first argument. If this value would not overflow into an assignment
 * of the second argument's type, it returns true. Otherwise, this falls
 * back to __same_type().
 */





/**
 * size_mul() - Calculate size_t multiplication with saturation at SIZE_MAX
 * @factor1: first factor
 * @factor2: second factor
 *
 * Returns: calculate @factor1 * @factor2, both promoted to size_t,
 * with any overflow causing the return value to be SIZE_MAX. The
 * lvalue must be size_t to avoid implicit type conversion.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t __attribute__((__warn_unused_result__)) size_mul(size_t factor1, size_t factor2)
{
 size_t bytes;

 if (__must_check_overflow(__builtin_mul_overflow(factor1, factor2, &bytes)))
  return (~(size_t)0);

 return bytes;
}

/**
 * size_add() - Calculate size_t addition with saturation at SIZE_MAX
 * @addend1: first addend
 * @addend2: second addend
 *
 * Returns: calculate @addend1 + @addend2, both promoted to size_t,
 * with any overflow causing the return value to be SIZE_MAX. The
 * lvalue must be size_t to avoid implicit type conversion.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t __attribute__((__warn_unused_result__)) size_add(size_t addend1, size_t addend2)
{
 size_t bytes;

 if (__must_check_overflow(__builtin_add_overflow(addend1, addend2, &bytes)))
  return (~(size_t)0);

 return bytes;
}

/**
 * size_sub() - Calculate size_t subtraction with saturation at SIZE_MAX
 * @minuend: value to subtract from
 * @subtrahend: value to subtract from @minuend
 *
 * Returns: calculate @minuend - @subtrahend, both promoted to size_t,
 * with any overflow causing the return value to be SIZE_MAX. For
 * composition with the size_add() and size_mul() helpers, neither
 * argument may be SIZE_MAX (or the result with be forced to SIZE_MAX).
 * The lvalue must be size_t to avoid implicit type conversion.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t __attribute__((__warn_unused_result__)) size_sub(size_t minuend, size_t subtrahend)
{
 size_t bytes;

 if (minuend == (~(size_t)0) || subtrahend == (~(size_t)0) ||
     __must_check_overflow(__builtin_sub_overflow(minuend, subtrahend, &bytes)))
  return (~(size_t)0);

 return bytes;
}

/**
 * array_size() - Calculate size of 2-dimensional array.
 * @a: dimension one
 * @b: dimension two
 *
 * Calculates size of 2-dimensional array: @a * @b.
 *
 * Returns: number of bytes needed to represent the array or SIZE_MAX on
 * overflow.
 */


/**
 * array3_size() - Calculate size of 3-dimensional array.
 * @a: dimension one
 * @b: dimension two
 * @c: dimension three
 *
 * Calculates size of 3-dimensional array: @a * @b * @c.
 *
 * Returns: number of bytes needed to represent the array or SIZE_MAX on
 * overflow.
 */


/**
 * flex_array_size() - Calculate size of a flexible array member
 *                     within an enclosing structure.
 * @p: Pointer to the structure.
 * @member: Name of the flexible array member.
 * @count: Number of elements in the array.
 *
 * Calculates size of a flexible array of @count number of @member
 * elements, at the end of structure @p.
 *
 * Return: number of bytes needed or SIZE_MAX on overflow.
 */





/**
 * struct_size() - Calculate size of structure with trailing flexible array.
 * @p: Pointer to the structure.
 * @member: Name of the array member.
 * @count: Number of elements in the array.
 *
 * Calculates size of memory needed for structure @p followed by an
 * array of @count number of @member elements.
 *
 * Return: number of bytes needed or SIZE_MAX on overflow.
 */
# 17 "./include/linux/slab.h" 2


# 1 "./include/linux/percpu-refcount.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Percpu refcounts:
 * (C) 2012 Google, Inc.
 * Author: Kent Overstreet <koverstreet@google.com>
 *
 * This implements a refcount with similar semantics to atomic_t - atomic_inc(),
 * atomic_dec_and_test() - but percpu.
 *
 * There's one important difference between percpu refs and normal atomic_t
 * refcounts; you have to keep track of your initial refcount, and then when you
 * start shutting down you call percpu_ref_kill() _before_ dropping the initial
 * refcount.
 *
 * The refcount will have a range of 0 to ((1U << 31) - 1), i.e. one bit less
 * than an atomic_t - this is because of the way shutdown works, see
 * percpu_ref_kill()/PERCPU_COUNT_BIAS.
 *
 * Before you call percpu_ref_kill(), percpu_ref_put() does not check for the
 * refcount hitting 0 - it can't, if it was in percpu mode. percpu_ref_kill()
 * puts the ref back in single atomic_t mode, collecting the per cpu refs and
 * issuing the appropriate barriers, and then marks the ref as shutting down so
 * that percpu_ref_put() will check for the ref hitting 0.  After it returns,
 * it's safe to drop the initial ref.
 *
 * USAGE:
 *
 * See fs/aio.c for some example usage; it's used there for struct kioctx, which
 * is created when userspaces calls io_setup(), and destroyed when userspace
 * calls io_destroy() or the process exits.
 *
 * In the aio code, kill_ioctx() is called when we wish to destroy a kioctx; it
 * removes the kioctx from the proccess's table of kioctxs and kills percpu_ref.
 * After that, there can't be any new users of the kioctx (from lookup_ioctx())
 * and it's then safe to drop the initial ref with percpu_ref_put().
 *
 * Note that the free path, free_ioctx(), needs to go through explicit call_rcu()
 * to synchronize with RCU protected lookup_ioctx().  percpu_ref operations don't
 * imply RCU grace periods of any kind and if a user wants to combine percpu_ref
 * with RCU protection, it must be done explicitly.
 *
 * Code that does a two stage shutdown like this often needs some kind of
 * explicit synchronization to ensure the initial refcount can only be dropped
 * once - percpu_ref_kill() does this for you, it returns true once and false if
 * someone else already called it. The aio code uses it this way, but it's not
 * necessary if the code has some other mechanism to synchronize teardown.
 * around.
 */
# 59 "./include/linux/percpu-refcount.h"
struct percpu_ref;
typedef void (percpu_ref_func_t)(struct percpu_ref *);

/* flags set in the lower bits of percpu_ref->percpu_count_ptr */
enum {
 __PERCPU_REF_ATOMIC = 1LU << 0, /* operating in atomic mode */
 __PERCPU_REF_DEAD = 1LU << 1, /* (being) killed */
 __PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,

 __PERCPU_REF_FLAG_BITS = 2,
};

/* @flags for percpu_ref_init() */
enum {
 /*
	 * Start w/ ref == 1 in atomic mode.  Can be switched to percpu
	 * operation using percpu_ref_switch_to_percpu().  If initialized
	 * with this flag, the ref will stay in atomic mode until
	 * percpu_ref_switch_to_percpu() is invoked on it.
	 * Implies ALLOW_REINIT.
	 */
 PERCPU_REF_INIT_ATOMIC = 1 << 0,

 /*
	 * Start dead w/ ref == 0 in atomic mode.  Must be revived with
	 * percpu_ref_reinit() before used.  Implies INIT_ATOMIC and
	 * ALLOW_REINIT.
	 */
 PERCPU_REF_INIT_DEAD = 1 << 1,

 /*
	 * Allow switching from atomic mode to percpu mode.
	 */
 PERCPU_REF_ALLOW_REINIT = 1 << 2,
};

struct percpu_ref_data {
 atomic_long_t count;
 percpu_ref_func_t *release;
 percpu_ref_func_t *confirm_switch;
 bool force_atomic:1;
 bool allow_reinit:1;
 struct callback_head rcu;
 struct percpu_ref *ref;
};

struct percpu_ref {
 /*
	 * The low bit of the pointer indicates whether the ref is in percpu
	 * mode; if set, then get/put will manipulate the atomic_t.
	 */
 unsigned long percpu_count_ptr;

 /*
	 * 'percpu_ref' is often embedded into user structure, and only
	 * 'percpu_count_ptr' is required in fast path, move other fields
	 * into 'percpu_ref_data', so we can reduce memory footprint in
	 * fast path.
	 */
 struct percpu_ref_data *data;
};

int __attribute__((__warn_unused_result__)) percpu_ref_init(struct percpu_ref *ref,
     percpu_ref_func_t *release, unsigned int flags,
     gfp_t gfp);
void percpu_ref_exit(struct percpu_ref *ref);
void percpu_ref_switch_to_atomic(struct percpu_ref *ref,
     percpu_ref_func_t *confirm_switch);
void percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref);
void percpu_ref_switch_to_percpu(struct percpu_ref *ref);
void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
     percpu_ref_func_t *confirm_kill);
void percpu_ref_resurrect(struct percpu_ref *ref);
void percpu_ref_reinit(struct percpu_ref *ref);
bool percpu_ref_is_zero(struct percpu_ref *ref);

/**
 * percpu_ref_kill - drop the initial ref
 * @ref: percpu_ref to kill
 *
 * Must be used to drop the initial ref on a percpu refcount; must be called
 * precisely once before shutdown.
 *
 * Switches @ref into atomic mode before gathering up the percpu counters
 * and dropping the initial ref.
 *
 * There are no implied RCU grace periods between kill and release.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_ref_kill(struct percpu_ref *ref)
{
 percpu_ref_kill_and_confirm(ref, ((void *)0));
}

/*
 * Internal helper.  Don't use outside percpu-refcount proper.  The
 * function doesn't return the pointer and let the caller test it for NULL
 * because doing so forces the compiler to generate two conditional
 * branches as it can't assume that @ref->percpu_count is not NULL.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __ref_is_percpu(struct percpu_ref *ref,
       unsigned long /* nothing */ **percpu_countp)
{
 unsigned long percpu_ptr;

 /*
	 * The value of @ref->percpu_count_ptr is tested for
	 * !__PERCPU_REF_ATOMIC, which may be set asynchronously, and then
	 * used as a pointer.  If the compiler generates a separate fetch
	 * when using it as a pointer, __PERCPU_REF_ATOMIC may be set in
	 * between contaminating the pointer value, meaning that
	 * READ_ONCE() is required when fetching it.
	 *
	 * The dependency ordering from the READ_ONCE() pairs
	 * with smp_store_release() in __percpu_ref_switch_to_percpu().
	 */
 percpu_ptr = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_311(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ref->percpu_count_ptr) == sizeof(char) || sizeof(ref->percpu_count_ptr) == sizeof(short) || sizeof(ref->percpu_count_ptr) == sizeof(int) || sizeof(ref->percpu_count_ptr) == sizeof(long)) || sizeof(ref->percpu_count_ptr) == sizeof(long long))) __compiletime_assert_311(); } while (0); (*(const volatile typeof( _Generic((ref->percpu_count_ptr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (ref->percpu_count_ptr))) *)&(ref->percpu_count_ptr)); });
# 176 "./include/linux/percpu-refcount.h"
 /*
	 * Theoretically, the following could test just ATOMIC; however,
	 * then we'd have to mask off DEAD separately as DEAD may be
	 * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
	 * implies ATOMIC anyway.  Test them together.
	 */
 if (__builtin_expect(!!(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD), 0))
  return false;

 *percpu_countp = (unsigned long /* nothing */ *)percpu_ptr;
 return true;
}

/**
 * percpu_ref_get_many - increment a percpu refcount
 * @ref: percpu_ref to get
 * @nr: number of references to get
 *
 * Analogous to atomic_long_add().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
{
 unsigned long /* nothing */ *percpu_count;

 rcu_read_lock();

 if (__ref_is_percpu(ref, &percpu_count))
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
 else
  atomic_long_add(nr, &ref->data->count);

 rcu_read_unlock();
}

/**
 * percpu_ref_get - increment a percpu refcount
 * @ref: percpu_ref to get
 *
 * Analogous to atomic_long_inc().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_ref_get(struct percpu_ref *ref)
{
 percpu_ref_get_many(ref, 1);
}

/**
 * percpu_ref_tryget_many - try to increment a percpu refcount
 * @ref: percpu_ref to try-get
 * @nr: number of references to get
 *
 * Increment a percpu refcount  by @nr unless its count already reached zero.
 * Returns %true on success; %false on failure.
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_ref_tryget_many(struct percpu_ref *ref,
       unsigned long nr)
{
 unsigned long /* nothing */ *percpu_count;
 bool ret;

 rcu_read_lock();

 if (__ref_is_percpu(ref, &percpu_count)) {
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), nr); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
  ret = true;
 } else {
  ret = atomic_long_add_unless(&ref->data->count, nr, 0);
 }

 rcu_read_unlock();

 return ret;
}

/**
 * percpu_ref_tryget - try to increment a percpu refcount
 * @ref: percpu_ref to try-get
 *
 * Increment a percpu refcount unless its count already reached zero.
 * Returns %true on success; %false on failure.
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_ref_tryget(struct percpu_ref *ref)
{
 return percpu_ref_tryget_many(ref, 1);
}

/**
 * percpu_ref_tryget_live_rcu - same as percpu_ref_tryget_live() but the
 * caller is responsible for taking RCU.
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_ref_tryget_live_rcu(struct percpu_ref *ref)
{
 unsigned long /* nothing */ *percpu_count;
 bool ret = false;

 ({ int __ret_warn_on = !!(!rcu_read_lock_held()); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/percpu-refcount.h\"; .popsection; .long 14472b - .; .short 280; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });

 if (__builtin_expect(!!(__ref_is_percpu(ref, &percpu_count)), 1)) {
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
  ret = true;
 } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
  ret = atomic_long_inc_not_zero(&ref->data->count);
 }
 return ret;
}

/**
 * percpu_ref_tryget_live - try to increment a live percpu refcount
 * @ref: percpu_ref to try-get
 *
 * Increment a percpu refcount unless it has already been killed.  Returns
 * %true on success; %false on failure.
 *
 * Completion of percpu_ref_kill() in itself doesn't guarantee that this
 * function will fail.  For such guarantee, percpu_ref_kill_and_confirm()
 * should be used.  After the confirm_kill callback is invoked, it's
 * guaranteed that no new reference will be given out by
 * percpu_ref_tryget_live().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_ref_tryget_live(struct percpu_ref *ref)
{
 bool ret = false;

 rcu_read_lock();
 ret = percpu_ref_tryget_live_rcu(ref);
 rcu_read_unlock();
 return ret;
}

/**
 * percpu_ref_put_many - decrement a percpu refcount
 * @ref: percpu_ref to put
 * @nr: number of references to put
 *
 * Decrement the refcount, and if 0, call the release function (which was passed
 * to percpu_ref_init())
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
{
 unsigned long /* nothing */ *percpu_count;

 rcu_read_lock();

 if (__ref_is_percpu(ref, &percpu_count))
  do { do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*percpu_count))(nr)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*percpu_count))(nr)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*percpu_count))(nr)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__kern_my_cpu_offset()))); }); }), -(typeof(*percpu_count))(nr)); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
 else if (__builtin_expect(!!(atomic_long_sub_and_test(nr, &ref->data->count)), 0))
  ref->data->release(ref);

 rcu_read_unlock();
}

/**
 * percpu_ref_put - decrement a percpu refcount
 * @ref: percpu_ref to put
 *
 * Decrement the refcount, and if 0, call the release function (which was passed
 * to percpu_ref_init())
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void percpu_ref_put(struct percpu_ref *ref)
{
 percpu_ref_put_many(ref, 1);
}

/**
 * percpu_ref_is_dying - test whether a percpu refcount is dying or dead
 * @ref: percpu_ref to test
 *
 * Returns %true if @ref is dying or dead.
 *
 * This function is safe to call as long as @ref is between init and exit
 * and the caller is responsible for synchronizing against state changes.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool percpu_ref_is_dying(struct percpu_ref *ref)
{
 return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
}
# 20 "./include/linux/slab.h" 2


/*
 * Flags to pass to kmem_cache_create().
 * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
 */
/* DEBUG: Perform (expensive) checks on alloc/free */

/* DEBUG: Red zone objs in a cache */

/* DEBUG: Poison objects */

/* Indicate a kmalloc slab */

/* Align objs on cache lines */

/* Use GFP_DMA memory */

/* Use GFP_DMA32 memory */

/* DEBUG: Store the last owner for bug hunting */

/* Panic if kmem_cache_create() fails */

/*
 * SLAB_TYPESAFE_BY_RCU - **WARNING** READ THIS!
 *
 * This delays freeing the SLAB page by a grace period, it does _NOT_
 * delay object freeing. This means that if you do kmem_cache_free()
 * that memory location is free to be reused at any time. Thus it may
 * be possible to see another object there in the same RCU grace period.
 *
 * This feature only ensures the memory location backing the object
 * stays valid, the trick to using this is relying on an independent
 * object validation pass. Something like:
 *
 *  rcu_read_lock()
 * again:
 *  obj = lockless_lookup(key);
 *  if (obj) {
 *    if (!try_get_ref(obj)) // might fail for free objects
 *      goto again;
 *
 *    if (obj->key != key) { // not the object we expected
 *      put_ref(obj);
 *      goto again;
 *    }
 *  }
 *  rcu_read_unlock();
 *
 * This is useful if we need to approach a kernel structure obliquely,
 * from its address obtained without the usual locking. We can lock
 * the structure to stabilize it and check it's still at the given address,
 * only if we can be sure that the memory has not been meanwhile reused
 * for some other kind of object (which our subsystem's lock might corrupt).
 *
 * rcu_read_lock before reading the address, then rcu_read_unlock after
 * taking the spinlock within the structure expected at that address.
 *
 * Note that it is not possible to acquire a lock within a structure
 * allocated with SLAB_TYPESAFE_BY_RCU without first acquiring a reference
 * as described above.  The reason is that SLAB_TYPESAFE_BY_RCU pages
 * are not zeroed before being given to the slab, which means that any
 * locks must be initialized after each and every kmem_struct_alloc().
 * Alternatively, make the ctor passed to kmem_cache_create() initialize
 * the locks at page-allocation time, as is done in __i915_request_ctor(),
 * sighand_ctor(), and anon_vma_ctor().  Such a ctor permits readers
 * to safely acquire those ctor-initialized locks under rcu_read_lock()
 * protection.
 *
 * Note that SLAB_TYPESAFE_BY_RCU was originally named SLAB_DESTROY_BY_RCU.
 */
/* Defer freeing slabs to RCU */

/* Spread some memory over cpuset */

/* Trace allocations and frees */


/* Flag to prevent checks on free */






/* Avoid kmemleak tracing */


/* Fault injection mark */





/* Account to memcg */
# 128 "./include/linux/slab.h"
/*
 * Ignore user specified debugging flags.
 * Intended for caches created for self-tests so they have only flags
 * specified in the code and other flags are ignored.
 */
# 141 "./include/linux/slab.h"
/* The following flags affect the page allocator grouping pages by mobility */
/* Objects are reclaimable */







/*
 * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
 *
 * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
 *
 * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
 * Both make kfree a no-op.
 */





# 1 "./include/linux/kasan.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






# 1 "./include/linux/static_key.h" 1
# 9 "./include/linux/kasan.h" 2


struct kmem_cache;
struct page;
struct slab;
struct vm_struct;
struct task_struct;
# 24 "./include/linux/kasan.h"
typedef unsigned int kasan_vmalloc_flags_t;
# 74 "./include/linux/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kasan_add_zero_shadow(void *start, unsigned long size)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_remove_zero_shadow(void *start,
     unsigned long size)
{}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_enable_current(void) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_disable_current(void) {}
# 93 "./include/linux/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kasan_has_integrated_init(void)
{
 return kasan_hw_tags_enabled();
}
# 249 "./include/linux/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_unpoison_range(const void *address, size_t size) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_poison_pages(struct page *page, unsigned int order,
          bool init) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_unpoison_pages(struct page *page, unsigned int order,
     bool init) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_cache_create_kmalloc(struct kmem_cache *cache) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_poison_slab(struct slab *slab) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_unpoison_object_data(struct kmem_cache *cache,
     void *object) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_poison_object_data(struct kmem_cache *cache,
     void *object) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kasan_init_slab_obj(struct kmem_cache *cache,
    const void *object)
{
 return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kasan_slab_free(struct kmem_cache *s, void *object, bool init)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_kfree_large(void *ptr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_slab_free_mempool(void *ptr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kasan_slab_alloc(struct kmem_cache *s, void *object,
       gfp_t flags, bool init)
{
 return object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kasan_kmalloc(struct kmem_cache *s, const void *object,
    size_t size, gfp_t flags)
{
 return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
{
 return (void *)ptr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kasan_krealloc(const void *object, size_t new_size,
     gfp_t flags)
{
 return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kasan_check_byte(const void *address)
{
 return true;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_unpoison_task_stack(struct task_struct *task) {}
# 317 "./include/linux/kasan.h"
/* Tag-based KASAN modes do not use per-object metadata. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t kasan_metadata_size(struct kmem_cache *cache,
      bool in_object)
{
 return 0;
}
/* And thus nothing prevents cache merging. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) slab_flags_t kasan_never_merge(void)
{
 return 0;
}
/* And no cache-related metadata initialization is required. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_cache_create(struct kmem_cache *cache,
          unsigned int *size,
          slab_flags_t *flags) {}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_cache_shrink(struct kmem_cache *cache) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_cache_shutdown(struct kmem_cache *cache) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_record_aux_stack(void *ptr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_record_aux_stack_noalloc(void *ptr) {}
# 359 "./include/linux/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kasan_reset_tag(const void *addr)
{
 return (void *)addr;
}
# 375 "./include/linux/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_init_sw_tags(void) { }






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_init_hw_tags_cpu(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_init_hw_tags(void) { }
# 434 "./include/linux/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_populate_early_vm_area_shadow(void *start,
             unsigned long size) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kasan_populate_vmalloc(unsigned long start,
     unsigned long size)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_release_vmalloc(unsigned long start,
      unsigned long end,
      unsigned long free_region_start,
      unsigned long free_region_end) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kasan_unpoison_vmalloc(const void *start,
        unsigned long size,
        kasan_vmalloc_flags_t flags)
{
 return (void *)start;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_poison_vmalloc(const void *start, unsigned long size)
{ }
# 470 "./include/linux/kasan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kasan_alloc_module_shadow(void *addr, size_t size, gfp_t gfp_mask) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_free_module_shadow(const struct vm_struct *vm) {}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kasan_non_canonical_hook(unsigned long addr) { }
# 164 "./include/linux/slab.h" 2

struct list_lru;
struct mem_cgroup;
/*
 * struct kmem_cache related prototypes
 */
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) kmem_cache_init(void);
bool slab_is_available(void);

struct kmem_cache *kmem_cache_create(const char *name, unsigned int size,
   unsigned int align, slab_flags_t flags,
   void (*ctor)(void *));
struct kmem_cache *kmem_cache_create_usercopy(const char *name,
   unsigned int size, unsigned int align,
   slab_flags_t flags,
   unsigned int useroffset, unsigned int usersize,
   void (*ctor)(void *));
void kmem_cache_destroy(struct kmem_cache *s);
int kmem_cache_shrink(struct kmem_cache *s);

/*
 * Please use this macro to create slab caches. Simply specify the
 * name of the structure and maybe some flags that are listed above.
 *
 * The alignment of the struct determines object alignment. If you
 * f.e. add ____cacheline_aligned_in_smp to the struct declaration
 * then the objects will be properly aligned in SMP configurations.
 */




/*
 * To whitelist a single field for copying to/from usercopy, use this
 * macro instead for KMEM_CACHE() above.
 */







/*
 * Common kmalloc functions provided by all allocators
 */
void * __attribute__((__warn_unused_result__)) krealloc(const void *objp, size_t new_size, gfp_t flags) __attribute__((__alloc_size__(2)));
void kfree(const void *objp);
void kfree_sensitive(const void *objp);
size_t __ksize(const void *objp);

/**
 * ksize - Report actual allocation size of associated object
 *
 * @objp: Pointer returned from a prior kmalloc()-family allocation.
 *
 * This should not be used for writing beyond the originally requested
 * allocation size. Either use krealloc() or round up the allocation size
 * with kmalloc_size_roundup() prior to allocation. If this is used to
 * access beyond the originally requested allocation size, UBSAN_BOUNDS
 * and/or FORTIFY_SOURCE may trip, since they only know about the
 * originally allocated size via the __alloc_size attribute.
 */
size_t ksize(const void *objp);


bool kmem_valid_obj(void *object);
void kmem_dump_obj(void *object);


/*
 * Some archs want to perform DMA into kmalloc caches and need a guaranteed
 * alignment larger than the alignment of a 64-bit integer.
 * Setting ARCH_DMA_MINALIGN in arch headers allows that.
 */
# 247 "./include/linux/slab.h"
/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */




/*
 * Arches can define this function if they want to decide the minimum slab
 * alignment at runtime. The value returned by the function must be a power
 * of two and >= ARCH_SLAB_MINALIGN.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int arch_slab_minalign(void)
{
 return __alignof__(unsigned long long);
}


/*
 * kmem_cache_alloc and friends return pointers aligned to ARCH_SLAB_MINALIGN.
 * kmalloc and friends return pointers aligned to both ARCH_KMALLOC_MINALIGN
 * and ARCH_SLAB_MINALIGN, but here we only assume the former alignment.
 */




/*
 * Kmalloc array related definitions
 */
# 314 "./include/linux/slab.h"
/* Maximum allocatable size */

/* Maximum size for which we actually use a slab cache */

/* Maximum order allocatable via the slab allocator */


/*
 * Kmalloc subsystem.
 */




/*
 * This restriction comes from byte sized index implementation.
 * Page size is normally 2^12 bytes and, in this case, if we want to use
 * byte sized index which can represent 2^8 entries, the size of the object
 * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.
 * If minimum size of kmalloc is less than 16, we use it as minimum object
 * size and give up to use byte sized index.
 */



/*
 * Whenever changing this, take care of that kmalloc_type() and
 * create_kmalloc_caches() still work as intended.
 *
 * KMALLOC_NORMAL can contain only unaccounted objects whereas KMALLOC_CGROUP
 * is for accounted but unreclaimable and non-dma objects. All the other
 * kmem caches can have both accounted and unaccounted objects.
 */
enum kmalloc_cache_type {
 KMALLOC_NORMAL = 0,
# 358 "./include/linux/slab.h"
 KMALLOC_RECLAIM,


 KMALLOC_DMA,


 KMALLOC_CGROUP,

 NR_KMALLOC_TYPES
};


extern struct kmem_cache *
kmalloc_caches[NR_KMALLOC_TYPES][(12 + 1) + 1];

/*
 * Define gfp bits that should not be set for KMALLOC_NORMAL.
 */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) enum kmalloc_cache_type kmalloc_type(gfp_t flags)
{
 /*
	 * The most common case is KMALLOC_NORMAL, so test for it
	 * with a single branch for all the relevant flags.
	 */
 if (__builtin_expect(!!((flags & ((( gfp_t)0x10u) | (1 ? (( gfp_t)0x01u) : 0) | (1 ? (( gfp_t)0x400000u) : 0))) == 0), 1))
  return KMALLOC_NORMAL;

 /*
	 * At least one of the flags has to be set. Their priorities in
	 * decreasing order are:
	 *  1) __GFP_DMA
	 *  2) __GFP_RECLAIMABLE
	 *  3) __GFP_ACCOUNT
	 */
 if (1 && (flags & (( gfp_t)0x01u)))
  return KMALLOC_DMA;
 if (!1 || (flags & (( gfp_t)0x10u)))
  return KMALLOC_RECLAIM;
 else
  return KMALLOC_CGROUP;
}

/*
 * Figure out which kmalloc slab an allocation of a certain size
 * belongs to.
 * 0 = zero alloc
 * 1 =  65 .. 96 bytes
 * 2 = 129 .. 192 bytes
 * n = 2^(n-1)+1 .. 2^n
 *
 * Note: __kmalloc_index() is compile-time optimized, and not runtime optimized;
 * typical usage is via kmalloc_index() and therefore evaluated at compile-time.
 * Callers where !size_is_constant should only be test modules, where runtime
 * overheads of __kmalloc_index() can be tolerated.  Also see kmalloc_slab().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned int __kmalloc_index(size_t size,
          bool size_is_constant)
{
 if (!size)
  return 0;

 if (size <= (128))
  return ( __builtin_constant_p((128)) ? (((128)) < 2 ? 0 : 63 - __builtin_clzll((128))) : (sizeof((128)) <= 4) ? __ilog2_u32((128)) : __ilog2_u64((128)) );

 if ((128) <= 32 && size > 64 && size <= 96)
  return 1;
 if ((128) <= 64 && size > 128 && size <= 192)
  return 2;
 if (size <= 8) return 3;
 if (size <= 16) return 4;
 if (size <= 32) return 5;
 if (size <= 64) return 6;
 if (size <= 128) return 7;
 if (size <= 256) return 8;
 if (size <= 512) return 9;
 if (size <= 1024) return 10;
 if (size <= 2 * 1024) return 11;
 if (size <= 4 * 1024) return 12;
 if (size <= 8 * 1024) return 13;
 if (size <= 16 * 1024) return 14;
 if (size <= 32 * 1024) return 15;
 if (size <= 64 * 1024) return 16;
 if (size <= 128 * 1024) return 17;
 if (size <= 256 * 1024) return 18;
 if (size <= 512 * 1024) return 19;
 if (size <= 1024 * 1024) return 20;
 if (size <= 2 * 1024 * 1024) return 21;

 if (!0 && size_is_constant)
  do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_312(void) __attribute__((__error__("unexpected size in kmalloc_index()"))); if (!(!(1))) __compiletime_assert_312(); } while (0);
# 453 "./include/linux/slab.h"
 else
  do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/slab.h\"; .popsection; .long 14472b - .; .short 454; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0);

 /* Will never be reached. Needed because the compiler may complain */
 return -1;
}
_Static_assert(12 <= 20, "PAGE_SHIFT <= 20");



void *__kmalloc(size_t size, gfp_t flags) __attribute__((__assume_aligned__((128)))) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));

/**
 * kmem_cache_alloc - Allocate an object
 * @cachep: The cache to allocate from.
 * @flags: See kmalloc().
 *
 * Allocate an object from this cache.
 * See kmem_cache_zalloc() for a shortcut of adding __GFP_ZERO to flags.
 *
 * Return: pointer to the new object or %NULL in case of error
 */
void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags) __attribute__((__assume_aligned__(__alignof__(unsigned long long)))) __attribute__((__malloc__));
void *kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,
      gfp_t gfpflags) __attribute__((__assume_aligned__(__alignof__(unsigned long long)))) __attribute__((__malloc__));
void kmem_cache_free(struct kmem_cache *s, void *objp);

/*
 * Bulk allocation and freeing operations. These are accelerated in an
 * allocator specific way to avoid taking locks repeatedly or building
 * metadata structures unnecessarily.
 *
 * Note that interrupts must be enabled when calling these functions.
 */
void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p);
int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size, void **p);

/*
 * Caller must not use kfree_bulk() on memory not originally allocated
 * by kmalloc(), because the SLOB allocator cannot handle this.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void kfree_bulk(size_t size, void **p)
{
 kmem_cache_free_bulk(((void *)0), size, p);
}

void *__kmalloc_node(size_t size, gfp_t flags, int node) __attribute__((__assume_aligned__((128))))
        __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t flags, int node) __attribute__((__assume_aligned__(__alignof__(unsigned long long))))
          __attribute__((__malloc__));

void *kmalloc_trace(struct kmem_cache *s, gfp_t flags, size_t size)
      __attribute__((__assume_aligned__((128)))) __attribute__((__alloc_size__(3))) __attribute__((__malloc__));

void *kmalloc_node_trace(struct kmem_cache *s, gfp_t gfpflags,
    int node, size_t size) __attribute__((__assume_aligned__((128))))
      __attribute__((__alloc_size__(4))) __attribute__((__malloc__));
void *kmalloc_large(size_t size, gfp_t flags) __attribute__((__assume_aligned__(((1UL) << 12))))
           __attribute__((__alloc_size__(1))) __attribute__((__malloc__));

void *kmalloc_large_node(size_t size, gfp_t flags, int node) __attribute__((__assume_aligned__(((1UL) << 12))))
            __attribute__((__alloc_size__(1))) __attribute__((__malloc__));

/**
 * kmalloc - allocate kernel memory
 * @size: how many bytes of memory are required.
 * @flags: describe the allocation context
 *
 * kmalloc is the normal method of allocating memory
 * for objects smaller than page size in the kernel.
 *
 * The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN
 * bytes. For @size of power of two bytes, the alignment is also guaranteed
 * to be at least to the size.
 *
 * The @flags argument may be one of the GFP flags defined at
 * include/linux/gfp.h and described at
 * :ref:`Documentation/core-api/mm-api.rst <mm-api-gfp-flags>`
 *
 * The recommended usage of the @flags is described at
 * :ref:`Documentation/core-api/memory-allocation.rst <memory_allocation>`
 *
 * Below is a brief outline of the most useful GFP flags
 *
 * %GFP_KERNEL
 *	Allocate normal kernel ram. May sleep.
 *
 * %GFP_NOWAIT
 *	Allocation will not sleep.
 *
 * %GFP_ATOMIC
 *	Allocation will not sleep.  May use emergency pools.
 *
 * Also it is possible to set different flags by OR'ing
 * in one or more of the following additional @flags:
 *
 * %__GFP_ZERO
 *	Zero the allocated memory before returning. Also see kzalloc().
 *
 * %__GFP_HIGH
 *	This allocation has high priority and may use emergency pools.
 *
 * %__GFP_NOFAIL
 *	Indicate that this allocation is in no way allowed to fail
 *	(think twice before using).
 *
 * %__GFP_NORETRY
 *	If memory is not immediately available,
 *	then give up at once.
 *
 * %__GFP_NOWARN
 *	If allocation fails, don't issue any warnings.
 *
 * %__GFP_RETRY_MAYFAIL
 *	Try really hard to succeed the allocation but fail
 *	eventually.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kmalloc(size_t size, gfp_t flags)
{
 if (__builtin_constant_p(size) && size) {
  unsigned int index;

  if (size > (1UL << (12 + 1)))
   return kmalloc_large(size, flags);

  index = __kmalloc_index(size, true);
  return kmalloc_trace(
    kmalloc_caches[kmalloc_type(flags)][index],
    flags, size);
 }
 return __kmalloc(size, flags);
}
# 597 "./include/linux/slab.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kmalloc_node(size_t size, gfp_t flags, int node)
{
 if (__builtin_constant_p(size) && size) {
  unsigned int index;

  if (size > (1UL << (12 + 1)))
   return kmalloc_large_node(size, flags, node);

  index = __kmalloc_index(size, true);
  return kmalloc_node_trace(
    kmalloc_caches[kmalloc_type(flags)][index],
    flags, node, size);
 }
 return __kmalloc_node(size, flags, node);
}
# 622 "./include/linux/slab.h"
/**
 * kmalloc_array - allocate memory for an array.
 * @n: number of elements.
 * @size: element size.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kmalloc_array(size_t n, size_t size, gfp_t flags)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(n, size, &bytes))), 0))
  return ((void *)0);
 if (__builtin_constant_p(n) && __builtin_constant_p(size))
  return kmalloc(bytes, flags);
 return __kmalloc(bytes, flags);
}

/**
 * krealloc_array - reallocate memory for an array.
 * @p: pointer to the memory chunk to reallocate
 * @new_n: new number of elements to alloc
 * @new_size: new size of a single member of the array
 * @flags: the type of memory to allocate (see kmalloc)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(2, 3))) void * __attribute__((__warn_unused_result__)) krealloc_array(void *p,
              size_t new_n,
              size_t new_size,
              gfp_t flags)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(new_n, new_size, &bytes))), 0))
  return ((void *)0);

 return krealloc(p, bytes, flags);
}

/**
 * kcalloc - allocate memory for an array. The memory is set to zero.
 * @n: number of elements.
 * @size: element size.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kcalloc(size_t n, size_t size, gfp_t flags)
{
 return kmalloc_array(n, size, flags | (( gfp_t)0x100u));
}

void *__kmalloc_node_track_caller(size_t size, gfp_t flags, int node,
      unsigned long caller) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));




/*
 * kmalloc_track_caller is a special version of kmalloc that records the
 * calling function of the routine calling it for slab leak tracking instead
 * of just the calling function (confusing, eh?).
 * It's useful when the call to kmalloc comes from a widely-used standard
 * allocator where we care about the real place the memory allocation
 * request comes from.
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,
         int node)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(n, size, &bytes))), 0))
  return ((void *)0);
 if (__builtin_constant_p(n) && __builtin_constant_p(size))
  return kmalloc_node(bytes, flags, node);
 return __kmalloc_node(bytes, flags, node);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)
{
 return kmalloc_array_node(n, size, flags | (( gfp_t)0x100u), node);
}

/*
 * Shortcuts
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
{
 return kmem_cache_alloc(k, flags | (( gfp_t)0x100u));
}

/**
 * kzalloc - allocate memory. The memory is set to zero.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kzalloc(size_t size, gfp_t flags)
{
 return kmalloc(size, flags | (( gfp_t)0x100u));
}

/**
 * kzalloc_node - allocate zeroed memory from a particular memory node.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 * @node: memory node from which to allocate
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kzalloc_node(size_t size, gfp_t flags, int node)
{
 return kmalloc_node(size, flags | (( gfp_t)0x100u), node);
}

extern void *kvmalloc_node(size_t size, gfp_t flags, int node) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kvmalloc(size_t size, gfp_t flags)
{
 return kvmalloc_node(size, flags, (-1));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kvzalloc_node(size_t size, gfp_t flags, int node)
{
 return kvmalloc_node(size, flags | (( gfp_t)0x100u), node);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kvzalloc(size_t size, gfp_t flags)
{
 return kvmalloc(size, flags | (( gfp_t)0x100u));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kvmalloc_array(size_t n, size_t size, gfp_t flags)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(n, size, &bytes))), 0))
  return ((void *)0);

 return kvmalloc(bytes, flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kvcalloc(size_t n, size_t size, gfp_t flags)
{
 return kvmalloc_array(n, size, flags | (( gfp_t)0x100u));
}

extern void *kvrealloc(const void *p, size_t oldsize, size_t newsize, gfp_t flags)
        __attribute__((__alloc_size__(3)));
extern void kvfree(const void *addr);
extern void kvfree_sensitive(const void *addr, size_t len);

unsigned int kmem_cache_size(struct kmem_cache *s);

/**
 * kmalloc_size_roundup - Report allocation bucket size for the given size
 *
 * @size: Number of bytes to round up from.
 *
 * This returns the number of bytes that would be available in a kmalloc()
 * allocation of @size bytes. For example, a 126 byte request would be
 * rounded up to the next sized kmalloc bucket, 128 bytes. (This is strictly
 * for the general-purpose kmalloc()-based allocations, and is not for the
 * pre-sized kmem_cache_alloc()-based allocations.)
 *
 * Use this to kmalloc() the full bucket size ahead of time instead of using
 * ksize() to query the size after an allocation.
 */
size_t kmalloc_size_roundup(size_t size);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) kmem_cache_init_late(void);
# 46 "./include/linux/fs.h" 2


# 1 "./include/uapi/linux/fs.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/*
 * This file has definitions for some important file table structures
 * and constants and structures used by various generic file system
 * ioctl's.  Please do not make any changes in this file before
 * sending patches for review to linux-fsdevel@vger.kernel.org and
 * linux-api@vger.kernel.org.
 */
# 20 "./include/uapi/linux/fs.h"
/* Use of MS_* flags within the kernel is restricted to core mount(2) code. */




/*
 * It's silly to have NR_OPEN bigger than NR_FILE, but you can change
 * the file limit at runtime and only root can increase the per-process
 * nr_file rlimit, so it's safe to set up a ridiculously high absolute
 * upper limit on files-per-process.
 *
 * Some programs (notably those using select()) may have to be
 * recompiled to take full advantage of the new limits..
 */

/* Fixed constants first: */
# 54 "./include/uapi/linux/fs.h"
struct file_clone_range {
 __s64 src_fd;
 __u64 src_offset;
 __u64 src_length;
 __u64 dest_offset;
};

struct fstrim_range {
 __u64 start;
 __u64 len;
 __u64 minlen;
};

/* extent-same (dedupe) ioctls; these MUST match the btrfs ioctl definitions */



/* from struct btrfs_ioctl_file_extent_same_info */
struct file_dedupe_range_info {
 __s64 dest_fd; /* in - destination file */
 __u64 dest_offset; /* in - start of extent in destination */
 __u64 bytes_deduped; /* out - total # of bytes we were able
				 * to dedupe from this file. */
 /* status of this dedupe operation:
	 * < 0 for error
	 * == FILE_DEDUPE_RANGE_SAME if dedupe succeeds
	 * == FILE_DEDUPE_RANGE_DIFFERS if data differs
	 */
 __s32 status; /* out - see above description */
 __u32 reserved; /* must be zero */
};

/* from struct btrfs_ioctl_file_extent_same_args */
struct file_dedupe_range {
 __u64 src_offset; /* in - start of extent in source */
 __u64 src_length; /* in - length of extent */
 __u16 dest_count; /* in - total elements in info array */
 __u16 reserved1; /* must be zero */
 __u32 reserved2; /* must be zero */
 struct file_dedupe_range_info info[];
};

/* And dynamically-tunable limits and defaults: */
struct files_stat_struct {
 unsigned long nr_files; /* read only */
 unsigned long nr_free_files; /* read only */
 unsigned long max_files; /* tunable */
};

struct inodes_stat_t {
 long nr_inodes;
 long nr_unused;
 long dummy[5]; /* padding for sysctl ABI compatibility */
};




/*
 * Structure for FS_IOC_FSGETXATTR[A] and FS_IOC_FSSETXATTR.
 */
struct fsxattr {
 __u32 fsx_xflags; /* xflags field value (get/set) */
 __u32 fsx_extsize; /* extsize field value (get/set)*/
 __u32 fsx_nextents; /* nextents field value (get)	*/
 __u32 fsx_projid; /* project identifier (get/set) */
 __u32 fsx_cowextsize; /* CoW extsize field value (get/set)*/
 unsigned char fsx_pad[8];
};

/*
 * Flags for the fsx_xflags field
 */
# 145 "./include/uapi/linux/fs.h"
/* the read-only stuff doesn't really belong here, but any other place is
   probably as bad and I don't want to create yet another include file. */
# 170 "./include/uapi/linux/fs.h"
/* A jump here: 108-111 have been used for various private purposes. */
# 188 "./include/uapi/linux/fs.h"
/*
 * A jump here: 130-136 are reserved for zoned block devices
 * (see uapi/linux/blkzoned.h)
 */
# 219 "./include/uapi/linux/fs.h"
/*
 * Inode flags (FS_IOC_GETFLAGS / FS_IOC_SETFLAGS)
 *
 * Note: for historical reasons, these flags were originally used and
 * defined for use by ext2/ext3, and then other file systems started
 * using these flags so they wouldn't need to write their own version
 * of chattr/lsattr (which was shipped as part of e2fsprogs).  You
 * should think twice before trying to use these flags in new
 * contexts, or trying to assign these flags, since they are used both
 * as the UAPI and the on-disk encoding for ext2/3/4.  Also, we are
 * almost out of 32-bit flags.  :-)
 *
 * We have recently hoisted FS_IOC_FSGETXATTR / FS_IOC_FSSETXATTR from
 * XFS to the generic FS level interface.  This uses a structure that
 * has padding and hence has more room to grow, so it may be more
 * appropriate for many new use cases.
 *
 * Please do not change these flags or interfaces before checking with
 * linux-fsdevel@vger.kernel.org and linux-api@vger.kernel.org.
 */
# 247 "./include/uapi/linux/fs.h"
/* Reserved for compression usage... */



/* End compression flags --- maybe not all used */
# 283 "./include/uapi/linux/fs.h"
/*
 * Flags for preadv2/pwritev2:
 */

typedef int __kernel_rwf_t;

/* high priority request, poll if possible */


/* per-IO O_DSYNC */


/* per-IO O_SYNC */


/* per-IO, return -EAGAIN if operation would block */


/* per-IO O_APPEND */


/* mask of flags supported by the kernel */
# 49 "./include/linux/fs.h" 2

struct backing_dev_info;
struct bdi_writeback;
struct bio;
struct io_comp_batch;
struct export_operations;
struct fiemap_extent_info;
struct hd_geometry;
struct iovec;
struct kiocb;
struct kobject;
struct pipe_inode_info;
struct poll_table_struct;
struct kstatfs;
struct vm_area_struct;
struct vfsmount;
struct cred;
struct swap_info_struct;
struct seq_file;
struct workqueue_struct;
struct iov_iter;
struct fscrypt_info;
struct fscrypt_operations;
struct fsverity_info;
struct fsverity_operations;
struct fs_context;
struct fs_parameter_spec;
struct fileattr;
struct iomap_ops;

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) inode_init(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) inode_init_early(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) files_init(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) files_maxfiles_init(void);

extern unsigned long get_max_files(void);
extern unsigned int sysctl_nr_open;

typedef __kernel_rwf_t rwf_t;

struct buffer_head;
typedef int (get_block_t)(struct inode *inode, sector_t iblock,
   struct buffer_head *bh_result, int create);
typedef int (dio_iodone_t)(struct kiocb *iocb, loff_t offset,
   ssize_t bytes, void *private);
# 102 "./include/linux/fs.h"
/* called from RCU mode, don't block */


/*
 * flags in file.f_mode.  Note that FMODE_READ and FMODE_WRITE must correspond
 * to O_WRONLY and O_RDWR via the strange trick in do_dentry_open()
 */

/* file is open for reading */

/* file is open for writing */

/* file is seekable */

/* file can be accessed using pread */

/* file can be accessed using pwrite */

/* File is opened for execution with sys_execve / sys_uselib */

/* File is opened with O_NDELAY (only set for block devices) */

/* File is opened with O_EXCL (only set for block devices) */

/* File is opened using open(.., 3, ..) and is writeable only for ioctls
   (specialy hack for floppy.c) */

/* 32bit hashes as llseek() offset (for directories) */

/* 64bit hashes as llseek() offset (for directories) */


/*
 * Don't update ctime and mtime.
 *
 * Currently a special hack for the XFS open_by_handle ioctl, but we'll
 * hopefully graduate it to a proper O_CMTIME flag supported by open(2) soon.
 */


/* Expect random access pattern */


/* File is huge (eg. /dev/mem): treat loff_t as unsigned */


/* File is opened with O_PATH; almost nothing can be done with it */


/* File needs atomic accesses to f_pos */

/* Write access to underlying fs */

/* Has read method(s) */

/* Has write method(s) */





/* File is stream-like */


/* File supports DIRECT IO */


/* File was opened by fanotify and shouldn't generate fanotify events */


/* File is capable of returning -EAGAIN if I/O will block */


/* File represents mount that needs unmounting */


/* File does not contribute to nr_files count */


/* File supports async buffered reads */


/* File supports async nowait buffered writes */


/*
 * Attribute flags.  These should be or-ed together to figure out what
 * has been changed!
 */
# 209 "./include/linux/fs.h"
/*
 * Whiteout is represented by a char device.  The following constants define the
 * mode and device number to use.
 */



/*
 * This is the Inode Attributes structure, used for notify_change().  It
 * uses the above definitions as flags, to know which values have changed.
 * Also, in this manner, a Filesystem can look at only the values it cares
 * about.  Basically, these are the attributes that the VFS layer can
 * request to change from the FS layer.
 *
 * Derek Atkins <warlord@MIT.EDU> 94-10-20
 */
struct iattr {
 unsigned int ia_valid;
 umode_t ia_mode;
 /*
	 * The two anonymous unions wrap structures with the same member.
	 *
	 * Filesystems raising FS_ALLOW_IDMAP need to use ia_vfs{g,u}id which
	 * are a dedicated type requiring the filesystem to use the dedicated
	 * helpers. Other filesystem can continue to use ia_{g,u}id until they
	 * have been ported.
	 *
	 * They always contain the same value. In other words FS_ALLOW_IDMAP
	 * pass down the same value on idmapped mounts as they would on regular
	 * mounts.
	 */
 union {
  kuid_t ia_uid;
  vfsuid_t ia_vfsuid;
 };
 union {
  kgid_t ia_gid;
  vfsgid_t ia_vfsgid;
 };
 loff_t ia_size;
 struct timespec64 ia_atime;
 struct timespec64 ia_mtime;
 struct timespec64 ia_ctime;

 /*
	 * Not an attribute, but an auxiliary info for filesystems wanting to
	 * implement an ftruncate() like method.  NOTE: filesystem should
	 * check for (ia_valid & ATTR_FILE), and not for (ia_file != NULL).
	 */
 struct file *ia_file;
};

/*
 * Includes for diskquotas.
 */
# 1 "./include/linux/quota.h" 1
/*
 * Copyright (c) 1982, 1986 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Robert Elz at The University of Melbourne.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */
# 42 "./include/linux/quota.h"
# 1 "./include/uapi/linux/dqblk_xfs.h" 1
/* SPDX-License-Identifier: LGPL-2.1+ WITH Linux-syscall-note */
/*
 * Copyright (c) 1995-2001,2004 Silicon Graphics, Inc.  All Rights Reserved.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesset General Public License
 * along with this program; if not, write to the Free Software Foundation,
 * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 */





/*
 * Disk quota - quotactl(2) commands for the XFS Quota Manager (XQM).
 */
# 45 "./include/uapi/linux/dqblk_xfs.h"
/*
 * fs_disk_quota structure:
 *
 * This contains the current quota information regarding a user/proj/group.
 * It is 64-bit aligned, and all the blk units are in BBs (Basic Blocks) of
 * 512 bytes.
 */

typedef struct fs_disk_quota {
 __s8 d_version; /* version of this structure */
 __s8 d_flags; /* FS_{USER,PROJ,GROUP}_QUOTA */
 __u16 d_fieldmask; /* field specifier */
 __u32 d_id; /* user, project, or group ID */
 __u64 d_blk_hardlimit;/* absolute limit on disk blks */
 __u64 d_blk_softlimit;/* preferred limit on disk blks */
 __u64 d_ino_hardlimit;/* maximum # allocated inodes */
 __u64 d_ino_softlimit;/* preferred inode limit */
 __u64 d_bcount; /* # disk blocks owned by the user */
 __u64 d_icount; /* # inodes owned by the user */
 __s32 d_itimer; /* Zero if within inode limits. If
					 * not, we refuse service at this time
					 * (in seconds since Unix epoch) */
 __s32 d_btimer; /* similar to above; for disk blocks */
 __u16 d_iwarns; /* # warnings issued wrt num inodes */
 __u16 d_bwarns; /* # warnings issued wrt disk blocks */
 __s8 d_itimer_hi; /* upper 8 bits of timer values */
 __s8 d_btimer_hi;
 __s8 d_rtbtimer_hi;
 __s8 d_padding2; /* padding2 - for future use */
 __u64 d_rtb_hardlimit;/* absolute limit on realtime blks */
 __u64 d_rtb_softlimit;/* preferred limit on RT disk blks */
 __u64 d_rtbcount; /* # realtime blocks owned */
 __s32 d_rtbtimer; /* similar to above; for RT disk blks */
 __u16 d_rtbwarns; /* # warnings issued wrt RT disk blks */
 __s16 d_padding3; /* padding3 - for future use */
 char d_padding4[8]; /* yet more padding */
} fs_disk_quota_t;

/*
 * These fields are sent to Q_XSETQLIM to specify fields that need to change.
 */
# 94 "./include/uapi/linux/dqblk_xfs.h"
/*
 * These timers can only be set in super user's dquot. For others, timers are
 * automatically started and stopped. Superusers timer values set the limits
 * for the rest.  In case these values are zero, the DQ_{F,B}TIMELIMIT values
 * defined below are used.
 * These values also apply only to the d_fieldmask field for Q_XSETQLIM.
 */





/*
 * Warning counts are set in both super user's dquot and others. For others,
 * warnings are set/cleared by the administrators (or automatically by going
 * below the soft limit).  Superusers warning values set the warning limits
 * for the rest.  In case these values are zero, the DQ_{F,B}WARNLIMIT values
 * defined below are used.
 * These values also apply only to the d_fieldmask field for Q_XSETQLIM.
 */





/*
 * Accounting values.  These can only be set for filesystem with
 * non-transactional quotas that require quotacheck(8) in userspace.
 */





/*
 * Quota expiration timestamps are 40-bit signed integers, with the upper 8
 * bits encoded in the _hi fields.
 */


/*
 * Various flags related to quotactl(2).
 */
# 148 "./include/uapi/linux/dqblk_xfs.h"
/*
 * fs_quota_stat is the struct returned in Q_XGETQSTAT for a given file system.
 * Provides a centralized way to get meta information about the quota subsystem.
 * eg. space taken up for user and group quotas, number of dquots currently
 * incore.
 */


/*
 * Some basic information about 'quota files'.
 */
typedef struct fs_qfilestat {
 __u64 qfs_ino; /* inode number */
 __u64 qfs_nblks; /* number of BBs 512-byte-blks */
 __u32 qfs_nextents; /* number of extents */
} fs_qfilestat_t;

typedef struct fs_quota_stat {
 __s8 qs_version; /* version number for future changes */
 __u16 qs_flags; /* FS_QUOTA_{U,P,G}DQ_{ACCT,ENFD} */
 __s8 qs_pad; /* unused */
 fs_qfilestat_t qs_uquota; /* user quota storage information */
 fs_qfilestat_t qs_gquota; /* group quota storage information */
 __u32 qs_incoredqs; /* number of dquots incore */
 __s32 qs_btimelimit; /* limit for blks timer */
 __s32 qs_itimelimit; /* limit for inodes timer */
 __s32 qs_rtbtimelimit;/* limit for rt blks timer */
 __u16 qs_bwarnlimit; /* limit for num warnings */
 __u16 qs_iwarnlimit; /* limit for num warnings */
} fs_quota_stat_t;

/*
 * fs_quota_statv is used by Q_XGETQSTATV for a given file system. It provides
 * a centralized way to get meta information about the quota subsystem. eg.
 * space taken up for user, group, and project quotas, number of dquots
 * currently incore.
 *
 * This version has proper versioning support with appropriate padding for
 * future expansions, and ability to expand for future without creating any
 * backward compatibility issues.
 *
 * Q_XGETQSTATV uses the passed in value of the requested version via
 * fs_quota_statv.qs_version to determine the return data layout of
 * fs_quota_statv.  The kernel will fill the data fields relevant to that
 * version.
 *
 * If kernel does not support user space caller specified version, EINVAL will
 * be returned. User space caller can then reduce the version number and retry
 * the same command.
 */

/*
 * Some basic information about 'quota files' for Q_XGETQSTATV command
 */
struct fs_qfilestatv {
 __u64 qfs_ino; /* inode number */
 __u64 qfs_nblks; /* number of BBs 512-byte-blks */
 __u32 qfs_nextents; /* number of extents */
 __u32 qfs_pad; /* pad for 8-byte alignment */
};

struct fs_quota_statv {
 __s8 qs_version; /* version for future changes */
 __u8 qs_pad1; /* pad for 16bit alignment */
 __u16 qs_flags; /* FS_QUOTA_.* flags */
 __u32 qs_incoredqs; /* number of dquots incore */
 struct fs_qfilestatv qs_uquota; /* user quota information */
 struct fs_qfilestatv qs_gquota; /* group quota information */
 struct fs_qfilestatv qs_pquota; /* project quota information */
 __s32 qs_btimelimit; /* limit for blks timer */
 __s32 qs_itimelimit; /* limit for inodes timer */
 __s32 qs_rtbtimelimit;/* limit for rt blks timer */
 __u16 qs_bwarnlimit; /* limit for num warnings */
 __u16 qs_iwarnlimit; /* limit for num warnings */
 __u16 qs_rtbwarnlimit;/* limit for rt blks warnings */
 __u16 qs_pad3;
 __u32 qs_pad4;
 __u64 qs_pad2[7]; /* for future proofing */
};
# 43 "./include/linux/quota.h" 2
# 1 "./include/linux/dqblk_v1.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 *	File with in-memory structures of old quota format
 */




/* Numbers of blocks needed for updates */
# 44 "./include/linux/quota.h" 2
# 1 "./include/linux/dqblk_v2.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 *  Definitions for vfsv0 quota format
 */




# 1 "./include/linux/dqblk_qtree.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 *	Definitions of structures and functions for quota formats using trie
 */






/* Numbers of blocks needed for updates - we count with the smallest
 * possible block size (1024) */





struct dquot;
struct kqid;

/* Operations */
struct qtree_fmt_operations {
 void (*mem2disk_dqblk)(void *disk, struct dquot *dquot); /* Convert given entry from in memory format to disk one */
 void (*disk2mem_dqblk)(struct dquot *dquot, void *disk); /* Convert given entry from disk format to in memory one */
 int (*is_id)(void *disk, struct dquot *dquot); /* Is this structure for given id? */
};

/* Inmemory copy of version specific information */
struct qtree_mem_dqinfo {
 struct super_block *dqi_sb; /* Sb quota is on */
 int dqi_type; /* Quota type */
 unsigned int dqi_blocks; /* # of blocks in quota file */
 unsigned int dqi_free_blk; /* First block in list of free blocks */
 unsigned int dqi_free_entry; /* First block with free entry */
 unsigned int dqi_blocksize_bits; /* Block size of quota file */
 unsigned int dqi_entry_size; /* Size of quota entry in quota file */
 unsigned int dqi_usable_bs; /* Space usable in block for quota data */
 unsigned int dqi_qtree_depth; /* Precomputed depth of quota tree */
 const struct qtree_fmt_operations *dqi_ops; /* Operations for entry manipulation */
};

int qtree_write_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_read_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_delete_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_release_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_entry_unused(struct qtree_mem_dqinfo *info, char *disk);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int qtree_depth(struct qtree_mem_dqinfo *info)
{
 unsigned int epb = info->dqi_usable_bs >> 2;
 unsigned long long entries = epb;
 int i;

 for (i = 1; entries < (1ULL << 32); i++)
  entries *= epb;
 return i;
}
int qtree_get_next_id(struct qtree_mem_dqinfo *info, struct kqid *qid);
# 10 "./include/linux/dqblk_v2.h" 2

/* Numbers of blocks needed for updates */
# 45 "./include/linux/quota.h" 2



# 1 "./include/linux/projid.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * A set of types for the internal kernel types representing project ids.
 *
 * The types defined in this header allow distinguishing which project ids in
 * the kernel are values used by userspace and which project id values are
 * the internal kernel values.  With the addition of user namespaces the values
 * can be different.  Using the type system makes it possible for the compiler
 * to detect when we overlook these differences.
 *
 */


struct user_namespace;
extern struct user_namespace init_user_ns;

typedef __kernel_uid32_t projid_t;

typedef struct {
 projid_t val;
} kprojid_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) projid_t __kprojid_val(kprojid_t projid)
{
 return projid.val;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool projid_eq(kprojid_t left, kprojid_t right)
{
 return __kprojid_val(left) == __kprojid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool projid_lt(kprojid_t left, kprojid_t right)
{
 return __kprojid_val(left) < __kprojid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool projid_valid(kprojid_t projid)
{
 return !projid_eq(projid, (kprojid_t){ -1 });
}



extern kprojid_t make_kprojid(struct user_namespace *from, projid_t projid);

extern projid_t from_kprojid(struct user_namespace *to, kprojid_t projid);
extern projid_t from_kprojid_munged(struct user_namespace *to, kprojid_t projid);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kprojid_has_mapping(struct user_namespace *ns, kprojid_t projid)
{
 return from_kprojid(ns, projid) != (projid_t)-1;
}
# 49 "./include/linux/quota.h" 2
# 1 "./include/uapi/linux/quota.h" 1
/*
 * Copyright (c) 1982, 1986 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Robert Elz at The University of Melbourne.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */
# 45 "./include/uapi/linux/quota.h"
/*
 * Definitions for the default names of the quotas files.
 */







/*
 * Command definitions for the 'quotactl' system call.
 * The commands are broken into a main command defined below
 * and a subcommand that is used to convey the type of
 * quota that is being manipulated (see above).
 */
# 75 "./include/uapi/linux/quota.h"
/* Quota format type IDs */





/* Size of block in which space limits are passed through the quota
 * interface */



/*
 * Quota structure used for communication with userspace via quotactl
 * Following flags are used to specify which fields are valid
 */
enum {
 QIF_BLIMITS_B = 0,
 QIF_SPACE_B,
 QIF_ILIMITS_B,
 QIF_INODES_B,
 QIF_BTIME_B,
 QIF_ITIME_B,
};
# 110 "./include/uapi/linux/quota.h"
struct if_dqblk {
 __u64 dqb_bhardlimit;
 __u64 dqb_bsoftlimit;
 __u64 dqb_curspace;
 __u64 dqb_ihardlimit;
 __u64 dqb_isoftlimit;
 __u64 dqb_curinodes;
 __u64 dqb_btime;
 __u64 dqb_itime;
 __u32 dqb_valid;
};

struct if_nextdqblk {
 __u64 dqb_bhardlimit;
 __u64 dqb_bsoftlimit;
 __u64 dqb_curspace;
 __u64 dqb_ihardlimit;
 __u64 dqb_isoftlimit;
 __u64 dqb_curinodes;
 __u64 dqb_btime;
 __u64 dqb_itime;
 __u32 dqb_valid;
 __u32 dqb_id;
};

/*
 * Structure used for setting quota information about file via quotactl
 * Following flags are used to specify which fields are valid
 */





enum {
 DQF_ROOT_SQUASH_B = 0,
 DQF_SYS_FILE_B = 16,
 /* Kernel internal flags invisible to userspace */
 DQF_PRIVATE
};

/* Root squash enabled (for v1 quota format) */

/* Quota stored in a system file */


struct if_dqinfo {
 __u64 dqi_bgrace;
 __u64 dqi_igrace;
 __u32 dqi_flags; /* DFQ_* */
 __u32 dqi_valid;
};

/*
 * Definitions for quota netlink interface
 */
# 178 "./include/uapi/linux/quota.h"
enum {
 QUOTA_NL_C_UNSPEC,
 QUOTA_NL_C_WARNING,
 __QUOTA_NL_C_MAX,
};


enum {
 QUOTA_NL_A_UNSPEC,
 QUOTA_NL_A_QTYPE,
 QUOTA_NL_A_EXCESS_ID,
 QUOTA_NL_A_WARNING,
 QUOTA_NL_A_DEV_MAJOR,
 QUOTA_NL_A_DEV_MINOR,
 QUOTA_NL_A_CAUSED_ID,
 QUOTA_NL_A_PAD,
 __QUOTA_NL_A_MAX,
};
# 50 "./include/linux/quota.h" 2




enum quota_type {
 USRQUOTA = 0, /* element used for user quotas */
 GRPQUOTA = 1, /* element used for group quotas */
 PRJQUOTA = 2, /* element used for project quotas */
};

/* Masks for quota types when used as a bitmask */




typedef __kernel_uid32_t qid_t; /* Type in which we store ids in memory */
typedef long long qsize_t; /* Type in which we store sizes */

struct kqid { /* Type in which we store the quota identifier */
 union {
  kuid_t uid;
  kgid_t gid;
  kprojid_t projid;
 };
 enum quota_type type; /* USRQUOTA (uid) or GRPQUOTA (gid) or PRJQUOTA (projid) */
};

extern bool qid_eq(struct kqid left, struct kqid right);
extern bool qid_lt(struct kqid left, struct kqid right);
extern qid_t from_kqid(struct user_namespace *to, struct kqid qid);
extern qid_t from_kqid_munged(struct user_namespace *to, struct kqid qid);
extern bool qid_valid(struct kqid qid);

/**
 *	make_kqid - Map a user-namespace, type, qid tuple into a kqid.
 *	@from: User namespace that the qid is in
 *	@type: The type of quota
 *	@qid: Quota identifier
 *
 *	Maps a user-namespace, type qid tuple into a kernel internal
 *	kqid, and returns that kqid.
 *
 *	When there is no mapping defined for the user-namespace, type,
 *	qid tuple an invalid kqid is returned.  Callers are expected to
 *	test for and handle invalid kqids being returned.
 *	Invalid kqids may be tested for using qid_valid().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kqid make_kqid(struct user_namespace *from,
        enum quota_type type, qid_t qid)
{
 struct kqid kqid;

 kqid.type = type;
 switch (type) {
 case USRQUOTA:
  kqid.uid = make_kuid(from, qid);
  break;
 case GRPQUOTA:
  kqid.gid = make_kgid(from, qid);
  break;
 case PRJQUOTA:
  kqid.projid = make_kprojid(from, qid);
  break;
 default:
  do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/quota.h\"; .popsection; .long 14472b - .; .short 114; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0);
 }
 return kqid;
}

/**
 *	make_kqid_invalid - Explicitly make an invalid kqid
 *	@type: The type of quota identifier
 *
 *	Returns an invalid kqid with the specified type.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kqid make_kqid_invalid(enum quota_type type)
{
 struct kqid kqid;

 kqid.type = type;
 switch (type) {
 case USRQUOTA:
  kqid.uid = (kuid_t){ -1 };
  break;
 case GRPQUOTA:
  kqid.gid = (kgid_t){ -1 };
  break;
 case PRJQUOTA:
  kqid.projid = (kprojid_t){ -1 };
  break;
 default:
  do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/quota.h\"; .popsection; .long 14472b - .; .short 141; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0);
 }
 return kqid;
}

/**
 *	make_kqid_uid - Make a kqid from a kuid
 *	@uid: The kuid to make the quota identifier from
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kqid make_kqid_uid(kuid_t uid)
{
 struct kqid kqid;
 kqid.type = USRQUOTA;
 kqid.uid = uid;
 return kqid;
}

/**
 *	make_kqid_gid - Make a kqid from a kgid
 *	@gid: The kgid to make the quota identifier from
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kqid make_kqid_gid(kgid_t gid)
{
 struct kqid kqid;
 kqid.type = GRPQUOTA;
 kqid.gid = gid;
 return kqid;
}

/**
 *	make_kqid_projid - Make a kqid from a projid
 *	@projid: The kprojid to make the quota identifier from
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kqid make_kqid_projid(kprojid_t projid)
{
 struct kqid kqid;
 kqid.type = PRJQUOTA;
 kqid.projid = projid;
 return kqid;
}

/**
 *	qid_has_mapping - Report if a qid maps into a user namespace.
 *	@ns:  The user namespace to see if a value maps into.
 *	@qid: The kernel internal quota identifier to test.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool qid_has_mapping(struct user_namespace *ns, struct kqid qid)
{
 return from_kqid(ns, qid) != (qid_t) -1;
}


extern spinlock_t dq_data_lock;

/* Maximal numbers of writes for quota operation (insert/delete/update)
 * (over VFS all formats) */





/*
 * Data for one user/group kept in memory
 */
struct mem_dqblk {
 qsize_t dqb_bhardlimit; /* absolute limit on disk blks alloc */
 qsize_t dqb_bsoftlimit; /* preferred limit on disk blks */
 qsize_t dqb_curspace; /* current used space */
 qsize_t dqb_rsvspace; /* current reserved space for delalloc*/
 qsize_t dqb_ihardlimit; /* absolute limit on allocated inodes */
 qsize_t dqb_isoftlimit; /* preferred inode limit */
 qsize_t dqb_curinodes; /* current # allocated inodes */
 time64_t dqb_btime; /* time limit for excessive disk use */
 time64_t dqb_itime; /* time limit for excessive inode use */
};

/*
 * Data for one quotafile kept in memory
 */
struct quota_format_type;

struct mem_dqinfo {
 struct quota_format_type *dqi_format;
 int dqi_fmt_id; /* Id of the dqi_format - used when turning
				 * quotas on after remount RW */
 struct list_head dqi_dirty_list; /* List of dirty dquots [dq_list_lock] */
 unsigned long dqi_flags; /* DFQ_ flags [dq_data_lock] */
 unsigned int dqi_bgrace; /* Space grace time [dq_data_lock] */
 unsigned int dqi_igrace; /* Inode grace time [dq_data_lock] */
 qsize_t dqi_max_spc_limit; /* Maximum space limit [static] */
 qsize_t dqi_max_ino_limit; /* Maximum inode limit [static] */
 void *dqi_priv;
};

struct super_block;

/* Mask for flags passed to userspace */

/* Mask for flags modifiable from userspace */


enum {
 DQF_INFO_DIRTY_B = DQF_PRIVATE,
};


extern void mark_info_dirty(struct super_block *sb, int type);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int info_dirty(struct mem_dqinfo *info)
{
 return ((__builtin_constant_p(DQF_INFO_DIRTY_B) && __builtin_constant_p((uintptr_t)(&info->dqi_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&info->dqi_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&info->dqi_flags))) ? const_test_bit(DQF_INFO_DIRTY_B, &info->dqi_flags) : generic_test_bit(DQF_INFO_DIRTY_B, &info->dqi_flags));
}

enum {
 DQST_LOOKUPS,
 DQST_DROPS,
 DQST_READS,
 DQST_WRITES,
 DQST_CACHE_HITS,
 DQST_ALLOC_DQUOTS,
 DQST_FREE_DQUOTS,
 DQST_SYNCS,
 _DQST_DQSTAT_LAST
};

struct dqstats {
 unsigned long stat[_DQST_DQSTAT_LAST];
 struct percpu_counter counter[_DQST_DQSTAT_LAST];
};

extern struct dqstats dqstats;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dqstats_inc(unsigned int type)
{
 percpu_counter_inc(&dqstats.counter[type]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dqstats_dec(unsigned int type)
{
 percpu_counter_dec(&dqstats.counter[type]);
}
# 294 "./include/linux/quota.h"
struct dquot {
 struct hlist_node dq_hash; /* Hash list in memory [dq_list_lock] */
 struct list_head dq_inuse; /* List of all quotas [dq_list_lock] */
 struct list_head dq_free; /* Free list element [dq_list_lock] */
 struct list_head dq_dirty; /* List of dirty dquots [dq_list_lock] */
 struct mutex dq_lock; /* dquot IO lock */
 spinlock_t dq_dqb_lock; /* Lock protecting dq_dqb changes */
 atomic_t dq_count; /* Use count */
 struct super_block *dq_sb; /* superblock this applies to */
 struct kqid dq_id; /* ID this applies to (uid, gid, projid) */
 loff_t dq_off; /* Offset of dquot on disk [dq_lock, stable once set] */
 unsigned long dq_flags; /* See DQ_* */
 struct mem_dqblk dq_dqb; /* Diskquota usage [dq_dqb_lock] */
};

/* Operations which must be implemented by each quota format */
struct quota_format_ops {
 int (*check_quota_file)(struct super_block *sb, int type); /* Detect whether file is in our format */
 int (*read_file_info)(struct super_block *sb, int type); /* Read main info about file - called on quotaon() */
 int (*write_file_info)(struct super_block *sb, int type); /* Write main info about file */
 int (*free_file_info)(struct super_block *sb, int type); /* Called on quotaoff() */
 int (*read_dqblk)(struct dquot *dquot); /* Read structure for one user */
 int (*commit_dqblk)(struct dquot *dquot); /* Write structure for one user */
 int (*release_dqblk)(struct dquot *dquot); /* Called when last reference to dquot is being dropped */
 int (*get_next_id)(struct super_block *sb, struct kqid *qid); /* Get next ID with existing structure in the quota file */
};

/* Operations working with dquots */
struct dquot_operations {
 int (*write_dquot) (struct dquot *); /* Ordinary dquot write */
 struct dquot *(*alloc_dquot)(struct super_block *, int); /* Allocate memory for new dquot */
 void (*destroy_dquot)(struct dquot *); /* Free memory for dquot */
 int (*acquire_dquot) (struct dquot *); /* Quota is going to be created on disk */
 int (*release_dquot) (struct dquot *); /* Quota is going to be deleted from disk */
 int (*mark_dirty) (struct dquot *); /* Dquot is marked dirty */
 int (*write_info) (struct super_block *, int); /* Write of quota "superblock" */
 /* get reserved quota for delayed alloc, value returned is managed by
	 * quota code only */
 qsize_t *(*get_reserved_space) (struct inode *);
 int (*get_projid) (struct inode *, kprojid_t *);/* Get project ID */
 /* Get number of inodes that were charged for a given inode */
 int (*get_inode_usage) (struct inode *, qsize_t *);
 /* Get next ID with active quota structure */
 int (*get_next_id) (struct super_block *sb, struct kqid *qid);
};

struct path;

/* Structure for communicating via ->get_dqblk() & ->set_dqblk() */
struct qc_dqblk {
 int d_fieldmask; /* mask of fields to change in ->set_dqblk() */
 u64 d_spc_hardlimit; /* absolute limit on used space */
 u64 d_spc_softlimit; /* preferred limit on used space */
 u64 d_ino_hardlimit; /* maximum # allocated inodes */
 u64 d_ino_softlimit; /* preferred inode limit */
 u64 d_space; /* Space owned by the user */
 u64 d_ino_count; /* # inodes owned by the user */
 s64 d_ino_timer; /* zero if within inode limits */
    /* if not, we refuse service */
 s64 d_spc_timer; /* similar to above; for space */
 int d_ino_warns; /* # warnings issued wrt num inodes */
 int d_spc_warns; /* # warnings issued wrt used space */
 u64 d_rt_spc_hardlimit; /* absolute limit on realtime space */
 u64 d_rt_spc_softlimit; /* preferred limit on RT space */
 u64 d_rt_space; /* realtime space owned */
 s64 d_rt_spc_timer; /* similar to above; for RT space */
 int d_rt_spc_warns; /* # warnings issued wrt RT space */
};

/*
 * Field specifiers for ->set_dqblk() in struct qc_dqblk and also for
 * ->set_info() in struct qc_info
 */
# 394 "./include/linux/quota.h"
/* Structures for communicating via ->get_state */
struct qc_type_state {
 unsigned int flags; /* Flags QCI_* */
 unsigned int spc_timelimit; /* Time after which space softlimit is
					 * enforced */
 unsigned int ino_timelimit; /* Ditto for inode softlimit */
 unsigned int rt_spc_timelimit; /* Ditto for real-time space */
 unsigned int spc_warnlimit; /* Limit for number of space warnings */
 unsigned int ino_warnlimit; /* Ditto for inodes */
 unsigned int rt_spc_warnlimit; /* Ditto for real-time space */
 unsigned long long ino; /* Inode number of quota file */
 blkcnt_t blocks; /* Number of 512-byte blocks in the file */
 blkcnt_t nextents; /* Number of extents in the file */
};

struct qc_state {
 unsigned int s_incoredqs; /* Number of dquots in core */
 struct qc_type_state s_state[3]; /* Per quota type information */
};

/* Structure for communicating via ->set_info */
struct qc_info {
 int i_fieldmask; /* mask of fields to change in ->set_info() */
 unsigned int i_flags; /* Flags QCI_* */
 unsigned int i_spc_timelimit; /* Time after which space softlimit is
					 * enforced */
 unsigned int i_ino_timelimit; /* Ditto for inode softlimit */
 unsigned int i_rt_spc_timelimit;/* Ditto for real-time space */
 unsigned int i_spc_warnlimit; /* Limit for number of space warnings */
 unsigned int i_ino_warnlimit; /* Limit for number of inode warnings */
 unsigned int i_rt_spc_warnlimit; /* Ditto for real-time space */
};

/* Operations handling requests from userspace */
struct quotactl_ops {
 int (*quota_on)(struct super_block *, int, int, const struct path *);
 int (*quota_off)(struct super_block *, int);
 int (*quota_enable)(struct super_block *, unsigned int);
 int (*quota_disable)(struct super_block *, unsigned int);
 int (*quota_sync)(struct super_block *, int);
 int (*set_info)(struct super_block *, int, struct qc_info *);
 int (*get_dqblk)(struct super_block *, struct kqid, struct qc_dqblk *);
 int (*get_nextdqblk)(struct super_block *, struct kqid *,
        struct qc_dqblk *);
 int (*set_dqblk)(struct super_block *, struct kqid, struct qc_dqblk *);
 int (*get_state)(struct super_block *, struct qc_state *);
 int (*rm_xquota)(struct super_block *, unsigned int);
};

struct quota_format_type {
 int qf_fmt_id; /* Quota format id */
 const struct quota_format_ops *qf_ops; /* Operations of format */
 struct module *qf_owner; /* Module implementing quota format */
 struct quota_format_type *qf_next;
};

/**
 * Quota state flags - they come in three flavors - for users, groups and projects.
 *
 * Actual typed flags layout:
 *				USRQUOTA	GRPQUOTA	PRJQUOTA
 *  DQUOT_USAGE_ENABLED		0x0001		0x0002		0x0004
 *  DQUOT_LIMITS_ENABLED	0x0008		0x0010		0x0020
 *  DQUOT_SUSPENDED		0x0040		0x0080		0x0100
 *
 * Following bits are used for non-typed flags:
 *  DQUOT_QUOTA_SYS_FILE	0x0200
 *  DQUOT_NEGATIVE_USAGE	0x0400
 *  DQUOT_NOLIST_DIRTY		0x0800
 */
enum {
 _DQUOT_USAGE_ENABLED = 0, /* Track disk usage for users */
 _DQUOT_LIMITS_ENABLED, /* Enforce quota limits for users */
 _DQUOT_SUSPENDED, /* User diskquotas are off, but
						 * we have necessary info in
						 * memory to turn them on */
 _DQUOT_STATE_FLAGS
};





/* Other quota flags */


      /* Quota file is a special
						 * system file and user cannot
						 * touch it. Filesystem is
						 * responsible for setting
						 * S_NOQUOTA, S_NOATIME flags
						 */

            /* Allow negative quota usage */
/* Do not track dirty dquots in a list */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int dquot_state_flag(unsigned int flags, int type)
{
 return flags << type;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int dquot_generic_flag(unsigned int flags, int type)
{
 return (flags >> type) & ((1 << _DQUOT_USAGE_ENABLED * 3) | (1 << _DQUOT_LIMITS_ENABLED * 3) | (1 << _DQUOT_SUSPENDED * 3));
}

/* Bitmap of quota types where flag is set in flags */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned dquot_state_types(unsigned flags, unsigned flag)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_313(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(flag) == 0 || (((flag) & ((flag) - 1)) != 0)"))); if (!(!((flag) == 0 || (((flag) & ((flag) - 1)) != 0)))) __compiletime_assert_313(); } while (0);
# 505 "./include/linux/quota.h"
 return (flags / flag) & ((1 << 3) - 1);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void quota_send_warning(struct kqid qid, dev_t dev,
          const char warntype)
{
 return;
}


struct quota_info {
 unsigned int flags; /* Flags for diskquotas on this device */
 struct rw_semaphore dqio_sem; /* Lock quota file while I/O in progress */
 struct inode *files[3]; /* inodes of quotafiles */
 struct mem_dqinfo info[3]; /* Information for each quota type */
 const struct quota_format_ops *ops[3]; /* Operations for each type */
};

int register_quota_format(struct quota_format_type *fmt);
void unregister_quota_format(struct quota_format_type *fmt);

struct quota_module_name {
 int qm_fmt_id;
 char *qm_mod_name;
};
# 265 "./include/linux/fs.h" 2

/*
 * Maximum number of layers of fs stack.  Needs to be limited to
 * prevent kernel stack overflow
 */


/**
 * enum positive_aop_returns - aop return codes with specific semantics
 *
 * @AOP_WRITEPAGE_ACTIVATE: Informs the caller that page writeback has
 * 			    completed, that the page is still locked, and
 * 			    should be considered active.  The VM uses this hint
 * 			    to return the page to the active list -- it won't
 * 			    be a candidate for writeback again in the near
 * 			    future.  Other callers must be careful to unlock
 * 			    the page if they get this return.  Returned by
 * 			    writepage();
 *
 * @AOP_TRUNCATED_PAGE: The AOP method that was handed a locked page has
 *  			unlocked it and the page might have been truncated.
 *  			The caller should back up to acquiring a new page and
 *  			trying again.  The aop will be taking reasonable
 *  			precautions not to livelock.  If the caller held a page
 *  			reference, it should drop it before retrying.  Returned
 *  			by read_folio().
 *
 * address_space_operation functions return these large constants to indicate
 * special semantics to the caller.  These are much larger than the bytes in a
 * page to allow for functions that return the number of bytes operated on in a
 * given page.
 */

enum positive_aop_returns {
 AOP_WRITEPAGE_ACTIVATE = 0x80000,
 AOP_TRUNCATED_PAGE = 0x80001,
};

/*
 * oh the beauties of C type declarations.
 */
struct page;
struct address_space;
struct writeback_control;
struct readahead_control;

/*
 * Write life time hint values.
 * Stored in struct inode as u8.
 */
enum rw_hint {
 WRITE_LIFE_NOT_SET = 0,
 WRITE_LIFE_NONE = 1,
 WRITE_LIFE_SHORT = 2,
 WRITE_LIFE_MEDIUM = 3,
 WRITE_LIFE_LONG = 4,
 WRITE_LIFE_EXTREME = 5,
};

/* Match RWF_* bits to IOCB bits */






/* non-RWF related bits - start at 16 */



/* iocb->ki_waitq is valid */


/* can use bio alloc cache */


struct kiocb {
 struct file *ki_filp;
 loff_t ki_pos;
 void (*ki_complete)(struct kiocb *iocb, long ret);
 void *private;
 int ki_flags;
 u16 ki_ioprio; /* See linux/ioprio.h */
 struct wait_page_queue *ki_waitq; /* for async buffered IO */
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_sync_kiocb(struct kiocb *kiocb)
{
 return kiocb->ki_complete == ((void *)0);
}

struct address_space_operations {
 int (*writepage)(struct page *page, struct writeback_control *wbc);
 int (*read_folio)(struct file *, struct folio *);

 /* Write back some dirty pages from this mapping. */
 int (*writepages)(struct address_space *, struct writeback_control *);

 /* Mark a folio dirty.  Return true if this dirtied it */
 bool (*dirty_folio)(struct address_space *, struct folio *);

 void (*readahead)(struct readahead_control *);

 int (*write_begin)(struct file *, struct address_space *mapping,
    loff_t pos, unsigned len,
    struct page **pagep, void **fsdata);
 int (*write_end)(struct file *, struct address_space *mapping,
    loff_t pos, unsigned len, unsigned copied,
    struct page *page, void *fsdata);

 /* Unfortunately this kludge is needed for FIBMAP. Don't use it */
 sector_t (*bmap)(struct address_space *, sector_t);
 void (*invalidate_folio) (struct folio *, size_t offset, size_t len);
 bool (*release_folio)(struct folio *, gfp_t);
 void (*free_folio)(struct folio *folio);
 ssize_t (*direct_IO)(struct kiocb *, struct iov_iter *iter);
 /*
	 * migrate the contents of a folio to the specified target. If
	 * migrate_mode is MIGRATE_ASYNC, it must not block.
	 */
 int (*migrate_folio)(struct address_space *, struct folio *dst,
   struct folio *src, enum migrate_mode);
 int (*launder_folio)(struct folio *);
 bool (*is_partially_uptodate) (struct folio *, size_t from,
   size_t count);
 void (*is_dirty_writeback) (struct folio *, bool *dirty, bool *wb);
 int (*error_remove_page)(struct address_space *, struct page *);

 /* swapfile support */
 int (*swap_activate)(struct swap_info_struct *sis, struct file *file,
    sector_t *span);
 void (*swap_deactivate)(struct file *file);
 int (*swap_rw)(struct kiocb *iocb, struct iov_iter *iter);
};

extern const struct address_space_operations empty_aops;

/**
 * struct address_space - Contents of a cacheable, mappable object.
 * @host: Owner, either the inode or the block_device.
 * @i_pages: Cached pages.
 * @invalidate_lock: Guards coherency between page cache contents and
 *   file offset->disk block mappings in the filesystem during invalidates.
 *   It is also used to block modification of page cache contents through
 *   memory mappings.
 * @gfp_mask: Memory allocation flags to use for allocating pages.
 * @i_mmap_writable: Number of VM_SHARED mappings.
 * @nr_thps: Number of THPs in the pagecache (non-shmem only).
 * @i_mmap: Tree of private and shared mappings.
 * @i_mmap_rwsem: Protects @i_mmap and @i_mmap_writable.
 * @nrpages: Number of page entries, protected by the i_pages lock.
 * @writeback_index: Writeback starts here.
 * @a_ops: Methods.
 * @flags: Error bits and flags (AS_*).
 * @wb_err: The most recent error which has occurred.
 * @private_lock: For use by the owner of the address_space.
 * @private_list: For use by the owner of the address_space.
 * @private_data: For use by the owner of the address_space.
 */
struct address_space {
 struct inode *host;
 struct xarray i_pages;
 struct rw_semaphore invalidate_lock;
 gfp_t gfp_mask;
 atomic_t i_mmap_writable;




 struct rb_root_cached i_mmap;
 struct rw_semaphore i_mmap_rwsem;
 unsigned long nrpages;
 unsigned long writeback_index;
 const struct address_space_operations *a_ops;
 unsigned long flags;
 errseq_t wb_err;
 spinlock_t private_lock;
 struct list_head private_list;
 void *private_data;
} __attribute__((aligned(sizeof(long)))) ;
 /*
	 * On most architectures that alignment is already the case; but
	 * must be enforced here for CRIS, to let the least significant bit
	 * of struct page's "mapping" pointer be used for PAGE_MAPPING_ANON.
	 */

/* XArray tags, for tagging dirty and writeback pages in the pagecache. */




/*
 * Returns true if any of the pages in the mapping are marked with the tag.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mapping_tagged(struct address_space *mapping, xa_mark_t tag)
{
 return xa_marked(&mapping->i_pages, tag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_mmap_lock_write(struct address_space *mapping)
{
 down_write(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int i_mmap_trylock_write(struct address_space *mapping)
{
 return down_write_trylock(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_mmap_unlock_write(struct address_space *mapping)
{
 up_write(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int i_mmap_trylock_read(struct address_space *mapping)
{
 return down_read_trylock(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_mmap_lock_read(struct address_space *mapping)
{
 down_read(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_mmap_unlock_read(struct address_space *mapping)
{
 up_read(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_mmap_assert_locked(struct address_space *mapping)
{
 do { (void)(&mapping->i_mmap_rwsem); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_mmap_assert_write_locked(struct address_space *mapping)
{
 do { (void)(&mapping->i_mmap_rwsem); } while (0);
}

/*
 * Might pages of this file be mapped into userspace?
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mapping_mapped(struct address_space *mapping)
{
 return !(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_314(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((&mapping->i_mmap.rb_root)->rb_node) == sizeof(char) || sizeof((&mapping->i_mmap.rb_root)->rb_node) == sizeof(short) || sizeof((&mapping->i_mmap.rb_root)->rb_node) == sizeof(int) || sizeof((&mapping->i_mmap.rb_root)->rb_node) == sizeof(long)) || sizeof((&mapping->i_mmap.rb_root)->rb_node) == sizeof(long long))) __compiletime_assert_314(); } while (0); (*(const volatile typeof( _Generic(((&mapping->i_mmap.rb_root)->rb_node), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((&mapping->i_mmap.rb_root)->rb_node))) *)&((&mapping->i_mmap.rb_root)->rb_node)); }) == ((void *)0));
# 510 "./include/linux/fs.h"
}

/*
 * Might pages of this file have been modified in userspace?
 * Note that i_mmap_writable counts all VM_SHARED vmas: do_mmap
 * marks vma as VM_SHARED if it is shared, and the file was opened for
 * writing i.e. vma may be mprotected writable even if now readonly.
 *
 * If i_mmap_writable is negative, no new writable mappings are allowed. You
 * can only deny writable mappings, if none exists right now.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mapping_writably_mapped(struct address_space *mapping)
{
 return atomic_read(&mapping->i_mmap_writable) > 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mapping_map_writable(struct address_space *mapping)
{
 return atomic_inc_unless_negative(&mapping->i_mmap_writable) ?
  0 : -1 /* Operation not permitted */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mapping_unmap_writable(struct address_space *mapping)
{
 atomic_dec(&mapping->i_mmap_writable);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mapping_deny_writable(struct address_space *mapping)
{
 return atomic_dec_unless_positive(&mapping->i_mmap_writable) ?
  0 : -16 /* Device or resource busy */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mapping_allow_writable(struct address_space *mapping)
{
 atomic_inc(&mapping->i_mmap_writable);
}

/*
 * Use sequence counter to get consistent i_size on 32-bit processors.
 */
# 559 "./include/linux/fs.h"
struct posix_acl;

/*
 * ACL_DONT_CACHE is for stacked filesystems, that rely on underlying fs to
 * cache the ACL.  This also means that ->get_inode_acl() can be called in RCU
 * mode with the LOOKUP_RCU flag.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct posix_acl *
uncached_acl_sentinel(struct task_struct *task)
{
 return (void *)task + 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
is_uncached_acl(struct posix_acl *acl)
{
 return (long)acl & 1;
}







struct fsnotify_mark_connector;

/*
 * Keep mostly read-only and often accessed (especially for
 * the RCU path lookup and 'stat' data) fields at the beginning
 * of the 'struct inode'
 */
struct inode {
 umode_t i_mode;
 unsigned short i_opflags;
 kuid_t i_uid;
 kgid_t i_gid;
 unsigned int i_flags;


 struct posix_acl *i_acl;
 struct posix_acl *i_default_acl;


 const struct inode_operations *i_op;
 struct super_block *i_sb;
 struct address_space *i_mapping;


 void *i_security;


 /* Stat data, not accessed from path walking */
 unsigned long i_ino;
 /*
	 * Filesystems may only read i_nlink directly.  They shall use the
	 * following functions for modification:
	 *
	 *    (set|clear|inc|drop)_nlink
	 *    inode_(inc|dec)_link_count
	 */
 union {
  const unsigned int i_nlink;
  unsigned int __i_nlink;
 };
 dev_t i_rdev;
 loff_t i_size;
 struct timespec64 i_atime;
 struct timespec64 i_mtime;
 struct timespec64 i_ctime;
 spinlock_t i_lock; /* i_blocks, i_bytes, maybe i_size */
 unsigned short i_bytes;
 u8 i_blkbits;
 u8 i_write_hint;
 blkcnt_t i_blocks;





 /* Misc */
 unsigned long i_state;
 struct rw_semaphore i_rwsem;

 unsigned long dirtied_when; /* jiffies of first dirtying */
 unsigned long dirtied_time_when;

 struct hlist_node i_hash;
 struct list_head i_io_list; /* backing dev IO list */

 struct bdi_writeback *i_wb; /* the associated cgroup wb */

 /* foreign inode detection, see wbc_detach_inode() */
 int i_wb_frn_winner;
 u16 i_wb_frn_avg_time;
 u16 i_wb_frn_history;

 struct list_head i_lru; /* inode LRU list */
 struct list_head i_sb_list;
 struct list_head i_wb_list; /* backing dev writeback list */
 union {
  struct hlist_head i_dentry;
  struct callback_head i_rcu;
 };
 atomic64_t i_version;
 atomic64_t i_sequence; /* see futex */
 atomic_t i_count;
 atomic_t i_dio_count;
 atomic_t i_writecount;

 atomic_t i_readcount; /* struct files open RO */

 union {
  const struct file_operations *i_fop; /* former ->i_op->default_file_ops */
  void (*free_inode)(struct inode *);
 };
 struct file_lock_context *i_flctx;
 struct address_space i_data;
 struct list_head i_devices;
 union {
  struct pipe_inode_info *i_pipe;
  struct cdev *i_cdev;
  char *i_link;
  unsigned i_dir_seq;
 };

 __u32 i_generation;


 __u32 i_fsnotify_mask; /* all events this inode cares about */
 struct fsnotify_mark_connector /* nothing */ *i_fsnotify_marks;
# 702 "./include/linux/fs.h"
 void *i_private; /* fs or device private pointer */
} ;

struct timespec64 timestamp_truncate(struct timespec64 t, struct inode *inode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int i_blocksize(const struct inode *node)
{
 return (1 << node->i_blkbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int inode_unhashed(struct inode *inode)
{
 return hlist_unhashed(&inode->i_hash);
}

/*
 * __mark_inode_dirty expects inodes to be hashed.  Since we don't
 * want special inodes in the fileset inode space, we make them
 * appear hashed, but do not put on any lists.  hlist_del()
 * will work fine and require no locking.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_fake_hash(struct inode *inode)
{
 hlist_add_fake(&inode->i_hash);
}

/*
 * inode->i_mutex nesting subclasses for the lock validator:
 *
 * 0: the object of the current VFS operation
 * 1: parent
 * 2: child/target
 * 3: xattr
 * 4: second non-directory
 * 5: second parent (when locking independent directories in rename)
 *
 * I_MUTEX_NONDIR2 is for certain operations (such as rename) which lock two
 * non-directories at once.
 *
 * The locking order between these classes is
 * parent[2] -> child -> grandchild -> normal -> xattr -> second non-directory
 */
enum inode_i_mutex_lock_class
{
 I_MUTEX_NORMAL,
 I_MUTEX_PARENT,
 I_MUTEX_CHILD,
 I_MUTEX_XATTR,
 I_MUTEX_NONDIR2,
 I_MUTEX_PARENT2,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_lock(struct inode *inode)
{
 down_write(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_unlock(struct inode *inode)
{
 up_write(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_lock_shared(struct inode *inode)
{
 down_read(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_unlock_shared(struct inode *inode)
{
 up_read(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int inode_trylock(struct inode *inode)
{
 return down_write_trylock(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int inode_trylock_shared(struct inode *inode)
{
 return down_read_trylock(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int inode_is_locked(struct inode *inode)
{
 return rwsem_is_locked(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_lock_nested(struct inode *inode, unsigned subclass)
{
 down_write(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_lock_shared_nested(struct inode *inode, unsigned subclass)
{
 down_read(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void filemap_invalidate_lock(struct address_space *mapping)
{
 down_write(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void filemap_invalidate_unlock(struct address_space *mapping)
{
 up_write(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void filemap_invalidate_lock_shared(struct address_space *mapping)
{
 down_read(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int filemap_invalidate_trylock_shared(
     struct address_space *mapping)
{
 return down_read_trylock(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void filemap_invalidate_unlock_shared(
     struct address_space *mapping)
{
 up_read(&mapping->invalidate_lock);
}

void lock_two_nondirectories(struct inode *, struct inode*);
void unlock_two_nondirectories(struct inode *, struct inode*);

void filemap_invalidate_lock_two(struct address_space *mapping1,
     struct address_space *mapping2);
void filemap_invalidate_unlock_two(struct address_space *mapping1,
       struct address_space *mapping2);


/*
 * NOTE: in a 32bit arch with a preemptable kernel and
 * an UP compile the i_size_read/write must be atomic
 * with respect to the local cpu (unlike with preempt disabled),
 * but they don't need to be atomic with respect to other cpus like in
 * true SMP (so they need either to either locally disable irq around
 * the read or for example on x86 they can be still implemented as a
 * cmpxchg8b without the need of the lock prefix). For SMP compiles
 * and 64bit archs it makes no difference if preempt is enabled or not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) loff_t i_size_read(const struct inode *inode)
{
# 864 "./include/linux/fs.h"
 return inode->i_size;

}

/*
 * NOTE: unlike i_size_read(), i_size_write() does need locking around it
 * (normally i_mutex), otherwise on 32bit/SMP an update of i_size_seqcount
 * can be lost, resulting in subsequent i_size_read() calls spinning forever.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_size_write(struct inode *inode, loff_t i_size)
{
# 886 "./include/linux/fs.h"
 inode->i_size = i_size;

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned iminor(const struct inode *inode)
{
 return ((unsigned int) ((inode->i_rdev) & ((1U << 20) - 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned imajor(const struct inode *inode)
{
 return ((unsigned int) ((inode->i_rdev) >> 20));
}

struct fown_struct {
 rwlock_t lock; /* protects pid, uid, euid fields */
 struct pid *pid; /* pid or -pgrp where SIGIO should be sent */
 enum pid_type pid_type; /* Kind of process group SIGIO should be sent to */
 kuid_t uid, euid; /* uid/euid of process setting the owner */
 int signum; /* posix.1b rt signal to be delivered on IO */
};

/**
 * struct file_ra_state - Track a file's readahead state.
 * @start: Where the most recent readahead started.
 * @size: Number of pages read in the most recent readahead.
 * @async_size: Numer of pages that were/are not needed immediately
 *      and so were/are genuinely "ahead".  Start next readahead when
 *      the first of these pages is accessed.
 * @ra_pages: Maximum size of a readahead request, copied from the bdi.
 * @mmap_miss: How many mmap accesses missed in the page cache.
 * @prev_pos: The last byte in the most recent read request.
 *
 * When this structure is passed to ->readahead(), the "most recent"
 * readahead means the current readahead.
 */
struct file_ra_state {
 unsigned long start;
 unsigned int size;
 unsigned int async_size;
 unsigned int ra_pages;
 unsigned int mmap_miss;
 loff_t prev_pos;
};

/*
 * Check if @index falls in the readahead windows.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ra_has_index(struct file_ra_state *ra, unsigned long index)
{
 return (index >= ra->start &&
  index < ra->start + ra->size);
}

struct file {
 union {
  struct llist_node f_llist;
  struct callback_head f_rcuhead;
  unsigned int f_iocb_flags;
 };
 struct path f_path;
 struct inode *f_inode; /* cached value */
 const struct file_operations *f_op;

 /*
	 * Protects f_ep, f_flags.
	 * Must not be taken from IRQ context.
	 */
 spinlock_t f_lock;
 atomic_long_t f_count;
 unsigned int f_flags;
 fmode_t f_mode;
 struct mutex f_pos_lock;
 loff_t f_pos;
 struct fown_struct f_owner;
 const struct cred *f_cred;
 struct file_ra_state f_ra;

 u64 f_version;

 void *f_security;

 /* needed for tty driver, and maybe others */
 void *private_data;


 /* Used by fs/eventpoll.c to link all the hooks to this file */
 struct hlist_head *f_ep;

 struct address_space *f_mapping;
 errseq_t f_wb_err;
 errseq_t f_sb_err; /* for syncfs */
}
  __attribute__((aligned(4))); /* lest something weird decides that 2 is OK */

struct file_handle {
 __u32 handle_bytes;
 int handle_type;
 /* file identifier */
 unsigned char f_handle[];
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct file *get_file(struct file *f)
{
 atomic_long_inc(&f->f_count);
 return f;
}





/* Page cache limit. The filesystems should put that into their s_maxbytes
   limits, otherwise bad things can happen in VM. */
# 1022 "./include/linux/fs.h"
/*
 * Special return value from posix_lock_file() and vfs_lock_file() for
 * asynchronous locking.
 */


/* legacy typedef, should eventually be removed */
typedef void *fl_owner_t;

struct file_lock;

struct file_lock_operations {
 void (*fl_copy_lock)(struct file_lock *, struct file_lock *);
 void (*fl_release_private)(struct file_lock *);
};

struct lock_manager_operations {
 void *lm_mod_owner;
 fl_owner_t (*lm_get_owner)(fl_owner_t);
 void (*lm_put_owner)(fl_owner_t);
 void (*lm_notify)(struct file_lock *); /* unblock callback */
 int (*lm_grant)(struct file_lock *, int);
 bool (*lm_break)(struct file_lock *);
 int (*lm_change)(struct file_lock *, int, struct list_head *);
 void (*lm_setup)(struct file_lock *, void **);
 bool (*lm_breaker_owns_lease)(struct file_lock *);
 bool (*lm_lock_expirable)(struct file_lock *cfl);
 void (*lm_expire_lock)(void);
};

struct lock_manager {
 struct list_head list;
 /*
	 * NFSv4 and up also want opens blocked during the grace period;
	 * NLM doesn't care:
	 */
 bool block_opens;
};

struct net;
void locks_start_grace(struct net *, struct lock_manager *);
void locks_end_grace(struct lock_manager *);
bool locks_in_grace(struct net *);
bool opens_in_grace(struct net *);

/* that will die - we need it for nfs_lock_info */
# 1 "./include/linux/nfs_fs_i.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



struct nlm_lockowner;

/*
 * NFS lock info
 */
struct nfs_lock_info {
 u32 state;
 struct nlm_lockowner *owner;
 struct list_head list;
};

struct nfs4_lock_state;
struct nfs4_lock_info {
 struct nfs4_lock_state *owner;
};
# 1069 "./include/linux/fs.h" 2

/*
 * struct file_lock represents a generic "file lock". It's used to represent
 * POSIX byte range locks, BSD (flock) locks, and leases. It's important to
 * note that the same struct is used to represent both a request for a lock and
 * the lock itself, but the same object is never used for both.
 *
 * FIXME: should we create a separate "struct lock_request" to help distinguish
 * these two uses?
 *
 * The varous i_flctx lists are ordered by:
 *
 * 1) lock owner
 * 2) lock range start
 * 3) lock range end
 *
 * Obviously, the last two criteria only matter for POSIX locks.
 */
struct file_lock {
 struct file_lock *fl_blocker; /* The lock, that is blocking us */
 struct list_head fl_list; /* link into file_lock_context */
 struct hlist_node fl_link; /* node in global lists */
 struct list_head fl_blocked_requests; /* list of requests with
						 * ->fl_blocker pointing here
						 */
 struct list_head fl_blocked_member; /* node in
						 * ->fl_blocker->fl_blocked_requests
						 */
 fl_owner_t fl_owner;
 unsigned int fl_flags;
 unsigned char fl_type;
 unsigned int fl_pid;
 int fl_link_cpu; /* what cpu's list is this on? */
 wait_queue_head_t fl_wait;
 struct file *fl_file;
 loff_t fl_start;
 loff_t fl_end;

 struct fasync_struct * fl_fasync; /* for lease break notifications */
 /* for lease breaks: */
 unsigned long fl_break_time;
 unsigned long fl_downgrade_time;

 const struct file_lock_operations *fl_ops; /* Callbacks for filesystems */
 const struct lock_manager_operations *fl_lmops; /* Callbacks for lockmanagers */
 union {
  struct nfs_lock_info nfs_fl;
  struct nfs4_lock_info nfs4_fl;
  struct {
   struct list_head link; /* link in AFS vnode's pending_locks list */
   int state; /* state of grant or error if -ve */
   unsigned int debug_id;
  } afs;
 } fl_u;
} ;

struct file_lock_context {
 spinlock_t flc_lock;
 struct list_head flc_flock;
 struct list_head flc_posix;
 struct list_head flc_lease;
};

/* The following constant reflects the upper bound of the file/locking space */





extern void send_sigio(struct fown_struct *fown, int fd, int band);




extern int fcntl_getlk(struct file *, unsigned int, struct flock *);
extern int fcntl_setlk(unsigned int, struct file *, unsigned int,
   struct flock *);







extern int fcntl_setlease(unsigned int fd, struct file *filp, long arg);
extern int fcntl_getlease(struct file *filp);

/* fs/locks.c */
void locks_free_lock_context(struct inode *inode);
void locks_free_lock(struct file_lock *fl);
extern void locks_init_lock(struct file_lock *);
extern struct file_lock * locks_alloc_lock(void);
extern void locks_copy_lock(struct file_lock *, struct file_lock *);
extern void locks_copy_conflock(struct file_lock *, struct file_lock *);
extern void locks_remove_posix(struct file *, fl_owner_t);
extern void locks_remove_file(struct file *);
extern void locks_release_private(struct file_lock *);
extern void posix_test_lock(struct file *, struct file_lock *);
extern int posix_lock_file(struct file *, struct file_lock *, struct file_lock *);
extern int locks_delete_block(struct file_lock *);
extern int vfs_test_lock(struct file *, struct file_lock *);
extern int vfs_lock_file(struct file *, unsigned int, struct file_lock *, struct file_lock *);
extern int vfs_cancel_lock(struct file *filp, struct file_lock *fl);
bool vfs_inode_has_locks(struct inode *inode);
extern int locks_lock_inode_wait(struct inode *inode, struct file_lock *fl);
extern int __break_lease(struct inode *inode, unsigned int flags, unsigned int type);
extern void lease_get_mtime(struct inode *, struct timespec64 *time);
extern int generic_setlease(struct file *, long, struct file_lock **, void **priv);
extern int vfs_setlease(struct file *, long, struct file_lock **, void **);
extern int lease_modify(struct file_lock *, int, struct list_head *);

struct notifier_block;
extern int lease_register_notifier(struct notifier_block *);
extern void lease_unregister_notifier(struct notifier_block *);

struct files_struct;
extern void show_fd_locks(struct seq_file *f,
    struct file *filp, struct files_struct *files);
extern bool locks_owner_has_blockers(struct file_lock_context *flctx,
   fl_owner_t owner);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct file_lock_context *
locks_inode_context(const struct inode *inode)
{
 return ({ union { typeof( _Generic((*&inode->i_flctx), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&inode->i_flctx))) __val; char __c[1]; } __u; typeof(&inode->i_flctx) __p = (&inode->i_flctx); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_315(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&inode->i_flctx) == sizeof(char) || sizeof(*&inode->i_flctx) == sizeof(short) || sizeof(*&inode->i_flctx) == sizeof(int) || sizeof(*&inode->i_flctx) == sizeof(long)))) __compiletime_assert_315(); } while (0); kasan_check_read(__p, sizeof(*&inode->i_flctx)); switch (sizeof(*&inode->i_flctx)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&inode->i_flctx))__u.__val; });
# 1194 "./include/linux/fs.h"
}
# 1350 "./include/linux/fs.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct inode *file_inode(const struct file *f)
{
 return f->f_inode;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct dentry *file_dentry(const struct file *file)
{
 return d_real(file->f_path.dentry, file_inode(file));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int locks_lock_file_wait(struct file *filp, struct file_lock *fl)
{
 return locks_lock_inode_wait(file_inode(filp), fl);
}

struct fasync_struct {
 rwlock_t fa_lock;
 int magic;
 int fa_fd;
 struct fasync_struct *fa_next; /* singly linked list */
 struct file *fa_file;
 struct callback_head fa_rcu;
};



/* SMP safe fasync helpers: */
extern int fasync_helper(int, struct file *, int, struct fasync_struct **);
extern struct fasync_struct *fasync_insert_entry(int, struct file *, struct fasync_struct **, struct fasync_struct *);
extern int fasync_remove_entry(struct file *, struct fasync_struct **);
extern struct fasync_struct *fasync_alloc(void);
extern void fasync_free(struct fasync_struct *);

/* can be called from interrupts */
extern void kill_fasync(struct fasync_struct **, int, int);

extern void __f_setown(struct file *filp, struct pid *, enum pid_type, int force);
extern int f_setown(struct file *filp, unsigned long arg, int force);
extern void f_delown(struct file *filp);
extern pid_t f_getown(struct file *filp);
extern int send_sigurg(struct fown_struct *fown);

/*
 * sb->s_flags.  Note that these mirror the equivalent MS_* flags where
 * represented in both.
 */
# 1412 "./include/linux/fs.h"
/* These sb flags are internal to the kernel */







/* These flags relate to encoding and casefolding */





/*
 *	Umount options
 */







/* sb->s_iflags */





/* sb->s_iflags to limit user namespace mounts */
# 1452 "./include/linux/fs.h"
/* Possible states of 'frozen' field */
enum {
 SB_UNFROZEN = 0, /* FS is unfrozen */
 SB_FREEZE_WRITE = 1, /* Writes, dir ops, ioctls frozen */
 SB_FREEZE_PAGEFAULT = 2, /* Page faults stopped as well */
 SB_FREEZE_FS = 3, /* For internal FS use (e.g. to stop
					 * internal threads if needed) */
 SB_FREEZE_COMPLETE = 4, /* ->freeze_fs finished successfully */
};



struct sb_writers {
 int frozen; /* Is sb frozen? */
 wait_queue_head_t wait_unfrozen; /* wait for thaw */
 struct percpu_rw_semaphore rw_sem[(SB_FREEZE_COMPLETE - 1)];
};

struct super_block {
 struct list_head s_list; /* Keep this first */
 dev_t s_dev; /* search index; _not_ kdev_t */
 unsigned char s_blocksize_bits;
 unsigned long s_blocksize;
 loff_t s_maxbytes; /* Max file size */
 struct file_system_type *s_type;
 const struct super_operations *s_op;
 const struct dquot_operations *dq_op;
 const struct quotactl_ops *s_qcop;
 const struct export_operations *s_export_op;
 unsigned long s_flags;
 unsigned long s_iflags; /* internal SB_I_* flags */
 unsigned long s_magic;
 struct dentry *s_root;
 struct rw_semaphore s_umount;
 int s_count;
 atomic_t s_active;

 void *s_security;

 const struct xattr_handler **s_xattr;
# 1503 "./include/linux/fs.h"
 struct hlist_bl_head s_roots; /* alternate root dentries for NFS */
 struct list_head s_mounts; /* list of mounts; _not_ for fs use */
 struct block_device *s_bdev;
 struct backing_dev_info *s_bdi;
 struct mtd_info *s_mtd;
 struct hlist_node s_instances;
 unsigned int s_quota_types; /* Bitmask of supported quota types */
 struct quota_info s_dquot; /* Diskquota specific options */

 struct sb_writers s_writers;

 /*
	 * Keep s_fs_info, s_time_gran, s_fsnotify_mask, and
	 * s_fsnotify_marks together for cache efficiency. They are frequently
	 * accessed and rarely modified.
	 */
 void *s_fs_info; /* Filesystem private info */

 /* Granularity of c/m/atime in ns (cannot be worse than a second) */
 u32 s_time_gran;
 /* Time limits for c/m/atime in seconds */
 time64_t s_time_min;
 time64_t s_time_max;

 __u32 s_fsnotify_mask;
 struct fsnotify_mark_connector /* nothing */ *s_fsnotify_marks;


 char s_id[32]; /* Informational name */
 uuid_t s_uuid; /* UUID */

 unsigned int s_max_links;
 fmode_t s_mode;

 /*
	 * The next field is for VFS *only*. No filesystems have any business
	 * even looking at it. You had been warned.
	 */
 struct mutex s_vfs_rename_mutex; /* Kludge */

 /*
	 * Filesystem subtype.  If non-empty the filesystem type field
	 * in /proc/mounts will be "type.subtype"
	 */
 const char *s_subtype;

 const struct dentry_operations *s_d_op; /* default d_op for dentries */

 struct shrinker s_shrink; /* per-sb shrinker handle */

 /* Number of inodes with nlink == 0 but still referenced */
 atomic_long_t s_remove_count;

 /*
	 * Number of inode/mount/sb objects that are being watched, note that
	 * inodes objects are currently double-accounted.
	 */
 atomic_long_t s_fsnotify_connectors;

 /* Being remounted read-only */
 int s_readonly_remount;

 /* per-sb errseq_t for reporting writeback errors via syncfs */
 errseq_t s_wb_err;

 /* AIO completions deferred from interrupt context */
 struct workqueue_struct *s_dio_done_wq;
 struct hlist_head s_pins;

 /*
	 * Owning user namespace and default context in which to
	 * interpret filesystem uids, gids, quotas, device nodes,
	 * xattrs and security labels.
	 */
 struct user_namespace *s_user_ns;

 /*
	 * The list_lru structure is essentially just a pointer to a table
	 * of per-node lru lists, each of which has its own spinlock.
	 * There is no need to put them into separate cachelines.
	 */
 struct list_lru s_dentry_lru;
 struct list_lru s_inode_lru;
 struct callback_head rcu;
 struct work_struct destroy_work;

 struct mutex s_sync_lock; /* sync serialisation lock */

 /*
	 * Indicates how deep in a filesystem stack this SB is
	 */
 int s_stack_depth;

 /* s_inode_list_lock protects s_inodes */
 spinlock_t s_inode_list_lock __attribute__((__aligned__((1 << (6)))));
 struct list_head s_inodes; /* all inodes */

 spinlock_t s_inode_wblist_lock;
 struct list_head s_inodes_wb; /* writeback inodes */
} ;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct user_namespace *i_user_ns(const struct inode *inode)
{
 return inode->i_sb->s_user_ns;
}

/* Helper functions so that in most cases filesystems will
 * not need to deal directly with kuid_t and kgid_t and can
 * instead deal with the raw numeric values that are stored
 * in the filesystem.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) uid_t i_uid_read(const struct inode *inode)
{
 return from_kuid(i_user_ns(inode), inode->i_uid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) gid_t i_gid_read(const struct inode *inode)
{
 return from_kgid(i_user_ns(inode), inode->i_gid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_uid_write(struct inode *inode, uid_t uid)
{
 inode->i_uid = make_kuid(i_user_ns(inode), uid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_gid_write(struct inode *inode, gid_t gid)
{
 inode->i_gid = make_kgid(i_user_ns(inode), gid);
}

/**
 * i_uid_into_vfsuid - map an inode's i_uid down into a mnt_userns
 * @mnt_userns: user namespace of the mount the inode was found from
 * @inode: inode to map
 *
 * Return: whe inode's i_uid mapped down according to @mnt_userns.
 * If the inode's i_uid has no mapping INVALID_VFSUID is returned.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vfsuid_t i_uid_into_vfsuid(struct user_namespace *mnt_userns,
      const struct inode *inode)
{
 return make_vfsuid(mnt_userns, i_user_ns(inode), inode->i_uid);
}

/**
 * i_uid_needs_update - check whether inode's i_uid needs to be updated
 * @mnt_userns: user namespace of the mount the inode was found from
 * @attr: the new attributes of @inode
 * @inode: the inode to update
 *
 * Check whether the $inode's i_uid field needs to be updated taking idmapped
 * mounts into account if the filesystem supports it.
 *
 * Return: true if @inode's i_uid field needs to be updated, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool i_uid_needs_update(struct user_namespace *mnt_userns,
          const struct iattr *attr,
          const struct inode *inode)
{
 return ((attr->ia_valid & (1 << 1)) &&
  !vfsuid_eq(attr->ia_vfsuid,
      i_uid_into_vfsuid(mnt_userns, inode)));
}

/**
 * i_uid_update - update @inode's i_uid field
 * @mnt_userns: user namespace of the mount the inode was found from
 * @attr: the new attributes of @inode
 * @inode: the inode to update
 *
 * Safely update @inode's i_uid field translating the vfsuid of any idmapped
 * mount into the filesystem kuid.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_uid_update(struct user_namespace *mnt_userns,
    const struct iattr *attr,
    struct inode *inode)
{
 if (attr->ia_valid & (1 << 1))
  inode->i_uid = from_vfsuid(mnt_userns, i_user_ns(inode),
        attr->ia_vfsuid);
}

/**
 * i_gid_into_vfsgid - map an inode's i_gid down into a mnt_userns
 * @mnt_userns: user namespace of the mount the inode was found from
 * @inode: inode to map
 *
 * Return: the inode's i_gid mapped down according to @mnt_userns.
 * If the inode's i_gid has no mapping INVALID_VFSGID is returned.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vfsgid_t i_gid_into_vfsgid(struct user_namespace *mnt_userns,
      const struct inode *inode)
{
 return make_vfsgid(mnt_userns, i_user_ns(inode), inode->i_gid);
}

/**
 * i_gid_needs_update - check whether inode's i_gid needs to be updated
 * @mnt_userns: user namespace of the mount the inode was found from
 * @attr: the new attributes of @inode
 * @inode: the inode to update
 *
 * Check whether the $inode's i_gid field needs to be updated taking idmapped
 * mounts into account if the filesystem supports it.
 *
 * Return: true if @inode's i_gid field needs to be updated, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool i_gid_needs_update(struct user_namespace *mnt_userns,
          const struct iattr *attr,
          const struct inode *inode)
{
 return ((attr->ia_valid & (1 << 2)) &&
  !vfsgid_eq(attr->ia_vfsgid,
      i_gid_into_vfsgid(mnt_userns, inode)));
}

/**
 * i_gid_update - update @inode's i_gid field
 * @mnt_userns: user namespace of the mount the inode was found from
 * @attr: the new attributes of @inode
 * @inode: the inode to update
 *
 * Safely update @inode's i_gid field translating the vfsgid of any idmapped
 * mount into the filesystem kgid.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_gid_update(struct user_namespace *mnt_userns,
    const struct iattr *attr,
    struct inode *inode)
{
 if (attr->ia_valid & (1 << 2))
  inode->i_gid = from_vfsgid(mnt_userns, i_user_ns(inode),
        attr->ia_vfsgid);
}

/**
 * inode_fsuid_set - initialize inode's i_uid field with callers fsuid
 * @inode: inode to initialize
 * @mnt_userns: user namespace of the mount the inode was found from
 *
 * Initialize the i_uid field of @inode. If the inode was found/created via
 * an idmapped mount map the caller's fsuid according to @mnt_users.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_fsuid_set(struct inode *inode,
       struct user_namespace *mnt_userns)
{
 inode->i_uid = mapped_fsuid(mnt_userns, i_user_ns(inode));
}

/**
 * inode_fsgid_set - initialize inode's i_gid field with callers fsgid
 * @inode: inode to initialize
 * @mnt_userns: user namespace of the mount the inode was found from
 *
 * Initialize the i_gid field of @inode. If the inode was found/created via
 * an idmapped mount map the caller's fsgid according to @mnt_users.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_fsgid_set(struct inode *inode,
       struct user_namespace *mnt_userns)
{
 inode->i_gid = mapped_fsgid(mnt_userns, i_user_ns(inode));
}

/**
 * fsuidgid_has_mapping() - check whether caller's fsuid/fsgid is mapped
 * @sb: the superblock we want a mapping in
 * @mnt_userns: user namespace of the relevant mount
 *
 * Check whether the caller's fsuid and fsgid have a valid mapping in the
 * s_user_ns of the superblock @sb. If the caller is on an idmapped mount map
 * the caller's fsuid and fsgid according to the @mnt_userns first.
 *
 * Return: true if fsuid and fsgid is mapped, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool fsuidgid_has_mapping(struct super_block *sb,
     struct user_namespace *mnt_userns)
{
 struct user_namespace *fs_userns = sb->s_user_ns;
 kuid_t kuid;
 kgid_t kgid;

 kuid = mapped_fsuid(mnt_userns, fs_userns);
 if (!uid_valid(kuid))
  return false;
 kgid = mapped_fsgid(mnt_userns, fs_userns);
 if (!gid_valid(kgid))
  return false;
 return kuid_has_mapping(fs_userns, kuid) &&
        kgid_has_mapping(fs_userns, kgid);
}

extern struct timespec64 current_time(struct inode *inode);

/*
 * Snapshotting support.
 */

/*
 * These are internal functions, please use sb_start_{write,pagefault,intwrite}
 * instead.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __sb_end_write(struct super_block *sb, int level)
{
 percpu_up_read(sb->s_writers.rw_sem + level-1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __sb_start_write(struct super_block *sb, int level)
{
 percpu_down_read(sb->s_writers.rw_sem + level - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __sb_start_write_trylock(struct super_block *sb, int level)
{
 return percpu_down_read_trylock(sb->s_writers.rw_sem + level - 1);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sb_write_started(const struct super_block *sb)
{
 return (1);
}

/**
 * sb_end_write - drop write access to a superblock
 * @sb: the super we wrote to
 *
 * Decrement number of writers to the filesystem. Wake up possible waiters
 * wanting to freeze the filesystem.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sb_end_write(struct super_block *sb)
{
 __sb_end_write(sb, SB_FREEZE_WRITE);
}

/**
 * sb_end_pagefault - drop write access to a superblock from a page fault
 * @sb: the super we wrote to
 *
 * Decrement number of processes handling write page fault to the filesystem.
 * Wake up possible waiters wanting to freeze the filesystem.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sb_end_pagefault(struct super_block *sb)
{
 __sb_end_write(sb, SB_FREEZE_PAGEFAULT);
}

/**
 * sb_end_intwrite - drop write access to a superblock for internal fs purposes
 * @sb: the super we wrote to
 *
 * Decrement fs-internal number of writers to the filesystem.  Wake up possible
 * waiters wanting to freeze the filesystem.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sb_end_intwrite(struct super_block *sb)
{
 __sb_end_write(sb, SB_FREEZE_FS);
}

/**
 * sb_start_write - get write access to a superblock
 * @sb: the super we write to
 *
 * When a process wants to write data or metadata to a file system (i.e. dirty
 * a page or an inode), it should embed the operation in a sb_start_write() -
 * sb_end_write() pair to get exclusion against file system freezing. This
 * function increments number of writers preventing freezing. If the file
 * system is already frozen, the function waits until the file system is
 * thawed.
 *
 * Since freeze protection behaves as a lock, users have to preserve
 * ordering of freeze protection and other filesystem locks. Generally,
 * freeze protection should be the outermost lock. In particular, we have:
 *
 * sb_start_write
 *   -> i_mutex			(write path, truncate, directory ops, ...)
 *   -> s_umount		(freeze_super, thaw_super)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sb_start_write(struct super_block *sb)
{
 __sb_start_write(sb, SB_FREEZE_WRITE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sb_start_write_trylock(struct super_block *sb)
{
 return __sb_start_write_trylock(sb, SB_FREEZE_WRITE);
}

/**
 * sb_start_pagefault - get write access to a superblock from a page fault
 * @sb: the super we write to
 *
 * When a process starts handling write page fault, it should embed the
 * operation into sb_start_pagefault() - sb_end_pagefault() pair to get
 * exclusion against file system freezing. This is needed since the page fault
 * is going to dirty a page. This function increments number of running page
 * faults preventing freezing. If the file system is already frozen, the
 * function waits until the file system is thawed.
 *
 * Since page fault freeze protection behaves as a lock, users have to preserve
 * ordering of freeze protection and other filesystem locks. It is advised to
 * put sb_start_pagefault() close to mmap_lock in lock ordering. Page fault
 * handling code implies lock dependency:
 *
 * mmap_lock
 *   -> sb_start_pagefault
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sb_start_pagefault(struct super_block *sb)
{
 __sb_start_write(sb, SB_FREEZE_PAGEFAULT);
}

/**
 * sb_start_intwrite - get write access to a superblock for internal fs purposes
 * @sb: the super we write to
 *
 * This is the third level of protection against filesystem freezing. It is
 * free for use by a filesystem. The only requirement is that it must rank
 * below sb_start_pagefault.
 *
 * For example filesystem can call sb_start_intwrite() when starting a
 * transaction which somewhat eases handling of freezing for internal sources
 * of filesystem changes (internal fs threads, discarding preallocation on file
 * close, etc.).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sb_start_intwrite(struct super_block *sb)
{
 __sb_start_write(sb, SB_FREEZE_FS);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sb_start_intwrite_trylock(struct super_block *sb)
{
 return __sb_start_write_trylock(sb, SB_FREEZE_FS);
}

bool inode_owner_or_capable(struct user_namespace *mnt_userns,
       const struct inode *inode);

/*
 * VFS helper functions..
 */
int vfs_create(struct user_namespace *, struct inode *,
        struct dentry *, umode_t, bool);
int vfs_mkdir(struct user_namespace *, struct inode *,
       struct dentry *, umode_t);
int vfs_mknod(struct user_namespace *, struct inode *, struct dentry *,
              umode_t, dev_t);
int vfs_symlink(struct user_namespace *, struct inode *,
  struct dentry *, const char *);
int vfs_link(struct dentry *, struct user_namespace *, struct inode *,
      struct dentry *, struct inode **);
int vfs_rmdir(struct user_namespace *, struct inode *, struct dentry *);
int vfs_unlink(struct user_namespace *, struct inode *, struct dentry *,
        struct inode **);

/**
 * struct renamedata - contains all information required for renaming
 * @old_mnt_userns:    old user namespace of the mount the inode was found from
 * @old_dir:           parent of source
 * @old_dentry:                source
 * @new_mnt_userns:    new user namespace of the mount the inode was found from
 * @new_dir:           parent of destination
 * @new_dentry:                destination
 * @delegated_inode:   returns an inode needing a delegation break
 * @flags:             rename flags
 */
struct renamedata {
 struct user_namespace *old_mnt_userns;
 struct inode *old_dir;
 struct dentry *old_dentry;
 struct user_namespace *new_mnt_userns;
 struct inode *new_dir;
 struct dentry *new_dentry;
 struct inode **delegated_inode;
 unsigned int flags;
} ;

int vfs_rename(struct renamedata *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vfs_whiteout(struct user_namespace *mnt_userns,
          struct inode *dir, struct dentry *dentry)
{
 return vfs_mknod(mnt_userns, dir, dentry, 0020000 | 0,
    0);
}

struct file *vfs_tmpfile_open(struct user_namespace *mnt_userns,
   const struct path *parentpath,
   umode_t mode, int open_flag, const struct cred *cred);

int vfs_mkobj(struct dentry *, umode_t,
  int (*f)(struct dentry *, umode_t, void *),
  void *);

int vfs_fchown(struct file *file, uid_t user, gid_t group);
int vfs_fchmod(struct file *file, umode_t mode);
int vfs_utimes(const struct path *path, struct timespec64 *times);

extern long vfs_ioctl(struct file *file, unsigned int cmd, unsigned long arg);


extern long compat_ptr_ioctl(struct file *file, unsigned int cmd,
     unsigned long arg);




/*
 * VFS file helper functions.
 */
void inode_init_owner(struct user_namespace *mnt_userns, struct inode *inode,
        const struct inode *dir, umode_t mode);
extern bool may_open_dev(const struct path *path);
umode_t mode_strip_sgid(struct user_namespace *mnt_userns,
   const struct inode *dir, umode_t mode);

/*
 * This is the "filldir" function type, used by readdir() to let
 * the kernel specify what kind of dirent layout it wants to have.
 * This allows the kernel to read directories into kernel space or
 * to have different dirent layouts depending on the binary type.
 * Return 'true' to keep going and 'false' if there are no more entries.
 */
struct dir_context;
typedef bool (*filldir_t)(struct dir_context *, const char *, int, loff_t, u64,
    unsigned);

struct dir_context {
 filldir_t actor;
 loff_t pos;
};

/*
 * These flags let !MMU mmap() govern direct device mapping vs immediate
 * copying more easily for MAP_PRIVATE, especially for ROM filesystems.
 *
 * NOMMU_MAP_COPY:	Copy can be mapped (MAP_PRIVATE)
 * NOMMU_MAP_DIRECT:	Can be mapped directly (MAP_SHARED)
 * NOMMU_MAP_READ:	Can be mapped for reading
 * NOMMU_MAP_WRITE:	Can be mapped for writing
 * NOMMU_MAP_EXEC:	Can be mapped for execution
 */
# 2057 "./include/linux/fs.h"
/*
 * These flags control the behavior of the remap_file_range function pointer.
 * If it is called with len == 0 that means "remap to end of source file".
 * See Documentation/filesystems/vfs.rst for more details about this call.
 *
 * REMAP_FILE_DEDUP: only remap if contents identical (i.e. deduplicate)
 * REMAP_FILE_CAN_SHORTEN: caller can handle a shortened request
 */



/*
 * These flags signal that the caller is ok with altering various aspects of
 * the behavior of the remap operation.  The changes must be made by the
 * implementation; the vfs remap helper functions can take advantage of them.
 * Flags in this category exist to preserve the quirky behavior of the hoisted
 * btrfs clone/dedupe ioctls.
 */


/*
 * These flags control the behavior of vfs_copy_file_range().
 * They are not available to the user via syscall.
 *
 * COPY_FILE_SPLICE: call splice direct instead of fs clone/copy ops
 */


struct iov_iter;
struct io_uring_cmd;

struct file_operations {
 struct module *owner;
 loff_t (*llseek) (struct file *, loff_t, int);
 ssize_t (*read) (struct file *, char /* nothing */ *, size_t, loff_t *);
 ssize_t (*write) (struct file *, const char /* nothing */ *, size_t, loff_t *);
 ssize_t (*read_iter) (struct kiocb *, struct iov_iter *);
 ssize_t (*write_iter) (struct kiocb *, struct iov_iter *);
 int (*iopoll)(struct kiocb *kiocb, struct io_comp_batch *,
   unsigned int flags);
 int (*iterate) (struct file *, struct dir_context *);
 int (*iterate_shared) (struct file *, struct dir_context *);
 __poll_t (*poll) (struct file *, struct poll_table_struct *);
 long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
 long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
 int (*mmap) (struct file *, struct vm_area_struct *);
 unsigned long mmap_supported_flags;
 int (*open) (struct inode *, struct file *);
 int (*flush) (struct file *, fl_owner_t id);
 int (*release) (struct inode *, struct file *);
 int (*fsync) (struct file *, loff_t, loff_t, int datasync);
 int (*fasync) (int, struct file *, int);
 int (*lock) (struct file *, int, struct file_lock *);
 ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int);
 unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
 int (*check_flags)(int);
 int (*flock) (struct file *, int, struct file_lock *);
 ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);
 ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);
 int (*setlease)(struct file *, long, struct file_lock **, void **);
 long (*fallocate)(struct file *file, int mode, loff_t offset,
     loff_t len);
 void (*show_fdinfo)(struct seq_file *m, struct file *f);



 ssize_t (*copy_file_range)(struct file *, loff_t, struct file *,
   loff_t, size_t, unsigned int);
 loff_t (*remap_file_range)(struct file *file_in, loff_t pos_in,
       struct file *file_out, loff_t pos_out,
       loff_t len, unsigned int remap_flags);
 int (*fadvise)(struct file *, loff_t, loff_t, int);
 int (*uring_cmd)(struct io_uring_cmd *ioucmd, unsigned int issue_flags);
 int (*uring_cmd_iopoll)(struct io_uring_cmd *, struct io_comp_batch *,
    unsigned int poll_flags);
} ;

struct inode_operations {
 struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int);
 const char * (*get_link) (struct dentry *, struct inode *, struct delayed_call *);
 int (*permission) (struct user_namespace *, struct inode *, int);
 struct posix_acl * (*get_inode_acl)(struct inode *, int, bool);

 int (*readlink) (struct dentry *, char /* nothing */ *,int);

 int (*create) (struct user_namespace *, struct inode *,struct dentry *,
         umode_t, bool);
 int (*link) (struct dentry *,struct inode *,struct dentry *);
 int (*unlink) (struct inode *,struct dentry *);
 int (*symlink) (struct user_namespace *, struct inode *,struct dentry *,
   const char *);
 int (*mkdir) (struct user_namespace *, struct inode *,struct dentry *,
        umode_t);
 int (*rmdir) (struct inode *,struct dentry *);
 int (*mknod) (struct user_namespace *, struct inode *,struct dentry *,
        umode_t,dev_t);
 int (*rename) (struct user_namespace *, struct inode *, struct dentry *,
   struct inode *, struct dentry *, unsigned int);
 int (*setattr) (struct user_namespace *, struct dentry *,
   struct iattr *);
 int (*getattr) (struct user_namespace *, const struct path *,
   struct kstat *, u32, unsigned int);
 ssize_t (*listxattr) (struct dentry *, char *, size_t);
 int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start,
        u64 len);
 int (*update_time)(struct inode *, struct timespec64 *, int);
 int (*atomic_open)(struct inode *, struct dentry *,
      struct file *, unsigned open_flag,
      umode_t create_mode);
 int (*tmpfile) (struct user_namespace *, struct inode *,
   struct file *, umode_t);
 struct posix_acl *(*get_acl)(struct user_namespace *, struct dentry *,
         int);
 int (*set_acl)(struct user_namespace *, struct dentry *,
         struct posix_acl *, int);
 int (*fileattr_set)(struct user_namespace *mnt_userns,
       struct dentry *dentry, struct fileattr *fa);
 int (*fileattr_get)(struct dentry *dentry, struct fileattr *fa);
} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t call_read_iter(struct file *file, struct kiocb *kio,
         struct iov_iter *iter)
{
 return file->f_op->read_iter(kio, iter);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t call_write_iter(struct file *file, struct kiocb *kio,
          struct iov_iter *iter)
{
 return file->f_op->write_iter(kio, iter);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int call_mmap(struct file *file, struct vm_area_struct *vma)
{
 return file->f_op->mmap(file, vma);
}

extern ssize_t vfs_read(struct file *, char /* nothing */ *, size_t, loff_t *);
extern ssize_t vfs_write(struct file *, const char /* nothing */ *, size_t, loff_t *);
extern ssize_t vfs_copy_file_range(struct file *, loff_t , struct file *,
       loff_t, size_t, unsigned int);
extern ssize_t generic_copy_file_range(struct file *file_in, loff_t pos_in,
           struct file *file_out, loff_t pos_out,
           size_t len, unsigned int flags);
int __generic_remap_file_range_prep(struct file *file_in, loff_t pos_in,
        struct file *file_out, loff_t pos_out,
        loff_t *len, unsigned int remap_flags,
        const struct iomap_ops *dax_read_ops);
int generic_remap_file_range_prep(struct file *file_in, loff_t pos_in,
      struct file *file_out, loff_t pos_out,
      loff_t *count, unsigned int remap_flags);
extern loff_t do_clone_file_range(struct file *file_in, loff_t pos_in,
      struct file *file_out, loff_t pos_out,
      loff_t len, unsigned int remap_flags);
extern loff_t vfs_clone_file_range(struct file *file_in, loff_t pos_in,
       struct file *file_out, loff_t pos_out,
       loff_t len, unsigned int remap_flags);
extern int vfs_dedupe_file_range(struct file *file,
     struct file_dedupe_range *same);
extern loff_t vfs_dedupe_file_range_one(struct file *src_file, loff_t src_pos,
     struct file *dst_file, loff_t dst_pos,
     loff_t len, unsigned int remap_flags);


struct super_operations {
    struct inode *(*alloc_inode)(struct super_block *sb);
 void (*destroy_inode)(struct inode *);
 void (*free_inode)(struct inode *);

    void (*dirty_inode) (struct inode *, int flags);
 int (*write_inode) (struct inode *, struct writeback_control *wbc);
 int (*drop_inode) (struct inode *);
 void (*evict_inode) (struct inode *);
 void (*put_super) (struct super_block *);
 int (*sync_fs)(struct super_block *sb, int wait);
 int (*freeze_super) (struct super_block *);
 int (*freeze_fs) (struct super_block *);
 int (*thaw_super) (struct super_block *);
 int (*unfreeze_fs) (struct super_block *);
 int (*statfs) (struct dentry *, struct kstatfs *);
 int (*remount_fs) (struct super_block *, int *, char *);
 void (*umount_begin) (struct super_block *);

 int (*show_options)(struct seq_file *, struct dentry *);
 int (*show_devname)(struct seq_file *, struct dentry *);
 int (*show_path)(struct seq_file *, struct dentry *);
 int (*show_stats)(struct seq_file *, struct dentry *);

 ssize_t (*quota_read)(struct super_block *, int, char *, size_t, loff_t);
 ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);
 struct dquot **(*get_dquots)(struct inode *);

 long (*nr_cached_objects)(struct super_block *,
      struct shrink_control *);
 long (*free_cached_objects)(struct super_block *,
        struct shrink_control *);
};

/*
 * Inode flags - they have no relation to superblock flags now
 */
# 2281 "./include/linux/fs.h"
/*
 * Note that nosuid etc flags are inode-specific: setting some file-system
 * flags just means all the inodes inherit those flags by default. It might be
 * possible to override it selectively if you really wanted to with some
 * ioctl() that is not currently implemented.
 *
 * Exception: SB_RDONLY is always applied to the entire file system.
 *
 * Unfortunately, it is possible to change a filesystems flags with it mounted
 * with files in use.  This means that all of the inodes will not have their
 * i_flags updated.  Hence, i_flags no longer inherit the superblock mount
 * flags, so these have to be checked separately. -- rmk@arm.uk.linux.org
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sb_rdonly(const struct super_block *sb) { return sb->s_flags & 1 /* Mount read-only */; }
# 2326 "./include/linux/fs.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool HAS_UNMAPPED_ID(struct user_namespace *mnt_userns,
       struct inode *inode)
{
 return !vfsuid_valid(i_uid_into_vfsuid(mnt_userns, inode)) ||
        !vfsgid_valid(i_gid_into_vfsgid(mnt_userns, inode));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_sync_kiocb(struct kiocb *kiocb, struct file *filp)
{
 *kiocb = (struct kiocb) {
  .ki_filp = filp,
  .ki_flags = filp->f_iocb_flags,
  .ki_ioprio = get_current_ioprio(),
 };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kiocb_clone(struct kiocb *kiocb, struct kiocb *kiocb_src,
          struct file *filp)
{
 *kiocb = (struct kiocb) {
  .ki_filp = filp,
  .ki_flags = kiocb_src->ki_flags,
  .ki_ioprio = kiocb_src->ki_ioprio,
  .ki_pos = kiocb_src->ki_pos,
 };
}

/*
 * Inode state bits.  Protected by inode->i_lock
 *
 * Four bits determine the dirty state of the inode: I_DIRTY_SYNC,
 * I_DIRTY_DATASYNC, I_DIRTY_PAGES, and I_DIRTY_TIME.
 *
 * Four bits define the lifetime of an inode.  Initially, inodes are I_NEW,
 * until that flag is cleared.  I_WILL_FREE, I_FREEING and I_CLEAR are set at
 * various stages of removing an inode.
 *
 * Two bits are used for locking and completion notification, I_NEW and I_SYNC.
 *
 * I_DIRTY_SYNC		Inode is dirty, but doesn't have to be written on
 *			fdatasync() (unless I_DIRTY_DATASYNC is also set).
 *			Timestamp updates are the usual cause.
 * I_DIRTY_DATASYNC	Data-related inode changes pending.  We keep track of
 *			these changes separately from I_DIRTY_SYNC so that we
 *			don't have to write inode on fdatasync() when only
 *			e.g. the timestamps have changed.
 * I_DIRTY_PAGES	Inode has dirty pages.  Inode itself may be clean.
 * I_DIRTY_TIME		The inode itself has dirty timestamps, and the
 *			lazytime mount option is enabled.  We keep track of this
 *			separately from I_DIRTY_SYNC in order to implement
 *			lazytime.  This gets cleared if I_DIRTY_INODE
 *			(I_DIRTY_SYNC and/or I_DIRTY_DATASYNC) gets set. But
 *			I_DIRTY_TIME can still be set if I_DIRTY_SYNC is already
 *			in place because writeback might already be in progress
 *			and we don't want to lose the time update
 * I_NEW		Serves as both a mutex and completion notification.
 *			New inodes set I_NEW.  If two processes both create
 *			the same inode, one of them will release its inode and
 *			wait for I_NEW to be released before returning.
 *			Inodes in I_WILL_FREE, I_FREEING or I_CLEAR state can
 *			also cause waiting on I_NEW, without I_NEW actually
 *			being set.  find_inode() uses this to prevent returning
 *			nearly-dead inodes.
 * I_WILL_FREE		Must be set when calling write_inode_now() if i_count
 *			is zero.  I_FREEING must be set when I_WILL_FREE is
 *			cleared.
 * I_FREEING		Set when inode is about to be freed but still has dirty
 *			pages or buffers attached or the inode itself is still
 *			dirty.
 * I_CLEAR		Added by clear_inode().  In this state the inode is
 *			clean and can be destroyed.  Inode keeps I_FREEING.
 *
 *			Inodes that are I_WILL_FREE, I_FREEING or I_CLEAR are
 *			prohibited for many purposes.  iget() must wait for
 *			the inode to be completely released, then create it
 *			anew.  Other functions will just ignore such inodes,
 *			if appropriate.  I_NEW is used for waiting.
 *
 * I_SYNC		Writeback of inode is running. The bit is set during
 *			data writeback, and cleared with a wakeup on the bit
 *			address once it is done. The bit is also used to pin
 *			the inode in memory for flusher thread.
 *
 * I_REFERENCED		Marks the inode as recently references on the LRU list.
 *
 * I_DIO_WAKEUP		Never set.  Only used as a key for wait_on_bit().
 *
 * I_WB_SWITCH		Cgroup bdi_writeback switching in progress.  Used to
 *			synchronize competing switching instances and to tell
 *			wb stat updates to grab the i_pages lock.  See
 *			inode_switch_wbs_work_fn() for details.
 *
 * I_OVL_INUSE		Used by overlayfs to get exclusive ownership on upper
 *			and work dirs among overlayfs mounts.
 *
 * I_CREATING		New object's inode in the middle of setting up.
 *
 * I_DONTCACHE		Evict inode as soon as it is not used anymore.
 *
 * I_SYNC_QUEUED	Inode is queued in b_io or b_more_io writeback lists.
 *			Used to detect that mark_inode_dirty() should not move
 * 			inode between dirty lists.
 *
 * I_PINNING_FSCACHE_WB	Inode is pinning an fscache object for writeback.
 *
 * Q: What is the difference between I_WILL_FREE and I_FREEING?
 */
# 2459 "./include/linux/fs.h"
extern void __mark_inode_dirty(struct inode *, int);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mark_inode_dirty(struct inode *inode)
{
 __mark_inode_dirty(inode, (((1 << 0) | (1 << 1)) | (1 << 2)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mark_inode_dirty_sync(struct inode *inode)
{
 __mark_inode_dirty(inode, (1 << 0));
}

/*
 * Returns true if the given inode itself only has dirty timestamps (its pages
 * may still be dirty) and isn't currently being allocated or freed.
 * Filesystems should call this if when writing an inode when lazytime is
 * enabled, they want to opportunistically write the timestamps of other inodes
 * located very nearby on-disk, e.g. in the same inode block.  This returns true
 * if the given inode is in need of such an opportunistic update.  Requires
 * i_lock, or at least later re-checking under i_lock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool inode_is_dirtytime_only(struct inode *inode)
{
 return (inode->i_state & ((1 << 11) | (1 << 3) |
      (1 << 5) | (1 << 4))) == (1 << 11);
}

extern void inc_nlink(struct inode *inode);
extern void drop_nlink(struct inode *inode);
extern void clear_nlink(struct inode *inode);
extern void set_nlink(struct inode *inode, unsigned int nlink);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_inc_link_count(struct inode *inode)
{
 inc_nlink(inode);
 mark_inode_dirty(inode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_dec_link_count(struct inode *inode)
{
 drop_nlink(inode);
 mark_inode_dirty(inode);
}

enum file_time_flags {
 S_ATIME = 1,
 S_MTIME = 2,
 S_CTIME = 4,
 S_VERSION = 8,
};

extern bool atime_needs_update(const struct path *, struct inode *);
extern void touch_atime(const struct path *);
int inode_update_time(struct inode *inode, struct timespec64 *time, int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void file_accessed(struct file *file)
{
 if (!(file->f_flags & 01000000))
  touch_atime(&file->f_path);
}

extern int file_modified(struct file *file);
int kiocb_modified(struct kiocb *iocb);

int sync_inode_metadata(struct inode *inode, int wait);

struct file_system_type {
 const char *name;
 int fs_flags;







 int (*init_fs_context)(struct fs_context *);
 const struct fs_parameter_spec *parameters;
 struct dentry *(*mount) (struct file_system_type *, int,
         const char *, void *);
 void (*kill_sb) (struct super_block *);
 struct module *owner;
 struct file_system_type * next;
 struct hlist_head fs_supers;

 struct lock_class_key s_lock_key;
 struct lock_class_key s_umount_key;
 struct lock_class_key s_vfs_rename_key;
 struct lock_class_key s_writers_key[(SB_FREEZE_COMPLETE - 1)];

 struct lock_class_key i_lock_key;
 struct lock_class_key i_mutex_key;
 struct lock_class_key invalidate_lock_key;
 struct lock_class_key i_mutex_dir_key;
};



extern struct dentry *mount_bdev(struct file_system_type *fs_type,
 int flags, const char *dev_name, void *data,
 int (*fill_super)(struct super_block *, void *, int));
extern struct dentry *mount_single(struct file_system_type *fs_type,
 int flags, void *data,
 int (*fill_super)(struct super_block *, void *, int));
extern struct dentry *mount_nodev(struct file_system_type *fs_type,
 int flags, void *data,
 int (*fill_super)(struct super_block *, void *, int));
extern struct dentry *mount_subtree(struct vfsmount *mnt, const char *path);
void retire_super(struct super_block *sb);
void generic_shutdown_super(struct super_block *sb);
void kill_block_super(struct super_block *sb);
void kill_anon_super(struct super_block *sb);
void kill_litter_super(struct super_block *sb);
void deactivate_super(struct super_block *sb);
void deactivate_locked_super(struct super_block *sb);
int set_anon_super(struct super_block *s, void *data);
int set_anon_super_fc(struct super_block *s, struct fs_context *fc);
int get_anon_bdev(dev_t *);
void free_anon_bdev(dev_t);
struct super_block *sget_fc(struct fs_context *fc,
       int (*test)(struct super_block *, struct fs_context *),
       int (*set)(struct super_block *, struct fs_context *));
struct super_block *sget(struct file_system_type *type,
   int (*test)(struct super_block *,void *),
   int (*set)(struct super_block *,void *),
   int flags, void *data);

/* Alas, no aliases. Too much hassle with bringing module.h everywhere */




/*
 * This one is to be used *ONLY* from ->open() instances.
 * fops must be non-NULL, pinned down *and* module dependencies
 * should be sufficient to pin the caller down as well.
 */







extern int register_filesystem(struct file_system_type *);
extern int unregister_filesystem(struct file_system_type *);
extern int vfs_statfs(const struct path *, struct kstatfs *);
extern int user_statfs(const char /* nothing */ *, struct kstatfs *);
extern int fd_statfs(int, struct kstatfs *);
extern int freeze_super(struct super_block *super);
extern int thaw_super(struct super_block *super);
extern __attribute__((__format__(printf, 2, 3)))
int super_setup_bdi_name(struct super_block *sb, char *fmt, ...);
extern int super_setup_bdi(struct super_block *sb);

extern int current_umask(void);

extern void ihold(struct inode * inode);
extern void iput(struct inode *);
extern int generic_update_time(struct inode *, struct timespec64 *, int);

/* /sys/fs */
extern struct kobject *fs_kobj;




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int break_lease(struct inode *inode, unsigned int mode)
{
 /*
	 * Since this check is lockless, we must ensure that any refcounts
	 * taken are done before checking i_flctx->flc_lease. Otherwise, we
	 * could end up racing with tasks trying to set a new lease on this
	 * file.
	 */
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 if (inode->i_flctx && !list_empty_careful(&inode->i_flctx->flc_lease))
  return __break_lease(inode, mode, 32 /* lease held on this file */);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int break_deleg(struct inode *inode, unsigned int mode)
{
 /*
	 * Since this check is lockless, we must ensure that any refcounts
	 * taken are done before checking i_flctx->flc_lease. Otherwise, we
	 * could end up racing with tasks trying to set a new lease on this
	 * file.
	 */
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 if (inode->i_flctx && !list_empty_careful(&inode->i_flctx->flc_lease))
  return __break_lease(inode, mode, 4 /* NFSv4 delegation */);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int try_break_deleg(struct inode *inode, struct inode **delegated_inode)
{
 int ret;

 ret = break_deleg(inode, 00000001|00004000);
 if (ret == -11 /* Try again */ /* Operation would block */ && delegated_inode) {
  *delegated_inode = inode;
  ihold(inode);
 }
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int break_deleg_wait(struct inode **delegated_inode)
{
 int ret;

 ret = break_deleg(*delegated_inode, 00000001);
 iput(*delegated_inode);
 *delegated_inode = ((void *)0);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int break_layout(struct inode *inode, bool wait)
{
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 if (inode->i_flctx && !list_empty_careful(&inode->i_flctx->flc_lease))
  return __break_lease(inode,
    wait ? 00000001 : 00000001 | 00004000,
    2048 /* outstanding pNFS layout */);
 return 0;
}
# 2714 "./include/linux/fs.h"
/* fs/open.c */
struct audit_names;
struct filename {
 const char *name; /* pointer to actual string */
 const /* nothing */ char *uptr; /* original userland pointer */
 int refcnt;
 struct audit_names *aname;
 const char iname[];
};
_Static_assert(__builtin_offsetof(struct filename, iname) % sizeof(long) == 0, "offsetof(struct filename, iname) % sizeof(long) == 0");

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct user_namespace *file_mnt_user_ns(struct file *file)
{
 return mnt_user_ns(file->f_path.mnt);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct mnt_idmap *file_mnt_idmap(struct file *file)
{
 return mnt_idmap(file->f_path.mnt);
}

/**
 * is_idmapped_mnt - check whether a mount is mapped
 * @mnt: the mount to check
 *
 * If @mnt has an non @nop_mnt_idmap attached to it then @mnt is mapped.
 *
 * Return: true if mount is mapped, false if not.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_idmapped_mnt(const struct vfsmount *mnt)
{
 return mnt_idmap(mnt) != &nop_mnt_idmap;
}

extern long vfs_truncate(const struct path *, loff_t);
int do_truncate(struct user_namespace *, struct dentry *, loff_t start,
  unsigned int time_attrs, struct file *filp);
extern int vfs_fallocate(struct file *file, int mode, loff_t offset,
   loff_t len);
extern long do_sys_open(int dfd, const char /* nothing */ *filename, int flags,
   umode_t mode);
extern struct file *file_open_name(struct filename *, int, umode_t);
extern struct file *filp_open(const char *, int, umode_t);
extern struct file *file_open_root(const struct path *,
       const char *, int, umode_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct file *file_open_root_mnt(struct vfsmount *mnt,
       const char *name, int flags, umode_t mode)
{
 return file_open_root(&(struct path){.mnt = mnt, .dentry = mnt->mnt_root},
         name, flags, mode);
}
extern struct file * dentry_open(const struct path *, int, const struct cred *);
extern struct file *dentry_create(const struct path *path, int flags,
      umode_t mode, const struct cred *cred);
extern struct file * open_with_fake_path(const struct path *, int,
      struct inode*, const struct cred *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct file *file_clone_open(struct file *file)
{
 return dentry_open(&file->f_path, file->f_flags, file->f_cred);
}
extern int filp_close(struct file *, fl_owner_t id);

extern struct filename *getname_flags(const char /* nothing */ *, int, int *);
extern struct filename *getname_uflags(const char /* nothing */ *, int);
extern struct filename *getname(const char /* nothing */ *);
extern struct filename *getname_kernel(const char *);
extern void putname(struct filename *name);

extern int finish_open(struct file *file, struct dentry *dentry,
   int (*open)(struct inode *, struct file *));
extern int finish_no_open(struct file *file, struct dentry *dentry);

/* Helper for the simple case when original dentry is used */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int finish_open_simple(struct file *file, int error)
{
 if (error)
  return error;

 return finish_open(file, file->f_path.dentry, ((void *)0));
}

/* fs/dcache.c */
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) vfs_caches_init_early(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) vfs_caches_init(void);

extern struct kmem_cache *names_cachep;




extern struct super_block *blockdev_superblock;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sb_is_blkdev_sb(struct super_block *sb)
{
 return 1 && sb == blockdev_superblock;
}

void emergency_thaw_all(void);
extern int sync_filesystem(struct super_block *);
extern const struct file_operations def_blk_fops;
extern const struct file_operations def_chr_fops;

/* fs/char_dev.c */

/* Marks the bottom of the first segment of free char majors */

/* Marks the top and bottom of the second segment of free char majors */



extern int alloc_chrdev_region(dev_t *, unsigned, unsigned, const char *);
extern int register_chrdev_region(dev_t, unsigned, const char *);
extern int __register_chrdev(unsigned int major, unsigned int baseminor,
        unsigned int count, const char *name,
        const struct file_operations *fops);
extern void __unregister_chrdev(unsigned int major, unsigned int baseminor,
    unsigned int count, const char *name);
extern void unregister_chrdev_region(dev_t, unsigned);
extern void chrdev_show(struct seq_file *,off_t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_chrdev(unsigned int major, const char *name,
      const struct file_operations *fops)
{
 return __register_chrdev(major, 0, 256, name, fops);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unregister_chrdev(unsigned int major, const char *name)
{
 __unregister_chrdev(major, 0, 256, name);
}

extern void init_special_inode(struct inode *, umode_t, dev_t);

/* Invalid inode operations -- fs/bad_inode.c */
extern void make_bad_inode(struct inode *);
extern bool is_bad_inode(struct inode *);

extern int __attribute__((__warn_unused_result__)) file_fdatawait_range(struct file *file, loff_t lstart,
      loff_t lend);
extern int __attribute__((__warn_unused_result__)) file_check_and_advance_wb_err(struct file *file);
extern int __attribute__((__warn_unused_result__)) file_write_and_wait_range(struct file *file,
      loff_t start, loff_t end);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int file_write_and_wait(struct file *file)
{
 return file_write_and_wait_range(file, 0, ((long long)(~0ULL >> 1)));
}

extern int vfs_fsync_range(struct file *file, loff_t start, loff_t end,
      int datasync);
extern int vfs_fsync(struct file *file, int datasync);

extern int sync_file_range(struct file *file, loff_t offset, loff_t nbytes,
    unsigned int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iocb_is_dsync(const struct kiocb *iocb)
{
 return (iocb->ki_flags & ( int) (( __kernel_rwf_t)0x00000002)) ||
  (((iocb->ki_filp->f_mapping->host)->i_sb->s_flags & (16 /* Writes are synced at once */)) || ((iocb->ki_filp->f_mapping->host)->i_flags & (1 << 0) /* Writes are synced at once */));
}

/*
 * Sync the bytes written if this was a synchronous write.  Expect ki_pos
 * to already be updated for the write, and will return either the amount
 * of bytes passed in, or an error if syncing the file failed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t generic_write_sync(struct kiocb *iocb, ssize_t count)
{
 if (iocb_is_dsync(iocb)) {
  int ret = vfs_fsync_range(iocb->ki_filp,
    iocb->ki_pos - count, iocb->ki_pos - 1,
    (iocb->ki_flags & ( int) (( __kernel_rwf_t)0x00000004)) ? 0 : 1);
  if (ret)
   return ret;
 }

 return count;
}

extern void emergency_sync(void);
extern void emergency_remount(void);


extern int bmap(struct inode *inode, sector_t *block);







int notify_change(struct user_namespace *, struct dentry *,
    struct iattr *, struct inode **);
int inode_permission(struct user_namespace *, struct inode *, int);
int generic_permission(struct user_namespace *, struct inode *, int);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int file_permission(struct file *file, int mask)
{
 return inode_permission(file_mnt_user_ns(file),
    file_inode(file), mask);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int path_permission(const struct path *path, int mask)
{
 return inode_permission(mnt_user_ns(path->mnt),
    d_inode(path->dentry), mask);
}
int __check_sticky(struct user_namespace *mnt_userns, struct inode *dir,
     struct inode *inode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool execute_ok(struct inode *inode)
{
 return (inode->i_mode & (00100|00010|00001)) || (((inode->i_mode) & 00170000) == 0040000);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool inode_wrong_type(const struct inode *inode, umode_t mode)
{
 return (inode->i_mode ^ mode) & 00170000;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void file_start_write(struct file *file)
{
 if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
  return;
 sb_start_write(file_inode(file)->i_sb);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool file_start_write_trylock(struct file *file)
{
 if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
  return true;
 return sb_start_write_trylock(file_inode(file)->i_sb);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void file_end_write(struct file *file)
{
 if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
  return;
 __sb_end_write(file_inode(file)->i_sb, SB_FREEZE_WRITE);
}

/*
 * This is used for regular files where some users -- especially the
 * currently executed binary in a process, previously handled via
 * VM_DENYWRITE -- cannot handle concurrent write (and maybe mmap
 * read-write shared) accesses.
 *
 * get_write_access() gets write permission for a file.
 * put_write_access() releases this write permission.
 * deny_write_access() denies write access to a file.
 * allow_write_access() re-enables write access to a file.
 *
 * The i_writecount field of an inode can have the following values:
 * 0: no write access, no denied write access
 * < 0: (-i_writecount) users that denied write access to the file.
 * > 0: (i_writecount) users that have write access to the file.
 *
 * Normally we operate on that counter with atomic_{inc,dec} and it's safe
 * except for the cases where we don't hold i_writecount yet. Then we need to
 * use {get,deny}_write_access() - these functions check the sign and refuse
 * to do the change if sign is wrong.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_write_access(struct inode *inode)
{
 return atomic_inc_unless_negative(&inode->i_writecount) ? 0 : -26 /* Text file busy */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int deny_write_access(struct file *file)
{
 struct inode *inode = file_inode(file);
 return atomic_dec_unless_positive(&inode->i_writecount) ? 0 : -26 /* Text file busy */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void put_write_access(struct inode * inode)
{
 atomic_dec(&inode->i_writecount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void allow_write_access(struct file *file)
{
 if (file)
  atomic_inc(&file_inode(file)->i_writecount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool inode_is_open_for_write(const struct inode *inode)
{
 return atomic_read(&inode->i_writecount) > 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_readcount_dec(struct inode *inode)
{
 do { if (__builtin_expect(!!(!atomic_read(&inode->i_readcount)), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/fs.h\"; .popsection; .long 14472b - .; .short 2999; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 atomic_dec(&inode->i_readcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void i_readcount_inc(struct inode *inode)
{
 atomic_inc(&inode->i_readcount);
}
# 3016 "./include/linux/fs.h"
extern int do_pipe_flags(int *, int);

extern ssize_t kernel_read(struct file *, void *, size_t, loff_t *);
ssize_t __kernel_read(struct file *file, void *buf, size_t count, loff_t *pos);
extern ssize_t kernel_write(struct file *, const void *, size_t, loff_t *);
extern ssize_t __kernel_write(struct file *, const void *, size_t, loff_t *);
extern struct file * open_exec(const char *);

/* fs/dcache.c -- generic fs support functions */
extern bool is_subdir(struct dentry *, struct dentry *);
extern bool path_is_under(const struct path *, const struct path *);

extern char *file_path(struct file *, char *, int);



/* needed for stackable file system support */
extern loff_t default_llseek(struct file *file, loff_t offset, int whence);

extern loff_t vfs_llseek(struct file *file, loff_t offset, int whence);

extern int inode_init_always(struct super_block *, struct inode *);
extern void inode_init_once(struct inode *);
extern void address_space_init_once(struct address_space *mapping);
extern struct inode * igrab(struct inode *);
extern ino_t iunique(struct super_block *, ino_t);
extern int inode_needs_sync(struct inode *inode);
extern int generic_delete_inode(struct inode *inode);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int generic_drop_inode(struct inode *inode)
{
 return !inode->i_nlink || inode_unhashed(inode);
}
extern void d_mark_dontcache(struct inode *inode);

extern struct inode *ilookup5_nowait(struct super_block *sb,
  unsigned long hashval, int (*test)(struct inode *, void *),
  void *data);
extern struct inode *ilookup5(struct super_block *sb, unsigned long hashval,
  int (*test)(struct inode *, void *), void *data);
extern struct inode *ilookup(struct super_block *sb, unsigned long ino);

extern struct inode *inode_insert5(struct inode *inode, unsigned long hashval,
  int (*test)(struct inode *, void *),
  int (*set)(struct inode *, void *),
  void *data);
extern struct inode * iget5_locked(struct super_block *, unsigned long, int (*test)(struct inode *, void *), int (*set)(struct inode *, void *), void *);
extern struct inode * iget_locked(struct super_block *, unsigned long);
extern struct inode *find_inode_nowait(struct super_block *,
           unsigned long,
           int (*match)(struct inode *,
          unsigned long, void *),
           void *data);
extern struct inode *find_inode_rcu(struct super_block *, unsigned long,
        int (*)(struct inode *, void *), void *);
extern struct inode *find_inode_by_ino_rcu(struct super_block *, unsigned long);
extern int insert_inode_locked4(struct inode *, unsigned long, int (*test)(struct inode *, void *), void *);
extern int insert_inode_locked(struct inode *);



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lockdep_annotate_inode_mutex_key(struct inode *inode) { };

extern void unlock_new_inode(struct inode *);
extern void discard_new_inode(struct inode *);
extern unsigned int get_next_ino(void);
extern void evict_inodes(struct super_block *sb);
void dump_mapping(const struct address_space *);

/*
 * Userspace may rely on the the inode number being non-zero. For example, glibc
 * simply ignores files with zero i_ino in unlink() and other places.
 *
 * As an additional complication, if userspace was compiled with
 * _FILE_OFFSET_BITS=32 on a 64-bit kernel we'll only end up reading out the
 * lower 32 bits, so we need to check that those aren't zero explicitly. With
 * _FILE_OFFSET_BITS=64, this may cause some harmless false-negatives, but
 * better safe than sorry.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_zero_ino(ino_t ino)
{
 return (u32)ino == 0;
}

extern void __iget(struct inode * inode);
extern void iget_failed(struct inode *);
extern void clear_inode(struct inode *);
extern void __destroy_inode(struct inode *);
extern struct inode *new_inode_pseudo(struct super_block *sb);
extern struct inode *new_inode(struct super_block *sb);
extern void free_inode_nonrcu(struct inode *inode);
extern int setattr_should_drop_suidgid(struct user_namespace *, struct inode *);
extern int file_remove_privs(struct file *);

/*
 * This must be used for allocating filesystems specific inodes to set
 * up the inode reclaim context correctly.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *
alloc_inode_sb(struct super_block *sb, struct kmem_cache *cache, gfp_t gfp)
{
 return kmem_cache_alloc_lru(cache, &sb->s_inode_lru, gfp);
}

extern void __insert_inode_hash(struct inode *, unsigned long hashval);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void insert_inode_hash(struct inode *inode)
{
 __insert_inode_hash(inode, inode->i_ino);
}

extern void __remove_inode_hash(struct inode *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void remove_inode_hash(struct inode *inode)
{
 if (!inode_unhashed(inode) && !hlist_fake(&inode->i_hash))
  __remove_inode_hash(inode);
}

extern void inode_sb_list_add(struct inode *inode);
extern void inode_add_lru(struct inode *inode);

extern int sb_set_blocksize(struct super_block *, int);
extern int sb_min_blocksize(struct super_block *, int);

extern int generic_file_mmap(struct file *, struct vm_area_struct *);
extern int generic_file_readonly_mmap(struct file *, struct vm_area_struct *);
extern ssize_t generic_write_checks(struct kiocb *, struct iov_iter *);
int generic_write_checks_count(struct kiocb *iocb, loff_t *count);
extern int generic_write_check_limits(struct file *file, loff_t pos,
  loff_t *count);
extern int generic_file_rw_checks(struct file *file_in, struct file *file_out);
ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *to,
  ssize_t already_read);
extern ssize_t generic_file_read_iter(struct kiocb *, struct iov_iter *);
extern ssize_t __generic_file_write_iter(struct kiocb *, struct iov_iter *);
extern ssize_t generic_file_write_iter(struct kiocb *, struct iov_iter *);
extern ssize_t generic_file_direct_write(struct kiocb *, struct iov_iter *);
ssize_t generic_perform_write(struct kiocb *, struct iov_iter *);

ssize_t vfs_iter_read(struct file *file, struct iov_iter *iter, loff_t *ppos,
  rwf_t flags);
ssize_t vfs_iter_write(struct file *file, struct iov_iter *iter, loff_t *ppos,
  rwf_t flags);
ssize_t vfs_iocb_iter_read(struct file *file, struct kiocb *iocb,
      struct iov_iter *iter);
ssize_t vfs_iocb_iter_write(struct file *file, struct kiocb *iocb,
       struct iov_iter *iter);

/* fs/splice.c */
extern ssize_t generic_file_splice_read(struct file *, loff_t *,
  struct pipe_inode_info *, size_t, unsigned int);
extern ssize_t iter_file_splice_write(struct pipe_inode_info *,
  struct file *, loff_t *, size_t, unsigned int);
extern ssize_t generic_splice_sendpage(struct pipe_inode_info *pipe,
  struct file *out, loff_t *, size_t len, unsigned int flags);
extern long do_splice_direct(struct file *in, loff_t *ppos, struct file *out,
  loff_t *opos, size_t len, unsigned int flags);


extern void
file_ra_state_init(struct file_ra_state *ra, struct address_space *mapping);
extern loff_t noop_llseek(struct file *file, loff_t offset, int whence);

extern loff_t vfs_setpos(struct file *file, loff_t offset, loff_t maxsize);
extern loff_t generic_file_llseek(struct file *file, loff_t offset, int whence);
extern loff_t generic_file_llseek_size(struct file *file, loff_t offset,
  int whence, loff_t maxsize, loff_t eof);
extern loff_t fixed_size_llseek(struct file *file, loff_t offset,
  int whence, loff_t size);
extern loff_t no_seek_end_llseek_size(struct file *, loff_t, int, loff_t);
extern loff_t no_seek_end_llseek(struct file *, loff_t, int);
int rw_verify_area(int, struct file *, const loff_t *, size_t);
extern int generic_file_open(struct inode * inode, struct file * filp);
extern int nonseekable_open(struct inode * inode, struct file * filp);
extern int stream_open(struct inode * inode, struct file * filp);


typedef void (dio_submit_t)(struct bio *bio, struct inode *inode,
       loff_t file_offset);

enum {
 /* need locking between buffered and direct access */
 DIO_LOCKING = 0x01,

 /* filesystem does not support filling holes */
 DIO_SKIP_HOLES = 0x02,
};

ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
        struct block_device *bdev, struct iov_iter *iter,
        get_block_t get_block,
        dio_iodone_t end_io, dio_submit_t submit_io,
        int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t blockdev_direct_IO(struct kiocb *iocb,
      struct inode *inode,
      struct iov_iter *iter,
      get_block_t get_block)
{
 return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
   get_block, ((void *)0), ((void *)0), DIO_LOCKING | DIO_SKIP_HOLES);
}


void inode_dio_wait(struct inode *inode);

/**
 * inode_dio_begin - signal start of a direct I/O requests
 * @inode: inode the direct I/O happens on
 *
 * This is called once we've finished processing a direct I/O request,
 * and is used to wake up callers waiting for direct I/O to be quiesced.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_dio_begin(struct inode *inode)
{
 atomic_inc(&inode->i_dio_count);
}

/**
 * inode_dio_end - signal finish of a direct I/O requests
 * @inode: inode the direct I/O happens on
 *
 * This is called once we've finished processing a direct I/O request,
 * and is used to wake up callers waiting for direct I/O to be quiesced.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_dio_end(struct inode *inode)
{
 if (atomic_dec_and_test(&inode->i_dio_count))
  wake_up_bit(&inode->i_state, 9);
}

/*
 * Warn about a page cache invalidation failure diring a direct I/O write.
 */
void dio_warn_stale_pagecache(struct file *filp);

extern void inode_set_flags(struct inode *inode, unsigned int flags,
       unsigned int mask);

extern const struct file_operations generic_ro_fops;



extern int readlink_copy(char /* nothing */ *, int, const char *);
extern int page_readlink(struct dentry *, char /* nothing */ *, int);
extern const char *page_get_link(struct dentry *, struct inode *,
     struct delayed_call *);
extern void page_put_link(void *);
extern int page_symlink(struct inode *inode, const char *symname, int len);
extern const struct inode_operations page_symlink_inode_operations;
extern void kfree_link(void *);
void generic_fillattr(struct user_namespace *, struct inode *, struct kstat *);
void generic_fill_statx_attr(struct inode *inode, struct kstat *stat);
extern int vfs_getattr_nosec(const struct path *, struct kstat *, u32, unsigned int);
extern int vfs_getattr(const struct path *, struct kstat *, u32, unsigned int);
void __inode_add_bytes(struct inode *inode, loff_t bytes);
void inode_add_bytes(struct inode *inode, loff_t bytes);
void __inode_sub_bytes(struct inode *inode, loff_t bytes);
void inode_sub_bytes(struct inode *inode, loff_t bytes);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) loff_t __inode_get_bytes(struct inode *inode)
{
 return (((loff_t)inode->i_blocks) << 9) + inode->i_bytes;
}
loff_t inode_get_bytes(struct inode *inode);
void inode_set_bytes(struct inode *inode, loff_t bytes);
const char *simple_get_link(struct dentry *, struct inode *,
       struct delayed_call *);
extern const struct inode_operations simple_symlink_inode_operations;

extern int iterate_dir(struct file *, struct dir_context *);

int vfs_fstatat(int dfd, const char /* nothing */ *filename, struct kstat *stat,
  int flags);
int vfs_fstat(int fd, struct kstat *stat);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vfs_stat(const char /* nothing */ *filename, struct kstat *stat)
{
 return vfs_fstatat(-100 /* Special value used to indicate
                                           openat should use the current
                                           working directory. */, filename, stat, 0);
# 3292 "./include/linux/fs.h"
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vfs_lstat(const char /* nothing */ *name, struct kstat *stat)
{
 return vfs_fstatat(-100 /* Special value used to indicate
                                           openat should use the current
                                           working directory. */, name, stat, 0x100 /* Do not follow symbolic links.  */);
# 3296 "./include/linux/fs.h"
}

extern const char *vfs_get_link(struct dentry *, struct delayed_call *);
extern int vfs_readlink(struct dentry *, char /* nothing */ *, int);

extern struct file_system_type *get_filesystem(struct file_system_type *fs);
extern void put_filesystem(struct file_system_type *fs);
extern struct file_system_type *get_fs_type(const char *name);
extern struct super_block *get_super(struct block_device *);
extern struct super_block *get_active_super(struct block_device *bdev);
extern void drop_super(struct super_block *sb);
extern void drop_super_exclusive(struct super_block *sb);
extern void iterate_supers(void (*)(struct super_block *, void *), void *);
extern void iterate_supers_type(struct file_system_type *,
           void (*)(struct super_block *, void *), void *);

extern int dcache_dir_open(struct inode *, struct file *);
extern int dcache_dir_close(struct inode *, struct file *);
extern loff_t dcache_dir_lseek(struct file *, loff_t, int);
extern int dcache_readdir(struct file *, struct dir_context *);
extern int simple_setattr(struct user_namespace *, struct dentry *,
     struct iattr *);
extern int simple_getattr(struct user_namespace *, const struct path *,
     struct kstat *, u32, unsigned int);
extern int simple_statfs(struct dentry *, struct kstatfs *);
extern int simple_open(struct inode *inode, struct file *file);
extern int simple_link(struct dentry *, struct inode *, struct dentry *);
extern int simple_unlink(struct inode *, struct dentry *);
extern int simple_rmdir(struct inode *, struct dentry *);
extern int simple_rename_exchange(struct inode *old_dir, struct dentry *old_dentry,
      struct inode *new_dir, struct dentry *new_dentry);
extern int simple_rename(struct user_namespace *, struct inode *,
    struct dentry *, struct inode *, struct dentry *,
    unsigned int);
extern void simple_recursive_removal(struct dentry *,
                              void (*callback)(struct dentry *));
extern int noop_fsync(struct file *, loff_t, loff_t, int);
extern ssize_t noop_direct_IO(struct kiocb *iocb, struct iov_iter *iter);
extern int simple_empty(struct dentry *);
extern int simple_write_begin(struct file *file, struct address_space *mapping,
   loff_t pos, unsigned len,
   struct page **pagep, void **fsdata);
extern const struct address_space_operations ram_aops;
extern int always_delete_dentry(const struct dentry *);
extern struct inode *alloc_anon_inode(struct super_block *);
extern int simple_nosetlease(struct file *, long, struct file_lock **, void **);
extern const struct dentry_operations simple_dentry_operations;

extern struct dentry *simple_lookup(struct inode *, struct dentry *, unsigned int flags);
extern ssize_t generic_read_dir(struct file *, char /* nothing */ *, size_t, loff_t *);
extern const struct file_operations simple_dir_operations;
extern const struct inode_operations simple_dir_inode_operations;
extern void make_empty_dir_inode(struct inode *inode);
extern bool is_empty_dir_inode(struct inode *inode);
struct tree_descr { const char *name; const struct file_operations *ops; int mode; };
struct dentry *d_alloc_name(struct dentry *, const char *);
extern int simple_fill_super(struct super_block *, unsigned long,
        const struct tree_descr *);
extern int simple_pin_fs(struct file_system_type *, struct vfsmount **mount, int *count);
extern void simple_release_fs(struct vfsmount **mount, int *count);

extern ssize_t simple_read_from_buffer(void /* nothing */ *to, size_t count,
   loff_t *ppos, const void *from, size_t available);
extern ssize_t simple_write_to_buffer(void *to, size_t available, loff_t *ppos,
  const void /* nothing */ *from, size_t count);

extern int __generic_file_fsync(struct file *, loff_t, loff_t, int);
extern int generic_file_fsync(struct file *, loff_t, loff_t, int);

extern int generic_check_addressable(unsigned, u64);

extern void generic_set_encrypted_ci_d_ops(struct dentry *dentry);

int may_setattr(struct user_namespace *mnt_userns, struct inode *inode,
  unsigned int ia_valid);
int setattr_prepare(struct user_namespace *, struct dentry *, struct iattr *);
extern int inode_newsize_ok(const struct inode *, loff_t offset);
void setattr_copy(struct user_namespace *, struct inode *inode,
    const struct iattr *attr);

extern int file_update_time(struct file *file);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_is_dax(const struct vm_area_struct *vma)
{
 return vma->vm_file && ((vma->vm_file->f_mapping->host)->i_flags & 0 /* Make all the DAX code disappear */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_is_fsdax(struct vm_area_struct *vma)
{
 struct inode *inode;

 if (!0 || !vma->vm_file)
  return false;
 if (!vma_is_dax(vma))
  return false;
 inode = file_inode(vma->vm_file);
 if ((((inode->i_mode) & 00170000) == 0020000))
  return false; /* device-dax */
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int iocb_flags(struct file *file)
{
 int res = 0;
 if (file->f_flags & 00002000)
  res |= ( int) (( __kernel_rwf_t)0x00000010);
 if (file->f_flags & 0200000 /* direct disk access hint - currently ignored */)
  res |= (1 << 17);
 if (file->f_flags & 00010000 /* used to be O_SYNC, see below */)
  res |= ( int) (( __kernel_rwf_t)0x00000002);
 if (file->f_flags & 04000000)
  res |= ( int) (( __kernel_rwf_t)0x00000004);
 return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kiocb_set_rw_flags(struct kiocb *ki, rwf_t flags)
{
 int kiocb_flags = 0;

 /* make sure there's no overlap between RWF and private IOCB flags */
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_316(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(__force int) RWF_SUPPORTED & IOCB_EVENTFD"))); if (!(!(( int) ((( __kernel_rwf_t)0x00000001) | (( __kernel_rwf_t)0x00000002) | (( __kernel_rwf_t)0x00000004) | (( __kernel_rwf_t)0x00000008) | (( __kernel_rwf_t)0x00000010)) & (1 << 16)))) __compiletime_assert_316(); } while (0);
# 3418 "./include/linux/fs.h"
 if (!flags)
  return 0;
 if (__builtin_expect(!!(flags & ~((( __kernel_rwf_t)0x00000001) | (( __kernel_rwf_t)0x00000002) | (( __kernel_rwf_t)0x00000004) | (( __kernel_rwf_t)0x00000008) | (( __kernel_rwf_t)0x00000010))), 0))
  return -95 /* Operation not supported on transport endpoint */;

 if (flags & (( __kernel_rwf_t)0x00000008)) {
  if (!(ki->ki_filp->f_mode & (( fmode_t)0x8000000)))
   return -95 /* Operation not supported on transport endpoint */;
  kiocb_flags |= (1 << 20);
 }
 kiocb_flags |= ( int) (flags & ((( __kernel_rwf_t)0x00000001) | (( __kernel_rwf_t)0x00000002) | (( __kernel_rwf_t)0x00000004) | (( __kernel_rwf_t)0x00000008) | (( __kernel_rwf_t)0x00000010)));
 if (flags & (( __kernel_rwf_t)0x00000004))
  kiocb_flags |= ( int) (( __kernel_rwf_t)0x00000002);

 ki->ki_flags |= kiocb_flags;
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ino_t parent_ino(struct dentry *dentry)
{
 ino_t res;

 /*
	 * Don't strictly need d_lock here? If the parent ino could change
	 * then surely we'd have a deeper race in the caller?
	 */
 spin_lock(&dentry->d_lockref.lock);
 res = dentry->d_parent->d_inode->i_ino;
 spin_unlock(&dentry->d_lockref.lock);
 return res;
}

/* Transaction based IO helpers */

/*
 * An argresp is stored in an allocated page and holds the
 * size of the argument or response, along with its content
 */
struct simple_transaction_argresp {
 ssize_t size;
 char data[];
};



char *simple_transaction_get(struct file *file, const char /* nothing */ *buf,
    size_t size);
ssize_t simple_transaction_read(struct file *file, char /* nothing */ *buf,
    size_t size, loff_t *pos);
int simple_transaction_release(struct inode *inode, struct file *file);

void simple_transaction_set(struct file *file, size_t n);

/*
 * simple attribute files
 *
 * These attributes behave similar to those in sysfs:
 *
 * Writing to an attribute immediately sets a value, an open file can be
 * written to multiple times.
 *
 * Reading from an attribute creates a buffer from the value that might get
 * read with multiple read calls. When the attribute has been read
 * completely, no further read calls are possible until the file is opened
 * again.
 *
 * All attributes contain a text representation of a numeric value
 * that are accessed with the get() and set() functions.
 */
# 3508 "./include/linux/fs.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__format__(printf, 1, 2)))
void __simple_attr_check_format(const char *fmt, ...)
{
 /* don't do anything, just let the compiler check the arguments; */
}

int simple_attr_open(struct inode *inode, struct file *file,
       int (*get)(void *, u64 *), int (*set)(void *, u64),
       const char *fmt);
int simple_attr_release(struct inode *inode, struct file *file);
ssize_t simple_attr_read(struct file *file, char /* nothing */ *buf,
    size_t len, loff_t *ppos);
ssize_t simple_attr_write(struct file *file, const char /* nothing */ *buf,
     size_t len, loff_t *ppos);
ssize_t simple_attr_write_signed(struct file *file, const char /* nothing */ *buf,
     size_t len, loff_t *ppos);

struct ctl_table;
int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) list_bdev_fs_names(char *buf, size_t size);
# 3535 "./include/linux/fs.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_sxid(umode_t mode)
{
 return mode & (0004000 | 0002000);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int check_sticky(struct user_namespace *mnt_userns,
          struct inode *dir, struct inode *inode)
{
 if (!(dir->i_mode & 0001000))
  return 0;

 return __check_sticky(mnt_userns, dir, inode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inode_has_no_xattr(struct inode *inode)
{
 if (!is_sxid(inode->i_mode) && (inode->i_sb->s_flags & (1<<28)))
  inode->i_flags |= (1 << 12) /* no suid or xattr security attributes */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_root_inode(struct inode *inode)
{
 return inode == inode->i_sb->s_root->d_inode;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dir_emit(struct dir_context *ctx,
       const char *name, int namelen,
       u64 ino, unsigned type)
{
 return ctx->actor(ctx, name, namelen, ctx->pos, ino, type);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dir_emit_dot(struct file *file, struct dir_context *ctx)
{
 return ctx->actor(ctx, ".", 1, ctx->pos,
     file->f_path.dentry->d_inode->i_ino, 4);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dir_emit_dotdot(struct file *file, struct dir_context *ctx)
{
 return ctx->actor(ctx, "..", 2, ctx->pos,
     parent_ino(file->f_path.dentry), 4);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dir_emit_dots(struct file *file, struct dir_context *ctx)
{
 if (ctx->pos == 0) {
  if (!dir_emit_dot(file, ctx))
   return false;
  ctx->pos = 1;
 }
 if (ctx->pos == 1) {
  if (!dir_emit_dotdot(file, ctx))
   return false;
  ctx->pos = 2;
 }
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dir_relax(struct inode *inode)
{
 inode_unlock(inode);
 inode_lock(inode);
 return !((inode)->i_flags & (1 << 4) /* removed, but still open directory */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dir_relax_shared(struct inode *inode)
{
 inode_unlock_shared(inode);
 inode_lock_shared(inode);
 return !((inode)->i_flags & (1 << 4) /* removed, but still open directory */);
}

extern bool path_noexec(const struct path *path);
extern void inode_nohighmem(struct inode *inode);

/* mm/fadvise.c */
extern int vfs_fadvise(struct file *file, loff_t offset, loff_t len,
         int advice);
extern int generic_fadvise(struct file *file, loff_t offset, loff_t len,
      int advice);
# 6 "./include/linux/highmem.h" 2


# 1 "./include/linux/cacheflush.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./arch/arm64/include/asm/cacheflush.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/cacheflush.h
 *
 * Copyright (C) 1999-2002 Russell King.
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./include/linux/kgdb.h" 1
/*
 * This provides the callbacks and functions that KGDB needs to share between
 * the core, I/O and arch-specific portions.
 *
 * Author: Amit Kale <amitkale@linsyssoft.com> and
 *         Tom Rini <trini@kernel.crashing.org>
 *
 * 2001-2004 (c) Amit S. Kale and 2003-2005 (c) MontaVista Software, Inc.
 * This file is licensed under the terms of the GNU General Public License
 * version 2. This program is licensed "as is" without any warranty of any
 * kind, whether express or implied.
 */






# 1 "./include/linux/kprobes.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */


/*
 *  Kernel Probes (KProbes)
 *
 * Copyright (C) IBM Corporation, 2002, 2004
 *
 * 2002-Oct	Created by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel
 *		Probes initial implementation ( includes suggestions from
 *		Rusty Russell).
 * 2004-July	Suparna Bhattacharya <suparna@in.ibm.com> added jumper probes
 *		interface to access function arguments.
 * 2005-May	Hien Nguyen <hien@us.ibm.com> and Jim Keniston
 *		<jkenisto@us.ibm.com>  and Prasanna S Panchamukhi
 *		<prasanna@in.ibm.com> added function-return probes.
 */
# 28 "./include/linux/kprobes.h"
# 1 "./include/linux/ftrace.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Ftrace header.  For implementation details beyond the random comments
 * scattered below, see: Documentation/trace/ftrace-design.rst
 */




# 1 "./include/linux/trace_recursion.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/interrupt.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/* interrupt.h */






# 1 "./include/linux/irqreturn.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/**
 * enum irqreturn - irqreturn type values
 * @IRQ_NONE:		interrupt was not from this device or was not handled
 * @IRQ_HANDLED:	interrupt was handled by this device
 * @IRQ_WAKE_THREAD:	handler requests to wake the handler thread
 */
enum irqreturn {
 IRQ_NONE = (0 << 0),
 IRQ_HANDLED = (1 << 0),
 IRQ_WAKE_THREAD = (1 << 1),
};

typedef enum irqreturn irqreturn_t;
# 10 "./include/linux/interrupt.h" 2

# 1 "./include/linux/hardirq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/context_tracking_state.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/static_key.h" 1
# 7 "./include/linux/context_tracking_state.h" 2


/* Offset to allow distinguishing irq vs. task-based idle entry/exit. */


enum ctx_state {
 CONTEXT_DISABLED = -1, /* returned by ct_state() if unknown */
 CONTEXT_KERNEL = 0,
 CONTEXT_IDLE = 1,
 CONTEXT_USER = 2,
 CONTEXT_GUEST = 3,
 CONTEXT_MAX = 4,
};

/* Even value for idle, else odd. */





struct context_tracking {
# 39 "./include/linux/context_tracking_state.h"
 atomic_t state;


 long dynticks_nesting; /* Track process nesting level. */
 long dynticks_nmi_nesting; /* Track irq/NMI nesting level. */

};


extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct context_tracking) context_tracking;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int __ct_state(void)
{
 return (*(const volatile typeof( _Generic(((({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking.state) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&context_tracking.state)) *)(&context_tracking.state)); (typeof((typeof(*(&context_tracking.state)) *)(&context_tracking.state))) (__ptr + ((__kern_my_cpu_offset()))); }); }))->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking.state) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&context_tracking.state)) *)(&context_tracking.state)); (typeof((typeof(*(&context_tracking.state)) *)(&context_tracking.state))) (__ptr + ((__kern_my_cpu_offset()))); }); }))->counter))) *)&((({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking.state) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&context_tracking.state)) *)(&context_tracking.state)); (typeof((typeof(*(&context_tracking.state)) *)(&context_tracking.state))) (__ptr + ((__kern_my_cpu_offset()))); }); }))->counter)) & (CONTEXT_MAX - 1);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int ct_dynticks(void)
{
 return atomic_read(({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking.state) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&context_tracking.state)) *)(&context_tracking.state)); (typeof((typeof(*(&context_tracking.state)) *)(&context_tracking.state))) (__ptr + ((__kern_my_cpu_offset()))); }); })) & (~(CONTEXT_MAX - 1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int ct_dynticks_cpu(int cpu)
{
 struct context_tracking *ct = ({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&context_tracking))) *)((&context_tracking))); (typeof((typeof(*((&context_tracking))) *)((&context_tracking)))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); });

 return atomic_read(&ct->state) & (~(CONTEXT_MAX - 1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int ct_dynticks_cpu_acquire(int cpu)
{
 struct context_tracking *ct = ({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&context_tracking))) *)((&context_tracking))); (typeof((typeof(*((&context_tracking))) *)((&context_tracking)))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); });

 return atomic_read_acquire(&ct->state) & (~(CONTEXT_MAX - 1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long ct_dynticks_nesting(void)
{
 return ({ __this_cpu_preempt_check("read"); ({ typeof(context_tracking.dynticks_nesting) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(context_tracking.dynticks_nesting)) { case 1: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nesting))) *)(&(context_tracking.dynticks_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long ct_dynticks_nesting_cpu(int cpu)
{
 struct context_tracking *ct = ({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&context_tracking))) *)((&context_tracking))); (typeof((typeof(*((&context_tracking))) *)((&context_tracking)))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); });

 return ct->dynticks_nesting;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long ct_dynticks_nmi_nesting(void)
{
 return ({ __this_cpu_preempt_check("read"); ({ typeof(context_tracking.dynticks_nmi_nesting) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nmi_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(context_tracking.dynticks_nmi_nesting)) { case 1: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nmi_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nmi_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nmi_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(context_tracking.dynticks_nmi_nesting)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting))); (typeof((typeof(*(&(context_tracking.dynticks_nmi_nesting))) *)(&(context_tracking.dynticks_nmi_nesting)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) long ct_dynticks_nmi_nesting_cpu(int cpu)
{
 struct context_tracking *ct = ({ do { const void /* nothing */ *__vpp_verify = (typeof((&context_tracking) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&context_tracking))) *)((&context_tracking))); (typeof((typeof(*((&context_tracking))) *)((&context_tracking)))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); });

 return ct->dynticks_nmi_nesting;
}
# 141 "./include/linux/context_tracking_state.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool context_tracking_enabled(void) { return false; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool context_tracking_enabled_cpu(int cpu) { return false; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) bool context_tracking_enabled_this_cpu(void) { return false; }
# 6 "./include/linux/hardirq.h" 2


# 1 "./include/linux/ftrace_irq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 15 "./include/linux/ftrace_irq.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_nmi_enter(void)
{
# 25 "./include/linux/ftrace_irq.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_nmi_exit(void)
{
# 37 "./include/linux/ftrace_irq.h"
}
# 9 "./include/linux/hardirq.h" 2

# 1 "./include/linux/vtime.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 12 "./include/linux/vtime.h"
/*
 * Common vtime APIs
 */
# 28 "./include/linux/vtime.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_user_enter(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_user_exit(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_guest_enter(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_guest_exit(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_init_idle(struct task_struct *tsk, int cpu) { }
# 41 "./include/linux/vtime.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_account_irq(struct task_struct *tsk, unsigned int offset) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_account_softirq(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_account_hardirq(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_flush(struct task_struct *tsk) { }


/*
 * vtime_accounting_enabled_this_cpu() definitions/declarations
 */
# 116 "./include/linux/vtime.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vtime_accounting_enabled_this_cpu(void) { return false; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vtime_task_switch(struct task_struct *prev) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void vtime_account_guest_enter(void)
{
 get_current()->flags |= 0x00000001 /* I'm a virtual CPU */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void vtime_account_guest_exit(void)
{
 get_current()->flags &= ~0x00000001 /* I'm a virtual CPU */;
}





extern void irqtime_account_irq(struct task_struct *tsk, unsigned int offset);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void account_softirq_enter(struct task_struct *tsk)
{
 vtime_account_irq(tsk, (1UL << (0 + 8)));
 irqtime_account_irq(tsk, (1UL << (0 + 8)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void account_softirq_exit(struct task_struct *tsk)
{
 vtime_account_softirq(tsk);
 irqtime_account_irq(tsk, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void account_hardirq_enter(struct task_struct *tsk)
{
 vtime_account_irq(tsk, (1UL << ((0 + 8) + 8)));
 irqtime_account_irq(tsk, (1UL << ((0 + 8) + 8)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void account_hardirq_exit(struct task_struct *tsk)
{
 vtime_account_hardirq(tsk);
 irqtime_account_irq(tsk, 0);
}
# 11 "./include/linux/hardirq.h" 2
# 1 "./arch/arm64/include/asm/hardirq.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */







# 1 "./arch/arm64/include/asm/irq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/asm-generic/irq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * NR_IRQS is the upper bound of how many interrupts can be handled
 * in the platform. It is used to size the static irq_map array,
 * so don't make it too big.
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int irq_canonicalize(int irq)
{
 return irq;
}
# 8 "./arch/arm64/include/asm/irq.h" 2

struct pt_regs;

int set_handle_irq(void (*handle_irq)(struct pt_regs *));

int set_handle_fiq(void (*handle_fiq)(struct pt_regs *));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int nr_legacy_irqs(void)
{
 return 0;
}
# 13 "./arch/arm64/include/asm/hardirq.h" 2
# 1 "./arch/arm64/include/asm/kvm_arm.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012,2013 - ARM Ltd
 * Author: Marc Zyngier <marc.zyngier@arm.com>
 */




# 1 "./arch/arm64/include/asm/esr.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2013 - ARM Ltd
 * Author: Marc Zyngier <marc.zyngier@arm.com>
 */
# 15 "./arch/arm64/include/asm/esr.h"
/* Unallocated EC: 0x02 */







/* Unallocated EC: 0x0A - 0x0B */



/* Unallocated EC: 0x0F - 0x10 */



/* Unallocated EC: 0x14 */






/* Unallocated EC: 0x1B */


/* Unallocated EC: 0x1E */




/* Unallocated EC: 0x23 */



/* Unallocated EC: 0x27 */

/* Unallocated EC: 0x29 - 0x2B */

/* Unallocated EC: 0x2D - 0x2E */







/* Unallocated EC: 0x36 - 0x37 */

/* Unallocated EC: 0x39 */

/* Unallocated EC: 0x3B */

/* Unallocated EC: 0x3D - 0x3F */
# 81 "./arch/arm64/include/asm/esr.h"
/* ISS field definitions shared by different classes */



/* Asynchronous Error Type */
# 97 "./arch/arm64/include/asm/esr.h"
/* Shared ISS field definitions for Data/Instruction aborts */
# 107 "./arch/arm64/include/asm/esr.h"
/* Shared ISS fault status code(IFSC/DFSC) for Data/Instruction aborts */
# 118 "./arch/arm64/include/asm/esr.h"
/* ISS field definitions for Data Aborts */
# 134 "./arch/arm64/include/asm/esr.h"
/* ISS field definitions for exceptions taken in to Hyp */
# 147 "./arch/arm64/include/asm/esr.h"
/*
 * DISR_EL1 and ESR_ELx share the bottom 13 bits, but the RES0 bits may mean
 * different things in the future...
 */


/* ESR value templates for specific events */





/* BRK instruction trap from AArch64 state */


/* ISS field definitions for System instruction traps */
# 197 "./arch/arm64/include/asm/esr.h"
/*
 * User space cache operations have the following sysreg encoding
 * in System instructions.
 * op0=1, op1=3, op2=1, crn=7, crm={ 5, 10, 11, 12, 13, 14 }, WRITE (L=0)
 */
# 217 "./arch/arm64/include/asm/esr.h"
/*
 * User space MRS operations which are supported for emulation
 * have the following sysreg encoding in System instructions.
 * op0 = 3, op1= 0, crn = 0, {crm = 0, 4-7}, READ (L = 1)
 */
# 266 "./arch/arm64/include/asm/esr.h"
/*
 * ISS field definitions for floating-point exception traps
 * (FP_EXC_32/FP_EXC_64).
 *
 * (The FPEXC_* constants are used instead for common bits.)
 */



/*
 * ISS field definitions for CP15 accesses
 */
# 336 "./arch/arm64/include/asm/esr.h"
/*
 * ISS values for SME traps
 */







# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 347 "./arch/arm64/include/asm/esr.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool esr_is_data_abort(unsigned long esr)
{
 const unsigned long ec = (((esr) & ((((0x3FUL))) << (26))) >> (26));

 return ec == (0x24) || ec == (0x25);
}

const char *esr_get_class_string(unsigned long esr);
# 11 "./arch/arm64/include/asm/kvm_arm.h" 2

# 1 "./arch/arm64/include/generated/uapi/asm/types.h" 1
# 13 "./arch/arm64/include/asm/kvm_arm.h" 2

/* Hyp Configuration Register (HCR) bits */
# 66 "./arch/arm64/include/asm/kvm_arm.h"
/*
 * The bits we set in HCR:
 * TLOR:	Trap LORegion register accesses
 * RW:		64bit by default, can be overridden for 32bit VMs
 * TACR:	Trap ACTLR
 * TSC:		Trap SMC
 * TSW:		Trap cache operations by set/way
 * TWE:		Trap WFE
 * TWI:		Trap WFI
 * TIDCP:	Trap L2CTLR/L2ECTLR
 * BSU_IS:	Upgrade barriers to the inner shareable domain
 * FB:		Force broadcast of all maintenance operations
 * AMO:		Override CPSR.A and enable signaling with VA
 * IMO:		Override CPSR.I and enable signaling with VI
 * FMO:		Override CPSR.F and enable signaling with VF
 * SWIO:	Turn set/way invalidates into set/way clean+invalidate
 * PTW:		Take a stage2 fault if a stage1 walk steps in device memory
 * TID3:	Trap EL1 reads of group 3 ID registers
 */
# 94 "./arch/arm64/include/asm/kvm_arm.h"
/* TCR_EL2 Registers bits */
# 108 "./arch/arm64/include/asm/kvm_arm.h"
/* VTCR_EL2 Registers bits */
# 133 "./arch/arm64/include/asm/kvm_arm.h"
/*
 * We configure the Stage-2 page tables to always restrict the IPA space to be
 * 40 bits wide (T0SZ = 24).  Systems with a PARange smaller than 40 bits are
 * not known to exist and will break with this configuration.
 *
 * The VTCR_EL2 is configured per VM and is initialised in kvm_arm_setup_stage2().
 *
 * Note that when using 4K pages, we concatenate two first level page tables
 * together. With 16K pages, we concatenate 16 first level page tables.
 *
 */




/*
 * VTCR_EL2:SL0 indicates the entry level for Stage2 translation.
 * Interestingly, it depends on the page size.
 * See D.10.2.121, VTCR_EL2, in ARM DDI 0487C.a
 *
 *	-----------------------------------------
 *	| Entry level		|  4K  | 16K/64K |
 *	------------------------------------------
 *	| Level: 0		|  2   |   -     |
 *	------------------------------------------
 *	| Level: 1		|  1   |   2     |
 *	------------------------------------------
 *	| Level: 2		|  0   |   1     |
 *	------------------------------------------
 *	| Level: 3		|  -   |   0     |
 *	------------------------------------------
 *
 * The table roughly translates to :
 *
 *	SL0(PAGE_SIZE, Entry_level) = TGRAN_SL0_BASE - Entry_Level
 *
 * Where TGRAN_SL0_BASE is a magic number depending on the page size:
 * 	TGRAN_SL0_BASE(4K) = 2
 *	TGRAN_SL0_BASE(16K) = 3
 *	TGRAN_SL0_BASE(64K) = 3
 * provided we take care of ruling out the unsupported cases and
 * Entry_Level = 4 - Number_of_levels.
 *
 */
# 204 "./arch/arm64/include/asm/kvm_arm.h"
/*
 * ARM VMSAv8-64 defines an algorithm for finding the translation table
 * descriptors in section D4.2.8 in ARM DDI 0487C.a.
 *
 * The algorithm defines the expectations on the translation table
 * addresses for each level, based on PAGE_SIZE, entry level
 * and the translation table size (T0SZ). The variable "x" in the
 * algorithm determines the alignment of a table base address at a given
 * level and thus determines the alignment of VTTBR:BADDR for stage2
 * page table entry level.
 * Since the number of bits resolved at the entry level could vary
 * depending on the T0SZ, the value of "x" is defined based on a
 * Magic constant for a given PAGE_SIZE and Entry Level. The
 * intermediate levels must be always aligned to the PAGE_SIZE (i.e,
 * x = PAGE_SHIFT).
 *
 * The value of "x" for entry level is calculated as :
 *    x = Magic_N - T0SZ
 *
 * where Magic_N is an integer depending on the page size and the entry
 * level of the page table as below:
 *
 *	--------------------------------------------
 *	| Entry level		|  4K    16K   64K |
 *	--------------------------------------------
 *	| Level: 0 (4 levels)	| 28   |  -  |  -  |
 *	--------------------------------------------
 *	| Level: 1 (3 levels)	| 37   | 31  | 25  |
 *	--------------------------------------------
 *	| Level: 2 (2 levels)	| 46   | 42  | 38  |
 *	--------------------------------------------
 *	| Level: 3 (1 level)	| -    | 53  | 51  |
 *	--------------------------------------------
 *
 * We have a magic formula for the Magic_N below:
 *
 *  Magic_N(PAGE_SIZE, Level) = 64 - ((PAGE_SHIFT - 3) * Number_of_levels)
 *
 * where Number_of_levels = (4 - Level). We are only interested in the
 * value for Entry_Level for the stage2 page table.
 *
 * So, given that T0SZ = (64 - IPA_SHIFT), we can compute 'x' as follows:
 *
 *	x = (64 - ((PAGE_SHIFT - 3) * Number_of_levels)) - (64 - IPA_SHIFT)
 *	  = IPA_SHIFT - ((PAGE_SHIFT - 3) * Number of levels)
 *
 * Here is one way to explain the Magic Formula:
 *
 *  x = log2(Size_of_Entry_Level_Table)
 *
 * Since, we can resolve (PAGE_SHIFT - 3) bits at each level, and another
 * PAGE_SHIFT bits in the PTE, we have :
 *
 *  Bits_Entry_level = IPA_SHIFT - ((PAGE_SHIFT - 3) * (n - 1) + PAGE_SHIFT)
 *		     = IPA_SHIFT - (PAGE_SHIFT - 3) * n - 3
 *  where n = number of levels, and since each pointer is 8bytes, we have:
 *
 *  x = Bits_Entry_Level + 3
 *    = IPA_SHIFT - (PAGE_SHIFT - 3) * n
 *
 * The only constraint here is that, we have to find the number of page table
 * levels for a given IPA size (which we do, see stage2_pt_levels())
 */






/* Hyp System Trap Register */


/* Hyp Coprocessor Trap Register Shifts */


/* Hyp Coprocessor Trap Register */
# 293 "./arch/arm64/include/asm/kvm_arm.h"
/* Hyp Debug Configuration Register bits */
# 322 "./arch/arm64/include/asm/kvm_arm.h"
/* For compatibility with fault code shared with 32-bit */
# 337 "./arch/arm64/include/asm/kvm_arm.h"
/* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */

/*
 * We have
 *	PAR	[PA_Shift - 1	: 12] = PA	[PA_Shift - 1 : 12]
 *	HPFAR	[PA_Shift - 9	: 4]  = FIPA	[PA_Shift - 1 : 12]
 */
# 14 "./arch/arm64/include/asm/hardirq.h" 2



# 1 "./include/asm-generic/hardirq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






typedef struct {
 unsigned int __softirq_pending;



} __attribute__((__aligned__((1 << (6))))) irq_cpustat_t;

extern /* nothing */ __attribute__((section(".data..percpu" "..shared_aligned"))) __typeof__(irq_cpustat_t) irq_stat __attribute__((__aligned__((1 << (6)))));

# 1 "./include/linux/irq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Please do not include this file in generic code.  There is currently
 * no requirement for any architecture to implement anything held
 * within this file.
 *
 * Thanks. --rmk
 */




# 1 "./include/linux/irqhandler.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Interrupt flow handler typedefs are defined here to avoid circular
 * include dependencies.
 */

struct irq_desc;
struct irq_data;
typedef void (*irq_flow_handler_t)(struct irq_desc *desc);
# 17 "./include/linux/irq.h" 2



# 1 "./include/linux/io.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright 2006 PathScale, Inc.  All Rights Reserved.
 */
# 13 "./include/linux/io.h"
# 1 "./arch/arm64/include/asm/io.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/io.h
 *
 * Copyright (C) 1996-2000 Russell King
 * Copyright (C) 2012 ARM Ltd.
 */




# 1 "./include/linux/pgtable.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/asm/pgtable.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */




# 1 "./arch/arm64/include/asm/proc-fns.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/proc-fns.h
 *
 * Copyright (C) 1997-1999 Russell King
 * Copyright (C) 2000 Deep Blue Solutions Ltd
 * Copyright (C) 2012 ARM Ltd.
 */







struct cpu_suspend_ctx;

extern void cpu_do_idle(void);
extern void cpu_do_suspend(struct cpu_suspend_ctx *ptr);
extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
# 10 "./arch/arm64/include/asm/pgtable.h" 2




# 1 "./arch/arm64/include/asm/pgtable-prot.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2016 ARM Ltd.
 */
# 13 "./arch/arm64/include/asm/pgtable-prot.h"
/*
 * Software defined PTE bits definition.
 */







/*
 * This bit indicates that the entry is present i.e. pmd_page()
 * still points to a valid huge page in memory even if the pmd
 * has been invalidated.
 */







extern bool arm64_use_ng_mappings;







/*
 * If we have userspace only BTI we don't want to mark kernel pages
 * guarded even if the system does support BTI.
 */
# 85 "./arch/arm64/include/asm/pgtable-prot.h"
/* shared+writable pages are clean by default, hence PTE_RDONLY|PTE_WRITE */
# 15 "./arch/arm64/include/asm/pgtable.h" 2
# 1 "./arch/arm64/include/asm/tlbflush.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Based on arch/arm/include/asm/tlbflush.h
 *
 * Copyright (C) 1999-2003 Russell King
 * Copyright (C) 2012 ARM Ltd.
 */
# 19 "./arch/arm64/include/asm/tlbflush.h"
/*
 * Raw TLBI operations.
 *
 * Where necessary, use the __tlbi() macro to avoid asm()
 * boilerplate. Drivers and most kernel code should use the TLB
 * management routines in preference to the macro below.
 *
 * The macro can be used as __tlbi(op) or __tlbi(op, arg), depending
 * on whether a particular TLBI operation takes an argument or
 * not. The macros handles invoking the asm with or without the
 * register argument as appropriate.
 */
# 56 "./arch/arm64/include/asm/tlbflush.h"
/* This macro creates a properly formatted VA operand for the TLBI */
# 65 "./arch/arm64/include/asm/tlbflush.h"
/*
 * Get translation granule of the system, which is decided by
 * PAGE_SIZE.  Used by TTL.
 *  - 4KB	: 1
 *  - 16KB	: 2
 *  - 64KB	: 3
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_trans_granule(void)
{
 switch (((1UL) << 12)) {
 case 0x00001000:
  return 1;
 case 0x00004000:
  return 2;
 case 0x00010000:
  return 3;
 default:
  return 0;
 }
}

/*
 * Level-based TLBI operations.
 *
 * When ARMv8.4-TTL exists, TLBI operations take an additional hint for
 * the level at which the invalidation must take place. If the level is
 * wrong, no invalidation may take place. In the case where the level
 * cannot be easily determined, a 0 value for the level parameter will
 * perform a non-hinted invalidation.
 *
 * For Stage-2 invalidation, use the level values provided to that effect
 * in asm/stage2_pgtable.h.
 */
# 123 "./arch/arm64/include/asm/tlbflush.h"
/*
 * This macro creates a properly formatted VA operand for the TLB RANGE.
 * The value bit assignments are:
 *
 * +----------+------+-------+-------+-------+----------------------+
 * |   ASID   |  TG  | SCALE |  NUM  |  TTL  |        BADDR         |
 * +-----------------+-------+-------+-------+----------------------+
 * |63      48|47  46|45   44|43   39|38   37|36                   0|
 *
 * The address range is determined by below formula:
 * [BADDR, BADDR + (NUM + 1) * 2^(5*SCALE + 1) * PAGESIZE)
 *
 */
# 148 "./arch/arm64/include/asm/tlbflush.h"
/* These macros are used by the TLBI RANGE feature. */




/*
 * Generate 'num' values from -1 to 30 with -1 rejected by the
 * __flush_tlb_range() loop below.
 */




/*
 *	TLB Invalidation
 *	================
 *
 * 	This header file implements the low-level TLB invalidation routines
 *	(sometimes referred to as "flushing" in the kernel) for arm64.
 *
 *	Every invalidation operation uses the following template:
 *
 *	DSB ISHST	// Ensure prior page-table updates have completed
 *	TLBI ...	// Invalidate the TLB
 *	DSB ISH		// Ensure the TLB invalidation has completed
 *      if (invalidated kernel mappings)
 *		ISB	// Discard any instructions fetched from the old mapping
 *
 *
 *	The following functions form part of the "core" TLB invalidation API,
 *	as documented in Documentation/core-api/cachetlb.rst:
 *
 *	flush_tlb_all()
 *		Invalidate the entire TLB (kernel + user) on all CPUs
 *
 *	flush_tlb_mm(mm)
 *		Invalidate an entire user address space on all CPUs.
 *		The 'mm' argument identifies the ASID to invalidate.
 *
 *	flush_tlb_range(vma, start, end)
 *		Invalidate the virtual-address range '[start, end)' on all
 *		CPUs for the user address space corresponding to 'vma->mm'.
 *		Note that this operation also invalidates any walk-cache
 *		entries associated with translations for the specified address
 *		range.
 *
 *	flush_tlb_kernel_range(start, end)
 *		Same as flush_tlb_range(..., start, end), but applies to
 * 		kernel mappings rather than a particular user address space.
 *		Whilst not explicitly documented, this function is used when
 *		unmapping pages from vmalloc/io space.
 *
 *	flush_tlb_page(vma, addr)
 *		Invalidate a single user mapping for address 'addr' in the
 *		address space corresponding to 'vma->mm'.  Note that this
 *		operation only invalidates a single, last-level page-table
 *		entry and therefore does not affect any walk-caches.
 *
 *
 *	Next, we have some undocumented invalidation routines that you probably
 *	don't want to call unless you know what you're doing:
 *
 *	local_flush_tlb_all()
 *		Same as flush_tlb_all(), but only applies to the calling CPU.
 *
 *	__flush_tlb_kernel_pgtable(addr)
 *		Invalidate a single kernel mapping for address 'addr' on all
 *		CPUs, ensuring that any walk-cache entries associated with the
 *		translation are also invalidated.
 *
 *	__flush_tlb_range(vma, start, end, stride, last_level)
 *		Invalidate the virtual-address range '[start, end)' on all
 *		CPUs for the user address space corresponding to 'vma->mm'.
 *		The invalidation operations are issued at a granularity
 *		determined by 'stride' and only affect any walk-cache entries
 *		if 'last_level' is equal to false.
 *
 *
 *	Finally, take a look at asm/tlb.h to see how tlb_flush() is implemented
 *	on top of these routines, since that is our interface to the mmu_gather
 *	API as used by munmap() and friends.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void local_flush_tlb_all(void)
{
 asm volatile("dsb " "nshst" : : : "memory");
 asm (".arch " "armv8.5-a" "\n" "tlbi " "vmalle1" "\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vmalle1" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : );
 asm volatile("dsb " "nsh" : : : "memory");
 asm volatile("isb" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_tlb_all(void)
{
 asm volatile("dsb " "ishst" : : : "memory");
 asm (".arch " "armv8.5-a" "\n" "tlbi " "vmalle1is" "\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vmalle1is" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : );
 asm volatile("dsb " "ish" : : : "memory");
 asm volatile("isb" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_tlb_mm(struct mm_struct *mm)
{
 unsigned long asid;

 asm volatile("dsb " "ishst" : : : "memory");
 asid = ({ unsigned long __ta = (0) >> 12; __ta &= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (43)) * 0l)) : (int *)8))), (0) > (43), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (43))))); __ta |= (unsigned long)((atomic64_read(&(mm)->context.id) & 0xffff)) << 48; __ta; });
 asm (".arch " "armv8.5-a" "\n" "tlbi " "aside1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "aside1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (asid));
 do { if (arm64_kernel_unmapped_at_el0()) asm (".arch " "armv8.5-a" "\n" "tlbi " "aside1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "aside1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" ((asid) | ((((1UL))) << 48))); } while (0);
 asm volatile("dsb " "ish" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_tlb_page_nosync(struct vm_area_struct *vma,
      unsigned long uaddr)
{
 unsigned long addr;

 asm volatile("dsb " "ishst" : : : "memory");
 addr = ({ unsigned long __ta = (uaddr) >> 12; __ta &= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (43)) * 0l)) : (int *)8))), (0) > (43), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (43))))); __ta |= (unsigned long)((atomic64_read(&(vma->vm_mm)->context.id) & 0xffff)) << 48; __ta; });
 asm (".arch " "armv8.5-a" "\n" "tlbi " "vale1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vale1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (addr));
 do { if (arm64_kernel_unmapped_at_el0()) asm (".arch " "armv8.5-a" "\n" "tlbi " "vale1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vale1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" ((addr) | ((((1UL))) << 48))); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_tlb_page(struct vm_area_struct *vma,
      unsigned long uaddr)
{
 flush_tlb_page_nosync(vma, uaddr);
 asm volatile("dsb " "ish" : : : "memory");
}

/*
 * This is meant to avoid soft lock-ups on large TLB flushing ranges and not
 * necessarily a performance improvement.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __flush_tlb_range(struct vm_area_struct *vma,
         unsigned long start, unsigned long end,
         unsigned long stride, bool last_level,
         int tlb_level)
{
 int num = 0;
 int scale = 0;
 unsigned long asid, addr, pages;

 start = ((start) & ~((__typeof__(start))((stride)-1)));
 end = ((((end)-1) | ((__typeof__(end))((stride)-1)))+1);
 pages = (end - start) >> 12;

 /*
	 * When not uses TLB range ops, we can handle up to
	 * (MAX_TLBI_OPS - 1) pages;
	 * When uses TLB range ops, we can handle up to
	 * (MAX_TLBI_RANGE_PAGES - 1) pages.
	 */
 if ((!system_supports_tlb_range() &&
      (end - start) >= ((1 << (12 - 3)) * stride)) ||
     pages >= ((unsigned long)((31) + 1) << (5 * (3) + 1))) {
  flush_tlb_mm(vma->vm_mm);
  return;
 }

 asm volatile("dsb " "ishst" : : : "memory");
 asid = (atomic64_read(&(vma->vm_mm)->context.id) & 0xffff);

 /*
	 * When the CPU does not support TLB range operations, flush the TLB
	 * entries one by one at the granularity of 'stride'. If the TLB
	 * range ops are supported, then:
	 *
	 * 1. If 'pages' is odd, flush the first page through non-range
	 *    operations;
	 *
	 * 2. For remaining pages: the minimum range granularity is decided
	 *    by 'scale', so multiple range TLBI operations may be required.
	 *    Start from scale = 0, flush the corresponding number of pages
	 *    ((num+1)*2^(5*scale+1) starting from 'addr'), then increase it
	 *    until no pages left.
	 *
	 * Note that certain ranges can be represented by either num = 31 and
	 * scale or num = 0 and scale + 1. The loop below favours the latter
	 * since num is limited to 30 by the __TLBI_RANGE_NUM() macro.
	 */
 while (pages > 0) {
  if (!system_supports_tlb_range() ||
      pages % 2 == 1) {
   addr = ({ unsigned long __ta = (start) >> 12; __ta &= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (43)) * 0l)) : (int *)8))), (0) > (43), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (43))))); __ta |= (unsigned long)(asid) << 48; __ta; });
   if (last_level) {
    do { u64 arg = addr; if (cpus_have_const_cap(10) && tlb_level) { u64 ttl = tlb_level & 3; ttl |= get_trans_granule() << 2; arg &= ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))); arg |= ({ ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_317(void) __attribute__((__error__("FIELD_PREP: " "mask is not constant"))); if (!(!(!__builtin_constant_p(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))))) __compiletime_assert_317(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_318(void) __attribute__((__error__("FIELD_PREP: " "mask is zero"))); if (!(!((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) == 0))) __compiletime_assert_318(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_319(void) __attribute__((__error__("FIELD_PREP: " "value too large for the field"))); if (!(!(__builtin_constant_p(ttl) ? ~((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) >> (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (ttl) : 0))) __compiletime_assert_319(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_320(void) __attribute__((__error__("FIELD_PREP: " "type of reg too small for mask"))); if (!(!(((typeof( _Generic((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))))(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))) > ((typeof( _Generic((0ULL), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (0ULL))))(~0ull))))) __compiletime_assert_320(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_321(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0"))); if (!(!((((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0))) __compiletime_assert_321(); } while (0); }); ((typeof(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))(ttl) << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))); }); } asm (".arch " "armv8.5-a" "\n" "tlbi " "vale1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vale1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (arg)); } while(0);
# 334 "./arch/arm64/include/asm/tlbflush.h"
    do { if (arm64_kernel_unmapped_at_el0()) do { u64 arg = (addr | ((((1UL))) << 48)); if (cpus_have_const_cap(10) && tlb_level) { u64 ttl = tlb_level & 3; ttl |= get_trans_granule() << 2; arg &= ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))); arg |= ({ ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_322(void) __attribute__((__error__("FIELD_PREP: " "mask is not constant"))); if (!(!(!__builtin_constant_p(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))))) __compiletime_assert_322(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_323(void) __attribute__((__error__("FIELD_PREP: " "mask is zero"))); if (!(!((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) == 0))) __compiletime_assert_323(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_324(void) __attribute__((__error__("FIELD_PREP: " "value too large for the field"))); if (!(!(__builtin_constant_p(ttl) ? ~((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) >> (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (ttl) : 0))) __compiletime_assert_324(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_325(void) __attribute__((__error__("FIELD_PREP: " "type of reg too small for mask"))); if (!(!(((typeof( _Generic((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))))(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))) > ((typeof( _Generic((0ULL), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (0ULL))))(~0ull))))) __compiletime_assert_325(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_326(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0"))); if (!(!((((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0))) __compiletime_assert_326(); } while (0); }); ((typeof(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))(ttl) << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))); }); } asm (".arch " "armv8.5-a" "\n" "tlbi " "vale1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vale1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (arg)); } while(0); } while (0);
# 335 "./arch/arm64/include/asm/tlbflush.h"
   } else {
    do { u64 arg = addr; if (cpus_have_const_cap(10) && tlb_level) { u64 ttl = tlb_level & 3; ttl |= get_trans_granule() << 2; arg &= ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))); arg |= ({ ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_327(void) __attribute__((__error__("FIELD_PREP: " "mask is not constant"))); if (!(!(!__builtin_constant_p(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))))) __compiletime_assert_327(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_328(void) __attribute__((__error__("FIELD_PREP: " "mask is zero"))); if (!(!((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) == 0))) __compiletime_assert_328(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_329(void) __attribute__((__error__("FIELD_PREP: " "value too large for the field"))); if (!(!(__builtin_constant_p(ttl) ? ~((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) >> (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (ttl) : 0))) __compiletime_assert_329(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_330(void) __attribute__((__error__("FIELD_PREP: " "type of reg too small for mask"))); if (!(!(((typeof( _Generic((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))))(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))) > ((typeof( _Generic((0ULL), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (0ULL))))(~0ull))))) __compiletime_assert_330(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_331(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0"))); if (!(!((((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0))) __compiletime_assert_331(); } while (0); }); ((typeof(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))(ttl) << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))); }); } asm (".arch " "armv8.5-a" "\n" "tlbi " "vae1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vae1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (arg)); } while(0);
# 337 "./arch/arm64/include/asm/tlbflush.h"
    do { if (arm64_kernel_unmapped_at_el0()) do { u64 arg = (addr | ((((1UL))) << 48)); if (cpus_have_const_cap(10) && tlb_level) { u64 ttl = tlb_level & 3; ttl |= get_trans_granule() << 2; arg &= ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))); arg |= ({ ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_332(void) __attribute__((__error__("FIELD_PREP: " "mask is not constant"))); if (!(!(!__builtin_constant_p(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))))) __compiletime_assert_332(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_333(void) __attribute__((__error__("FIELD_PREP: " "mask is zero"))); if (!(!((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) == 0))) __compiletime_assert_333(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_334(void) __attribute__((__error__("FIELD_PREP: " "value too large for the field"))); if (!(!(__builtin_constant_p(ttl) ? ~((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) >> (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (ttl) : 0))) __compiletime_assert_334(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_335(void) __attribute__((__error__("FIELD_PREP: " "type of reg too small for mask"))); if (!(!(((typeof( _Generic((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))))(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47))))))) > ((typeof( _Generic((0ULL), char: (unsigned char)0, unsigned char: (unsigned char)0, signed char: (unsigned char)0, unsigned short: (unsigned short)0, signed short: (unsigned short)0, unsigned int: (unsigned int)0, signed int: (unsigned int)0, unsigned long: (unsigned long)0, signed long: (unsigned long)0, unsigned long long: (unsigned long long)0, signed long long: (unsigned long long)0, default: (0ULL))))(~0ull))))) __compiletime_assert_335(); } while (0); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_336(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0"))); if (!(!((((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1))) - 1)) != 0))) __compiletime_assert_336(); } while (0); }); ((typeof(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))))(ttl) << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))) - 1)) & (((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((44) > (47)) * 0l)) : (int *)8))), (44) > (47), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (44)) + 1) & (~(((0ULL))) >> (64 - 1 - (47)))))); }); } asm (".arch " "armv8.5-a" "\n" "tlbi " "vae1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vae1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (arg)); } while(0); } while (0);
# 338 "./arch/arm64/include/asm/tlbflush.h"
   }
   start += stride;
   pages -= stride >> 12;
   continue;
  }

  num = ((((pages) >> (5 * (scale) + 1)) & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (4)) * 0l)) : (int *)8))), (0) > (4), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (4)))))) - 1);
  if (num >= 0) {
   addr = ({ unsigned long __ta = (start) >> 12; __ta &= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (36)) * 0l)) : (int *)8))), (0) > (36), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (36))))); __ta |= (unsigned long)(tlb_level) << 37; __ta |= (unsigned long)(num) << 39; __ta |= (unsigned long)(scale) << 44; __ta |= get_trans_granule() << 46; __ta |= (unsigned long)(asid) << 48; __ta; });

   if (last_level) {
    asm (".arch " "armv8.5-a" "\n" "tlbi " "rvale1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "rvale1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (addr));
    do { if (arm64_kernel_unmapped_at_el0()) asm (".arch " "armv8.5-a" "\n" "tlbi " "rvale1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "rvale1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" ((addr) | ((((1UL))) << 48))); } while (0);
   } else {
    asm (".arch " "armv8.5-a" "\n" "tlbi " "rvae1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "rvae1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (addr));
    do { if (arm64_kernel_unmapped_at_el0()) asm (".arch " "armv8.5-a" "\n" "tlbi " "rvae1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "rvae1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" ((addr) | ((((1UL))) << 48))); } while (0);
   }
   start += ((unsigned long)((num) + 1) << (5 * (scale) + 1)) << 12;
   pages -= ((unsigned long)((num) + 1) << (5 * (scale) + 1));
  }
  scale++;
 }
 asm volatile("dsb " "ish" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_tlb_range(struct vm_area_struct *vma,
       unsigned long start, unsigned long end)
{
 /*
	 * We cannot use leaf-only invalidation here, since we may be invalidating
	 * table entries as part of collapsing hugepages or moving page tables.
	 * Set the tlb_level to 0 because we can not get enough information here.
	 */
 __flush_tlb_range(vma, start, end, ((1UL) << 12), false, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_tlb_kernel_range(unsigned long start, unsigned long end)
{
 unsigned long addr;

 if ((end - start) > ((1 << (12 - 3)) * ((1UL) << 12))) {
  flush_tlb_all();
  return;
 }

 start = ({ unsigned long __ta = (start) >> 12; __ta &= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (43)) * 0l)) : (int *)8))), (0) > (43), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (43))))); __ta |= (unsigned long)(0) << 48; __ta; });
 end = ({ unsigned long __ta = (end) >> 12; __ta &= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (43)) * 0l)) : (int *)8))), (0) > (43), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (43))))); __ta |= (unsigned long)(0) << 48; __ta; });

 asm volatile("dsb " "ishst" : : : "memory");
 for (addr = start; addr < end; addr += 1 << (12 - 12))
  asm (".arch " "armv8.5-a" "\n" "tlbi " "vaale1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vaale1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (addr));
 asm volatile("dsb " "ish" : : : "memory");
 asm volatile("isb" : : : "memory");
}

/*
 * Used to invalidate the TLB (walk caches) corresponding to intermediate page
 * table levels (pgd/pud/pmd).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __flush_tlb_kernel_pgtable(unsigned long kaddr)
{
 unsigned long addr = ({ unsigned long __ta = (kaddr) >> 12; __ta &= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (43)) * 0l)) : (int *)8))), (0) > (43), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (0)) + 1) & (~(((0ULL))) >> (64 - 1 - (43))))); __ta |= (unsigned long)(0) << 48; __ta; });

 asm volatile("dsb " "ishst" : : : "memory");
 asm (".arch " "armv8.5-a" "\n" "tlbi " "vaae1is" ", %0\n" ".if ""1"" == 1\n" "661:\n\t" "nop\n			nop" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "82" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "dsb ish\n		tlbi " "vaae1is" ", %0" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n" : : "r" (addr));
 asm volatile("dsb " "ish" : : : "memory");
 asm volatile("isb" : : : "memory");
}
# 16 "./arch/arm64/include/asm/pgtable.h" 2

/*
 * VMALLOC range.
 *
 * VMALLOC_START: beginning of the kernel vmalloc space
 * VMALLOC_END: extends to the available space below vmemmap, PCI I/O space
 *	and fixed mappings
 */
# 32 "./arch/arm64/include/asm/pgtable.h"
# 1 "./arch/arm64/include/asm/fixmap.h" 1
/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 * Copyright (C) 2013 Mark Salter <msalter@redhat.com>
 *
 * Adapted from arch/x86 version.
 *
 */
# 25 "./arch/arm64/include/asm/fixmap.h"
/*
 * Here we define all the compile-time 'special' virtual
 * addresses. The point is to have a constant address at
 * compile time, but to set the physical address only
 * in the boot process.
 *
 * Each enum increment in these 'compile-time allocated'
 * memory buffers is page-sized. Use set_fixmap(idx,phys)
 * to associate physical memory with a fixmap index.
 */
enum fixed_addresses {
 FIX_HOLE,

 /*
	 * Reserve a virtual window for the FDT that is 2 MB larger than the
	 * maximum supported size, and put it at the top of the fixmap region.
	 * The additional space ensures that any FDT that does not exceed
	 * MAX_FDT_SIZE can be mapped regardless of whether it crosses any
	 * 2 MB alignment boundaries.
	 *
	 * Keep this at the top so it remains 2 MB aligned.
	 */

 FIX_FDT_END,
 FIX_FDT = FIX_FDT_END + (0x00200000 + 0x00200000) / ((1UL) << 12) - 1,

 FIX_EARLYCON_MEM_BASE,
 FIX_TEXT_POKE0,


 /* Used for GHES mapping from assorted contexts */
 FIX_APEI_GHES_IRQ,
 FIX_APEI_GHES_SEA,
# 66 "./arch/arm64/include/asm/fixmap.h"
 FIX_ENTRY_TRAMP_TEXT4, /* one extra slot for the data page */

 FIX_ENTRY_TRAMP_TEXT3,
 FIX_ENTRY_TRAMP_TEXT2,
 FIX_ENTRY_TRAMP_TEXT1,


 __end_of_permanent_fixed_addresses,

 /*
	 * Temporary boot-time mappings, used by early_ioremap(),
	 * before ioremap() is functional.
	 */




 FIX_BTMAP_END = __end_of_permanent_fixed_addresses,
 FIX_BTMAP_BEGIN = FIX_BTMAP_END + ((0x00040000 / ((1UL) << 12)) * 7) - 1,

 /*
	 * Used for kernel page table creation, so unmapped memory may be used
	 * for tables.
	 */
 FIX_PTE,
 FIX_PMD,
 FIX_PUD,
 FIX_PGD,

 __end_of_fixed_addresses
};






void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) early_fixmap_init(void);






extern void __set_fixmap(enum fixed_addresses idx, phys_addr_t phys, pgprot_t prot);

# 1 "./include/asm-generic/fixmap.h" 1
/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 * Break out common bits to asm-generic by Mark Salter, November 2013
 */
# 25 "./include/asm-generic/fixmap.h"
/*
 * 'index to address' translation. If anyone tries to use the idx
 * directly without translation, we catch the bug with a NULL-deference
 * kernel oops. Illegal ranges of incoming indices are caught too.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) unsigned long fix_to_virt(const unsigned int idx)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_337(void) __attribute__((__error__("BUILD_BUG_ON failed: " "idx >= __end_of_fixed_addresses"))); if (!(!(idx >= __end_of_fixed_addresses))) __compiletime_assert_337(); } while (0);
# 33 "./include/asm-generic/fixmap.h"
 return (((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x02000000) - ((idx) << 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long virt_to_fix(const unsigned long vaddr)
{
 do { if (__builtin_expect(!!(vaddr >= ((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x02000000) || vaddr < (((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x02000000) - (__end_of_permanent_fixed_addresses << 12))), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/asm-generic/fixmap.h\"; .popsection; .long 14472b - .; .short 38; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 return ((((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x02000000) - ((vaddr)&(~(((1UL) << 12)-1)))) >> 12);
}

/*
 * Provide some reasonable defaults for page flags.
 * Not all architectures use all of these different types and some
 * architectures use different names.
 */
# 73 "./include/asm-generic/fixmap.h"
/* Return a pointer with offset calculated */
# 85 "./include/asm-generic/fixmap.h"
/*
 * Some hardware wants to get fixmapped without caching.
 */






/*
 * Some fixmaps are for IO
 */
# 113 "./arch/arm64/include/asm/fixmap.h" 2
# 33 "./arch/arm64/include/asm/pgtable.h" 2



# 1 "./include/linux/page_table_check.h" 1
/* SPDX-License-Identifier: GPL-2.0 */

/*
 * Copyright (c) 2021, Google LLC.
 * Pasha Tatashin <pasha.tatashin@soleen.com>
 */
# 118 "./include/linux/page_table_check.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_alloc(struct page *page, unsigned int order)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_free(struct page *page, unsigned int order)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_pte_clear(struct mm_struct *mm,
           unsigned long addr, pte_t pte)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_pmd_clear(struct mm_struct *mm,
           unsigned long addr, pmd_t pmd)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_pud_clear(struct mm_struct *mm,
           unsigned long addr, pud_t pud)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_pte_set(struct mm_struct *mm,
         unsigned long addr, pte_t *ptep,
         pte_t pte)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_pmd_set(struct mm_struct *mm,
         unsigned long addr, pmd_t *pmdp,
         pmd_t pmd)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_pud_set(struct mm_struct *mm,
         unsigned long addr, pud_t *pudp,
         pud_t pud)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_table_check_pte_clear_range(struct mm_struct *mm,
          unsigned long addr,
          pmd_t pmd)
{
}
# 37 "./arch/arm64/include/asm/pgtable.h" 2




/* Set stride and tlb_level in flush_*_tlb_range */






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_thp_swp_supported(void)
{
 return !system_supports_mte();
}


/*
 * Outside of a few very special situations (e.g. hibernation), we always
 * use broadcast TLB invalidation instructions, therefore a spurious page
 * fault on one CPU which has been handled concurrently by another CPU
 * does not need to perform additional invalidation.
 */


/*
 * ZERO_PAGE is a global shared page that is always zero: used
 * for zero-mapped memory areas etc..
 */
extern unsigned long empty_zero_page[((1UL) << 12) / sizeof(unsigned long)];





/*
 * Macros to convert between a physical address and its placement in a
 * page table entry, taking care of 52-bit addresses.
 */
# 99 "./arch/arm64/include/asm/pgtable.h"
/*
 * The following only work if pte_present(). Undefined behaviour otherwise.
 */
# 128 "./arch/arm64/include/asm/pgtable.h"
/*
 * Execute-only user mappings do not have the PTE_USER bit set. All valid
 * kernel mappings have the PTE_UXN bit set.
 */


/*
 * Could the pte be present in the TLB? We must check mm_tlb_flush_pending
 * so that we don't erroneously return false for pages that have been
 * remapped as PROT_NONE but are yet to be flushed from the TLB.
 * Note that we can't make any assumptions based on the state of the access
 * flag, since ptep_clear_flush_young() elides a DSB when invalidating the
 * TLB.
 */



/*
 * p??_access_permitted() is true for valid user mappings (PTE_USER
 * bit set, subject to the write permission check). For execute-only
 * mappings, like PROT_EXEC with EPAN (both PTE_USER and PTE_UXN bits
 * not set) must return false. PROT_NONE mappings do not have the
 * PTE_VALID bit set.
 */







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t clear_pte_bit(pte_t pte, pgprot_t prot)
{
 ((pte).pte) &= ~((prot).pgprot);
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t set_pte_bit(pte_t pte, pgprot_t prot)
{
 ((pte).pte) |= ((prot).pgprot);
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t clear_pmd_bit(pmd_t pmd, pgprot_t prot)
{
 ((pmd).pmd) &= ~((prot).pgprot);
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t set_pmd_bit(pmd_t pmd, pgprot_t prot)
{
 ((pmd).pmd) |= ((prot).pgprot);
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkwrite(pte_t pte)
{
 pte = set_pte_bit(pte, ((pgprot_t) { (((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */) } ));
 pte = clear_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 7) /* AP[2] */) } ));
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkclean(pte_t pte)
{
 pte = clear_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 55)) } ));
 pte = set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 7) /* AP[2] */) } ));

 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkdirty(pte_t pte)
{
 pte = set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 55)) } ));

 if ((!!(((pte).pte) & ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */)))
  pte = clear_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 7) /* AP[2] */) } ));

 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_wrprotect(pte_t pte)
{
 /*
	 * If hardware-dirty (PTE_WRITE/DBM bit set and PTE_RDONLY
	 * clear), set the PTE_DIRTY bit.
	 */
 if (((!!(((pte).pte) & ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */)) && !(((pte).pte) & (((pteval_t)(1)) << 7) /* AP[2] */)))
  pte = pte_mkdirty(pte);

 pte = clear_pte_bit(pte, ((pgprot_t) { (((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */) } ));
 pte = set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 7) /* AP[2] */) } ));
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkold(pte_t pte)
{
 return clear_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 10) /* Access Flag */) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkyoung(pte_t pte)
{
 return set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 10) /* Access Flag */) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkspecial(pte_t pte)
{
 return set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 56)) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkcont(pte_t pte)
{
 pte = set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 52) /* Contiguous range */) } ));
 return set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(3)) << 0)) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mknoncont(pte_t pte)
{
 return clear_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 52) /* Contiguous range */) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkpresent(pte_t pte)
{
 return set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 0)) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_mkcont(pmd_t pmd)
{
 return ((pmd_t) { (((pmd).pmd) | (((pmdval_t)(1)) << 52)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mkdevmap(pte_t pte)
{
 return set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 57) | (((pteval_t)(1)) << 56)) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_pte(pte_t *ptep, pte_t pte)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_338(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*ptep) == sizeof(char) || sizeof(*ptep) == sizeof(short) || sizeof(*ptep) == sizeof(int) || sizeof(*ptep) == sizeof(long)) || sizeof(*ptep) == sizeof(long long))) __compiletime_assert_338(); } while (0); do { *(volatile typeof(*ptep) *)&(*ptep) = (pte); } while (0); } while (0);
# 267 "./arch/arm64/include/asm/pgtable.h"
 /*
	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
	 * or update_mmu_cache() have the necessary barriers.
	 */
 if (((((pte).pte) & ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 6) /* AP[1] */ | (((pteval_t)(1)) << 54) /* User XN */)) == ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 54) /* User XN */))) {
  asm volatile("dsb " "ishst" : : : "memory");
  asm volatile("isb" : : : "memory");
 }
}

extern void __sync_icache_dcache(pte_t pteval);

/*
 * PTE bits configuration in the presence of hardware Dirty Bit Management
 * (PTE_WRITE == PTE_DBM):
 *
 * Dirty  Writable | PTE_RDONLY  PTE_WRITE  PTE_DIRTY (sw)
 *   0      0      |   1           0          0
 *   0      1      |   1           1          0
 *   1      0      |   1           0          1
 *   1      1      |   0           1          x
 *
 * When hardware DBM is not present, the sofware PTE_DIRTY bit is updated via
 * the page fault mechanism. Checking the dirty status of a pte becomes:
 *
 *   PTE_DIRTY || (PTE_WRITE && !PTE_RDONLY)
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __check_racy_pte_update(struct mm_struct *mm, pte_t *ptep,
        pte_t pte)
{
 pte_t old_pte;

 if (!0)
  return;

 old_pte = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_339(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*ptep) == sizeof(char) || sizeof(*ptep) == sizeof(short) || sizeof(*ptep) == sizeof(int) || sizeof(*ptep) == sizeof(long)) || sizeof(*ptep) == sizeof(long long))) __compiletime_assert_339(); } while (0); (*(const volatile typeof( _Generic((*ptep), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*ptep))) *)&(*ptep)); });
# 305 "./arch/arm64/include/asm/pgtable.h"
 if (!(!!(((old_pte).pte) & (((pteval_t)(1)) << 0))) || !(!!(((pte).pte) & (((pteval_t)(1)) << 0))))
  return;
 if (mm != get_current()->active_mm && atomic_read(&mm->mm_users) <= 1)
  return;

 /*
	 * Check for potential race with hardware updates of the pte
	 * (ptep_set_access_flags safely changes valid ptes without going
	 * through an invalid entry).
	 */
 ((void)(sizeof(( long)(!(!!(((pte).pte) & (((pteval_t)(1)) << 10) /* Access Flag */))))));


 ((void)(sizeof(( long)((!!(((old_pte).pte) & ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */)) && !((!!(((pte).pte) & (((pteval_t)(1)) << 55))) || ((!!(((pte).pte) & ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */)) && !(((pte).pte) & (((pteval_t)(1)) << 7) /* AP[2] */)))))));


}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __set_pte_at(struct mm_struct *mm, unsigned long addr,
    pte_t *ptep, pte_t pte)
{
 if ((!!(((pte).pte) & ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 58) /* only when !PTE_VALID */))) && (!(((pte).pte) & (((pteval_t)(1)) << 54) /* User XN */)) && !(!!(((pte).pte) & (((pteval_t)(1)) << 56))))
  __sync_icache_dcache(pte);

 /*
	 * If the PTE would provide user space access to the tags associated
	 * with it then ensure that the MTE tags are synchronised.  Although
	 * pte_access_permitted() returns false for exec only mappings, they
	 * don't expose tags (instruction fetches don't check tags).
	 */
 if (system_supports_mte() && (((((pte).pte) & ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 6) /* AP[1] */)) == ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 6) /* AP[1] */)) && (!(false) || (!!(((pte).pte) & ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */)))) &&
     !(!!(((pte).pte) & (((pteval_t)(1)) << 56)))) {
  pte_t old_pte = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_340(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*ptep) == sizeof(char) || sizeof(*ptep) == sizeof(short) || sizeof(*ptep) == sizeof(int) || sizeof(*ptep) == sizeof(long)) || sizeof(*ptep) == sizeof(long long))) __compiletime_assert_340(); } while (0); (*(const volatile typeof( _Generic((*ptep), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*ptep))) *)&(*ptep)); });
# 338 "./arch/arm64/include/asm/pgtable.h"
  /*
		 * We only need to synchronise if the new PTE has tags enabled
		 * or if swapping in (in which case another mapping may have
		 * set tags in the past even if this PTE isn't tagged).
		 * (!pte_none() && !pte_present()) is an open coded version of
		 * is_swap_pte()
		 */
  if (((((pte).pte) & (((pteval_t)(7)) << 2)) == (((pteval_t)((1))) << 2)) || (!(!((old_pte).pte)) && !(!!(((old_pte).pte) & ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 58) /* only when !PTE_VALID */)))))
   mte_sync_tags(old_pte, pte);
 }

 __check_racy_pte_update(mm, ptep, pte);

 set_pte(ptep, pte);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_pte_at(struct mm_struct *mm, unsigned long addr,
         pte_t *ptep, pte_t pte)
{
 page_table_check_pte_set(mm, addr, ptep, pte);
 return __set_pte_at(mm, addr, ptep, pte);
}

/*
 * Huge pte definitions.
 */


/*
 * Hugetlb definitions.
 */






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pgd_pte(pgd_t pgd)
{
 return ((pte_t) { (((pgd).pgd)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t p4d_pte(p4d_t p4d)
{
 return ((pte_t) { (((((p4d).pgd).pgd))) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pud_pte(pud_t pud)
{
 return ((pte_t) { (((pud).pud)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pud_t pte_pud(pte_t pte)
{
 return ((pud_t) { (((pte).pte)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pud_pmd(pud_t pud)
{
 return ((pmd_t) { (((pud).pud)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pmd_pte(pmd_t pmd)
{
 return ((pte_t) { (((pmd).pmd)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pte_pmd(pte_t pte)
{
 return ((pmd_t) { (((pte).pte)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pgprot_t mk_pud_sect_prot(pgprot_t prot)
{
 return ((pgprot_t) { ((((prot).pgprot) & ~(((pudval_t)(1)) << 1)) | (((pudval_t)(1)) << 0)) } );
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pgprot_t mk_pmd_sect_prot(pgprot_t prot)
{
 return ((pgprot_t) { ((((prot).pgprot) & ~(((pmdval_t)(1)) << 1)) | (((pmdval_t)(1)) << 0)) } );
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_swp_mkexclusive(pte_t pte)
{
 return set_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 2) /* only for swp ptes */) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pte_swp_exclusive(pte_t pte)
{
 return ((pte).pte) & (((pteval_t)(1)) << 2) /* only for swp ptes */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_swp_clear_exclusive(pte_t pte)
{
 return clear_pte_bit(pte, ((pgprot_t) { ((((pteval_t)(1)) << 2) /* only for swp ptes */) } ));
}

/*
 * Select all bits except the pfn
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pgprot_t pte_pgprot(pte_t pte)
{
 unsigned long pfn = ((((pte).pte) & (((((pteval_t)(1)) << (48 - 12)) - 1) << 12)) >> 12);

 return ((pgprot_t) { (((((pte_t) { (((phys_addr_t)(pfn) << 12) | ((((pgprot_t) { (0) } )).pgprot)) } )).pte) ^ ((pte).pte)) } );
}


/*
 * See the comment in include/linux/pgtable.h
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pte_protnone(pte_t pte)
{
 return (((pte).pte) & ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 58) /* only when !PTE_VALID */)) == (((pteval_t)(1)) << 58) /* only when !PTE_VALID */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_protnone(pmd_t pmd)
{
 return pte_protnone(pmd_pte(pmd));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_present(pmd_t pmd)
{
 return (!!(((pmd_pte(pmd)).pte) & ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 58) /* only when !PTE_VALID */))) || (!!(((pmd).pmd) & (((pteval_t)(1)) << 59) /* only when !PMD_SECT_VALID */));
}

/*
 * THP definitions.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_trans_huge(pmd_t pmd)
{
 return ((pmd).pmd) && pmd_present(pmd) && !(((pmd).pmd) & (((pmdval_t)(1)) << 1));
}
# 492 "./arch/arm64/include/asm/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_mkinvalid(pmd_t pmd)
{
 pmd = set_pmd_bit(pmd, ((pgprot_t) { ((((pteval_t)(1)) << 59) /* only when !PMD_SECT_VALID */) } ));
 pmd = clear_pmd_bit(pmd, ((pgprot_t) { ((((pmdval_t)(1)) << 0)) } ));

 return pmd;
}
# 509 "./arch/arm64/include/asm/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_mkdevmap(pmd_t pmd)
{
 return pte_pmd(set_pte_bit(pmd_pte(pmd), ((pgprot_t) { ((((pteval_t)(1)) << 57)) } )));
}
# 531 "./arch/arm64/include/asm/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_pmd_at(struct mm_struct *mm, unsigned long addr,
         pmd_t *pmdp, pmd_t pmd)
{
 page_table_check_pmd_set(mm, addr, pmdp, pmd);
 return __set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_pud_at(struct mm_struct *mm, unsigned long addr,
         pud_t *pudp, pud_t pud)
{
 page_table_check_pud_set(mm, addr, pudp, pud);
 return __set_pte_at(mm, addr, (pte_t *)pudp, pud_pte(pud));
}
# 557 "./arch/arm64/include/asm/pgtable.h"
/*
 * Mark the prot value as uncacheable and unbufferable.
 */
# 569 "./arch/arm64/include/asm/pgtable.h"
/*
 * DMA allocations for non-coherent devices use what the Arm architecture calls
 * "Normal non-cacheable" memory, which permits speculation, unaligned accesses
 * and merging of writes.  This is different from "Device-nGnR[nE]" memory which
 * is intended for MMIO and thus forbids speculation, preserves access size,
 * requires strict alignment and can also force write responses to come from the
 * endpoint.
 */





struct file;
extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
         unsigned long size, pgprot_t vma_prot);
# 608 "./arch/arm64/include/asm/pgtable.h"
extern pgd_t init_pg_dir[(1 << ((48) - ((12 - 3) * (4 - (4 - 4)) + 3)))];
extern pgd_t init_pg_end[];
extern pgd_t swapper_pg_dir[(1 << ((48) - ((12 - 3) * (4 - (4 - 4)) + 3)))];
extern pgd_t idmap_pg_dir[(1 << ((48) - ((12 - 3) * (4 - (4 - 4)) + 3)))];
extern pgd_t tramp_pg_dir[(1 << ((48) - ((12 - 3) * (4 - (4 - 4)) + 3)))];
extern pgd_t reserved_pg_dir[(1 << ((48) - ((12 - 3) * (4 - (4 - 4)) + 3)))];

extern void set_swapper_pgd(pgd_t *pgdp, pgd_t pgd);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool in_swapper_pgdir(void *addr)
{
 return ((unsigned long)addr & (~(((1UL) << 12)-1))) ==
         ((unsigned long)swapper_pg_dir & (~(((1UL) << 12)-1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_pmd(pmd_t *pmdp, pmd_t pmd)
{







 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_341(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pmdp) == sizeof(char) || sizeof(*pmdp) == sizeof(short) || sizeof(*pmdp) == sizeof(int) || sizeof(*pmdp) == sizeof(long)) || sizeof(*pmdp) == sizeof(long long))) __compiletime_assert_341(); } while (0); do { *(volatile typeof(*pmdp) *)&(*pmdp) = (pmd); } while (0); } while (0);
# 634 "./arch/arm64/include/asm/pgtable.h"
 if ((!!(((pmd_pte(pmd)).pte) & (((pteval_t)(1)) << 0)))) {
  asm volatile("dsb " "ishst" : : : "memory");
  asm volatile("isb" : : : "memory");
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pmd_clear(pmd_t *pmdp)
{
 set_pmd(pmdp, ((pmd_t) { (0) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) phys_addr_t pmd_page_paddr(pmd_t pmd)
{
 return (((pmd_pte(pmd)).pte) & (((((pteval_t)(1)) << (48 - 12)) - 1) << 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pmd_page_vaddr(pmd_t pmd)
{
 return (unsigned long)((void *)((unsigned long)(((phys_addr_t)(pmd_page_paddr(pmd))) - ({ ((void)(sizeof(( long)(memstart_addr & 1)))); memstart_addr; })) | ((-((((1UL))) << ((48)))))));
}

/* Find an entry in the third-level page table. */
# 664 "./arch/arm64/include/asm/pgtable.h"
/* use ONLY for statically allocated translation tables */


/*
 * Conversion functions: convert a page and protection to a page entry,
 * and a page entry and page directory to the page they refer to.
 */
# 686 "./arch/arm64/include/asm/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_pud(pud_t *pudp, pud_t pud)
{







 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_342(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pudp) == sizeof(char) || sizeof(*pudp) == sizeof(short) || sizeof(*pudp) == sizeof(int) || sizeof(*pudp) == sizeof(long)) || sizeof(*pudp) == sizeof(long long))) __compiletime_assert_342(); } while (0); do { *(volatile typeof(*pudp) *)&(*pudp) = (pud); } while (0); } while (0);
# 697 "./arch/arm64/include/asm/pgtable.h"
 if ((!!(((pud_pte(pud)).pte) & (((pteval_t)(1)) << 0)))) {
  asm volatile("dsb " "ishst" : : : "memory");
  asm volatile("isb" : : : "memory");
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pud_clear(pud_t *pudp)
{
 set_pud(pudp, ((pud_t) { (0) } ));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) phys_addr_t pud_page_paddr(pud_t pud)
{
 return (((pud_pte(pud)).pte) & (((((pteval_t)(1)) << (48 - 12)) - 1) << 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t *pud_pgtable(pud_t pud)
{
 return (pmd_t *)((void *)((unsigned long)(((phys_addr_t)(pud_page_paddr(pud))) - ({ ((void)(sizeof(( long)(memstart_addr & 1)))); memstart_addr; })) | ((-((((1UL))) << ((48)))))));
}

/* Find an entry in the second-level page table. */
# 727 "./arch/arm64/include/asm/pgtable.h"
/* use ONLY for statically allocated translation tables */
# 752 "./arch/arm64/include/asm/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_p4d(p4d_t *p4dp, p4d_t p4d)
{
 if (in_swapper_pgdir(p4dp)) {
  set_swapper_pgd((pgd_t *)p4dp, ((pgd_t) { (((((p4d).pgd).pgd))) } ));
  return;
 }

 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_343(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*p4dp) == sizeof(char) || sizeof(*p4dp) == sizeof(short) || sizeof(*p4dp) == sizeof(int) || sizeof(*p4dp) == sizeof(long)) || sizeof(*p4dp) == sizeof(long long))) __compiletime_assert_343(); } while (0); do { *(volatile typeof(*p4dp) *)&(*p4dp) = (p4d); } while (0); } while (0);
# 760 "./arch/arm64/include/asm/pgtable.h"
 asm volatile("dsb " "ishst" : : : "memory");
 asm volatile("isb" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void p4d_clear(p4d_t *p4dp)
{
 set_p4d(p4dp, ((p4d_t) { ((pgd_t) { (0) } ) }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) phys_addr_t p4d_page_paddr(p4d_t p4d)
{
 return (((p4d_pte(p4d)).pte) & (((((pteval_t)(1)) << (48 - 12)) - 1) << 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pud_t *p4d_pgtable(p4d_t p4d)
{
 return (pud_t *)((void *)((unsigned long)(((phys_addr_t)(p4d_page_paddr(p4d))) - ({ ((void)(sizeof(( long)(memstart_addr & 1)))); memstart_addr; })) | ((-((((1UL))) << ((48)))))));
}

/* Find an entry in the first-level page table. */
# 788 "./arch/arm64/include/asm/pgtable.h"
/* use ONLY for statically allocated translation tables */
# 811 "./arch/arm64/include/asm/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_modify(pte_t pte, pgprot_t newprot)
{
 /*
	 * Normal and Normal-Tagged are two different memory types and indices
	 * in MAIR_EL1. The mask below has to include PTE_ATTRINDX_MASK.
	 */
 const pteval_t mask = (((pteval_t)(1)) << 6) /* AP[1] */ | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */ | (((pteval_t)(1)) << 7) /* AP[2] */ |
         (((pteval_t)(1)) << 58) /* only when !PTE_VALID */ | (((pteval_t)(1)) << 0) | ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */ | (((pteval_t)(1)) << 50) /* BTI guarded */ |
         (((pteval_t)(7)) << 2);
 /* preserve the hardware dirty information */
 if (((!!(((pte).pte) & ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */)) && !(((pte).pte) & (((pteval_t)(1)) << 7) /* AP[2] */)))
  pte = pte_mkdirty(pte);
 ((pte).pte) = (((pte).pte) & ~mask) | (((newprot).pgprot) & mask);
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
{
 return pte_pmd(pte_modify(pmd_pte(pmd), newprot));
}


extern int ptep_set_access_flags(struct vm_area_struct *vma,
     unsigned long address, pte_t *ptep,
     pte_t entry, int dirty);



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmdp_set_access_flags(struct vm_area_struct *vma,
     unsigned long address, pmd_t *pmdp,
     pmd_t entry, int dirty)
{
 return ptep_set_access_flags(vma, address, (pte_t *)pmdp, pmd_pte(entry), dirty);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pud_devmap(pud_t pud)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pgd_devmap(pgd_t pgd)
{
 return 0;
}
# 874 "./arch/arm64/include/asm/pgtable.h"
/*
 * Atomic pte/pmd modifications.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __ptep_test_and_clear_young(pte_t *ptep)
{
 pte_t old_pte, pte;

 pte = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_344(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*ptep) == sizeof(char) || sizeof(*ptep) == sizeof(short) || sizeof(*ptep) == sizeof(int) || sizeof(*ptep) == sizeof(long)) || sizeof(*ptep) == sizeof(long long))) __compiletime_assert_344(); } while (0); (*(const volatile typeof( _Generic((*ptep), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*ptep))) *)&(*ptep)); });
# 883 "./arch/arm64/include/asm/pgtable.h"
 do {
  old_pte = pte;
  pte = pte_mkold(pte);
  ((pte).pte) = ({ typeof(&((*ptep).pte)) __ai_ptr = (&((*ptep).pte)); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __ret; __ret = (__typeof__(*(__ai_ptr))) __cmpxchg((__ai_ptr), (unsigned long)(((old_pte).pte)), (unsigned long)(((pte).pte)), sizeof(*(__ai_ptr))); __ret; }); });

 } while (((pte).pte) != ((old_pte).pte));

 return (!!(((pte).pte) & (((pteval_t)(1)) << 10) /* Access Flag */));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ptep_test_and_clear_young(struct vm_area_struct *vma,
         unsigned long address,
         pte_t *ptep)
{
 return __ptep_test_and_clear_young(ptep);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ptep_clear_flush_young(struct vm_area_struct *vma,
      unsigned long address, pte_t *ptep)
{
 int young = ptep_test_and_clear_young(vma, address, ptep);

 if (young) {
  /*
		 * We can elide the trailing DSB here since the worst that can
		 * happen is that a CPU continues to use the young entry in its
		 * TLB and we mistakenly reclaim the associated page. The
		 * window for such an event is bounded by the next
		 * context-switch, which provides a DSB to complete the TLB
		 * invalidation.
		 */
  flush_tlb_page_nosync(vma, address);
 }

 return young;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmdp_test_and_clear_young(struct vm_area_struct *vma,
         unsigned long address,
         pmd_t *pmdp)
{
 return ptep_test_and_clear_young(vma, address, (pte_t *)pmdp);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t ptep_get_and_clear(struct mm_struct *mm,
           unsigned long address, pte_t *ptep)
{
 pte_t pte = ((pte_t) { (({ typeof(&((*ptep).pte)) __ai_ptr = (&((*ptep).pte)); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __ret; __ret = (__typeof__(*(__ai_ptr))) __xchg((unsigned long)(0), (__ai_ptr), sizeof(*(__ai_ptr))); __ret; }); })) } );

 page_table_check_pte_clear(mm, address, pte);

 return pte;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
         unsigned long address, pmd_t *pmdp)
{
 pmd_t pmd = ((pmd_t) { (({ typeof(&((*pmdp).pmd)) __ai_ptr = (&((*pmdp).pmd)); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __ret; __ret = (__typeof__(*(__ai_ptr))) __xchg((unsigned long)(0), (__ai_ptr), sizeof(*(__ai_ptr))); __ret; }); })) } );

 page_table_check_pmd_clear(mm, address, pmd);

 return pmd;
}


/*
 * ptep_set_wrprotect - mark read-only while trasferring potential hardware
 * dirty status (PTE_DBM && !PTE_RDONLY) to the software PTE_DIRTY bit.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptep_set_wrprotect(struct mm_struct *mm, unsigned long address, pte_t *ptep)
{
 pte_t old_pte, pte;

 pte = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_345(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*ptep) == sizeof(char) || sizeof(*ptep) == sizeof(short) || sizeof(*ptep) == sizeof(int) || sizeof(*ptep) == sizeof(long)) || sizeof(*ptep) == sizeof(long long))) __compiletime_assert_345(); } while (0); (*(const volatile typeof( _Generic((*ptep), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*ptep))) *)&(*ptep)); });
# 965 "./arch/arm64/include/asm/pgtable.h"
 do {
  old_pte = pte;
  pte = pte_wrprotect(pte);
  ((pte).pte) = ({ typeof(&((*ptep).pte)) __ai_ptr = (&((*ptep).pte)); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __ret; __ret = (__typeof__(*(__ai_ptr))) __cmpxchg((__ai_ptr), (unsigned long)(((old_pte).pte)), (unsigned long)(((pte).pte)), sizeof(*(__ai_ptr))); __ret; }); });

 } while (((pte).pte) != ((old_pte).pte));
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pmdp_set_wrprotect(struct mm_struct *mm,
          unsigned long address, pmd_t *pmdp)
{
 ptep_set_wrprotect(mm, address, (pte_t *)pmdp);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmdp_establish(struct vm_area_struct *vma,
  unsigned long address, pmd_t *pmdp, pmd_t pmd)
{
 page_table_check_pmd_set(vma->vm_mm, address, pmdp, pmd);
 return ((pmd_t) { (({ typeof(&((*pmdp).pmd)) __ai_ptr = (&((*pmdp).pmd)); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __ret; __ret = (__typeof__(*(__ai_ptr))) __xchg((unsigned long)(((pmd).pmd)), (__ai_ptr), sizeof(*(__ai_ptr))); __ret; }); })) } );
}


/*
 * Encode and decode a swap entry:
 *	bits 0-1:	present (must be zero)
 *	bits 2:		remember PG_anon_exclusive
 *	bits 3-7:	swap type
 *	bits 8-57:	swap offset
 *	bit  58:	PTE_PROT_NONE (must be zero)
 */
# 1017 "./arch/arm64/include/asm/pgtable.h"
/*
 * Ensure that there are not more swap files than can be encoded in the kernel
 * PTEs.
 */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_prepare_to_swap(struct page *page)
{
 if (system_supports_mte())
  return mte_save_tags(page);
 return 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_swap_invalidate_page(int type, unsigned long offset)
{
 if (system_supports_mte())
  mte_invalidate_tags(type, offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_swap_invalidate_area(int type)
{
 if (system_supports_mte())
  mte_invalidate_tags_area(type);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_swap_restore(swp_entry_t entry, struct folio *folio)
{
 if (system_supports_mte() && mte_restore_tags(entry, &folio->page))
  set_bit(PG_arch_2, &folio->flags);
}



/*
 * On AArch64, the cache coherency is handled via the set_pte_at() function.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void update_mmu_cache(struct vm_area_struct *vma,
        unsigned long addr, pte_t *ptep)
{
 /*
	 * We don't do anything here, so there's a very small chance of
	 * us retaking a user fault which we just fixed up. The alternative
	 * is doing a dsb(ishst), but that penalises the fastpath.
	 */
}
# 1076 "./arch/arm64/include/asm/pgtable.h"
/*
 * On arm64 without hardware Access Flag, copying from user will fail because
 * the pte is old and cannot be marked young. So we always end up with zeroed
 * page after fork() + CoW for pfn mappings. We don't always have a
 * hardware-managed access flag on arm64.
 */


/*
 * Experimentally, it's cheap to set the access flag in hardware and we
 * benefit from prefaulting mappings as 'old' to start with.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pud_sect_supported(void)
{
 return ((1UL) << 12) == 0x00001000;
}




extern pte_t ptep_modify_prot_start(struct vm_area_struct *vma,
        unsigned long addr, pte_t *ptep);


extern void ptep_modify_prot_commit(struct vm_area_struct *vma,
        unsigned long addr, pte_t *ptep,
        pte_t old_pte, pte_t new_pte);
# 7 "./include/linux/pgtable.h" 2







# 1 "./include/asm-generic/pgtable_uffd.h" 1




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int pte_uffd_wp(pte_t pte)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int pmd_uffd_wp(pmd_t pmd)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) pte_t pte_mkuffd_wp(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) pmd_t pmd_mkuffd_wp(pmd_t pmd)
{
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) pte_t pte_clear_uffd_wp(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) pmd_t pmd_clear_uffd_wp(pmd_t pmd)
{
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) pte_t pte_swp_mkuffd_wp(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) int pte_swp_uffd_wp(pte_t pte)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) pte_t pte_swp_clear_uffd_wp(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_swp_mkuffd_wp(pmd_t pmd)
{
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_swp_uffd_wp(pmd_t pmd)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_swp_clear_uffd_wp(pmd_t pmd)
{
 return pmd;
}
# 15 "./include/linux/pgtable.h" 2







/*
 * On almost all architectures and configurations, 0 can be used as the
 * upper ceiling to free_pgtables(): on many architectures it has the same
 * effect as using TASK_SIZE.  However, there is one configuration which
 * must impose a more careful limit, to avoid freeing kernel pgtables.
 */




/*
 * This defines the first usable user address. Platforms
 * can override its value with custom FIRST_USER_ADDRESS
 * defined in their respective <asm/pgtable.h>.
 */




/*
 * This defines the generic helper for accessing PMD page
 * table page. Although platforms can still override this
 * via their respective <asm/pgtable.h>.
 */




/*
 * A page table page can be thought of an array like this: pXd_t[PTRS_PER_PxD]
 *
 * The pXx_index() functions return the index of the entry in the page
 * table page which would control the given virtual address
 *
 * As these functions may be used by the same code for different levels of
 * the page table folding, they are always available, regardless of
 * CONFIG_PGTABLE_LEVELS value. For the folded levels they simply return 0
 * because in such cases PTRS_PER_PxD equals 1.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pte_index(unsigned long address)
{
 return (address >> 12) & ((1 << (12 - 3)) - 1);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pmd_index(unsigned long address)
{
 return (address >> ((12 - 3) * (4 - (2)) + 3)) & ((1 << (12 - 3)) - 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pud_index(unsigned long address)
{
 return (address >> ((12 - 3) * (4 - (1)) + 3)) & ((1 << (12 - 3)) - 1);
}




/* Must be a compile-time constant, so implement it as a macro */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
 return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
}
# 107 "./include/linux/pgtable.h"
/* Find an entry in the second-level page table.. */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
 return pud_pgtable(*pud) + pmd_index(address);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pud_t *pud_offset(p4d_t *p4d, unsigned long address)
{
 return p4d_pgtable(*p4d) + pud_index(address);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pgd_t *pgd_offset_pgd(pgd_t *pgd, unsigned long address)
{
 return (pgd + (((address) >> ((12 - 3) * (4 - (4 - 4)) + 3)) & ((1 << ((48) - ((12 - 3) * (4 - (4 - 4)) + 3))) - 1)));
};

/*
 * a shortcut to get a pgd_t in a given mm
 */




/*
 * a shortcut which implies the use of the kernel's pgd, instead
 * of a process's
 */




/*
 * In many cases it is known that a virtual address is mapped at PMD or PTE
 * level, so instead of traversing all the page table levels, we can get a
 * pointer to the PMD entry in user or kernel page table or translate a virtual
 * address to the pointer in the PTE in the kernel page tables with simple
 * helpers.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t *pmd_off(struct mm_struct *mm, unsigned long va)
{
 return pmd_offset(pud_offset(p4d_offset(pgd_offset_pgd((mm)->pgd, (va)), va), va), va);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t *pmd_off_k(unsigned long va)
{
 return pmd_offset(pud_offset(p4d_offset(pgd_offset_pgd((&init_mm)->pgd, ((va))), va), va), va);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t *virt_to_kpte(unsigned long vaddr)
{
 pmd_t *pmd = pmd_off_k(vaddr);

 return (!((*pmd).pmd)) ? ((void *)0) : pte_offset_kernel(pmd, vaddr);
}
# 254 "./include/linux/pgtable.h"
extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
      unsigned long address, pmd_t *pmdp);
# 271 "./include/linux/pgtable.h"
/*
 * Return whether the accessed bit in non-leaf PMD entries is supported on the
 * local CPU.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_has_hw_nonleaf_pmd_young(void)
{
 return 0;
}
# 306 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptep_clear(struct mm_struct *mm, unsigned long addr,
         pte_t *ptep)
{
 ptep_get_and_clear(mm, addr, ptep);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t ptep_get(pte_t *ptep)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_346(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*ptep) == sizeof(char) || sizeof(*ptep) == sizeof(short) || sizeof(*ptep) == sizeof(int) || sizeof(*ptep) == sizeof(long)) || sizeof(*ptep) == sizeof(long long))) __compiletime_assert_346(); } while (0); (*(const volatile typeof( _Generic((*ptep), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*ptep))) *)&(*ptep)); });
# 316 "./include/linux/pgtable.h"
}
# 365 "./include/linux/pgtable.h"
/*
 * We require that the PTE can be read atomically.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t ptep_get_lockless(pte_t *ptep)
{
 return ptep_get(ptep);
}
# 389 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
         unsigned long address,
         pud_t *pudp)
{
 pud_t pud = *pudp;

 pud_clear(pudp);
 page_table_check_pud_clear(mm, address, pud);

 return pud;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmdp_huge_get_and_clear_full(struct vm_area_struct *vma,
         unsigned long address, pmd_t *pmdp,
         int full)
{
 return pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pud_t pudp_huge_get_and_clear_full(struct mm_struct *mm,
         unsigned long address, pud_t *pudp,
         int full)
{
 return pudp_huge_get_and_clear(mm, address, pudp);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t ptep_get_and_clear_full(struct mm_struct *mm,
         unsigned long address, pte_t *ptep,
         int full)
{
 return ptep_get_and_clear(mm, address, ptep);
}



/*
 * If two threads concurrently fault at the same page, the thread that
 * won the race updates the PTE and its local TLB/Cache. The other thread
 * gives up, simply does nothing, and continues; on architectures where
 * software can update TLB,  local TLB can be updated here to avoid next page
 * fault. This function updates TLB only, do nothing with cache or others.
 * It is the difference with function update_mmu_cache.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void update_mmu_tlb(struct vm_area_struct *vma,
    unsigned long address, pte_t *ptep)
{
}



/*
 * Some architectures may be able to avoid expensive synchronization
 * primitives when modifications are made to PTE's which are already
 * not present, or in the process of an address space destruction.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pte_clear_not_present_full(struct mm_struct *mm,
           unsigned long address,
           pte_t *ptep,
           int full)
{
 set_pte(ptep, ((pte_t) { (0) } ));
}



extern pte_t ptep_clear_flush(struct vm_area_struct *vma,
         unsigned long address,
         pte_t *ptep);



extern pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma,
         unsigned long address,
         pmd_t *pmdp);
extern pud_t pudp_huge_clear_flush(struct vm_area_struct *vma,
         unsigned long address,
         pud_t *pudp);
# 488 "./include/linux/pgtable.h"
/*
 * On some architectures hardware does not set page access bit when accessing
 * memory page, it is responsibility of software setting this bit. It brings
 * out extra page fault penalty to track page access bit. For optimization page
 * access bit can be set during all page fault flow on these arches.
 * To be differentiate with macro pte_mkyoung, this macro is used on platforms
 * where software maintains page access bit.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_sw_mkyoung(pte_t pte)
{
 return pte;
}
# 530 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pudp_set_wrprotect(struct mm_struct *mm,
          unsigned long address, pud_t *pudp)
{
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_347(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_347(); } while (0);
# 534 "./include/linux/pgtable.h"
}





extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,
     unsigned long address, pmd_t *pmdp);
# 555 "./include/linux/pgtable.h"
extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
           pgtable_t pgtable);



extern pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp);



/*
 * This is an implementation of pmdp_establish() that is only suitable for an
 * architecture that doesn't have hardware dirty/accessed bits. In this case we
 * can't race with CPU which sets these bits and non-atomic approach is fine.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t generic_pmdp_establish(struct vm_area_struct *vma,
  unsigned long address, pmd_t *pmdp, pmd_t pmd)
{
 pmd_t old_pmd = *pmdp;
 set_pmd_at(vma->vm_mm, address, pmdp, pmd);
 return old_pmd;
}



extern pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,
       pmd_t *pmdp);




/*
 * pmdp_invalidate_ad() invalidates the PMD while changing a transparent
 * hugepage mapping in the page tables. This function is similar to
 * pmdp_invalidate(), but should only be used if the access and dirty bits would
 * not be cleared by the software in the new PMD value. The function ensures
 * that hardware changes of the access and dirty bits updates would not be lost.
 *
 * Doing so can allow in certain architectures to avoid a TLB flush in most
 * cases. Yet, another TLB flush might be necessary later if the PMD update
 * itself requires such flush (e.g., if protection was set to be stricter). Yet,
 * even when a TLB flush is needed because of the update, the caller may be able
 * to batch these TLB flushing operations, so fewer TLB flush operations are
 * needed.
 */
extern pmd_t pmdp_invalidate_ad(struct vm_area_struct *vma,
    unsigned long address, pmd_t *pmdp);



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pte_same(pte_t pte_a, pte_t pte_b)
{
 return ((pte_a).pte) == ((pte_b).pte);
}



/*
 * Some architectures provide facilities to virtualization guests
 * so that they can flag allocated pages as unused. This allows the
 * host to transparently reclaim unused pages. This function returns
 * whether the pte's page is unused.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pte_unused(pte_t pte)
{
 return 0;
}
# 649 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_same(pmd_t pmd_a, pmd_t pmd_b)
{
 return ((pmd_a).pmd) == ((pmd_b).pmd);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pud_same(pud_t pud_a, pud_t pud_b)
{
 return ((pud_a).pud) == ((pud_b).pud);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int p4d_same(p4d_t p4d_a, p4d_t p4d_b)
{
 return ((((p4d_a).pgd).pgd)) == ((((p4d_b).pgd).pgd));
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pgd_same(pgd_t pgd_a, pgd_t pgd_b)
{
 return ((pgd_a).pgd) == ((pgd_b).pgd);
}


/*
 * Use set_p*_safe(), and elide TLB flushing, when confident that *no*
 * TLB flush will be required as a result of the "set". For example, use
 * in scenarios where it is known ahead of time that the routine is
 * setting non-present entries, or re-setting an existing entry to the
 * same value. Otherwise, use the typical "set" helpers and flush the
 * TLB.
 */
# 713 "./include/linux/pgtable.h"
/*
 * Some architectures support metadata associated with a page. When a
 * page is being swapped out, this metadata must be saved so it can be
 * restored when the page is swapped back in. SPARC M7 and newer
 * processors support an ADI (Application Data Integrity) tag for the
 * page as metadata for the page. arch_do_swap_page() can restore this
 * metadata when a page is swapped back in.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_do_swap_page(struct mm_struct *mm,
         struct vm_area_struct *vma,
         unsigned long addr,
         pte_t pte, pte_t oldpte)
{

}



/*
 * Some architectures support metadata associated with a page. When a
 * page is being swapped out, this metadata must be saved so it can be
 * restored when the page is swapped back in. SPARC M7 and newer
 * processors support an ADI (Application Data Integrity) tag for the
 * page as metadata for the page. arch_unmap_one() can save this
 * metadata on a swap-out of a page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_unmap_one(struct mm_struct *mm,
      struct vm_area_struct *vma,
      unsigned long addr,
      pte_t orig_pte)
{
 return 0;
}


/*
 * Allow architectures to preserve additional metadata associated with
 * swapped-out pages. The corresponding __HAVE_ARCH_SWAP_* macros and function
 * prototypes must be defined in the arch-specific asm/pgtable.h file.
 */
# 792 "./include/linux/pgtable.h"
/*
 * When walking page tables, get the address of the next boundary,
 * or the end address of the range if that comes earlier.  Although no
 * vma end wraps to 0, rounded up __boundary may wrap to 0 throughout.
 */
# 824 "./include/linux/pgtable.h"
/*
 * When walking page tables, we usually want to skip any p?d_none entries;
 * and any p?d_bad entries - reporting the error before resetting to none.
 * Do the tests inline, but report and clear the bad entry in mm/memory.c.
 */
void pgd_clear_bad(pgd_t *);
# 838 "./include/linux/pgtable.h"
void pud_clear_bad(pud_t *);




void pmd_clear_bad(pmd_t *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pgd_none_or_clear_bad(pgd_t *pgd)
{
 if (pgd_none(*pgd))
  return 1;
 if (__builtin_expect(!!(pgd_bad(*pgd)), 0)) {
  pgd_clear_bad(pgd);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int p4d_none_or_clear_bad(p4d_t *p4d)
{
 if ((!((((*p4d).pgd).pgd))))
  return 1;
 if (__builtin_expect(!!((!(((((*p4d).pgd).pgd)) & 2))), 0)) {
  do { } while (0);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pud_none_or_clear_bad(pud_t *pud)
{
 if ((!((*pud).pud)))
  return 1;
 if (__builtin_expect(!!((!((((*pud).pud) & (((pudval_t)(3)) << 0)) == (((pudval_t)(3)) << 0)))), 0)) {
  pud_clear_bad(pud);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_none_or_clear_bad(pmd_t *pmd)
{
 if ((!((*pmd).pmd)))
  return 1;
 if (__builtin_expect(!!((!((((*pmd).pmd) & (((pmdval_t)(3)) << 0)) == (((pmdval_t)(3)) << 0)))), 0)) {
  pmd_clear_bad(pmd);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t __ptep_modify_prot_start(struct vm_area_struct *vma,
          unsigned long addr,
          pte_t *ptep)
{
 /*
	 * Get the current pte state, but zero it out to make it
	 * non-present, preventing the hardware from asynchronously
	 * updating it.
	 */
 return ptep_get_and_clear(vma->vm_mm, addr, ptep);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __ptep_modify_prot_commit(struct vm_area_struct *vma,
          unsigned long addr,
          pte_t *ptep, pte_t pte)
{
 /*
	 * The pte is non-present, so there's no hardware state to
	 * preserve.
	 */
 set_pte_at(vma->vm_mm, addr, ptep, pte);
}
# 947 "./include/linux/pgtable.h"
/*
 * No-op macros that just return the current protection value. Defined here
 * because these macros can be used even if CONFIG_MMU is not defined.
 */
# 979 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)
{
 if (((oldprot).pgprot) == ((((pgprot_t) { ((((oldprot).pgprot) & ~((((pteval_t)(7)) << 2))) | ((((pteval_t)((3))) << 2) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */)) } )).pgprot))
  newprot = ((pgprot_t) { ((((newprot).pgprot) & ~((((pteval_t)(7)) << 2))) | ((((pteval_t)((3))) << 2) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */)) } );
 if (((oldprot).pgprot) == ((((pgprot_t) { ((((oldprot).pgprot) & ~((((pteval_t)(7)) << 2))) | ((((pteval_t)((2))) << 2) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */)) } )).pgprot))
  newprot = ((pgprot_t) { ((((newprot).pgprot) & ~((((pteval_t)(7)) << 2))) | ((((pteval_t)((2))) << 2) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */)) } );
 if (((oldprot).pgprot) == ((((pgprot_t) { ((((oldprot).pgprot) & ~((((pteval_t)(7)) << 2))) | ((((pteval_t)((4))) << 2) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */)) } )).pgprot))
  newprot = ((pgprot_t) { ((((newprot).pgprot) & ~((((pteval_t)(7)) << 2))) | ((((pteval_t)((4))) << 2) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */)) } );
 return newprot;
}
# 1000 "./include/linux/pgtable.h"
/*
 * A facility to provide lazy MMU batching.  This allows PTE updates and
 * page invalidations to be delayed until a call to leave lazy MMU mode
 * is issued.  Some architectures may benefit from doing this, and it is
 * beneficial for both shadow and direct mode hypervisors, which may batch
 * the PTE updates which happen during this window.  Note that using this
 * interface requires that read hazards be removed from the code.  A read
 * hazard could result in the direct mode hypervisor case, since the actual
 * write to the page tables may not yet have taken place, so reads though
 * a raw PTE pointer after it has been modified are not guaranteed to be
 * up to date.  This mode can only be entered and left under the protection of
 * the page table locks for all page tables which may be modified.  In the UP
 * case, this is required so that preemption is disabled, and in the SMP case,
 * it must synchronize the delayed page table writes properly on other CPUs.
 */






/*
 * A facility to provide batching of the reload of page tables and
 * other process state with the actual context switch code for
 * paravirtualized guests.  By convention, only one of the batched
 * update (lazy) modes (CPU, MMU) should be active at any given time,
 * entry should never be nested, and entry and exits should always be
 * paired.  This is for sanity of maintaining and reasoning about the
 * kernel code.  In this case, the exit (end of the context switch) is
 * in architecture-specific code, and so doesn't need a generic
 * definition.
 */




/*
 * When replacing an anonymous page by a real (!non) swap entry, we clear
 * PG_anon_exclusive from the page and instead remember whether the flag was
 * set in the swp pte. During fork(), we have to mark the entry as !exclusive
 * (possibly shared). On swapin, we use that information to restore
 * PG_anon_exclusive, which is very helpful in cases where we might have
 * additional (e.g., FOLL_GET) references on a page and wouldn't be able to
 * detect exclusivity.
 *
 * These functions don't apply to non-swap entries (e.g., migration, hwpoison,
 * ...).
 */
# 1083 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pte_soft_dirty(pte_t pte)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_soft_dirty(pmd_t pmd)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_mksoft_dirty(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_mksoft_dirty(pmd_t pmd)
{
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_clear_soft_dirty(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_clear_soft_dirty(pmd_t pmd)
{
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_swp_mksoft_dirty(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pte_swp_soft_dirty(pte_t pte)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t pte_swp_clear_soft_dirty(pte_t pte)
{
 return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)
{
 return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_swp_soft_dirty(pmd_t pmd)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)
{
 return pmd;
}



/*
 * Interfaces that can be used by architecture code to keep track of
 * memory type of pfn mappings specified by the remap_pfn_range,
 * vmf_insert_pfn.
 */

/*
 * track_pfn_remap is called when a _new_ pfn mapping is being established
 * by remap_pfn_range() for physical range indicated by pfn and size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,
      unsigned long pfn, unsigned long addr,
      unsigned long size)
{
 return 0;
}

/*
 * track_pfn_insert is called when a _new_ single pfn is established
 * by vmf_insert_pfn().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot,
        pfn_t pfn)
{
}

/*
 * track_pfn_copy is called when vma that is covering the pfnmap gets
 * copied through copy_page_range().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int track_pfn_copy(struct vm_area_struct *vma)
{
 return 0;
}

/*
 * untrack_pfn is called while unmapping a pfnmap for a region.
 * untrack can be called for a specific region indicated by pfn and size or
 * can be for the entire vma (in which case pfn, size are zero).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void untrack_pfn(struct vm_area_struct *vma,
          unsigned long pfn, unsigned long size)
{
}

/*
 * untrack_pfn_moved is called while mremapping a pfnmap for a new region.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void untrack_pfn_moved(struct vm_area_struct *vma)
{
}
# 1220 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_zero_pfn(unsigned long pfn)
{
 extern unsigned long zero_pfn;
 return pfn == zero_pfn;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long my_zero_pfn(unsigned long addr)
{
 extern unsigned long zero_pfn;
 return zero_pfn;
}
# 1285 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pud_trans_huge(pud_t pud)
{
 return 0;
}


/* See pmd_none_or_trans_huge_or_clear_bad for discussion. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pud_none_or_trans_huge_or_dev_or_clear_bad(pud_t *pud)
{
 pud_t pudval = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_348(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pud) == sizeof(char) || sizeof(*pud) == sizeof(short) || sizeof(*pud) == sizeof(int) || sizeof(*pud) == sizeof(long)) || sizeof(*pud) == sizeof(long long))) __compiletime_assert_348(); } while (0); (*(const volatile typeof( _Generic((*pud), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*pud))) *)&(*pud)); });
# 1296 "./include/linux/pgtable.h"
 if ((!((pudval).pud)) || pud_trans_huge(pudval) || pud_devmap(pudval))
  return 1;
 if (__builtin_expect(!!((!((((pudval).pud) & (((pudval_t)(3)) << 0)) == (((pudval_t)(3)) << 0)))), 0)) {
  pud_clear_bad(pud);
  return 1;
 }
 return 0;
}

/* See pmd_trans_unstable for discussion. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pud_trans_unstable(pud_t *pud)
{




 return 0;

}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t pmd_read_atomic(pmd_t *pmdp)
{
 /*
	 * Depend on compiler for an atomic pmd read. NOTE: this is
	 * only going to work, if the pmdval_t isn't larger than
	 * an unsigned long.
	 */
 return *pmdp;
}





/*
 * This function is meant to be used by sites walking pagetables with
 * the mmap_lock held in read mode to protect against MADV_DONTNEED and
 * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd
 * into a null pmd and the transhuge page fault can convert a null pmd
 * into an hugepmd or into a regular pmd (if the hugepage allocation
 * fails). While holding the mmap_lock in read mode the pmd becomes
 * stable and stops changing under us only if it's not null and not a
 * transhuge pmd. When those races occurs and this function makes a
 * difference vs the standard pmd_none_or_clear_bad, the result is
 * undefined so behaving like if the pmd was none is safe (because it
 * can return none anyway). The compiler level barrier() is critically
 * important to compute the two checks atomically on the same pmdval.
 *
 * For 32bit kernels with a 64bit large pmd_t this automatically takes
 * care of reading the pmd atomically to avoid SMP race conditions
 * against pmd_populate() when the mmap_lock is hold for reading by the
 * caller (a special atomic read not done by "gcc" as in the generic
 * version above, is also needed when THP is disabled because the page
 * fault can populate the pmd from under us).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)
{
 pmd_t pmdval = pmd_read_atomic(pmd);
 /*
	 * The barrier will stabilize the pmdval in a register or on
	 * the stack so that it will stop changing under the code.
	 *
	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,
	 * pmd_read_atomic is allowed to return a not atomic pmdval
	 * (for example pointing to an hugepage that has never been
	 * mapped in the pmd). The below checks will only care about
	 * the low part of the pmd with 32bit PAE x86 anyway, with the
	 * exception of pmd_none(). So the important thing is that if
	 * the low part of the pmd is found null, the high part will
	 * be also null or the pmd_none() check below would be
	 * confused.
	 */

 __asm__ __volatile__("": : :"memory");

 /*
	 * !pmd_present() checks for pmd migration entries
	 *
	 * The complete check uses is_pmd_migration_entry() in linux/swapops.h
	 * But using that requires moving current function and pmd_trans_unstable()
	 * to linux/swapops.h to resolve dependency, which is too much code move.
	 *
	 * !pmd_present() is equivalent to is_pmd_migration_entry() currently,
	 * because !pmd_present() pages can only be under migration not swapped
	 * out.
	 *
	 * pmd_none() is preserved for future condition checks on pmd migration
	 * entries and not confusing with this function name, although it is
	 * redundant with !pmd_present().
	 */
 if ((!((pmdval).pmd)) || pmd_trans_huge(pmdval) ||
  (1 && !pmd_present(pmdval)))
  return 1;
 if (__builtin_expect(!!((!((((pmdval).pmd) & (((pmdval_t)(3)) << 0)) == (((pmdval_t)(3)) << 0)))), 0)) {
  pmd_clear_bad(pmd);
  return 1;
 }
 return 0;
}

/*
 * This is a noop if Transparent Hugepage Support is not built into
 * the kernel. Otherwise it is equivalent to
 * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in
 * places that already verified the pmd is not none and they want to
 * walk ptes while holding the mmap sem in read mode (write mode don't
 * need this). If THP is not enabled, the pmd can't go away under the
 * code even if MADV_DONTNEED runs, but if THP is enabled we need to
 * run a pmd_trans_unstable before walking the ptes after
 * split_huge_pmd returns (because it may have run when the pmd become
 * null, but then a page fault can map in a THP and not a regular page).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_trans_unstable(pmd_t *pmd)
{

 return pmd_none_or_trans_huge_or_clear_bad(pmd);



}

/*
 * the ordering of these checks is important for pmds with _page_devmap set.
 * if we check pmd_trans_unstable() first we will trip the bad_pmd() check
 * inside of pmd_none_or_trans_huge_or_clear_bad(). this will end up correctly
 * returning 1 but not before it spams dmesg with the pmd_clear_bad() output.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pmd_devmap_trans_unstable(pmd_t *pmd)
{
 return (!!(((pmd_pte(*pmd)).pte) & (((pteval_t)(1)) << 57))) || pmd_trans_unstable(pmd);
}
# 1457 "./include/linux/pgtable.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void p4d_clear_huge(p4d_t *p4d) { }


int pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot);
int pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot);
int pud_clear_huge(pud_t *pud);
int pmd_clear_huge(pmd_t *pmd);
int p4d_free_pud_page(p4d_t *p4d, unsigned long addr);
int pud_free_pmd_page(pud_t *pud, unsigned long addr);
int pmd_free_pte_page(pmd_t *pmd, unsigned long addr);
# 1526 "./include/linux/pgtable.h"
struct file;
int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,
   unsigned long size, pgprot_t *vma_prot);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_espfix_bsp(void) { }


extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pgtable_cache_init(void);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
{
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_has_pfn_modify_check(void)
{
 return false;
}


/*
 * Architecture PAGE_KERNEL_* fallbacks
 *
 * Some architectures don't define certain PAGE_KERNEL_* flags. This is either
 * because they really don't support them, or the port needs to be updated to
 * reflect the required functionality. Below are a set of relatively safe
 * fallbacks, as best effort, which we can count on in lieu of the architectures
 * not defining them on their own yet.
 */
# 1566 "./include/linux/pgtable.h"
/*
 * Page Table Modification bits for pgtbl_mod_mask.
 *
 * These are used by the p?d_alloc_track*() set of functions an in the generic
 * vmalloc/ioremap code to track at which page-table levels entries have been
 * modified. Based on that the code can better decide when vmalloc and ioremap
 * mapping changes need to be synchronized to other page-tables in the system.
 */
# 1586 "./include/linux/pgtable.h"
/* Page-Table Modification Mask */
typedef unsigned int pgtbl_mod_mask;
# 1608 "./include/linux/pgtable.h"
/*
 * On some architectures it depends on the mm if the p4d/pud or pmd
 * layer of the page table hierarchy is folded or not.
 */
# 1634 "./include/linux/pgtable.h"
/*
 * p?d_leaf() - true if this entry is a final mapping to a physical address.
 * This differs from p?d_huge() by the fact that they are always available (if
 * the architecture supports large pages at the appropriate level) even
 * if CONFIG_HUGETLB_PAGE is not defined.
 * Only meaningful when called on a valid entry.
 */
# 1670 "./include/linux/pgtable.h"
/*
 * Some architectures have MMUs that are configurable or selectable at boot
 * time. These lead to variable PTRS_PER_x. For statically allocated arrays it
 * helps to have a static maximum value.
 */
# 1692 "./include/linux/pgtable.h"
/* description of effects of mapping type and prot in current implementation.
 * this is due to the limited x86 page protection hardware.  The expected
 * behavior is in parens:
 *
 * map_type	prot
 *		PROT_NONE	PROT_READ	PROT_WRITE	PROT_EXEC
 * MAP_SHARED	r: (no) no	r: (yes) yes	r: (no) yes	r: (no) yes
 *		w: (no) no	w: (no) no	w: (yes) yes	w: (no) no
 *		x: (no) no	x: (no) yes	x: (no) yes	x: (yes) yes
 *
 * MAP_PRIVATE	r: (no) no	r: (yes) yes	r: (no) yes	r: (no) yes
 *		w: (no) no	w: (no) no	w: (copy) copy	w: (no) no
 *		x: (no) no	x: (no) yes	x: (no) yes	x: (yes) yes
 *
 * On arm64, PROT_EXEC has the following behaviour for both MAP_SHARED and
 * MAP_PRIVATE (with Enhanced PAN supported):
 *								r: (no) no
 *								w: (no) no
 *								x: (yes) yes
 */
# 13 "./arch/arm64/include/asm/io.h" 2




# 1 "./arch/arm64/include/generated/asm/early_ioremap.h" 1
# 1 "./include/asm-generic/early_ioremap.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * early_ioremap() and early_iounmap() are for temporary early boot-time
 * mappings, before the real ioremap() is functional.
 */
extern void *early_ioremap(resource_size_t phys_addr,
       unsigned long size);
extern void *early_memremap(resource_size_t phys_addr,
       unsigned long size);
extern void *early_memremap_ro(resource_size_t phys_addr,
          unsigned long size);
extern void *early_memremap_prot(resource_size_t phys_addr,
     unsigned long size, unsigned long prot_val);
extern void early_iounmap(void *addr, unsigned long size);
extern void early_memunmap(void *addr, unsigned long size);


/* Arch-specific initialization */
extern void early_ioremap_init(void);

/* Generic initialization called by architecture code */
extern void early_ioremap_setup(void);

/*
 * Called as last step in paging_init() so library can act
 * accordingly for subsequent map/unmap requests.
 */
extern void early_ioremap_reset(void);

/*
 * Early copy from unmapped memory to kernel mapped memory.
 */
extern void copy_from_early_mem(void *dest, phys_addr_t src,
    unsigned long size);
# 2 "./arch/arm64/include/generated/asm/early_ioremap.h" 2
# 18 "./arch/arm64/include/asm/io.h" 2



/*
 * Generic IO read/write.  These perform native-endian accesses.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_writeb(u8 val, volatile void *addr)
{
 asm volatile("strb %w0, [%1]" : : "rZ" (val), "r" (addr));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_writew(u16 val, volatile void *addr)
{
 asm volatile("strh %w0, [%1]" : : "rZ" (val), "r" (addr));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void __raw_writel(u32 val, volatile void *addr)
{
 asm volatile("str %w0, [%1]" : : "rZ" (val), "r" (addr));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __raw_writeq(u64 val, volatile void *addr)
{
 asm volatile("str %x0, [%1]" : : "rZ" (val), "r" (addr));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 __raw_readb(const volatile void *addr)
{
 u8 val;
 asm volatile(".if ""1"" == 1\n" "661:\n\t" "ldrb %w0, [%1]" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "79" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "ldarb %w0, [%1]" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (val) : "r" (addr));
 return val;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 __raw_readw(const volatile void *addr)
{
 u16 val;

 asm volatile(".if ""1"" == 1\n" "661:\n\t" "ldrh %w0, [%1]" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "79" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "ldarh %w0, [%1]" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (val) : "r" (addr));
 return val;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) u32 __raw_readl(const volatile void *addr)
{
 u32 val;
 asm volatile(".if ""1"" == 1\n" "661:\n\t" "ldr %w0, [%1]" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "79" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "ldar %w0, [%1]" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (val) : "r" (addr));
 return val;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __raw_readq(const volatile void *addr)
{
 u64 val;
 asm volatile(".if ""1"" == 1\n" "661:\n\t" "ldr %0, [%1]" "\n" "662:\n" ".pushsection .altinstructions,\"a\"\n" " .word 661b - .\n" /* label           */ " .word 663f - .\n" /* new instruction */ " .hword " "79" "\n" /* feature bit     */ " .byte 662b-661b\n" /* source len      */ " .byte 664f-663f\n" /* replacement len */ ".popsection\n" ".subsection 1\n" "663:\n\t" "ldar %0, [%1]" "\n" "664:\n\t" ".org	. - (664b-663b) + (662b-661b)\n\t" ".org	. - (662b-661b) + (664b-663b)\n\t" ".previous\n" ".endif\n"


       : "=r" (val) : "r" (addr));
 return val;
}

/* IO barriers */
# 115 "./arch/arm64/include/asm/io.h"
/* arm64-specific, don't use in portable drivers */




/*
 *  I/O port access primitives.
 */




/*
 * String version of I/O memory access operations.
 */
extern void __memcpy_fromio(void *, const volatile void *, size_t);
extern void __memcpy_toio(volatile void *, const void *, size_t);
extern void __memset_io(volatile void *, int, size_t);





/*
 * I/O memory mapping functions.
 */

bool ioremap_allowed(phys_addr_t phys_addr, size_t size, unsigned long prot);
# 152 "./arch/arm64/include/asm/io.h"
/*
 * io{read,write}{16,32,64}be() macros
 */
# 163 "./arch/arm64/include/asm/io.h"
# 1 "./include/asm-generic/io.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/* Generic I/O port emulation.
 *
 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 */
# 19 "./include/asm-generic/io.h"
# 1 "./arch/arm64/include/generated/asm/mmiowb.h" 1
# 20 "./include/asm-generic/io.h" 2
# 1 "./include/asm-generic/pci_iomap.h" 1
/* SPDX-License-Identifier: GPL-2.0+ */
/* Generic I/O port emulation.
 *
 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 */



struct pci_dev;

/* Create a virtual mapping cookie for a PCI BAR (memory or IO) */
extern void *pci_iomap(struct pci_dev *dev, int bar, unsigned long max);
extern void *pci_iomap_wc(struct pci_dev *dev, int bar, unsigned long max);
extern void *pci_iomap_range(struct pci_dev *dev, int bar,
         unsigned long offset,
         unsigned long maxlen);
extern void *pci_iomap_wc_range(struct pci_dev *dev, int bar,
     unsigned long offset,
     unsigned long maxlen);
extern void pci_iounmap(struct pci_dev *dev, void *);
/* Create a virtual mapping cookie for a port on a given PCI device.
 * Do not call this directly, it exists to make it easier for architectures
 * to override */
# 21 "./include/asm-generic/io.h" 2





/* prevent prefetching of coherent DMA data ahead of a dma-complete */
# 35 "./include/asm-generic/io.h"
/* flush writes to coherent DMA data before possibly triggering a DMA read */
# 44 "./include/asm-generic/io.h"
/* serialize device access against a spin_unlock, usually handled there. */
# 65 "./include/asm-generic/io.h"
/*
 * "__DISABLE_TRACE_MMIO__" flag can be used to disable MMIO tracing for
 * specific kernel drivers in case of excessive/unwanted logging.
 *
 * Usage: Add a #define flag at the beginning of the driver file.
 * Ex: #define __DISABLE_TRACE_MMIO__
 *     #include <...>
 *     ...
 */
# 93 "./include/asm-generic/io.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void log_write_mmio(u64 val, u8 width, volatile void *addr,
      unsigned long caller_addr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void log_post_write_mmio(u64 val, u8 width, volatile void *addr,
           unsigned long caller_addr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void log_read_mmio(u8 width, const volatile void *addr,
     unsigned long caller_addr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void log_post_read_mmio(u64 val, u8 width, const volatile void *addr,
          unsigned long caller_addr) {}



/*
 * __raw_{read,write}{b,w,l,q}() access memory in native endianness.
 *
 * On some architectures memory mapped IO needs to be accessed differently.
 * On the simple architectures, we just read/write the memory location
 * directly.
 */
# 180 "./include/asm-generic/io.h"
/*
 * {read,write}{b,w,l,q}() access little endian memory and return result in
 * native endianness.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 readb(const volatile void *addr)
{
 u8 val;

 log_read_mmio(8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
          ;
 val = __raw_readb(addr);
 ({ unsigned long tmp; do { do { } while (0); asm volatile("dmb " "oshld" : : : "memory"); } while (0); /*								\
	 * Create a dummy control dependency from the IO read to any	\
	 * later instructions. This ensures that a subsequent call to	\
	 * udelay() will be ordered due to the ISB in get_cycles().	\
	 */ asm volatile("eor	%0, %1, %1\n" "cbnz	%0, ." : "=r" (tmp) : "r" ((unsigned long)(val)) : "memory"); });
# 195 "./include/asm-generic/io.h"
 log_post_read_mmio(val, 8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 readw(const volatile void *addr)
{
 u16 val;

 log_read_mmio(16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
          ;
 val = (( __u16)(__le16)((__le16 )__raw_readw(addr)));
 ({ unsigned long tmp; do { do { } while (0); asm volatile("dmb " "oshld" : : : "memory"); } while (0); /*								\
	 * Create a dummy control dependency from the IO read to any	\
	 * later instructions. This ensures that a subsequent call to	\
	 * udelay() will be ordered due to the ISB in get_cycles().	\
	 */ asm volatile("eor	%0, %1, %1\n" "cbnz	%0, ." : "=r" (tmp) : "r" ((unsigned long)(val)) : "memory"); });
# 210 "./include/asm-generic/io.h"
 log_post_read_mmio(val, 16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 readl(const volatile void *addr)
{
 u32 val;

 log_read_mmio(32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
          ;
 val = (( __u32)(__le32)((__le32 )__raw_readl(addr)));
 ({ unsigned long tmp; do { do { } while (0); asm volatile("dmb " "oshld" : : : "memory"); } while (0); /*								\
	 * Create a dummy control dependency from the IO read to any	\
	 * later instructions. This ensures that a subsequent call to	\
	 * udelay() will be ordered due to the ISB in get_cycles().	\
	 */ asm volatile("eor	%0, %1, %1\n" "cbnz	%0, ." : "=r" (tmp) : "r" ((unsigned long)(val)) : "memory"); });
# 225 "./include/asm-generic/io.h"
 log_post_read_mmio(val, 32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 readq(const volatile void *addr)
{
 u64 val;

 log_read_mmio(64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
          ;
 val = (( __u64)(__le64)(__raw_readq(addr)));
 ({ unsigned long tmp; do { do { } while (0); asm volatile("dmb " "oshld" : : : "memory"); } while (0); /*								\
	 * Create a dummy control dependency from the IO read to any	\
	 * later instructions. This ensures that a subsequent call to	\
	 * udelay() will be ordered due to the ISB in get_cycles().	\
	 */ asm volatile("eor	%0, %1, %1\n" "cbnz	%0, ." : "=r" (tmp) : "r" ((unsigned long)(val)) : "memory"); });
# 241 "./include/asm-generic/io.h"
 log_post_read_mmio(val, 64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writeb(u8 value, volatile void *addr)
{
 log_write_mmio(value, 8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 do { do { } while (0); asm volatile("dmb " "oshst" : : : "memory"); } while (0);
 __raw_writeb(value, addr);
          ;
 log_post_write_mmio(value, 8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writew(u16 value, volatile void *addr)
{
 log_write_mmio(value, 16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 do { do { } while (0); asm volatile("dmb " "oshst" : : : "memory"); } while (0);
 __raw_writew((u16 )(( __le16)(__u16)(value)), addr);
          ;
 log_post_write_mmio(value, 16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writel(u32 value, volatile void *addr)
{
 log_write_mmio(value, 32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 do { do { } while (0); asm volatile("dmb " "oshst" : : : "memory"); } while (0);
 __raw_writel((u32 )(( __le32)(__u32)(value)), addr);
          ;
 log_post_write_mmio(value, 32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writeq(u64 value, volatile void *addr)
{
 log_write_mmio(value, 64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 do { do { } while (0); asm volatile("dmb " "oshst" : : : "memory"); } while (0);
 __raw_writeq((( __le64)(__u64)(value)), addr);
          ;
 log_post_write_mmio(value, 64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}



/*
 * {read,write}{b,w,l,q}_relaxed() are like the regular version, but
 * are not guaranteed to provide ordering against spinlocks or memory
 * accesses.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 readb_relaxed(const volatile void *addr)
{
 u8 val;

 log_read_mmio(8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 val = __raw_readb(addr);
 log_post_read_mmio(val, 8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 readw_relaxed(const volatile void *addr)
{
 u16 val;

 log_read_mmio(16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 val = (( __u16)(__le16)(__raw_readw(addr)));
 log_post_read_mmio(val, 16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 readl_relaxed(const volatile void *addr)
{
 u32 val;

 log_read_mmio(32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 val = (( __u32)(__le32)(__raw_readl(addr)));
 log_post_read_mmio(val, 32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 readq_relaxed(const volatile void *addr)
{
 u64 val;

 log_read_mmio(64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 val = (( __u64)(__le64)(__raw_readq(addr)));
 log_post_read_mmio(val, 64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writeb_relaxed(u8 value, volatile void *addr)
{
 log_write_mmio(value, 8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 __raw_writeb(value, addr);
 log_post_write_mmio(value, 8, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writew_relaxed(u16 value, volatile void *addr)
{
 log_write_mmio(value, 16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 __raw_writew((( __le16)(__u16)(value)), addr);
 log_post_write_mmio(value, 16, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writel_relaxed(u32 value, volatile void *addr)
{
 log_write_mmio(value, 32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 __raw_writel((( __le32)(__u32)(value)), addr);
 log_post_write_mmio(value, 32, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writeq_relaxed(u64 value, volatile void *addr)
{
 log_write_mmio(value, 64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
 __raw_writeq((( __le64)(__u64)(value)), addr);
 log_post_write_mmio(value, 64, addr, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}


/*
 * {read,write}s{b,w,l,q}() repeatedly access the same memory address in
 * native endianness in 8-, 16-, 32- or 64-bit chunks (@count times).
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void readsb(const volatile void *addr, void *buffer,
     unsigned int count)
{
 if (count) {
  u8 *buf = buffer;

  do {
   u8 x = __raw_readb(addr);
   *buf++ = x;
  } while (--count);
 }
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void readsw(const volatile void *addr, void *buffer,
     unsigned int count)
{
 if (count) {
  u16 *buf = buffer;

  do {
   u16 x = __raw_readw(addr);
   *buf++ = x;
  } while (--count);
 }
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void readsl(const volatile void *addr, void *buffer,
     unsigned int count)
{
 if (count) {
  u32 *buf = buffer;

  do {
   u32 x = __raw_readl(addr);
   *buf++ = x;
  } while (--count);
 }
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void readsq(const volatile void *addr, void *buffer,
     unsigned int count)
{
 if (count) {
  u64 *buf = buffer;

  do {
   u64 x = __raw_readq(addr);
   *buf++ = x;
  } while (--count);
 }
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writesb(volatile void *addr, const void *buffer,
      unsigned int count)
{
 if (count) {
  const u8 *buf = buffer;

  do {
   __raw_writeb(*buf++, addr);
  } while (--count);
 }
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writesw(volatile void *addr, const void *buffer,
      unsigned int count)
{
 if (count) {
  const u16 *buf = buffer;

  do {
   __raw_writew(*buf++, addr);
  } while (--count);
 }
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writesl(volatile void *addr, const void *buffer,
      unsigned int count)
{
 if (count) {
  const u32 *buf = buffer;

  do {
   __raw_writel(*buf++, addr);
  } while (--count);
 }
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void writesq(volatile void *addr, const void *buffer,
      unsigned int count)
{
 if (count) {
  const u64 *buf = buffer;

  do {
   __raw_writeq(*buf++, addr);
  } while (--count);
 }
}
# 534 "./include/asm-generic/io.h"
/*
 * {in,out}{b,w,l}() access little endian I/O. {in,out}{b,w,l}_p() can be
 * implemented on hardware that needs an additional delay for I/O accesses to
 * take effect.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 _inb(unsigned long addr)
{
 u8 val;

           ;
 val = __raw_readb(((void *)(((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x00800000) - 0x01000000)) + addr);
 ({ unsigned long tmp; do { do { } while (0); asm volatile("dmb " "oshld" : : : "memory"); } while (0); /*								\
	 * Create a dummy control dependency from the IO read to any	\
	 * later instructions. This ensures that a subsequent call to	\
	 * udelay() will be ordered due to the ISB in get_cycles().	\
	 */ asm volatile("eor	%0, %1, %1\n" "cbnz	%0, ." : "=r" (tmp) : "r" ((unsigned long)(val)) : "memory"); });
# 549 "./include/asm-generic/io.h"
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 _inw(unsigned long addr)
{
 u16 val;

           ;
 val = (( __u16)(__le16)((__le16 )__raw_readw(((void *)(((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x00800000) - 0x01000000)) + addr)));
 ({ unsigned long tmp; do { do { } while (0); asm volatile("dmb " "oshld" : : : "memory"); } while (0); /*								\
	 * Create a dummy control dependency from the IO read to any	\
	 * later instructions. This ensures that a subsequent call to	\
	 * udelay() will be ordered due to the ISB in get_cycles().	\
	 */ asm volatile("eor	%0, %1, %1\n" "cbnz	%0, ." : "=r" (tmp) : "r" ((unsigned long)(val)) : "memory"); });
# 562 "./include/asm-generic/io.h"
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 _inl(unsigned long addr)
{
 u32 val;

           ;
 val = (( __u32)(__le32)((__le32 )__raw_readl(((void *)(((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x00800000) - 0x01000000)) + addr)));
 ({ unsigned long tmp; do { do { } while (0); asm volatile("dmb " "oshld" : : : "memory"); } while (0); /*								\
	 * Create a dummy control dependency from the IO read to any	\
	 * later instructions. This ensures that a subsequent call to	\
	 * udelay() will be ordered due to the ISB in get_cycles().	\
	 */ asm volatile("eor	%0, %1, %1\n" "cbnz	%0, ." : "=r" (tmp) : "r" ((unsigned long)(val)) : "memory"); });
# 575 "./include/asm-generic/io.h"
 return val;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void _outb(u8 value, unsigned long addr)
{
 do { do { } while (0); asm volatile("dmb " "oshst" : : : "memory"); } while (0);
 __raw_writeb(value, ((void *)(((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x00800000) - 0x01000000)) + addr);
           ;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void _outw(u16 value, unsigned long addr)
{
 do { do { } while (0); asm volatile("dmb " "oshst" : : : "memory"); } while (0);
 __raw_writew((u16 )(( __le16)(__u16)(value)), ((void *)(((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x00800000) - 0x01000000)) + addr);
           ;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void _outl(u32 value, unsigned long addr)
{
 do { do { } while (0); asm volatile("dmb " "oshst" : : : "memory"); } while (0);
 __raw_writel((u32 )(( __le32)(__u32)(value)), ((void *)(((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x00800000) - 0x01000000)) + addr);
           ;
}


# 1 "./include/linux/logic_pio.h" 1
// SPDX-License-Identifier: GPL-2.0+
/*
 * Copyright (C) 2017 HiSilicon Limited, All Rights Reserved.
 * Author: Gabriele Paoloni <gabriele.paoloni@huawei.com>
 * Author: Zhichang Yuan <yuanzhichang@hisilicon.com>
 */




# 1 "./include/linux/fwnode.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * fwnode.h - Firmware device node object handle type definition.
 *
 * Copyright (C) 2015, Intel Corporation
 * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
 */
# 17 "./include/linux/fwnode.h"
struct fwnode_operations;
struct device;

/*
 * fwnode link flags
 *
 * LINKS_ADDED:	The fwnode has already be parsed to add fwnode links.
 * NOT_DEVICE:	The fwnode will never be populated as a struct device.
 * INITIALIZED: The hardware corresponding to fwnode has been initialized.
 * NEEDS_CHILD_BOUND_ON_ADD: For this fwnode/device to probe successfully, its
 *			     driver needs its child devices to be bound with
 *			     their respective drivers as soon as they are
 *			     added.
 * BEST_EFFORT: The fwnode/device needs to probe early and might be missing some
 *		suppliers. Only enforce ordering with suppliers that have
 *		drivers.
 */






struct fwnode_handle {
 struct fwnode_handle *secondary;
 const struct fwnode_operations *ops;
 struct device *dev;
 struct list_head suppliers;
 struct list_head consumers;
 u8 flags;
};

struct fwnode_link {
 struct fwnode_handle *supplier;
 struct list_head s_hook;
 struct fwnode_handle *consumer;
 struct list_head c_hook;
};

/**
 * struct fwnode_endpoint - Fwnode graph endpoint
 * @port: Port number
 * @id: Endpoint id
 * @local_fwnode: reference to the related fwnode
 */
struct fwnode_endpoint {
 unsigned int port;
 unsigned int id;
 const struct fwnode_handle *local_fwnode;
};

/*
 * ports and endpoints defined as software_nodes should all follow a common
 * naming scheme; use these macros to ensure commonality.
 */





/**
 * struct fwnode_reference_args - Fwnode reference with additional arguments
 * @fwnode:- A reference to the base fwnode
 * @nargs: Number of elements in @args array
 * @args: Integer arguments on the fwnode
 */
struct fwnode_reference_args {
 struct fwnode_handle *fwnode;
 unsigned int nargs;
 u64 args[8];
};

/**
 * struct fwnode_operations - Operations for fwnode interface
 * @get: Get a reference to an fwnode.
 * @put: Put a reference to an fwnode.
 * @device_is_available: Return true if the device is available.
 * @device_get_match_data: Return the device driver match data.
 * @property_present: Return true if a property is present.
 * @property_read_int_array: Read an array of integer properties. Return zero on
 *			     success, a negative error code otherwise.
 * @property_read_string_array: Read an array of string properties. Return zero
 *				on success, a negative error code otherwise.
 * @get_name: Return the name of an fwnode.
 * @get_name_prefix: Get a prefix for a node (for printing purposes).
 * @get_parent: Return the parent of an fwnode.
 * @get_next_child_node: Return the next child node in an iteration.
 * @get_named_child_node: Return a child node with a given name.
 * @get_reference_args: Return a reference pointed to by a property, with args
 * @graph_get_next_endpoint: Return an endpoint node in an iteration.
 * @graph_get_remote_endpoint: Return the remote endpoint node of a local
 *			       endpoint node.
 * @graph_get_port_parent: Return the parent node of a port node.
 * @graph_parse_endpoint: Parse endpoint for port and endpoint id.
 * @add_links:	Create fwnode links to all the suppliers of the fwnode. Return
 *		zero on success, a negative error code otherwise.
 */
struct fwnode_operations {
 struct fwnode_handle *(*get)(struct fwnode_handle *fwnode);
 void (*put)(struct fwnode_handle *fwnode);
 bool (*device_is_available)(const struct fwnode_handle *fwnode);
 const void *(*device_get_match_data)(const struct fwnode_handle *fwnode,
          const struct device *dev);
 bool (*device_dma_supported)(const struct fwnode_handle *fwnode);
 enum dev_dma_attr
 (*device_get_dma_attr)(const struct fwnode_handle *fwnode);
 bool (*property_present)(const struct fwnode_handle *fwnode,
     const char *propname);
 int (*property_read_int_array)(const struct fwnode_handle *fwnode,
           const char *propname,
           unsigned int elem_size, void *val,
           size_t nval);
 int
 (*property_read_string_array)(const struct fwnode_handle *fwnode_handle,
          const char *propname, const char **val,
          size_t nval);
 const char *(*get_name)(const struct fwnode_handle *fwnode);
 const char *(*get_name_prefix)(const struct fwnode_handle *fwnode);
 struct fwnode_handle *(*get_parent)(const struct fwnode_handle *fwnode);
 struct fwnode_handle *
 (*get_next_child_node)(const struct fwnode_handle *fwnode,
          struct fwnode_handle *child);
 struct fwnode_handle *
 (*get_named_child_node)(const struct fwnode_handle *fwnode,
    const char *name);
 int (*get_reference_args)(const struct fwnode_handle *fwnode,
      const char *prop, const char *nargs_prop,
      unsigned int nargs, unsigned int index,
      struct fwnode_reference_args *args);
 struct fwnode_handle *
 (*graph_get_next_endpoint)(const struct fwnode_handle *fwnode,
       struct fwnode_handle *prev);
 struct fwnode_handle *
 (*graph_get_remote_endpoint)(const struct fwnode_handle *fwnode);
 struct fwnode_handle *
 (*graph_get_port_parent)(struct fwnode_handle *fwnode);
 int (*graph_parse_endpoint)(const struct fwnode_handle *fwnode,
        struct fwnode_endpoint *endpoint);
 void *(*iomap)(struct fwnode_handle *fwnode, int index);
 int (*irq_get)(const struct fwnode_handle *fwnode, unsigned int index);
 int (*add_links)(struct fwnode_handle *fwnode);
};
# 181 "./include/linux/fwnode.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void fwnode_init(struct fwnode_handle *fwnode,
          const struct fwnode_operations *ops)
{
 fwnode->ops = ops;
 INIT_LIST_HEAD(&fwnode->consumers);
 INIT_LIST_HEAD(&fwnode->suppliers);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void fwnode_dev_initialized(struct fwnode_handle *fwnode,
       bool initialized)
{
 if (IS_ERR_OR_NULL(fwnode))
  return;

 if (initialized)
  fwnode->flags |= ((((1UL))) << (2));
 else
  fwnode->flags &= ~((((1UL))) << (2));
}

extern u32 fw_devlink_get_flags(void);
extern bool fw_devlink_is_strict(void);
int fwnode_link_add(struct fwnode_handle *con, struct fwnode_handle *sup);
void fwnode_links_purge(struct fwnode_handle *fwnode);
void fw_devlink_purge_absent_suppliers(struct fwnode_handle *fwnode);
# 12 "./include/linux/logic_pio.h" 2

enum {
 LOGIC_PIO_INDIRECT, /* Indirect IO flag */
 LOGIC_PIO_CPU_MMIO, /* Memory-mapped IO flag */
};

struct logic_pio_hwaddr {
 struct list_head list;
 struct fwnode_handle *fwnode;
 resource_size_t hw_start;
 resource_size_t io_start;
 resource_size_t size; /* range size populated */
 unsigned long flags;

 void *hostdata;
 const struct logic_pio_host_ops *ops;
};

struct logic_pio_host_ops {
 u32 (*in)(void *hostdata, unsigned long addr, size_t dwidth);
 void (*out)(void *hostdata, unsigned long addr, u32 val,
      size_t dwidth);
 u32 (*ins)(void *hostdata, unsigned long addr, void *buffer,
     size_t dwidth, unsigned int count);
 void (*outs)(void *hostdata, unsigned long addr, const void *buffer,
       size_t dwidth, unsigned int count);
};


u8 logic_inb(unsigned long addr);
void logic_outb(u8 value, unsigned long addr);
void logic_outw(u16 value, unsigned long addr);
void logic_outl(u32 value, unsigned long addr);
u16 logic_inw(unsigned long addr);
u32 logic_inl(unsigned long addr);
void logic_outb(u8 value, unsigned long addr);
void logic_outw(u16 value, unsigned long addr);
void logic_outl(u32 value, unsigned long addr);
void logic_insb(unsigned long addr, void *buffer, unsigned int count);
void logic_insl(unsigned long addr, void *buffer, unsigned int count);
void logic_insw(unsigned long addr, void *buffer, unsigned int count);
void logic_outsb(unsigned long addr, const void *buffer, unsigned int count);
void logic_outsw(unsigned long addr, const void *buffer, unsigned int count);
void logic_outsl(unsigned long addr, const void *buffer, unsigned int count);
# 105 "./include/linux/logic_pio.h"
/*
 * We reserve 0x4000 bytes for Indirect IO as so far this library is only
 * used by the HiSilicon LPC Host. If needed, we can reserve a wider IO
 * area by redefining the macro below.
 */






struct logic_pio_hwaddr *find_io_range_by_fwnode(struct fwnode_handle *fwnode);
unsigned long logic_pio_trans_hwaddr(struct fwnode_handle *fwnode,
   resource_size_t hw_addr, resource_size_t size);
int logic_pio_register_range(struct logic_pio_hwaddr *newrange);
void logic_pio_unregister_range(struct logic_pio_hwaddr *range);
resource_size_t logic_pio_to_hwaddr(unsigned long pio);
unsigned long logic_pio_trans_cpuaddr(resource_size_t hw_addr);
# 610 "./include/asm-generic/io.h" 2
# 637 "./include/asm-generic/io.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 inb_p(unsigned long addr)
{
 return logic_inb(addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 inw_p(unsigned long addr)
{
 return logic_inw(addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 inl_p(unsigned long addr)
{
 return logic_inl(addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void outb_p(u8 value, unsigned long addr)
{
 logic_outb(value, addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void outw_p(u16 value, unsigned long addr)
{
 logic_outw(value, addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void outl_p(u32 value, unsigned long addr)
{
 logic_outl(value, addr);
}


/*
 * {in,out}s{b,w,l}{,_p}() are variants of the above that repeatedly access a
 * single I/O port multiple times.
 */
# 741 "./include/asm-generic/io.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void insb_p(unsigned long addr, void *buffer, unsigned int count)
{
 logic_insb(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void insw_p(unsigned long addr, void *buffer, unsigned int count)
{
 logic_insw(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void insl_p(unsigned long addr, void *buffer, unsigned int count)
{
 logic_insl(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void outsb_p(unsigned long addr, const void *buffer,
      unsigned int count)
{
 logic_outsb(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void outsw_p(unsigned long addr, const void *buffer,
      unsigned int count)
{
 logic_outsw(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void outsl_p(unsigned long addr, const void *buffer,
      unsigned int count)
{
 logic_outsl(addr, buffer, count);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 ioread8(const volatile void *addr)
{
 return readb(addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 ioread16(const volatile void *addr)
{
 return readw(addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 ioread32(const volatile void *addr)
{
 return readl(addr);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 ioread64(const volatile void *addr)
{
 return readq(addr);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite8(u8 value, volatile void *addr)
{
 writeb(value, addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite16(u16 value, volatile void *addr)
{
 writew(value, addr);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite32(u32 value, volatile void *addr)
{
 writel(value, addr);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite64(u64 value, volatile void *addr)
{
 writeq(value, addr);
}
# 913 "./include/asm-generic/io.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ioread8_rep(const volatile void *addr, void *buffer,
          unsigned int count)
{
 readsb(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ioread16_rep(const volatile void *addr,
    void *buffer, unsigned int count)
{
 readsw(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ioread32_rep(const volatile void *addr,
    void *buffer, unsigned int count)
{
 readsl(addr, buffer, count);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ioread64_rep(const volatile void *addr,
    void *buffer, unsigned int count)
{
 readsq(addr, buffer, count);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite8_rep(volatile void *addr,
    const void *buffer,
    unsigned int count)
{
 writesb(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite16_rep(volatile void *addr,
     const void *buffer,
     unsigned int count)
{
 writesw(addr, buffer, count);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite32_rep(volatile void *addr,
     const void *buffer,
     unsigned int count)
{
 writesl(addr, buffer, count);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iowrite64_rep(volatile void *addr,
     const void *buffer,
     unsigned int count)
{
 writesq(addr, buffer, count);
}






# 1 "./include/linux/vmalloc.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 13 "./include/linux/vmalloc.h"
# 1 "./arch/arm64/include/asm/vmalloc.h" 1
# 10 "./arch/arm64/include/asm/vmalloc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_vmap_pud_supported(pgprot_t prot)
{
 /*
	 * SW table walks can't handle removal of intermediate entries.
	 */
 return pud_sect_supported() &&
        !0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_vmap_pmd_supported(pgprot_t prot)
{
 /* See arch_vmap_pud_supported() */
 return !0;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pgprot_t arch_vmap_pgprot_tagged(pgprot_t prot)
{
 return ((pgprot_t) { ((((prot).pgprot) & ~((((pteval_t)(7)) << 2))) | ((((pteval_t)((1))) << 2))) } );
}
# 14 "./include/linux/vmalloc.h" 2

struct vm_area_struct; /* vma defining user mapping in mm_types.h */
struct notifier_block; /* in notifier.h */

/* bits in flags of vmalloc's vm_struct below */
# 38 "./include/linux/vmalloc.h"
/* bits [20..32] reserved for arch specific ioremap internals */

/*
 * Maximum alignment for ioremap() regions.
 * Can be overridden by arch-specific value.
 */




struct vm_struct {
 struct vm_struct *next;
 void *addr;
 unsigned long size;
 unsigned long flags;
 struct page **pages;

 unsigned int page_order;

 unsigned int nr_pages;
 phys_addr_t phys_addr;
 const void *caller;
};

struct vmap_area {
 unsigned long va_start;
 unsigned long va_end;

 struct rb_node rb_node; /* address sorted rbtree */
 struct list_head list; /* address sorted list */

 /*
	 * The following two variables can be packed, because
	 * a vmap_area object can be either:
	 *    1) in "free" tree (root is free_vmap_area_root)
	 *    2) or "busy" tree (root is vmap_area_root)
	 */
 union {
  unsigned long subtree_max_size; /* in "free" tree */
  struct vm_struct *vm; /* in "busy" tree */
 };
};

/* archs that select HAVE_ARCH_HUGE_VMAP should override one or more of these */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_vmap_p4d_supported(pgprot_t prot)
{
 return false;
}
# 104 "./include/linux/vmalloc.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long arch_vmap_pte_range_map_size(unsigned long addr, unsigned long end,
        u64 pfn, unsigned int max_page_shift)
{
 return ((1UL) << 12);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_vmap_pte_supported_shift(unsigned long size)
{
 return 12;
}
# 125 "./include/linux/vmalloc.h"
/*
 *	Highlevel APIs for driver use
 */
extern void vm_unmap_ram(const void *mem, unsigned int count);
extern void *vm_map_ram(struct page **pages, unsigned int count, int node);
extern void vm_unmap_aliases(void);


extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) vmalloc_init(void);
extern unsigned long vmalloc_nr_pages(void);







extern void *vmalloc(unsigned long size) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *vzalloc(unsigned long size) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *vmalloc_user(unsigned long size) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *vmalloc_node(unsigned long size, int node) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *vzalloc_node(unsigned long size, int node) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *vmalloc_32(unsigned long size) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *vmalloc_32_user(unsigned long size) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *__vmalloc(unsigned long size, gfp_t gfp_mask) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *__vmalloc_node_range(unsigned long size, unsigned long align,
   unsigned long start, unsigned long end, gfp_t gfp_mask,
   pgprot_t prot, unsigned long vm_flags, int node,
   const void *caller) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
void *__vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask,
  int node, const void *caller) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
void *vmalloc_huge(unsigned long size, gfp_t gfp_mask) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));

extern void *__vmalloc_array(size_t n, size_t size, gfp_t flags) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__));
extern void *vmalloc_array(size_t n, size_t size) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__));
extern void *__vcalloc(size_t n, size_t size, gfp_t flags) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__));
extern void *vcalloc(size_t n, size_t size) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__));

extern void vfree(const void *addr);
extern void vfree_atomic(const void *addr);

extern void *vmap(struct page **pages, unsigned int count,
   unsigned long flags, pgprot_t prot);
void *vmap_pfn(unsigned long *pfns, unsigned int count, pgprot_t prot);
extern void vunmap(const void *addr);

extern int remap_vmalloc_range_partial(struct vm_area_struct *vma,
           unsigned long uaddr, void *kaddr,
           unsigned long pgoff, unsigned long size);

extern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
       unsigned long pgoff);

/*
 * Architectures can set this mask to a combination of PGTBL_P?D_MODIFIED values
 * and let generic vmalloc and ioremap code know when arch_sync_kernel_mappings()
 * needs to be called.
 */




/*
 * There is no default implementation for arch_sync_kernel_mappings(). It is
 * relied upon the compiler to optimize calls out if ARCH_PAGE_TABLE_SYNC_MASK
 * is 0.
 */
void arch_sync_kernel_mappings(unsigned long start, unsigned long end);

/*
 *	Lowlevel-APIs (not for driver use!)
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t get_vm_area_size(const struct vm_struct *area)
{
 if (!(area->flags & 0x00000040 /* ***DANGEROUS*** don't add guard page */))
  /* return actual size without guard page */
  return area->size - ((1UL) << 12);
 else
  return area->size;

}

extern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);
extern struct vm_struct *get_vm_area_caller(unsigned long size,
     unsigned long flags, const void *caller);
extern struct vm_struct *__get_vm_area_caller(unsigned long size,
     unsigned long flags,
     unsigned long start, unsigned long end,
     const void *caller);
void free_vm_area(struct vm_struct *area);
extern struct vm_struct *remove_vm_area(const void *addr);
extern struct vm_struct *find_vm_area(const void *addr);
struct vmap_area *find_vmap_area(unsigned long addr);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_vm_area_hugepages(const void *addr)
{
 /*
	 * This may not 100% tell if the area is mapped with > PAGE_SIZE
	 * page table entries, if for some reason the architecture indicates
	 * larger sizes are available but decides not to use them, nothing
	 * prevents that. This only indicates the size of the physical page
	 * allocated in the vmalloc layer.
	 */

 return find_vm_area(addr)->page_order > 0;



}


void vunmap_range(unsigned long addr, unsigned long end);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_vm_flush_reset_perms(void *addr)
{
 struct vm_struct *vm = find_vm_area(addr);

 if (vm)
  vm->flags |= 0x00000100 /* reset direct map and flush TLB on unmap, can't be freed in atomic context */;
}







/* for /proc/kcore */
extern long vread(char *buf, char *addr, unsigned long count);

/*
 *	Internals.  Don't use..
 */
extern struct list_head vmap_area_list;
extern __attribute__((__section__(".init.text"))) __attribute__((__cold__)) void vm_area_add_early(struct vm_struct *vm);
extern __attribute__((__section__(".init.text"))) __attribute__((__cold__)) void vm_area_register_early(struct vm_struct *vm, size_t align);



struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
         const size_t *sizes, int nr_vms,
         size_t align);

void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms);
# 291 "./include/linux/vmalloc.h"
int register_vmap_purge_notifier(struct notifier_block *nb);
int unregister_vmap_purge_notifier(struct notifier_block *nb);


bool vmalloc_dump_obj(void *object);
# 995 "./include/asm-generic/io.h" 2


/*
 * Change virtual addresses to physical addresses and vv.
 * These are pretty trivial
 */
# 1017 "./include/asm-generic/io.h"
/**
 * DOC: ioremap() and ioremap_*() variants
 *
 * Architectures with an MMU are expected to provide ioremap() and iounmap()
 * themselves or rely on GENERIC_IOREMAP.  For NOMMU architectures we provide
 * a default nop-op implementation that expect that the physical address used
 * for MMIO are already marked as uncached, and can be used as kernel virtual
 * addresses.
 *
 * ioremap_wc() and ioremap_wt() can provide more relaxed caching attributes
 * for specific drivers if the architecture choses to implement them.  If they
 * are not implemented we fall back to plain ioremap. Conversely, ioremap_np()
 * can provide stricter non-posted write semantics if the architecture
 * implements them.
 */
# 1050 "./include/asm-generic/io.h"
/*
 * Arch code can implement the following two hooks when using GENERIC_IOREMAP
 * ioremap_allowed() return a bool,
 *   - true means continue to remap
 *   - false means skip remap and return directly
 * iounmap_allowed() return a bool,
 *   - true means continue to vunmap
 *   - false means skip vunmap and return directly
 */
# 1070 "./include/asm-generic/io.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iounmap_allowed(void *addr)
{
 return true;
}


void *ioremap_prot(phys_addr_t phys_addr, size_t size,
      unsigned long prot);
void iounmap(volatile void *addr);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *ioremap(phys_addr_t addr, size_t size)
{
 /* _PAGE_IOREMAP needs to be supplied by the architecture */
 return ioremap_prot(addr, size, ((((((pteval_t)(3)) << 0) | (((pteval_t)(1)) << 10) /* Access Flag */ | (((pteval_t)(3)) << 8) /* SH[1:0], inner shareable */) | (arm64_use_ng_mappings ? (((pteval_t)(1)) << 11) /* nG */ : 0)) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */ | ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */ | (((pteval_t)((4))) << 2)));
}
# 1095 "./include/asm-generic/io.h"
/*
 * ioremap_uc is special in that we do require an explicit architecture
 * implementation.  In general you do not want to use this function in a
 * driver and use plain ioremap, which is uncached by default.  Similarly
 * architectures should not implement it unless they have a very good
 * reason.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *ioremap_uc(phys_addr_t offset, size_t size)
{
 return ((void *)0);
}


/*
 * ioremap_np needs an explicit architecture implementation, as it
 * requests stronger semantics than regular ioremap(). Portable drivers
 * should instead use one of the higher-level abstractions, like
 * devm_ioremap_resource(), to choose the correct variant for any given
 * device and bus. Portable drivers with a good reason to want non-posted
 * write semantics should always provide an ioremap() fallback in case
 * ioremap_np() is not available.
 */
# 1131 "./include/asm-generic/io.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *ioport_map(unsigned long port, unsigned int nr)
{
 port &= (0x01000000 - 1);
 return (port > ((0x01000000 - 1) - 0x4000)) ? ((void *)0) : ((void *)(((-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - 0x00800000) - 0x01000000)) + port;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ioport_unmap(void *p)
{
}
# 1159 "./include/asm-generic/io.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *xlate_dev_mem_ptr(phys_addr_t addr)
{
 return ((void *)((unsigned long)(((phys_addr_t)(addr)) - ({ ((void)(sizeof(( long)(memstart_addr & 1)))); memstart_addr; })) | ((-((((1UL))) << ((48)))))));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unxlate_dev_mem_ptr(phys_addr_t phys, void *addr)
{
}
# 1224 "./include/asm-generic/io.h"
extern int devmem_is_allowed(unsigned long pfn);
# 164 "./arch/arm64/include/asm/io.h" 2


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *ioremap_cache(phys_addr_t addr, size_t size)
{
 if (pfn_is_map_memory(((unsigned long)((addr) >> 12))))
  return (void *)((unsigned long)((addr) - ({ ((void)(sizeof(( long)(memstart_addr & 1)))); memstart_addr; })) | ((-((((1UL))) << ((48))))));

 return ioremap_prot(addr, size, ((((((pteval_t)(3)) << 0) | (((pteval_t)(1)) << 10) /* Access Flag */ | (((pteval_t)(3)) << 8) /* SH[1:0], inner shareable */) | (arm64_use_ng_mappings ? (((pteval_t)(1)) << 11) /* nG */ : 0)) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */ | ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */ | (((pteval_t)((0))) << 2)));
}

/*
 * More restrictive address range checking than the default implementation
 * (PHYS_OFFSET and PHYS_MASK taken into account).
 */

extern int valid_phys_addr_range(phys_addr_t addr, size_t size);
extern int valid_mmap_phys_addr_range(unsigned long pfn, size_t size);

extern bool arch_memremap_can_ram_remap(resource_size_t offset, size_t size,
     unsigned long flags);
# 14 "./include/linux/io.h" 2


struct device;
struct resource;

          void __iowrite32_copy(void *to, const void *from, size_t count);
void __ioread32_copy(void *to, const void *from, size_t count);
void __iowrite64_copy(void *to, const void *from, size_t count);


int ioremap_page_range(unsigned long addr, unsigned long end,
         phys_addr_t phys_addr, pgprot_t prot);
# 34 "./include/linux/io.h"
/*
 * Managed iomap interface
 */

void * devm_ioport_map(struct device *dev, unsigned long port,
          unsigned int nr);
void devm_ioport_unmap(struct device *dev, void *addr);
# 56 "./include/linux/io.h"
void *devm_ioremap(struct device *dev, resource_size_t offset,
      resource_size_t size);
void *devm_ioremap_uc(struct device *dev, resource_size_t offset,
       resource_size_t size);
void *devm_ioremap_wc(struct device *dev, resource_size_t offset,
       resource_size_t size);
void devm_iounmap(struct device *dev, void *addr);
int check_signature(const volatile void *io_addr,
   const unsigned char *signature, int length);
void devm_ioremap_release(struct device *dev, void *res);

void *devm_memremap(struct device *dev, resource_size_t offset,
  size_t size, unsigned long flags);
void devm_memunmap(struct device *dev, void *addr);


/*
 * The PCI specifications (Rev 3.0, 3.2.5 "Transaction Ordering and
 * Posting") mandate non-posted configuration transactions. This default
 * implementation attempts to use the ioremap_np() API to provide this
 * on arches that support it, and falls back to ioremap() on those that
 * don't. Overriding this function is deprecated; arches that properly
 * support non-posted accesses should implement ioremap_np() instead, which
 * this default implementation can then use to return mappings compliant with
 * the PCI specification.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *pci_remap_cfgspace(phys_addr_t offset,
            size_t size)
{
 return ioremap_prot((offset), (size), ((((((pteval_t)(3)) << 0) | (((pteval_t)(1)) << 10) /* Access Flag */ | (((pteval_t)(3)) << 8) /* SH[1:0], inner shareable */) | (arm64_use_ng_mappings ? (((pteval_t)(1)) << 11) /* nG */ : 0)) | (((pteval_t)(1)) << 53) /* Privileged XN */ | (((pteval_t)(1)) << 54) /* User XN */ | ((((pteval_t)(1)) << 51) /* Dirty Bit Management */) /* same as DBM (51) */ | (((pteval_t)((3))) << 2))) ?: ioremap(offset, size);
}



/*
 * Some systems do not have legacy ISA devices.
 * /dev/port is not a valid interface on these systems.
 * So for those archs, <asm/io.h> should define the following symbol.
 */




/*
 * Some systems (x86 without PAT) have a somewhat reliable way to mark a
 * physical address range such that uncached mappings will actually
 * end up write-combining.  This facility should be used in conjunction
 * with pgprot_writecombine, ioremap-wc, or set_memory_wc, since it has
 * no effect if the per-page mechanisms are functional.
 * (On x86 without PAT, these functions manipulate MTRRs.)
 *
 * arch_phys_del_wc(0) or arch_phys_del_wc(any error code) is guaranteed
 * to have no effect.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) arch_phys_wc_add(unsigned long base,
      unsigned long size)
{
 return 0; /* It worked (i.e. did nothing). */
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_phys_wc_del(int handle)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_phys_wc_index(int handle)
{
 return -1;
}




int devm_arch_phys_wc_add(struct device *dev, unsigned long base, unsigned long size);

enum {
 /* See memremap() kernel-doc for usage description... */
 MEMREMAP_WB = 1 << 0,
 MEMREMAP_WT = 1 << 1,
 MEMREMAP_WC = 1 << 2,
 MEMREMAP_ENC = 1 << 3,
 MEMREMAP_DEC = 1 << 4,
};

void *memremap(resource_size_t offset, size_t size, unsigned long flags);
void memunmap(void *addr);

/*
 * On x86 PAT systems we have memory tracking that keeps track of
 * the allowed mappings on memory ranges. This tracking works for
 * all the in-kernel mapping APIs (ioremap*), but where the user
 * wishes to map a range from a physical device into user memory
 * the tracking won't be updated. This API is to be used by
 * drivers which remap physical device pages into userspace,
 * and wants to make sure they are mapped WC and not UC.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_io_reserve_memtype_wc(resource_size_t base,
          resource_size_t size)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_io_free_memtype_wc(resource_size_t base,
        resource_size_t size)
{
}


int devm_arch_io_reserve_memtype_wc(struct device *dev, resource_size_t start,
        resource_size_t size);
# 21 "./include/linux/irq.h" 2




# 1 "./arch/arm64/include/generated/asm/irq_regs.h" 1
# 1 "./include/asm-generic/irq_regs.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/* Fallback per-CPU frame pointer holder
 *
 * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 */






/*
 * Per-cpu current frame pointer - the location of the last exception frame on
 * the stack
 */
extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct pt_regs *) __irq_regs;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pt_regs *get_irq_regs(void)
{
 return ({ __this_cpu_preempt_check("read"); ({ typeof(__irq_regs) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(__irq_regs)) { case 1: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pt_regs *set_irq_regs(struct pt_regs *new_regs)
{
 struct pt_regs *old_regs;

 old_regs = ({ __this_cpu_preempt_check("read"); ({ typeof(__irq_regs) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(__irq_regs)) { case 1: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); });
 ({ __this_cpu_preempt_check("write"); do { do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(__irq_regs)) { case 1: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }) = new_regs; } while (0);break; case 2: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }) = new_regs; } while (0);break; case 4: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }) = new_regs; } while (0);break; case 8: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(__irq_regs)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(__irq_regs))) *)(&(__irq_regs))); (typeof((typeof(*(&(__irq_regs))) *)(&(__irq_regs)))) (__ptr + ((__kern_my_cpu_offset()))); }); }) = new_regs; } while (0);break; default: __bad_size_call_parameter();break; } } while (0); });
 return old_regs;
}
# 2 "./arch/arm64/include/generated/asm/irq_regs.h" 2
# 26 "./include/linux/irq.h" 2

struct seq_file;
struct module;
struct msi_msg;
struct irq_affinity_desc;
enum irqchip_irq_state;

/*
 * IRQ line status.
 *
 * Bits 0-7 are the same as the IRQF_* bits in linux/interrupt.h
 *
 * IRQ_TYPE_NONE		- default, unspecified type
 * IRQ_TYPE_EDGE_RISING		- rising edge triggered
 * IRQ_TYPE_EDGE_FALLING	- falling edge triggered
 * IRQ_TYPE_EDGE_BOTH		- rising and falling edge triggered
 * IRQ_TYPE_LEVEL_HIGH		- high level triggered
 * IRQ_TYPE_LEVEL_LOW		- low level triggered
 * IRQ_TYPE_LEVEL_MASK		- Mask to filter out the level bits
 * IRQ_TYPE_SENSE_MASK		- Mask for all the above bits
 * IRQ_TYPE_DEFAULT		- For use by some PICs to ask irq_set_type
 *				  to setup the HW to a sane default (used
 *                                by irqdomain map() callbacks to synchronize
 *                                the HW state and SW flags for a newly
 *                                allocated descriptor).
 *
 * IRQ_TYPE_PROBE		- Special flag for probing in progress
 *
 * Bits which can be modified via irq_set/clear/modify_status_flags()
 * IRQ_LEVEL			- Interrupt is level type. Will be also
 *				  updated in the code when the above trigger
 *				  bits are modified via irq_set_irq_type()
 * IRQ_PER_CPU			- Mark an interrupt PER_CPU. Will protect
 *				  it from affinity setting
 * IRQ_NOPROBE			- Interrupt cannot be probed by autoprobing
 * IRQ_NOREQUEST		- Interrupt cannot be requested via
 *				  request_irq()
 * IRQ_NOTHREAD			- Interrupt cannot be threaded
 * IRQ_NOAUTOEN			- Interrupt is not automatically enabled in
 *				  request/setup_irq()
 * IRQ_NO_BALANCING		- Interrupt cannot be balanced (affinity set)
 * IRQ_MOVE_PCNTXT		- Interrupt can be migrated from process context
 * IRQ_NESTED_THREAD		- Interrupt nests into another thread
 * IRQ_PER_CPU_DEVID		- Dev_id is a per-cpu variable
 * IRQ_IS_POLLED		- Always polled by another interrupt. Exclude
 *				  it from the spurious interrupt detection
 *				  mechanism and from core side polling.
 * IRQ_DISABLE_UNLAZY		- Disable lazy irq disable
 * IRQ_HIDDEN			- Don't show up in /proc/interrupts
 * IRQ_NO_DEBUG			- Exclude from note_interrupt() debugging
 */
enum {
 IRQ_TYPE_NONE = 0x00000000,
 IRQ_TYPE_EDGE_RISING = 0x00000001,
 IRQ_TYPE_EDGE_FALLING = 0x00000002,
 IRQ_TYPE_EDGE_BOTH = (IRQ_TYPE_EDGE_FALLING | IRQ_TYPE_EDGE_RISING),
 IRQ_TYPE_LEVEL_HIGH = 0x00000004,
 IRQ_TYPE_LEVEL_LOW = 0x00000008,
 IRQ_TYPE_LEVEL_MASK = (IRQ_TYPE_LEVEL_LOW | IRQ_TYPE_LEVEL_HIGH),
 IRQ_TYPE_SENSE_MASK = 0x0000000f,
 IRQ_TYPE_DEFAULT = IRQ_TYPE_SENSE_MASK,

 IRQ_TYPE_PROBE = 0x00000010,

 IRQ_LEVEL = (1 << 8),
 IRQ_PER_CPU = (1 << 9),
 IRQ_NOPROBE = (1 << 10),
 IRQ_NOREQUEST = (1 << 11),
 IRQ_NOAUTOEN = (1 << 12),
 IRQ_NO_BALANCING = (1 << 13),
 IRQ_MOVE_PCNTXT = (1 << 14),
 IRQ_NESTED_THREAD = (1 << 15),
 IRQ_NOTHREAD = (1 << 16),
 IRQ_PER_CPU_DEVID = (1 << 17),
 IRQ_IS_POLLED = (1 << 18),
 IRQ_DISABLE_UNLAZY = (1 << 19),
 IRQ_HIDDEN = (1 << 20),
 IRQ_NO_DEBUG = (1 << 21),
};
# 114 "./include/linux/irq.h"
/*
 * Return value for chip->irq_set_affinity()
 *
 * IRQ_SET_MASK_OK	- OK, core updates irq_common_data.affinity
 * IRQ_SET_MASK_NOCPY	- OK, chip did update irq_common_data.affinity
 * IRQ_SET_MASK_OK_DONE	- Same as IRQ_SET_MASK_OK for core. Special code to
 *			  support stacked irqchips, which indicates skipping
 *			  all descendant irqchips.
 */
enum {
 IRQ_SET_MASK_OK = 0,
 IRQ_SET_MASK_OK_NOCOPY,
 IRQ_SET_MASK_OK_DONE,
};

struct msi_desc;
struct irq_domain;

/**
 * struct irq_common_data - per irq data shared by all irqchips
 * @state_use_accessors: status information for irq chip functions.
 *			Use accessor functions to deal with it
 * @node:		node index useful for balancing
 * @handler_data:	per-IRQ data for the irq_chip methods
 * @affinity:		IRQ affinity on SMP. If this is an IPI
 *			related irq, then this is the mask of the
 *			CPUs to which an IPI can be sent.
 * @effective_affinity:	The effective IRQ affinity on SMP as some irq
 *			chips do not allow multi CPU destinations.
 *			A subset of @affinity.
 * @msi_desc:		MSI descriptor
 * @ipi_offset:		Offset of first IPI target cpu in @affinity. Optional.
 */
struct irq_common_data {
 unsigned int state_use_accessors;

 unsigned int node;

 void *handler_data;
 struct msi_desc *msi_desc;

 cpumask_var_t affinity;


 cpumask_var_t effective_affinity;


 unsigned int ipi_offset;

};

/**
 * struct irq_data - per irq chip data passed down to chip functions
 * @mask:		precomputed bitmask for accessing the chip registers
 * @irq:		interrupt number
 * @hwirq:		hardware interrupt number, local to the interrupt domain
 * @common:		point to data shared by all irqchips
 * @chip:		low level interrupt hardware access
 * @domain:		Interrupt translation domain; responsible for mapping
 *			between hwirq number and linux irq number.
 * @parent_data:	pointer to parent struct irq_data to support hierarchy
 *			irq_domain
 * @chip_data:		platform-specific per-chip private data for the chip
 *			methods, to allow shared chip implementations
 */
struct irq_data {
 u32 mask;
 unsigned int irq;
 unsigned long hwirq;
 struct irq_common_data *common;
 struct irq_chip *chip;
 struct irq_domain *domain;

 struct irq_data *parent_data;

 void *chip_data;
};

/*
 * Bit masks for irq_common_data.state_use_accessors
 *
 * IRQD_TRIGGER_MASK		- Mask for the trigger type bits
 * IRQD_SETAFFINITY_PENDING	- Affinity setting is pending
 * IRQD_ACTIVATED		- Interrupt has already been activated
 * IRQD_NO_BALANCING		- Balancing disabled for this IRQ
 * IRQD_PER_CPU			- Interrupt is per cpu
 * IRQD_AFFINITY_SET		- Interrupt affinity was set
 * IRQD_LEVEL			- Interrupt is level triggered
 * IRQD_WAKEUP_STATE		- Interrupt is configured for wakeup
 *				  from suspend
 * IRQD_MOVE_PCNTXT		- Interrupt can be moved in process
 *				  context
 * IRQD_IRQ_DISABLED		- Disabled state of the interrupt
 * IRQD_IRQ_MASKED		- Masked state of the interrupt
 * IRQD_IRQ_INPROGRESS		- In progress state of the interrupt
 * IRQD_WAKEUP_ARMED		- Wakeup mode armed
 * IRQD_FORWARDED_TO_VCPU	- The interrupt is forwarded to a VCPU
 * IRQD_AFFINITY_MANAGED	- Affinity is auto-managed by the kernel
 * IRQD_IRQ_STARTED		- Startup state of the interrupt
 * IRQD_MANAGED_SHUTDOWN	- Interrupt was shutdown due to empty affinity
 *				  mask. Applies only to affinity managed irqs.
 * IRQD_SINGLE_TARGET		- IRQ allows only a single affinity target
 * IRQD_DEFAULT_TRIGGER_SET	- Expected trigger already been set
 * IRQD_CAN_RESERVE		- Can use reservation mode
 * IRQD_MSI_NOMASK_QUIRK	- Non-maskable MSI quirk for affinity change
 *				  required
 * IRQD_HANDLE_ENFORCE_IRQCTX	- Enforce that handle_irq_*() is only invoked
 *				  from actual interrupt context.
 * IRQD_AFFINITY_ON_ACTIVATE	- Affinity is set on activation. Don't call
 *				  irq_chip::irq_set_affinity() when deactivated.
 * IRQD_IRQ_ENABLED_ON_SUSPEND	- Interrupt is enabled on suspend by irq pm if
 *				  irqchip have flag IRQCHIP_ENABLE_WAKEUP_ON_SUSPEND set.
 */
enum {
 IRQD_TRIGGER_MASK = 0xf,
 IRQD_SETAFFINITY_PENDING = (1 << 8),
 IRQD_ACTIVATED = (1 << 9),
 IRQD_NO_BALANCING = (1 << 10),
 IRQD_PER_CPU = (1 << 11),
 IRQD_AFFINITY_SET = (1 << 12),
 IRQD_LEVEL = (1 << 13),
 IRQD_WAKEUP_STATE = (1 << 14),
 IRQD_MOVE_PCNTXT = (1 << 15),
 IRQD_IRQ_DISABLED = (1 << 16),
 IRQD_IRQ_MASKED = (1 << 17),
 IRQD_IRQ_INPROGRESS = (1 << 18),
 IRQD_WAKEUP_ARMED = (1 << 19),
 IRQD_FORWARDED_TO_VCPU = (1 << 20),
 IRQD_AFFINITY_MANAGED = (1 << 21),
 IRQD_IRQ_STARTED = (1 << 22),
 IRQD_MANAGED_SHUTDOWN = (1 << 23),
 IRQD_SINGLE_TARGET = (1 << 24),
 IRQD_DEFAULT_TRIGGER_SET = (1 << 25),
 IRQD_CAN_RESERVE = (1 << 26),
 IRQD_MSI_NOMASK_QUIRK = (1 << 27),
 IRQD_HANDLE_ENFORCE_IRQCTX = (1 << 28),
 IRQD_AFFINITY_ON_ACTIVATE = (1 << 29),
 IRQD_IRQ_ENABLED_ON_SUSPEND = (1 << 30),
};



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_setaffinity_pending(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_SETAFFINITY_PENDING;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_per_cpu(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_PER_CPU;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_can_balance(struct irq_data *d)
{
 return !((((d)->common)->state_use_accessors) & (IRQD_PER_CPU | IRQD_NO_BALANCING));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_affinity_was_set(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_AFFINITY_SET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_mark_affinity_was_set(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_AFFINITY_SET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_trigger_type_was_set(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_DEFAULT_TRIGGER_SET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 irqd_get_trigger_type(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_TRIGGER_MASK;
}

/*
 * Must only be called inside irq_chip.irq_set_type() functions or
 * from the DT/ACPI setup code.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_trigger_type(struct irq_data *d, u32 type)
{
 (((d)->common)->state_use_accessors) &= ~IRQD_TRIGGER_MASK;
 (((d)->common)->state_use_accessors) |= type & IRQD_TRIGGER_MASK;
 (((d)->common)->state_use_accessors) |= IRQD_DEFAULT_TRIGGER_SET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_level_type(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_LEVEL;
}

/*
 * Must only be called of irqchip.irq_set_affinity() or low level
 * hierarchy domain allocation functions.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_single_target(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_SINGLE_TARGET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_single_target(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_SINGLE_TARGET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_handle_enforce_irqctx(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_HANDLE_ENFORCE_IRQCTX;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_handle_enforce_irqctx(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_HANDLE_ENFORCE_IRQCTX;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_enabled_on_suspend(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_IRQ_ENABLED_ON_SUSPEND;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_wakeup_set(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_WAKEUP_STATE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_can_move_in_process_context(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_MOVE_PCNTXT;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_irq_disabled(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_IRQ_DISABLED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_irq_masked(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_IRQ_MASKED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_irq_inprogress(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_IRQ_INPROGRESS;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_wakeup_armed(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_WAKEUP_ARMED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_forwarded_to_vcpu(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_forwarded_to_vcpu(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_clr_forwarded_to_vcpu(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) &= ~IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_affinity_is_managed(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_AFFINITY_MANAGED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_activated(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_ACTIVATED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_activated(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_ACTIVATED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_clr_activated(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) &= ~IRQD_ACTIVATED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_started(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_IRQ_STARTED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_is_managed_and_shutdown(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_MANAGED_SHUTDOWN;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_can_reserve(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_CAN_RESERVE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_clr_can_reserve(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) &= ~IRQD_CAN_RESERVE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_can_reserve(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_CAN_RESERVE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_msi_nomask_quirk(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_MSI_NOMASK_QUIRK;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_clr_msi_nomask_quirk(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) &= ~IRQD_MSI_NOMASK_QUIRK;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_msi_nomask_quirk(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_MSI_NOMASK_QUIRK;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqd_set_affinity_on_activate(struct irq_data *d)
{
 (((d)->common)->state_use_accessors) |= IRQD_AFFINITY_ON_ACTIVATE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irqd_affinity_on_activate(struct irq_data *d)
{
 return (((d)->common)->state_use_accessors) & IRQD_AFFINITY_ON_ACTIVATE;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) irq_hw_number_t irqd_to_hwirq(struct irq_data *d)
{
 return d->hwirq;
}

/**
 * struct irq_chip - hardware interrupt chip descriptor
 *
 * @name:		name for /proc/interrupts
 * @irq_startup:	start up the interrupt (defaults to ->enable if NULL)
 * @irq_shutdown:	shut down the interrupt (defaults to ->disable if NULL)
 * @irq_enable:		enable the interrupt (defaults to chip->unmask if NULL)
 * @irq_disable:	disable the interrupt
 * @irq_ack:		start of a new interrupt
 * @irq_mask:		mask an interrupt source
 * @irq_mask_ack:	ack and mask an interrupt source
 * @irq_unmask:		unmask an interrupt source
 * @irq_eoi:		end of interrupt
 * @irq_set_affinity:	Set the CPU affinity on SMP machines. If the force
 *			argument is true, it tells the driver to
 *			unconditionally apply the affinity setting. Sanity
 *			checks against the supplied affinity mask are not
 *			required. This is used for CPU hotplug where the
 *			target CPU is not yet set in the cpu_online_mask.
 * @irq_retrigger:	resend an IRQ to the CPU
 * @irq_set_type:	set the flow type (IRQ_TYPE_LEVEL/etc.) of an IRQ
 * @irq_set_wake:	enable/disable power-management wake-on of an IRQ
 * @irq_bus_lock:	function to lock access to slow bus (i2c) chips
 * @irq_bus_sync_unlock:function to sync and unlock slow bus (i2c) chips
 * @irq_cpu_online:	configure an interrupt source for a secondary CPU
 * @irq_cpu_offline:	un-configure an interrupt source for a secondary CPU
 * @irq_suspend:	function called from core code on suspend once per
 *			chip, when one or more interrupts are installed
 * @irq_resume:		function called from core code on resume once per chip,
 *			when one ore more interrupts are installed
 * @irq_pm_shutdown:	function called from core code on shutdown once per chip
 * @irq_calc_mask:	Optional function to set irq_data.mask for special cases
 * @irq_print_chip:	optional to print special chip info in show_interrupts
 * @irq_request_resources:	optional to request resources before calling
 *				any other callback related to this irq
 * @irq_release_resources:	optional to release resources acquired with
 *				irq_request_resources
 * @irq_compose_msi_msg:	optional to compose message content for MSI
 * @irq_write_msi_msg:	optional to write message content for MSI
 * @irq_get_irqchip_state:	return the internal state of an interrupt
 * @irq_set_irqchip_state:	set the internal state of a interrupt
 * @irq_set_vcpu_affinity:	optional to target a vCPU in a virtual machine
 * @ipi_send_single:	send a single IPI to destination cpus
 * @ipi_send_mask:	send an IPI to destination cpus in cpumask
 * @irq_nmi_setup:	function called from core code before enabling an NMI
 * @irq_nmi_teardown:	function called from core code after disabling an NMI
 * @flags:		chip specific flags
 */
struct irq_chip {
 const char *name;
 unsigned int (*irq_startup)(struct irq_data *data);
 void (*irq_shutdown)(struct irq_data *data);
 void (*irq_enable)(struct irq_data *data);
 void (*irq_disable)(struct irq_data *data);

 void (*irq_ack)(struct irq_data *data);
 void (*irq_mask)(struct irq_data *data);
 void (*irq_mask_ack)(struct irq_data *data);
 void (*irq_unmask)(struct irq_data *data);
 void (*irq_eoi)(struct irq_data *data);

 int (*irq_set_affinity)(struct irq_data *data, const struct cpumask *dest, bool force);
 int (*irq_retrigger)(struct irq_data *data);
 int (*irq_set_type)(struct irq_data *data, unsigned int flow_type);
 int (*irq_set_wake)(struct irq_data *data, unsigned int on);

 void (*irq_bus_lock)(struct irq_data *data);
 void (*irq_bus_sync_unlock)(struct irq_data *data);





 void (*irq_suspend)(struct irq_data *data);
 void (*irq_resume)(struct irq_data *data);
 void (*irq_pm_shutdown)(struct irq_data *data);

 void (*irq_calc_mask)(struct irq_data *data);

 void (*irq_print_chip)(struct irq_data *data, struct seq_file *p);
 int (*irq_request_resources)(struct irq_data *data);
 void (*irq_release_resources)(struct irq_data *data);

 void (*irq_compose_msi_msg)(struct irq_data *data, struct msi_msg *msg);
 void (*irq_write_msi_msg)(struct irq_data *data, struct msi_msg *msg);

 int (*irq_get_irqchip_state)(struct irq_data *data, enum irqchip_irq_state which, bool *state);
 int (*irq_set_irqchip_state)(struct irq_data *data, enum irqchip_irq_state which, bool state);

 int (*irq_set_vcpu_affinity)(struct irq_data *data, void *vcpu_info);

 void (*ipi_send_single)(struct irq_data *data, unsigned int cpu);
 void (*ipi_send_mask)(struct irq_data *data, const struct cpumask *dest);

 int (*irq_nmi_setup)(struct irq_data *data);
 void (*irq_nmi_teardown)(struct irq_data *data);

 unsigned long flags;
};

/*
 * irq_chip specific flags
 *
 * IRQCHIP_SET_TYPE_MASKED:           Mask before calling chip.irq_set_type()
 * IRQCHIP_EOI_IF_HANDLED:            Only issue irq_eoi() when irq was handled
 * IRQCHIP_MASK_ON_SUSPEND:           Mask non wake irqs in the suspend path
 * IRQCHIP_ONOFFLINE_ENABLED:         Only call irq_on/off_line callbacks
 *                                    when irq enabled
 * IRQCHIP_SKIP_SET_WAKE:             Skip chip.irq_set_wake(), for this irq chip
 * IRQCHIP_ONESHOT_SAFE:              One shot does not require mask/unmask
 * IRQCHIP_EOI_THREADED:              Chip requires eoi() on unmask in threaded mode
 * IRQCHIP_SUPPORTS_LEVEL_MSI:        Chip can provide two doorbells for Level MSIs
 * IRQCHIP_SUPPORTS_NMI:              Chip can deliver NMIs, only for root irqchips
 * IRQCHIP_ENABLE_WAKEUP_ON_SUSPEND:  Invokes __enable_irq()/__disable_irq() for wake irqs
 *                                    in the suspend path if they are in disabled state
 * IRQCHIP_AFFINITY_PRE_STARTUP:      Default affinity update before startup
 * IRQCHIP_IMMUTABLE:		      Don't ever change anything in this chip
 */
enum {
 IRQCHIP_SET_TYPE_MASKED = (1 << 0),
 IRQCHIP_EOI_IF_HANDLED = (1 << 1),
 IRQCHIP_MASK_ON_SUSPEND = (1 << 2),
 IRQCHIP_ONOFFLINE_ENABLED = (1 << 3),
 IRQCHIP_SKIP_SET_WAKE = (1 << 4),
 IRQCHIP_ONESHOT_SAFE = (1 << 5),
 IRQCHIP_EOI_THREADED = (1 << 6),
 IRQCHIP_SUPPORTS_LEVEL_MSI = (1 << 7),
 IRQCHIP_SUPPORTS_NMI = (1 << 8),
 IRQCHIP_ENABLE_WAKEUP_ON_SUSPEND = (1 << 9),
 IRQCHIP_AFFINITY_PRE_STARTUP = (1 << 10),
 IRQCHIP_IMMUTABLE = (1 << 11),
};

# 1 "./include/linux/irqdesc.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/kobject.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 * kobject.h - generic kernel object infrastructure.
 *
 * Copyright (c) 2002-2003 Patrick Mochel
 * Copyright (c) 2002-2003 Open Source Development Labs
 * Copyright (c) 2006-2008 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (c) 2006-2008 Novell Inc.
 *
 * Please read Documentation/core-api/kobject.rst before using the kobject
 * interface, ESPECIALLY the parts about reference counts and object
 * destructors.
 */






# 1 "./include/linux/sysfs.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * sysfs.h - definitions for the device driver filesystem
 *
 * Copyright (c) 2001,2002 Patrick Mochel
 * Copyright (c) 2004 Silicon Graphics, Inc.
 * Copyright (c) 2007 SUSE Linux Products GmbH
 * Copyright (c) 2007 Tejun Heo <teheo@suse.de>
 *
 * Please see Documentation/filesystems/sysfs.rst for more information.
 */




# 1 "./include/linux/kernfs.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * kernfs.h - pseudo filesystem decoupled from vfs locking
 */







# 1 "./include/linux/idr.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * include/linux/idr.h
 *
 * 2002-10-18  written by Jim Houston jim.houston@ccur.com
 *	Copyright (C) 2002 by Concurrent Computer Corporation
 *
 * Small id to pointer translation service avoiding fixed sized
 * tables.
 */
# 19 "./include/linux/idr.h"
struct idr {
 struct xarray idr_rt;
 unsigned int idr_base;
 unsigned int idr_next;
};

/*
 * The IDR API does not expose the tagging functionality of the radix tree
 * to users.  Use tag 0 to track whether a node has free space below it.
 */


/* Set the IDR flag and the IDR_FREE tag */
# 41 "./include/linux/idr.h"
/**
 * IDR_INIT() - Initialise an IDR.
 * @name: Name of IDR.
 *
 * A freshly-initialised IDR contains no IDs.
 */


/**
 * DEFINE_IDR() - Define a statically-allocated IDR.
 * @name: Name of IDR.
 *
 * An IDR defined using this macro is ready for use with no additional
 * initialisation required.  It contains no IDs.
 */


/**
 * idr_get_cursor - Return the current position of the cyclic allocator
 * @idr: idr handle
 *
 * The value returned is the value that will be next returned from
 * idr_alloc_cyclic() if it is free (otherwise the search will start from
 * this position).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int idr_get_cursor(const struct idr *idr)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_349(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(idr->idr_next) == sizeof(char) || sizeof(idr->idr_next) == sizeof(short) || sizeof(idr->idr_next) == sizeof(int) || sizeof(idr->idr_next) == sizeof(long)) || sizeof(idr->idr_next) == sizeof(long long))) __compiletime_assert_349(); } while (0); (*(const volatile typeof( _Generic((idr->idr_next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (idr->idr_next))) *)&(idr->idr_next)); });
# 69 "./include/linux/idr.h"
}

/**
 * idr_set_cursor - Set the current position of the cyclic allocator
 * @idr: idr handle
 * @val: new position
 *
 * The next call to idr_alloc_cyclic() will return @val if it is free
 * (otherwise the search will start from this position).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void idr_set_cursor(struct idr *idr, unsigned int val)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_350(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(idr->idr_next) == sizeof(char) || sizeof(idr->idr_next) == sizeof(short) || sizeof(idr->idr_next) == sizeof(int) || sizeof(idr->idr_next) == sizeof(long)) || sizeof(idr->idr_next) == sizeof(long long))) __compiletime_assert_350(); } while (0); do { *(volatile typeof(idr->idr_next) *)&(idr->idr_next) = (val); } while (0); } while (0);
# 82 "./include/linux/idr.h"
}

/**
 * DOC: idr sync
 * idr synchronization (stolen from radix-tree.h)
 *
 * idr_find() is able to be called locklessly, using RCU. The caller must
 * ensure calls to this function are made within rcu_read_lock() regions.
 * Other readers (lock-free or otherwise) and modifications may be running
 * concurrently.
 *
 * It is still required that the caller manage the synchronization and
 * lifetimes of the items. So if RCU lock-free lookups are used, typically
 * this would mean that the items have their own locks, or are amenable to
 * lock-free access; and that the items are freed by RCU (or only freed after
 * having been deleted from the idr tree *and* a synchronize_rcu() grace
 * period).
 */
# 112 "./include/linux/idr.h"
void idr_preload(gfp_t gfp_mask);

int idr_alloc(struct idr *, void *ptr, int start, int end, gfp_t);
int __attribute__((__warn_unused_result__)) idr_alloc_u32(struct idr *, void *ptr, u32 *id,
    unsigned long max, gfp_t);
int idr_alloc_cyclic(struct idr *, void *ptr, int start, int end, gfp_t);
void *idr_remove(struct idr *, unsigned long id);
void *idr_find(const struct idr *, unsigned long id);
int idr_for_each(const struct idr *,
   int (*fn)(int id, void *p, void *data), void *data);
void *idr_get_next(struct idr *, int *nextid);
void *idr_get_next_ul(struct idr *, unsigned long *nextid);
void *idr_replace(struct idr *, void *, unsigned long id);
void idr_destroy(struct idr *);

/**
 * idr_init_base() - Initialise an IDR.
 * @idr: IDR handle.
 * @base: The base value for the IDR.
 *
 * This variation of idr_init() creates an IDR which will allocate IDs
 * starting at %base.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void idr_init_base(struct idr *idr, int base)
{
 xa_init_flags(&idr->idr_rt, ((( gfp_t)4) | ( gfp_t) (1 << (((27 + 0)) + 0))));
 idr->idr_base = base;
 idr->idr_next = 0;
}

/**
 * idr_init() - Initialise an IDR.
 * @idr: IDR handle.
 *
 * Initialise a dynamically allocated IDR.  To initialise a
 * statically allocated IDR, use DEFINE_IDR().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void idr_init(struct idr *idr)
{
 idr_init_base(idr, 0);
}

/**
 * idr_is_empty() - Are there any IDs allocated?
 * @idr: IDR handle.
 *
 * Return: %true if any IDs have been allocated from this IDR.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool idr_is_empty(const struct idr *idr)
{
 return radix_tree_empty(&idr->idr_rt) &&
  radix_tree_tagged(&idr->idr_rt, 0);
}

/**
 * idr_preload_end - end preload section started with idr_preload()
 *
 * Each idr_preload() should be matched with an invocation of this
 * function.  See idr_preload() for details.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void idr_preload_end(void)
{
 do { local_lock_release(({ do { const void /* nothing */ *__vpp_verify = (typeof((&radix_tree_preloads.lock) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&radix_tree_preloads.lock)) *)(&radix_tree_preloads.lock)); (typeof((typeof(*(&radix_tree_preloads.lock)) *)(&radix_tree_preloads.lock))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0); } while (0);
}

/**
 * idr_for_each_entry() - Iterate over an IDR's elements of a given type.
 * @idr: IDR handle.
 * @entry: The type * to use as cursor
 * @id: Entry ID.
 *
 * @entry and @id do not need to be initialized before the loop, and
 * after normal termination @entry is left with the value NULL.  This
 * is convenient for a "not found" value.
 */



/**
 * idr_for_each_entry_ul() - Iterate over an IDR's elements of a given type.
 * @idr: IDR handle.
 * @entry: The type * to use as cursor.
 * @tmp: A temporary placeholder for ID.
 * @id: Entry ID.
 *
 * @entry and @id do not need to be initialized before the loop, and
 * after normal termination @entry is left with the value NULL.  This
 * is convenient for a "not found" value.
 */





/**
 * idr_for_each_entry_continue() - Continue iteration over an IDR's elements of a given type
 * @idr: IDR handle.
 * @entry: The type * to use as a cursor.
 * @id: Entry ID.
 *
 * Continue to iterate over entries, continuing after the current position.
 */





/**
 * idr_for_each_entry_continue_ul() - Continue iteration over an IDR's elements of a given type
 * @idr: IDR handle.
 * @entry: The type * to use as a cursor.
 * @tmp: A temporary placeholder for ID.
 * @id: Entry ID.
 *
 * Continue to iterate over entries, continuing after the current position.
 */





/*
 * IDA - ID Allocator, use when translation from id to pointer isn't necessary.
 */




struct ida_bitmap {
 unsigned long bitmap[(128 /* 128 bytes per chunk */ / sizeof(long))];
};

struct ida {
 struct xarray xa;
};
# 255 "./include/linux/idr.h"
int ida_alloc_range(struct ida *, unsigned int min, unsigned int max, gfp_t);
void ida_free(struct ida *, unsigned int id);
void ida_destroy(struct ida *ida);

/**
 * ida_alloc() - Allocate an unused ID.
 * @ida: IDA handle.
 * @gfp: Memory allocation flags.
 *
 * Allocate an ID between 0 and %INT_MAX, inclusive.
 *
 * Context: Any context. It is safe to call this function without
 * locking in your code.
 * Return: The allocated ID, or %-ENOMEM if memory could not be allocated,
 * or %-ENOSPC if there are no free IDs.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ida_alloc(struct ida *ida, gfp_t gfp)
{
 return ida_alloc_range(ida, 0, ~0, gfp);
}

/**
 * ida_alloc_min() - Allocate an unused ID.
 * @ida: IDA handle.
 * @min: Lowest ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Allocate an ID between @min and %INT_MAX, inclusive.
 *
 * Context: Any context. It is safe to call this function without
 * locking in your code.
 * Return: The allocated ID, or %-ENOMEM if memory could not be allocated,
 * or %-ENOSPC if there are no free IDs.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ida_alloc_min(struct ida *ida, unsigned int min, gfp_t gfp)
{
 return ida_alloc_range(ida, min, ~0, gfp);
}

/**
 * ida_alloc_max() - Allocate an unused ID.
 * @ida: IDA handle.
 * @max: Highest ID to allocate.
 * @gfp: Memory allocation flags.
 *
 * Allocate an ID between 0 and @max, inclusive.
 *
 * Context: Any context. It is safe to call this function without
 * locking in your code.
 * Return: The allocated ID, or %-ENOMEM if memory could not be allocated,
 * or %-ENOSPC if there are no free IDs.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ida_alloc_max(struct ida *ida, unsigned int max, gfp_t gfp)
{
 return ida_alloc_range(ida, 0, max, gfp);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ida_init(struct ida *ida)
{
 xa_init_flags(&ida->xa, ((( gfp_t)XA_LOCK_IRQ) | ((( gfp_t)4U) | (( gfp_t)((1U << (27 + 0)) << ( unsigned)((( xa_mark_t)0U)))))));
}

/*
 * ida_simple_get() and ida_simple_remove() are deprecated. Use
 * ida_alloc() and ida_free() instead respectively.
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool ida_is_empty(const struct ida *ida)
{
 return xa_empty(&ida->xa);
}
# 13 "./include/linux/kernfs.h" 2
# 23 "./include/linux/kernfs.h"
struct file;
struct dentry;
struct iattr;
struct seq_file;
struct vm_area_struct;
struct vm_operations_struct;
struct super_block;
struct file_system_type;
struct poll_table_struct;
struct fs_context;

struct kernfs_fs_context;
struct kernfs_open_node;
struct kernfs_iattrs;

/*
 * NR_KERNFS_LOCK_BITS determines size (NR_KERNFS_LOCKS) of hash
 * table of locks.
 * Having a small hash table would impact scalability, since
 * more and more kernfs_node objects will end up using same lock
 * and having a very large hash table would waste memory.
 *
 * At the moment size of hash table of locks is being set based on
 * the number of CPUs as follows:
 *
 * NR_CPU      NR_KERNFS_LOCK_BITS      NR_KERNFS_LOCKS
 *   1                  1                       2
 *  2-3                 2                       4
 *  4-7                 4                       16
 *  8-15                6                       64
 *  16-31               8                       256
 *  32 and more         10                      1024
 *
 * The above relation between NR_CPU and number of locks is based
 * on some internal experimentation which involved booting qemu
 * with different values of smp, performing some sysfs operations
 * on all CPUs and observing how increase in number of locks impacts
 * completion time of these sysfs operations on each CPU.
 */
# 70 "./include/linux/kernfs.h"
/*
 * There's one kernfs_open_file for each open file and one kernfs_open_node
 * for each kernfs_node with one or more open files.
 *
 * filp->private_data points to seq_file whose ->private points to
 * kernfs_open_file.
 *
 * kernfs_open_files are chained at kernfs_open_node->files, which is
 * protected by kernfs_global_locks.open_file_mutex[i].
 *
 * To reduce possible contention in sysfs access, arising due to single
 * locks, use an array of locks (e.g. open_file_mutex) and use kernfs_node
 * object address as hash keys to get the index of these locks.
 *
 * Hashed mutexes are safe to use here because operations using these don't
 * rely on global exclusion.
 *
 * In future we intend to replace other global locks with hashed ones as well.
 * kernfs_global_locks acts as a holder for all such hash tables.
 */
struct kernfs_global_locks {
 struct mutex open_file_mutex[(1 << (2 * (( __builtin_constant_p(256 < 32 ? 256 : 32) ? ((256 < 32 ? 256 : 32) < 2 ? 0 : 63 - __builtin_clzll(256 < 32 ? 256 : 32)) : (sizeof(256 < 32 ? 256 : 32) <= 4) ? __ilog2_u32(256 < 32 ? 256 : 32) : __ilog2_u64(256 < 32 ? 256 : 32) ))))];
};

enum kernfs_node_type {
 KERNFS_DIR = 0x0001,
 KERNFS_FILE = 0x0002,
 KERNFS_LINK = 0x0004,
};






enum kernfs_node_flag {
 KERNFS_ACTIVATED = 0x0010,
 KERNFS_NS = 0x0020,
 KERNFS_HAS_SEQ_SHOW = 0x0040,
 KERNFS_HAS_MMAP = 0x0080,
 KERNFS_LOCKDEP = 0x0100,
 KERNFS_HIDDEN = 0x0200,
 KERNFS_SUICIDAL = 0x0400,
 KERNFS_SUICIDED = 0x0800,
 KERNFS_EMPTY_DIR = 0x1000,
 KERNFS_HAS_RELEASE = 0x2000,
 KERNFS_REMOVING = 0x4000,
};

/* @flags for kernfs_create_root() */
enum kernfs_root_flag {
 /*
	 * kernfs_nodes are created in the deactivated state and invisible.
	 * They require explicit kernfs_activate() to become visible.  This
	 * can be used to make related nodes become visible atomically
	 * after all nodes are created successfully.
	 */
 KERNFS_ROOT_CREATE_DEACTIVATED = 0x0001,

 /*
	 * For regular files, if the opener has CAP_DAC_OVERRIDE, open(2)
	 * succeeds regardless of the RW permissions.  sysfs had an extra
	 * layer of enforcement where open(2) fails with -EACCES regardless
	 * of CAP_DAC_OVERRIDE if the permission doesn't have the
	 * respective read or write access at all (none of S_IRUGO or
	 * S_IWUGO) or the respective operation isn't implemented.  The
	 * following flag enables that behavior.
	 */
 KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK = 0x0002,

 /*
	 * The filesystem supports exportfs operation, so userspace can use
	 * fhandle to access nodes of the fs.
	 */
 KERNFS_ROOT_SUPPORT_EXPORTOP = 0x0004,

 /*
	 * Support user xattrs to be written to nodes rooted at this root.
	 */
 KERNFS_ROOT_SUPPORT_USER_XATTR = 0x0008,
};

/* type-specific structures for kernfs_node union members */
struct kernfs_elem_dir {
 unsigned long subdirs;
 /* children rbtree starts here and goes through kn->rb */
 struct rb_root children;

 /*
	 * The kernfs hierarchy this directory belongs to.  This fits
	 * better directly in kernfs_node but is here to save space.
	 */
 struct kernfs_root *root;
 /*
	 * Monotonic revision counter, used to identify if a directory
	 * node has changed during negative dentry revalidation.
	 */
 unsigned long rev;
};

struct kernfs_elem_symlink {
 struct kernfs_node *target_kn;
};

struct kernfs_elem_attr {
 const struct kernfs_ops *ops;
 struct kernfs_open_node /* nothing */ *open;
 loff_t size;
 struct kernfs_node *notify_next; /* for kernfs_notify() */
};

/*
 * kernfs_node - the building block of kernfs hierarchy.  Each and every
 * kernfs node is represented by single kernfs_node.  Most fields are
 * private to kernfs and shouldn't be accessed directly by kernfs users.
 *
 * As long as count reference is held, the kernfs_node itself is
 * accessible.  Dereferencing elem or any other outer entity requires
 * active reference.
 */
struct kernfs_node {
 atomic_t count;
 atomic_t active;



 /*
	 * Use kernfs_get_parent() and kernfs_name/path() instead of
	 * accessing the following two fields directly.  If the node is
	 * never moved to a different parent, it is safe to access the
	 * parent directly.
	 */
 struct kernfs_node *parent;
 const char *name;

 struct rb_node rb;

 const void *ns; /* namespace tag */
 unsigned int hash; /* ns + name hash */
 union {
  struct kernfs_elem_dir dir;
  struct kernfs_elem_symlink symlink;
  struct kernfs_elem_attr attr;
 };

 void *priv;

 /*
	 * 64bit unique ID.  On 64bit ino setups, id is the ino.  On 32bit,
	 * the low 32bits are ino and upper generation.
	 */
 u64 id;

 unsigned short flags;
 umode_t mode;
 struct kernfs_iattrs *iattr;
};

/*
 * kernfs_syscall_ops may be specified on kernfs_create_root() to support
 * syscalls.  These optional callbacks are invoked on the matching syscalls
 * and can perform any kernfs operations which don't necessarily have to be
 * the exact operation requested.  An active reference is held for each
 * kernfs_node parameter.
 */
struct kernfs_syscall_ops {
 int (*show_options)(struct seq_file *sf, struct kernfs_root *root);

 int (*mkdir)(struct kernfs_node *parent, const char *name,
       umode_t mode);
 int (*rmdir)(struct kernfs_node *kn);
 int (*rename)(struct kernfs_node *kn, struct kernfs_node *new_parent,
        const char *new_name);
 int (*show_path)(struct seq_file *sf, struct kernfs_node *kn,
    struct kernfs_root *root);
};

struct kernfs_node *kernfs_root_to_node(struct kernfs_root *root);

struct kernfs_open_file {
 /* published fields */
 struct kernfs_node *kn;
 struct file *file;
 struct seq_file *seq_file;
 void *priv;

 /* private fields, do not use outside kernfs proper */
 struct mutex mutex;
 struct mutex prealloc_mutex;
 int event;
 struct list_head list;
 char *prealloc_buf;

 size_t atomic_write_len;
 bool mmapped:1;
 bool released:1;
 const struct vm_operations_struct *vm_ops;
};

struct kernfs_ops {
 /*
	 * Optional open/release methods.  Both are called with
	 * @of->seq_file populated.
	 */
 int (*open)(struct kernfs_open_file *of);
 void (*release)(struct kernfs_open_file *of);

 /*
	 * Read is handled by either seq_file or raw_read().
	 *
	 * If seq_show() is present, seq_file path is active.  Other seq
	 * operations are optional and if not implemented, the behavior is
	 * equivalent to single_open().  @sf->private points to the
	 * associated kernfs_open_file.
	 *
	 * read() is bounced through kernel buffer and a read larger than
	 * PAGE_SIZE results in partial operation of PAGE_SIZE.
	 */
 int (*seq_show)(struct seq_file *sf, void *v);

 void *(*seq_start)(struct seq_file *sf, loff_t *ppos);
 void *(*seq_next)(struct seq_file *sf, void *v, loff_t *ppos);
 void (*seq_stop)(struct seq_file *sf, void *v);

 ssize_t (*read)(struct kernfs_open_file *of, char *buf, size_t bytes,
   loff_t off);

 /*
	 * write() is bounced through kernel buffer.  If atomic_write_len
	 * is not set, a write larger than PAGE_SIZE results in partial
	 * operations of PAGE_SIZE chunks.  If atomic_write_len is set,
	 * writes upto the specified size are executed atomically but
	 * larger ones are rejected with -E2BIG.
	 */
 size_t atomic_write_len;
 /*
	 * "prealloc" causes a buffer to be allocated at open for
	 * all read/write requests.  As ->seq_show uses seq_read()
	 * which does its own allocation, it is incompatible with
	 * ->prealloc.  Provide ->read and ->write with ->prealloc.
	 */
 bool prealloc;
 ssize_t (*write)(struct kernfs_open_file *of, char *buf, size_t bytes,
    loff_t off);

 __poll_t (*poll)(struct kernfs_open_file *of,
    struct poll_table_struct *pt);

 int (*mmap)(struct kernfs_open_file *of, struct vm_area_struct *vma);
};

/*
 * The kernfs superblock creation/mount parameter context.
 */
struct kernfs_fs_context {
 struct kernfs_root *root; /* Root of the hierarchy being mounted */
 void *ns_tag; /* Namespace tag of the mount (or NULL) */
 unsigned long magic; /* File system specific magic number */

 /* The following are set/used by kernfs_mount() */
 bool new_sb_created; /* Set to T if we allocated a new sb */
};



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum kernfs_node_type kernfs_type(struct kernfs_node *kn)
{
 return kn->flags & 0x000f;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ino_t kernfs_id_ino(u64 id)
{
 /* id is ino if ino_t is 64bit; otherwise, low 32bits */
 if (sizeof(ino_t) >= sizeof(u64))
  return id;
 else
  return (u32)id;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 kernfs_id_gen(u64 id)
{
 /* gen is fixed at 1 if ino_t is 64bit; otherwise, high 32bits */
 if (sizeof(ino_t) >= sizeof(u64))
  return 1;
 else
  return id >> 32;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ino_t kernfs_ino(struct kernfs_node *kn)
{
 return kernfs_id_ino(kn->id);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ino_t kernfs_gen(struct kernfs_node *kn)
{
 return kernfs_id_gen(kn->id);
}

/**
 * kernfs_enable_ns - enable namespace under a directory
 * @kn: directory of interest, should be empty
 *
 * This is to be called right after @kn is created to enable namespace
 * under it.  All children of @kn must have non-NULL namespace tags and
 * only the ones which match the super_block's tag will be visible.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kernfs_enable_ns(struct kernfs_node *kn)
{
 ({ int __ret_warn_on = !!(kernfs_type(kn) != KERNFS_DIR); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/kernfs.h\"; .popsection; .long 14472b - .; .short 378; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 ({ int __ret_warn_on = !!(!(({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_351(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((&kn->dir.children)->rb_node) == sizeof(char) || sizeof((&kn->dir.children)->rb_node) == sizeof(short) || sizeof((&kn->dir.children)->rb_node) == sizeof(int) || sizeof((&kn->dir.children)->rb_node) == sizeof(long)) || sizeof((&kn->dir.children)->rb_node) == sizeof(long long))) __compiletime_assert_351(); } while (0); (*(const volatile typeof( _Generic(((&kn->dir.children)->rb_node), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((&kn->dir.children)->rb_node))) *)&((&kn->dir.children)->rb_node)); }) == ((void *)0))); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/kernfs.h\"; .popsection; .long 14472b - .; .short 379; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
# 380 "./include/linux/kernfs.h"
 kn->flags |= KERNFS_NS;
}

/**
 * kernfs_ns_enabled - test whether namespace is enabled
 * @kn: the node to test
 *
 * Test whether namespace filtering is enabled for the children of @ns.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kernfs_ns_enabled(struct kernfs_node *kn)
{
 return kn->flags & KERNFS_NS;
}

int kernfs_name(struct kernfs_node *kn, char *buf, size_t buflen);
int kernfs_path_from_node(struct kernfs_node *root_kn, struct kernfs_node *kn,
     char *buf, size_t buflen);
void pr_cont_kernfs_name(struct kernfs_node *kn);
void pr_cont_kernfs_path(struct kernfs_node *kn);
struct kernfs_node *kernfs_get_parent(struct kernfs_node *kn);
struct kernfs_node *kernfs_find_and_get_ns(struct kernfs_node *parent,
        const char *name, const void *ns);
struct kernfs_node *kernfs_walk_and_get_ns(struct kernfs_node *parent,
        const char *path, const void *ns);
void kernfs_get(struct kernfs_node *kn);
void kernfs_put(struct kernfs_node *kn);

struct kernfs_node *kernfs_node_from_dentry(struct dentry *dentry);
struct kernfs_root *kernfs_root_from_sb(struct super_block *sb);
struct inode *kernfs_get_inode(struct super_block *sb, struct kernfs_node *kn);

struct dentry *kernfs_node_dentry(struct kernfs_node *kn,
      struct super_block *sb);
struct kernfs_root *kernfs_create_root(struct kernfs_syscall_ops *scops,
           unsigned int flags, void *priv);
void kernfs_destroy_root(struct kernfs_root *root);

struct kernfs_node *kernfs_create_dir_ns(struct kernfs_node *parent,
      const char *name, umode_t mode,
      kuid_t uid, kgid_t gid,
      void *priv, const void *ns);
struct kernfs_node *kernfs_create_empty_dir(struct kernfs_node *parent,
         const char *name);
struct kernfs_node *__kernfs_create_file(struct kernfs_node *parent,
      const char *name, umode_t mode,
      kuid_t uid, kgid_t gid,
      loff_t size,
      const struct kernfs_ops *ops,
      void *priv, const void *ns,
      struct lock_class_key *key);
struct kernfs_node *kernfs_create_link(struct kernfs_node *parent,
           const char *name,
           struct kernfs_node *target);
void kernfs_activate(struct kernfs_node *kn);
void kernfs_show(struct kernfs_node *kn, bool show);
void kernfs_remove(struct kernfs_node *kn);
void kernfs_break_active_protection(struct kernfs_node *kn);
void kernfs_unbreak_active_protection(struct kernfs_node *kn);
bool kernfs_remove_self(struct kernfs_node *kn);
int kernfs_remove_by_name_ns(struct kernfs_node *parent, const char *name,
        const void *ns);
int kernfs_rename_ns(struct kernfs_node *kn, struct kernfs_node *new_parent,
       const char *new_name, const void *new_ns);
int kernfs_setattr(struct kernfs_node *kn, const struct iattr *iattr);
__poll_t kernfs_generic_poll(struct kernfs_open_file *of,
        struct poll_table_struct *pt);
void kernfs_notify(struct kernfs_node *kn);

int kernfs_xattr_get(struct kernfs_node *kn, const char *name,
       void *value, size_t size);
int kernfs_xattr_set(struct kernfs_node *kn, const char *name,
       const void *value, size_t size, int flags);

const void *kernfs_super_ns(struct super_block *sb);
int kernfs_get_tree(struct fs_context *fc);
void kernfs_free_fs_context(struct fs_context *fc);
void kernfs_kill_sb(struct super_block *sb);

void kernfs_init(void);

struct kernfs_node *kernfs_find_and_get_node_by_id(struct kernfs_root *root,
         u64 id);
# 577 "./include/linux/kernfs.h"
/**
 * kernfs_path - build full path of a given node
 * @kn: kernfs_node of interest
 * @buf: buffer to copy @kn's name into
 * @buflen: size of @buf
 *
 * If @kn is NULL result will be "(null)".
 *
 * Returns the length of the full path.  If the full length is equal to or
 * greater than @buflen, @buf contains the truncated path with the trailing
 * '\0'.  On error, -errno is returned.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kernfs_path(struct kernfs_node *kn, char *buf, size_t buflen)
{
 return kernfs_path_from_node(kn, ((void *)0), buf, buflen);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kernfs_node *
kernfs_find_and_get(struct kernfs_node *kn, const char *name)
{
 return kernfs_find_and_get_ns(kn, name, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kernfs_node *
kernfs_walk_and_get(struct kernfs_node *kn, const char *path)
{
 return kernfs_walk_and_get_ns(kn, path, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kernfs_node *
kernfs_create_dir(struct kernfs_node *parent, const char *name, umode_t mode,
    void *priv)
{
 return kernfs_create_dir_ns(parent, name, mode,
        (kuid_t){ 0 }, (kgid_t){ 0 },
        priv, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kernfs_remove_by_name(struct kernfs_node *parent,
     const char *name)
{
 return kernfs_remove_by_name_ns(parent, name, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kernfs_rename(struct kernfs_node *kn,
    struct kernfs_node *new_parent,
    const char *new_name)
{
 return kernfs_rename_ns(kn, new_parent, new_name, ((void *)0));
}
# 17 "./include/linux/sysfs.h" 2




# 1 "./include/linux/kobject_ns.h" 1
// SPDX-License-Identifier: GPL-2.0
/* Kernel object name space definitions
 *
 * Copyright (c) 2002-2003 Patrick Mochel
 * Copyright (c) 2002-2003 Open Source Development Labs
 * Copyright (c) 2006-2008 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (c) 2006-2008 Novell Inc.
 *
 * Split from kobject.h by David Howells (dhowells@redhat.com)
 *
 * Please read Documentation/core-api/kobject.rst before using the kobject
 * interface, ESPECIALLY the parts about reference counts and object
 * destructors.
 */




struct sock;
struct kobject;

/*
 * Namespace types which are used to tag kobjects and sysfs entries.
 * Network namespace will likely be the first.
 */
enum kobj_ns_type {
 KOBJ_NS_TYPE_NONE = 0,
 KOBJ_NS_TYPE_NET,
 KOBJ_NS_TYPES
};

/*
 * Callbacks so sysfs can determine namespaces
 *   @grab_current_ns: return a new reference to calling task's namespace
 *   @netlink_ns: return namespace to which a sock belongs (right?)
 *   @initial_ns: return the initial namespace (i.e. init_net_ns)
 *   @drop_ns: drops a reference to namespace
 */
struct kobj_ns_type_operations {
 enum kobj_ns_type type;
 bool (*current_may_mount)(void);
 void *(*grab_current_ns)(void);
 const void *(*netlink_ns)(struct sock *sk);
 const void *(*initial_ns)(void);
 void (*drop_ns)(void *);
};

int kobj_ns_type_register(const struct kobj_ns_type_operations *ops);
int kobj_ns_type_registered(enum kobj_ns_type type);
const struct kobj_ns_type_operations *kobj_child_ns_ops(struct kobject *parent);
const struct kobj_ns_type_operations *kobj_ns_ops(struct kobject *kobj);

bool kobj_ns_current_may_mount(enum kobj_ns_type type);
void *kobj_ns_grab_current(enum kobj_ns_type type);
const void *kobj_ns_netlink(enum kobj_ns_type type, struct sock *sk);
const void *kobj_ns_initial(enum kobj_ns_type type);
void kobj_ns_drop(enum kobj_ns_type type, void *ns);
# 22 "./include/linux/sysfs.h" 2



struct kobject;
struct module;
struct bin_attribute;
enum kobj_ns_type;

struct attribute {
 const char *name;
 umode_t mode;





};

/**
 *	sysfs_attr_init - initialize a dynamically allocated sysfs attribute
 *	@attr: struct attribute to initialize
 *
 *	Initialize a dynamically allocated struct attribute so we can
 *	make lockdep happy.  This is a new requirement for attributes
 *	and initially this is only needed when lockdep is enabled.
 *	Lockdep gives a nice error when your attribute is added to
 *	sysfs if you don't have this.
 */
# 61 "./include/linux/sysfs.h"
/**
 * struct attribute_group - data structure used to declare an attribute group.
 * @name:	Optional: Attribute group name
 *		If specified, the attribute group will be created in
 *		a new subdirectory with this name.
 * @is_visible:	Optional: Function to return permissions associated with an
 *		attribute of the group. Will be called repeatedly for each
 *		non-binary attribute in the group. Only read/write
 *		permissions as well as SYSFS_PREALLOC are accepted. Must
 *		return 0 if an attribute is not visible. The returned value
 *		will replace static permissions defined in struct attribute.
 * @is_bin_visible:
 *		Optional: Function to return permissions associated with a
 *		binary attribute of the group. Will be called repeatedly
 *		for each binary attribute in the group. Only read/write
 *		permissions as well as SYSFS_PREALLOC are accepted. Must
 *		return 0 if a binary attribute is not visible. The returned
 *		value will replace static permissions defined in
 *		struct bin_attribute.
 * @attrs:	Pointer to NULL terminated list of attributes.
 * @bin_attrs:	Pointer to NULL terminated list of binary attributes.
 *		Either attrs or bin_attrs or both must be provided.
 */
struct attribute_group {
 const char *name;
 umode_t (*is_visible)(struct kobject *,
           struct attribute *, int);
 umode_t (*is_bin_visible)(struct kobject *,
        struct bin_attribute *, int);
 struct attribute **attrs;
 struct bin_attribute **bin_attrs;
};

/*
 * Use these macros to make defining attributes easier.
 * See include/linux/device.h for examples..
 */
# 171 "./include/linux/sysfs.h"
struct file;
struct vm_area_struct;
struct address_space;

struct bin_attribute {
 struct attribute attr;
 size_t size;
 void *private;
 struct address_space *(*f_mapping)(void);
 ssize_t (*read)(struct file *, struct kobject *, struct bin_attribute *,
   char *, loff_t, size_t);
 ssize_t (*write)(struct file *, struct kobject *, struct bin_attribute *,
    char *, loff_t, size_t);
 int (*mmap)(struct file *, struct kobject *, struct bin_attribute *attr,
      struct vm_area_struct *vma);
};

/**
 *	sysfs_bin_attr_init - initialize a dynamically allocated bin_attribute
 *	@attr: struct bin_attribute to initialize
 *
 *	Initialize a dynamically allocated struct bin_attribute so we
 *	can make lockdep happy.  This is a new requirement for
 *	attributes and initially this is only needed when lockdep is
 *	enabled.  Lockdep gives a nice error when your attribute is
 *	added to sysfs if you don't have this.
 */


/* macros to create static binary attributes easier */
# 254 "./include/linux/sysfs.h"
struct sysfs_ops {
 ssize_t (*show)(struct kobject *, struct attribute *, char *);
 ssize_t (*store)(struct kobject *, struct attribute *, const char *, size_t);
};



int __attribute__((__warn_unused_result__)) sysfs_create_dir_ns(struct kobject *kobj, const void *ns);
void sysfs_remove_dir(struct kobject *kobj);
int __attribute__((__warn_unused_result__)) sysfs_rename_dir_ns(struct kobject *kobj, const char *new_name,
         const void *new_ns);
int __attribute__((__warn_unused_result__)) sysfs_move_dir_ns(struct kobject *kobj,
       struct kobject *new_parent_kobj,
       const void *new_ns);
int __attribute__((__warn_unused_result__)) sysfs_create_mount_point(struct kobject *parent_kobj,
       const char *name);
void sysfs_remove_mount_point(struct kobject *parent_kobj,
         const char *name);

int __attribute__((__warn_unused_result__)) sysfs_create_file_ns(struct kobject *kobj,
          const struct attribute *attr,
          const void *ns);
int __attribute__((__warn_unused_result__)) sysfs_create_files(struct kobject *kobj,
       const struct attribute * const *attr);
int __attribute__((__warn_unused_result__)) sysfs_chmod_file(struct kobject *kobj,
      const struct attribute *attr, umode_t mode);
struct kernfs_node *sysfs_break_active_protection(struct kobject *kobj,
        const struct attribute *attr);
void sysfs_unbreak_active_protection(struct kernfs_node *kn);
void sysfs_remove_file_ns(struct kobject *kobj, const struct attribute *attr,
     const void *ns);
bool sysfs_remove_file_self(struct kobject *kobj, const struct attribute *attr);
void sysfs_remove_files(struct kobject *kobj, const struct attribute * const *attr);

int __attribute__((__warn_unused_result__)) sysfs_create_bin_file(struct kobject *kobj,
           const struct bin_attribute *attr);
void sysfs_remove_bin_file(struct kobject *kobj,
      const struct bin_attribute *attr);

int __attribute__((__warn_unused_result__)) sysfs_create_link(struct kobject *kobj, struct kobject *target,
       const char *name);
int __attribute__((__warn_unused_result__)) sysfs_create_link_nowarn(struct kobject *kobj,
       struct kobject *target,
       const char *name);
void sysfs_remove_link(struct kobject *kobj, const char *name);

int sysfs_rename_link_ns(struct kobject *kobj, struct kobject *target,
    const char *old_name, const char *new_name,
    const void *new_ns);

void sysfs_delete_link(struct kobject *dir, struct kobject *targ,
   const char *name);

int __attribute__((__warn_unused_result__)) sysfs_create_group(struct kobject *kobj,
        const struct attribute_group *grp);
int __attribute__((__warn_unused_result__)) sysfs_create_groups(struct kobject *kobj,
         const struct attribute_group **groups);
int __attribute__((__warn_unused_result__)) sysfs_update_groups(struct kobject *kobj,
         const struct attribute_group **groups);
int sysfs_update_group(struct kobject *kobj,
         const struct attribute_group *grp);
void sysfs_remove_group(struct kobject *kobj,
   const struct attribute_group *grp);
void sysfs_remove_groups(struct kobject *kobj,
    const struct attribute_group **groups);
int sysfs_add_file_to_group(struct kobject *kobj,
   const struct attribute *attr, const char *group);
void sysfs_remove_file_from_group(struct kobject *kobj,
   const struct attribute *attr, const char *group);
int sysfs_merge_group(struct kobject *kobj,
         const struct attribute_group *grp);
void sysfs_unmerge_group(struct kobject *kobj,
         const struct attribute_group *grp);
int sysfs_add_link_to_group(struct kobject *kobj, const char *group_name,
       struct kobject *target, const char *link_name);
void sysfs_remove_link_from_group(struct kobject *kobj, const char *group_name,
      const char *link_name);
int compat_only_sysfs_link_entry_to_kobj(struct kobject *kobj,
      struct kobject *target_kobj,
      const char *target_name,
      const char *symlink_name);

void sysfs_notify(struct kobject *kobj, const char *dir, const char *attr);

int __attribute__((__warn_unused_result__)) sysfs_init(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sysfs_enable_ns(struct kernfs_node *kn)
{
 return kernfs_enable_ns(kn);
}

int sysfs_file_change_owner(struct kobject *kobj, const char *name, kuid_t kuid,
       kgid_t kgid);
int sysfs_change_owner(struct kobject *kobj, kuid_t kuid, kgid_t kgid);
int sysfs_link_change_owner(struct kobject *kobj, struct kobject *targ,
       const char *name, kuid_t kuid, kgid_t kgid);
int sysfs_groups_change_owner(struct kobject *kobj,
         const struct attribute_group **groups,
         kuid_t kuid, kgid_t kgid);
int sysfs_group_change_owner(struct kobject *kobj,
        const struct attribute_group *groups, kuid_t kuid,
        kgid_t kgid);
__attribute__((__format__(printf, 2, 3)))
int sysfs_emit(char *buf, const char *fmt, ...);
__attribute__((__format__(printf, 3, 4)))
int sysfs_emit_at(char *buf, int at, const char *fmt, ...);
# 620 "./include/linux/sysfs.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) sysfs_create_file(struct kobject *kobj,
       const struct attribute *attr)
{
 return sysfs_create_file_ns(kobj, attr, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sysfs_remove_file(struct kobject *kobj,
         const struct attribute *attr)
{
 sysfs_remove_file_ns(kobj, attr, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sysfs_rename_link(struct kobject *kobj, struct kobject *target,
        const char *old_name, const char *new_name)
{
 return sysfs_rename_link_ns(kobj, target, old_name, new_name, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sysfs_notify_dirent(struct kernfs_node *kn)
{
 kernfs_notify(kn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kernfs_node *sysfs_get_dirent(struct kernfs_node *parent,
         const char *name)
{
 return kernfs_find_and_get(parent, name);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kernfs_node *sysfs_get(struct kernfs_node *kn)
{
 kernfs_get(kn);
 return kn;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sysfs_put(struct kernfs_node *kn)
{
 kernfs_put(kn);
}
# 21 "./include/linux/kobject.h" 2
# 40 "./include/linux/kobject.h"
/* counter to tag the uevent, read only except for the kobject core */
extern u64 uevent_seqnum;

/*
 * The actions here must match the index to the string array
 * in lib/kobject_uevent.c
 *
 * Do not add new actions here without checking with the driver-core
 * maintainers. Action strings are not meant to express subsystem
 * or device specific properties. In most cases you want to send a
 * kobject_uevent_env(kobj, KOBJ_CHANGE, env) with additional event
 * specific variables added to the event environment.
 */
enum kobject_action {
 KOBJ_ADD,
 KOBJ_REMOVE,
 KOBJ_CHANGE,
 KOBJ_MOVE,
 KOBJ_ONLINE,
 KOBJ_OFFLINE,
 KOBJ_BIND,
 KOBJ_UNBIND,
};

struct kobject {
 const char *name;
 struct list_head entry;
 struct kobject *parent;
 struct kset *kset;
 const struct kobj_type *ktype;
 struct kernfs_node *sd; /* sysfs directory entry */
 struct kref kref;



 unsigned int state_initialized:1;
 unsigned int state_in_sysfs:1;
 unsigned int state_add_uevent_sent:1;
 unsigned int state_remove_uevent_sent:1;
 unsigned int uevent_suppress:1;
};

extern __attribute__((__format__(printf, 2, 3)))
int kobject_set_name(struct kobject *kobj, const char *name, ...);
extern __attribute__((__format__(printf, 2, 0)))
int kobject_set_name_vargs(struct kobject *kobj, const char *fmt,
      va_list vargs);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *kobject_name(const struct kobject *kobj)
{
 return kobj->name;
}

extern void kobject_init(struct kobject *kobj, const struct kobj_type *ktype);
extern __attribute__((__format__(printf, 3, 4))) __attribute__((__warn_unused_result__))
int kobject_add(struct kobject *kobj, struct kobject *parent,
  const char *fmt, ...);
extern __attribute__((__format__(printf, 4, 5))) __attribute__((__warn_unused_result__))
int kobject_init_and_add(struct kobject *kobj,
    const struct kobj_type *ktype, struct kobject *parent,
    const char *fmt, ...);

extern void kobject_del(struct kobject *kobj);

extern struct kobject * __attribute__((__warn_unused_result__)) kobject_create_and_add(const char *name,
      struct kobject *parent);

extern int __attribute__((__warn_unused_result__)) kobject_rename(struct kobject *, const char *new_name);
extern int __attribute__((__warn_unused_result__)) kobject_move(struct kobject *, struct kobject *);

extern struct kobject *kobject_get(struct kobject *kobj);
extern struct kobject * __attribute__((__warn_unused_result__)) kobject_get_unless_zero(
      struct kobject *kobj);
extern void kobject_put(struct kobject *kobj);

extern const void *kobject_namespace(struct kobject *kobj);
extern void kobject_get_ownership(struct kobject *kobj,
      kuid_t *uid, kgid_t *gid);
extern char *kobject_get_path(struct kobject *kobj, gfp_t flag);

struct kobj_type {
 void (*release)(struct kobject *kobj);
 const struct sysfs_ops *sysfs_ops;
 const struct attribute_group **default_groups;
 const struct kobj_ns_type_operations *(*child_ns_type)(struct kobject *kobj);
 const void *(*namespace)(struct kobject *kobj);
 void (*get_ownership)(struct kobject *kobj, kuid_t *uid, kgid_t *gid);
};

struct kobj_uevent_env {
 char *argv[3];
 char *envp[64 /* number of env pointers */];
 int envp_idx;
 char buf[2048 /* buffer for the variables */];
 int buflen;
};

struct kset_uevent_ops {
 int (* const filter)(struct kobject *kobj);
 const char *(* const name)(struct kobject *kobj);
 int (* const uevent)(struct kobject *kobj, struct kobj_uevent_env *env);
};

struct kobj_attribute {
 struct attribute attr;
 ssize_t (*show)(struct kobject *kobj, struct kobj_attribute *attr,
   char *buf);
 ssize_t (*store)(struct kobject *kobj, struct kobj_attribute *attr,
    const char *buf, size_t count);
};

extern const struct sysfs_ops kobj_sysfs_ops;

struct sock;

/**
 * struct kset - a set of kobjects of a specific type, belonging to a specific subsystem.
 *
 * A kset defines a group of kobjects.  They can be individually
 * different "types" but overall these kobjects all want to be grouped
 * together and operated on in the same manner.  ksets are used to
 * define the attribute callbacks and other common events that happen to
 * a kobject.
 *
 * @list: the list of all kobjects for this kset
 * @list_lock: a lock for iterating over the kobjects
 * @kobj: the embedded kobject for this kset (recursion, isn't it fun...)
 * @uevent_ops: the set of uevent operations for this kset.  These are
 * called whenever a kobject has something happen to it so that the kset
 * can add new environment variables, or filter out the uevents if so
 * desired.
 */
struct kset {
 struct list_head list;
 spinlock_t list_lock;
 struct kobject kobj;
 const struct kset_uevent_ops *uevent_ops;
} ;

extern void kset_init(struct kset *kset);
extern int __attribute__((__warn_unused_result__)) kset_register(struct kset *kset);
extern void kset_unregister(struct kset *kset);
extern struct kset * __attribute__((__warn_unused_result__)) kset_create_and_add(const char *name,
      const struct kset_uevent_ops *u,
      struct kobject *parent_kobj);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kset *to_kset(struct kobject *kobj)
{
 return kobj ? ({ void *__mptr = (void *)(kobj); _Static_assert(__builtin_types_compatible_p(typeof(*(kobj)), typeof(((struct kset *)0)->kobj)) || __builtin_types_compatible_p(typeof(*(kobj)), typeof(void)), "pointer type mismatch in container_of()"); ((struct kset *)(__mptr - __builtin_offsetof(struct kset, kobj))); }) : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kset *kset_get(struct kset *k)
{
 return k ? to_kset(kobject_get(&k->kobj)) : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kset_put(struct kset *k)
{
 kobject_put(&k->kobj);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct kobj_type *get_ktype(struct kobject *kobj)
{
 return kobj->ktype;
}

extern struct kobject *kset_find_obj(struct kset *, const char *);

/* The global /sys/kernel/ kobject for people to chain off of */
extern struct kobject *kernel_kobj;
/* The global /sys/kernel/mm/ kobject for people to chain off of */
extern struct kobject *mm_kobj;
/* The global /sys/hypervisor/ kobject for people to chain off of */
extern struct kobject *hypervisor_kobj;
/* The global /sys/power/ kobject for people to chain off of */
extern struct kobject *power_kobj;
/* The global /sys/firmware/ kobject for people to chain off of */
extern struct kobject *firmware_kobj;

int kobject_uevent(struct kobject *kobj, enum kobject_action action);
int kobject_uevent_env(struct kobject *kobj, enum kobject_action action,
   char *envp[]);
int kobject_synth_uevent(struct kobject *kobj, const char *buf, size_t count);

__attribute__((__format__(printf, 2, 3)))
int add_uevent_var(struct kobj_uevent_env *env, const char *format, ...);
# 7 "./include/linux/irqdesc.h" 2


/*
 * Core internal functions to deal with irq descriptors
 */

struct irq_affinity_notify;
struct proc_dir_entry;
struct module;
struct irq_desc;
struct irq_domain;
struct pt_regs;

/**
 * struct irq_desc - interrupt descriptor
 * @irq_common_data:	per irq and chip data passed down to chip functions
 * @kstat_irqs:		irq stats per cpu
 * @handle_irq:		highlevel irq-events handler
 * @action:		the irq action chain
 * @status_use_accessors: status information
 * @core_internal_state__do_not_mess_with_it: core internal status information
 * @depth:		disable-depth, for nested irq_disable() calls
 * @wake_depth:		enable depth, for multiple irq_set_irq_wake() callers
 * @tot_count:		stats field for non-percpu irqs
 * @irq_count:		stats field to detect stalled irqs
 * @last_unhandled:	aging timer for unhandled count
 * @irqs_unhandled:	stats field for spurious unhandled interrupts
 * @threads_handled:	stats field for deferred spurious detection of threaded handlers
 * @threads_handled_last: comparator field for deferred spurious detection of threaded handlers
 * @lock:		locking for SMP
 * @affinity_hint:	hint to user space for preferred irq affinity
 * @affinity_notify:	context for notification of affinity changes
 * @pending_mask:	pending rebalanced interrupts
 * @threads_oneshot:	bitfield to handle shared oneshot threads
 * @threads_active:	number of irqaction threads currently running
 * @wait_for_threads:	wait queue for sync_irq to wait for threaded handlers
 * @nr_actions:		number of installed actions on this descriptor
 * @no_suspend_depth:	number of irqactions on a irq descriptor with
 *			IRQF_NO_SUSPEND set
 * @force_resume_depth:	number of irqactions on a irq descriptor with
 *			IRQF_FORCE_RESUME set
 * @rcu:		rcu head for delayed free
 * @kobj:		kobject used to represent this struct in sysfs
 * @request_mutex:	mutex to protect request/free before locking desc->lock
 * @dir:		/proc/irq/ procfs entry
 * @debugfs_file:	dentry for the debugfs file
 * @name:		flow handler name for /proc/interrupts output
 */
struct irq_desc {
 struct irq_common_data irq_common_data;
 struct irq_data irq_data;
 unsigned int /* nothing */ *kstat_irqs;
 irq_flow_handler_t handle_irq;
 struct irqaction *action; /* IRQ action list */
 unsigned int status_use_accessors;
 unsigned int core_internal_state__do_not_mess_with_it;
 unsigned int depth; /* nested irq disables */
 unsigned int wake_depth; /* nested wake enables */
 unsigned int tot_count;
 unsigned int irq_count; /* For detecting broken IRQs */
 unsigned long last_unhandled; /* Aging timer for unhandled count */
 unsigned int irqs_unhandled;
 atomic_t threads_handled;
 int threads_handled_last;
 raw_spinlock_t lock;
 struct cpumask *percpu_enabled;
 const struct cpumask *percpu_affinity;

 const struct cpumask *affinity_hint;
 struct irq_affinity_notify *affinity_notify;




 unsigned long threads_oneshot;
 atomic_t threads_active;
 wait_queue_head_t wait_for_threads;

 unsigned int nr_actions;
 unsigned int no_suspend_depth;
 unsigned int cond_suspend_depth;
 unsigned int force_resume_depth;


 struct proc_dir_entry *dir;






 struct callback_head rcu;
 struct kobject kobj;

 struct mutex request_mutex;
 int parent_irq;
 struct module *owner;
 const char *name;
} __attribute__((__aligned__(1 << ((6)))));


extern void irq_lock_sparse(void);
extern void irq_unlock_sparse(void);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int irq_desc_kstat_cpu(struct irq_desc *desc,
           unsigned int cpu)
{
 return desc->kstat_irqs ? *({ do { const void /* nothing */ *__vpp_verify = (typeof((desc->kstat_irqs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((desc->kstat_irqs))) *)((desc->kstat_irqs))); (typeof((typeof(*((desc->kstat_irqs))) *)((desc->kstat_irqs)))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }) : 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct irq_desc *irq_data_to_desc(struct irq_data *data)
{
 return ({ void *__mptr = (void *)(data->common); _Static_assert(__builtin_types_compatible_p(typeof(*(data->common)), typeof(((struct irq_desc *)0)->irq_common_data)) || __builtin_types_compatible_p(typeof(*(data->common)), typeof(void)), "pointer type mismatch in container_of()"); ((struct irq_desc *)(__mptr - __builtin_offsetof(struct irq_desc, irq_common_data))); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int irq_desc_get_irq(struct irq_desc *desc)
{
 return desc->irq_data.irq;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct irq_data *irq_desc_get_irq_data(struct irq_desc *desc)
{
 return &desc->irq_data;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct irq_chip *irq_desc_get_chip(struct irq_desc *desc)
{
 return desc->irq_data.chip;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *irq_desc_get_chip_data(struct irq_desc *desc)
{
 return desc->irq_data.chip_data;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *irq_desc_get_handler_data(struct irq_desc *desc)
{
 return desc->irq_common_data.handler_data;
}

/*
 * Architectures call this to let the generic IRQ layer
 * handle an interrupt.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void generic_handle_irq_desc(struct irq_desc *desc)
{
 desc->handle_irq(desc);
}

int handle_irq_desc(struct irq_desc *desc);
int generic_handle_irq(unsigned int irq);
int generic_handle_irq_safe(unsigned int irq);


/*
 * Convert a HW interrupt number to a logical one using a IRQ domain,
 * and handle the result interrupt number. Return -EINVAL if
 * conversion failed.
 */
int generic_handle_domain_irq(struct irq_domain *domain, unsigned int hwirq);
int generic_handle_domain_irq_safe(struct irq_domain *domain, unsigned int hwirq);
int generic_handle_domain_nmi(struct irq_domain *domain, unsigned int hwirq);


/* Test to see if a driver has successfully requested an irq */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int irq_desc_has_action(struct irq_desc *desc)
{
 return desc && desc->action != ((void *)0);
}

/**
 * irq_set_handler_locked - Set irq handler from a locked region
 * @data:	Pointer to the irq_data structure which identifies the irq
 * @handler:	Flow control handler function for this interrupt
 *
 * Sets the handler in the irq descriptor associated to @data.
 *
 * Must be called with irq_desc locked and valid parameters. Typical
 * call site is the irq_set_type() callback.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_handler_locked(struct irq_data *data,
       irq_flow_handler_t handler)
{
 struct irq_desc *desc = irq_data_to_desc(data);

 desc->handle_irq = handler;
}

/**
 * irq_set_chip_handler_name_locked - Set chip, handler and name from a locked region
 * @data:	Pointer to the irq_data structure for which the chip is set
 * @chip:	Pointer to the new irq chip
 * @handler:	Flow control handler function for this interrupt
 * @name:	Name of the interrupt
 *
 * Replace the irq chip at the proper hierarchy level in @data and
 * sets the handler and name in the associated irq descriptor.
 *
 * Must be called with irq_desc locked and valid parameters.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
irq_set_chip_handler_name_locked(struct irq_data *data,
     const struct irq_chip *chip,
     irq_flow_handler_t handler, const char *name)
{
 struct irq_desc *desc = irq_data_to_desc(data);

 desc->handle_irq = handler;
 desc->name = name;
 data->chip = (struct irq_chip *)chip;
}

bool irq_check_status_bit(unsigned int irq, unsigned int bitmask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irq_balancing_disabled(unsigned int irq)
{
 return irq_check_status_bit(irq, (IRQ_PER_CPU | IRQ_NO_BALANCING));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irq_is_percpu(unsigned int irq)
{
 return irq_check_status_bit(irq, IRQ_PER_CPU);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool irq_is_percpu_devid(unsigned int irq)
{
 return irq_check_status_bit(irq, IRQ_PER_CPU_DEVID);
}

void __irq_set_lockdep_class(unsigned int irq, struct lock_class_key *lock_class,
        struct lock_class_key *request_class);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
irq_set_lockdep_class(unsigned int irq, struct lock_class_key *lock_class,
        struct lock_class_key *request_class)
{
 if (0)
  __irq_set_lockdep_class(irq, lock_class, request_class);
}
# 592 "./include/linux/irq.h" 2

/*
 * Pick up the arch-dependent methods:
 */
# 1 "./arch/arm64/include/generated/asm/hw_irq.h" 1
# 1 "./include/asm-generic/hw_irq.h" 1


/*
 * hw_irq.h has internal declarations for the low-level interrupt
 * controller, like the original i8259A.
 * In general, this is not needed for new architectures.
 */
# 2 "./arch/arm64/include/generated/asm/hw_irq.h" 2
# 597 "./include/linux/irq.h" 2
# 608 "./include/linux/irq.h"
struct irqaction;
extern int setup_percpu_irq(unsigned int irq, struct irqaction *new);
extern void remove_percpu_irq(unsigned int irq, struct irqaction *act);





extern int irq_set_affinity_locked(struct irq_data *data,
       const struct cpumask *cpumask, bool force);
extern int irq_set_vcpu_affinity(unsigned int irq, void *vcpu_info);


extern void irq_migrate_all_off_this_cpu(void);
extern int irq_affinity_online_cpu(unsigned int cpu);
# 637 "./include/linux/irq.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_move_irq(struct irq_data *data) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_move_masked_irq(struct irq_data *data) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_force_complete_move(struct irq_desc *desc) { }


extern int no_irq_affinity;


int irq_set_parent(int irq, int parent_irq);







/*
 * Built-in IRQ handlers for various IRQ types,
 * callable via desc->handle_irq()
 */
extern void handle_level_irq(struct irq_desc *desc);
extern void handle_fasteoi_irq(struct irq_desc *desc);
extern void handle_edge_irq(struct irq_desc *desc);
extern void handle_edge_eoi_irq(struct irq_desc *desc);
extern void handle_simple_irq(struct irq_desc *desc);
extern void handle_untracked_irq(struct irq_desc *desc);
extern void handle_percpu_irq(struct irq_desc *desc);
extern void handle_percpu_devid_irq(struct irq_desc *desc);
extern void handle_bad_irq(struct irq_desc *desc);
extern void handle_nested_irq(unsigned int irq);

extern void handle_fasteoi_nmi(struct irq_desc *desc);
extern void handle_percpu_devid_fasteoi_nmi(struct irq_desc *desc);

extern int irq_chip_compose_msi_msg(struct irq_data *data, struct msi_msg *msg);
extern int irq_chip_pm_get(struct irq_data *data);
extern int irq_chip_pm_put(struct irq_data *data);

extern void handle_fasteoi_ack_irq(struct irq_desc *desc);
extern void handle_fasteoi_mask_irq(struct irq_desc *desc);
extern int irq_chip_set_parent_state(struct irq_data *data,
         enum irqchip_irq_state which,
         bool val);
extern int irq_chip_get_parent_state(struct irq_data *data,
         enum irqchip_irq_state which,
         bool *state);
extern void irq_chip_enable_parent(struct irq_data *data);
extern void irq_chip_disable_parent(struct irq_data *data);
extern void irq_chip_ack_parent(struct irq_data *data);
extern int irq_chip_retrigger_hierarchy(struct irq_data *data);
extern void irq_chip_mask_parent(struct irq_data *data);
extern void irq_chip_mask_ack_parent(struct irq_data *data);
extern void irq_chip_unmask_parent(struct irq_data *data);
extern void irq_chip_eoi_parent(struct irq_data *data);
extern int irq_chip_set_affinity_parent(struct irq_data *data,
     const struct cpumask *dest,
     bool force);
extern int irq_chip_set_wake_parent(struct irq_data *data, unsigned int on);
extern int irq_chip_set_vcpu_affinity_parent(struct irq_data *data,
          void *vcpu_info);
extern int irq_chip_set_type_parent(struct irq_data *data, unsigned int type);
extern int irq_chip_request_resources_parent(struct irq_data *data);
extern void irq_chip_release_resources_parent(struct irq_data *data);


/* Handling of unhandled and spurious interrupts: */
extern void note_interrupt(struct irq_desc *desc, irqreturn_t action_ret);


/* Enable/disable irq debugging output: */
extern int noirqdebug_setup(char *str);

/* Checks whether the interrupt can be requested by request_irq(): */
extern int can_request_irq(unsigned int irq, unsigned long irqflags);

/* Dummy irq-chip implementations: */
extern struct irq_chip no_irq_chip;
extern struct irq_chip dummy_irq_chip;

extern void
irq_set_chip_and_handler_name(unsigned int irq, const struct irq_chip *chip,
         irq_flow_handler_t handle, const char *name);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_chip_and_handler(unsigned int irq,
         const struct irq_chip *chip,
         irq_flow_handler_t handle)
{
 irq_set_chip_and_handler_name(irq, chip, handle, ((void *)0));
}

extern int irq_set_percpu_devid(unsigned int irq);
extern int irq_set_percpu_devid_partition(unsigned int irq,
       const struct cpumask *affinity);
extern int irq_get_percpu_devid_partition(unsigned int irq,
       struct cpumask *affinity);

extern void
__irq_set_handler(unsigned int irq, irq_flow_handler_t handle, int is_chained,
    const char *name);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
irq_set_handler(unsigned int irq, irq_flow_handler_t handle)
{
 __irq_set_handler(irq, handle, 0, ((void *)0));
}

/*
 * Set a highlevel chained flow handler for a given IRQ.
 * (a chained handler is automatically enabled and set to
 *  IRQ_NOREQUEST, IRQ_NOPROBE, and IRQ_NOTHREAD)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
irq_set_chained_handler(unsigned int irq, irq_flow_handler_t handle)
{
 __irq_set_handler(irq, handle, 1, ((void *)0));
}

/*
 * Set a highlevel chained flow handler and its data for a given IRQ.
 * (a chained handler is automatically enabled and set to
 *  IRQ_NOREQUEST, IRQ_NOPROBE, and IRQ_NOTHREAD)
 */
void
irq_set_chained_handler_and_data(unsigned int irq, irq_flow_handler_t handle,
     void *data);

void irq_modify_status(unsigned int irq, unsigned long clr, unsigned long set);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_status_flags(unsigned int irq, unsigned long set)
{
 irq_modify_status(irq, 0, set);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_clear_status_flags(unsigned int irq, unsigned long clr)
{
 irq_modify_status(irq, clr, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_noprobe(unsigned int irq)
{
 irq_modify_status(irq, 0, IRQ_NOPROBE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_probe(unsigned int irq)
{
 irq_modify_status(irq, IRQ_NOPROBE, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_nothread(unsigned int irq)
{
 irq_modify_status(irq, 0, IRQ_NOTHREAD);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_thread(unsigned int irq)
{
 irq_modify_status(irq, IRQ_NOTHREAD, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_nested_thread(unsigned int irq, bool nest)
{
 if (nest)
  irq_set_status_flags(irq, IRQ_NESTED_THREAD);
 else
  irq_clear_status_flags(irq, IRQ_NESTED_THREAD);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_set_percpu_devid_flags(unsigned int irq)
{
 irq_set_status_flags(irq,
        IRQ_NOAUTOEN | IRQ_PER_CPU | IRQ_NOTHREAD |
        IRQ_NOPROBE | IRQ_PER_CPU_DEVID);
}

/* Set/get chip/data for an IRQ: */
extern int irq_set_chip(unsigned int irq, const struct irq_chip *chip);
extern int irq_set_handler_data(unsigned int irq, void *data);
extern int irq_set_chip_data(unsigned int irq, void *data);
extern int irq_set_irq_type(unsigned int irq, unsigned int type);
extern int irq_set_msi_desc(unsigned int irq, struct msi_desc *entry);
extern int irq_set_msi_desc_off(unsigned int irq_base, unsigned int irq_offset,
    struct msi_desc *entry);
extern struct irq_data *irq_get_irq_data(unsigned int irq);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct irq_chip *irq_get_chip(unsigned int irq)
{
 struct irq_data *d = irq_get_irq_data(irq);
 return d ? d->chip : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct irq_chip *irq_data_get_irq_chip(struct irq_data *d)
{
 return d->chip;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *irq_get_chip_data(unsigned int irq)
{
 struct irq_data *d = irq_get_irq_data(irq);
 return d ? d->chip_data : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *irq_data_get_irq_chip_data(struct irq_data *d)
{
 return d->chip_data;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *irq_get_handler_data(unsigned int irq)
{
 struct irq_data *d = irq_get_irq_data(irq);
 return d ? d->common->handler_data : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *irq_data_get_irq_handler_data(struct irq_data *d)
{
 return d->common->handler_data;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct msi_desc *irq_get_msi_desc(unsigned int irq)
{
 struct irq_data *d = irq_get_irq_data(irq);
 return d ? d->common->msi_desc : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct msi_desc *irq_data_get_msi_desc(struct irq_data *d)
{
 return d->common->msi_desc;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 irq_get_trigger_type(unsigned int irq)
{
 struct irq_data *d = irq_get_irq_data(irq);
 return d ? irqd_get_trigger_type(d) : 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int irq_common_data_get_node(struct irq_common_data *d)
{

 return d->node;



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int irq_data_get_node(struct irq_data *d)
{
 return irq_common_data_get_node(d->common);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
const struct cpumask *irq_data_get_affinity_mask(struct irq_data *d)
{

 return d->common->affinity;



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_data_update_affinity(struct irq_data *d,
         const struct cpumask *m)
{

 cpumask_copy(d->common->affinity, m);

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const struct cpumask *irq_get_affinity_mask(int irq)
{
 struct irq_data *d = irq_get_irq_data(irq);

 return d ? irq_data_get_affinity_mask(d) : ((void *)0);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
const struct cpumask *irq_data_get_effective_affinity_mask(struct irq_data *d)
{
 return d->common->effective_affinity;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_data_update_effective_affinity(struct irq_data *d,
            const struct cpumask *m)
{
 cpumask_copy(d->common->effective_affinity, m);
}
# 932 "./include/linux/irq.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
const struct cpumask *irq_get_effective_affinity_mask(unsigned int irq)
{
 struct irq_data *d = irq_get_irq_data(irq);

 return d ? irq_data_get_effective_affinity_mask(d) : ((void *)0);
}

unsigned int arch_dynirq_lower_bound(unsigned int from);

int __irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,
        struct module *owner,
        const struct irq_affinity_desc *affinity);

int __devm_irq_alloc_descs(struct device *dev, int irq, unsigned int from,
      unsigned int cnt, int node, struct module *owner,
      const struct irq_affinity_desc *affinity);

/* use macros to avoid needing export.h for THIS_MODULE */
# 981 "./include/linux/irq.h"
void irq_free_descs(unsigned int irq, unsigned int cnt);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_free_desc(unsigned int irq)
{
 irq_free_descs(irq, 1);
}





/**
 * struct irq_chip_regs - register offsets for struct irq_gci
 * @enable:	Enable register offset to reg_base
 * @disable:	Disable register offset to reg_base
 * @mask:	Mask register offset to reg_base
 * @ack:	Ack register offset to reg_base
 * @eoi:	Eoi register offset to reg_base
 * @type:	Type configuration register offset to reg_base
 * @polarity:	Polarity configuration register offset to reg_base
 */
struct irq_chip_regs {
 unsigned long enable;
 unsigned long disable;
 unsigned long mask;
 unsigned long ack;
 unsigned long eoi;
 unsigned long type;
 unsigned long polarity;
};

/**
 * struct irq_chip_type - Generic interrupt chip instance for a flow type
 * @chip:		The real interrupt chip which provides the callbacks
 * @regs:		Register offsets for this chip
 * @handler:		Flow handler associated with this chip
 * @type:		Chip can handle these flow types
 * @mask_cache_priv:	Cached mask register private to the chip type
 * @mask_cache:		Pointer to cached mask register
 *
 * A irq_generic_chip can have several instances of irq_chip_type when
 * it requires different functions and register offsets for different
 * flow types.
 */
struct irq_chip_type {
 struct irq_chip chip;
 struct irq_chip_regs regs;
 irq_flow_handler_t handler;
 u32 type;
 u32 mask_cache_priv;
 u32 *mask_cache;
};

/**
 * struct irq_chip_generic - Generic irq chip data structure
 * @lock:		Lock to protect register and cache data access
 * @reg_base:		Register base address (virtual)
 * @reg_readl:		Alternate I/O accessor (defaults to readl if NULL)
 * @reg_writel:		Alternate I/O accessor (defaults to writel if NULL)
 * @suspend:		Function called from core code on suspend once per
 *			chip; can be useful instead of irq_chip::suspend to
 *			handle chip details even when no interrupts are in use
 * @resume:		Function called from core code on resume once per chip;
 *			can be useful instead of irq_chip::suspend to handle
 *			chip details even when no interrupts are in use
 * @irq_base:		Interrupt base nr for this chip
 * @irq_cnt:		Number of interrupts handled by this chip
 * @mask_cache:		Cached mask register shared between all chip types
 * @type_cache:		Cached type register
 * @polarity_cache:	Cached polarity register
 * @wake_enabled:	Interrupt can wakeup from suspend
 * @wake_active:	Interrupt is marked as an wakeup from suspend source
 * @num_ct:		Number of available irq_chip_type instances (usually 1)
 * @private:		Private data for non generic chip callbacks
 * @installed:		bitfield to denote installed interrupts
 * @unused:		bitfield to denote unused interrupts
 * @domain:		irq domain pointer
 * @list:		List head for keeping track of instances
 * @chip_types:		Array of interrupt irq_chip_types
 *
 * Note, that irq_chip_generic can have multiple irq_chip_type
 * implementations which can be associated to a particular irq line of
 * an irq_chip_generic instance. That allows to share and protect
 * state in an irq_chip_generic instance when we need to implement
 * different flow mechanisms (level/edge) for it.
 */
struct irq_chip_generic {
 raw_spinlock_t lock;
 void *reg_base;
 u32 (*reg_readl)(void *addr);
 void (*reg_writel)(u32 val, void *addr);
 void (*suspend)(struct irq_chip_generic *gc);
 void (*resume)(struct irq_chip_generic *gc);
 unsigned int irq_base;
 unsigned int irq_cnt;
 u32 mask_cache;
 u32 type_cache;
 u32 polarity_cache;
 u32 wake_enabled;
 u32 wake_active;
 unsigned int num_ct;
 void *private;
 unsigned long installed;
 unsigned long unused;
 struct irq_domain *domain;
 struct list_head list;
 struct irq_chip_type chip_types[];
};

/**
 * enum irq_gc_flags - Initialization flags for generic irq chips
 * @IRQ_GC_INIT_MASK_CACHE:	Initialize the mask_cache by reading mask reg
 * @IRQ_GC_INIT_NESTED_LOCK:	Set the lock class of the irqs to nested for
 *				irq chips which need to call irq_set_wake() on
 *				the parent irq. Usually GPIO implementations
 * @IRQ_GC_MASK_CACHE_PER_TYPE:	Mask cache is chip type private
 * @IRQ_GC_NO_MASK:		Do not calculate irq_data->mask
 * @IRQ_GC_BE_IO:		Use big-endian register accesses (default: LE)
 */
enum irq_gc_flags {
 IRQ_GC_INIT_MASK_CACHE = 1 << 0,
 IRQ_GC_INIT_NESTED_LOCK = 1 << 1,
 IRQ_GC_MASK_CACHE_PER_TYPE = 1 << 2,
 IRQ_GC_NO_MASK = 1 << 3,
 IRQ_GC_BE_IO = 1 << 4,
};

/*
 * struct irq_domain_chip_generic - Generic irq chip data structure for irq domains
 * @irqs_per_chip:	Number of interrupts per chip
 * @num_chips:		Number of chips
 * @irq_flags_to_set:	IRQ* flags to set on irq setup
 * @irq_flags_to_clear:	IRQ* flags to clear on irq setup
 * @gc_flags:		Generic chip specific setup flags
 * @gc:			Array of pointers to generic interrupt chips
 */
struct irq_domain_chip_generic {
 unsigned int irqs_per_chip;
 unsigned int num_chips;
 unsigned int irq_flags_to_clear;
 unsigned int irq_flags_to_set;
 enum irq_gc_flags gc_flags;
 struct irq_chip_generic *gc[];
};

/* Generic chip callback functions */
void irq_gc_noop(struct irq_data *d);
void irq_gc_mask_disable_reg(struct irq_data *d);
void irq_gc_mask_set_bit(struct irq_data *d);
void irq_gc_mask_clr_bit(struct irq_data *d);
void irq_gc_unmask_enable_reg(struct irq_data *d);
void irq_gc_ack_set_bit(struct irq_data *d);
void irq_gc_ack_clr_bit(struct irq_data *d);
void irq_gc_mask_disable_and_ack_set(struct irq_data *d);
void irq_gc_eoi(struct irq_data *d);
int irq_gc_set_wake(struct irq_data *d, unsigned int on);

/* Setup functions for irq_chip_generic */
int irq_map_generic_chip(struct irq_domain *d, unsigned int virq,
    irq_hw_number_t hw_irq);
void irq_unmap_generic_chip(struct irq_domain *d, unsigned int virq);
struct irq_chip_generic *
irq_alloc_generic_chip(const char *name, int nr_ct, unsigned int irq_base,
         void *reg_base, irq_flow_handler_t handler);
void irq_setup_generic_chip(struct irq_chip_generic *gc, u32 msk,
       enum irq_gc_flags flags, unsigned int clr,
       unsigned int set);
int irq_setup_alt_chip(struct irq_data *d, unsigned int type);
void irq_remove_generic_chip(struct irq_chip_generic *gc, u32 msk,
        unsigned int clr, unsigned int set);

struct irq_chip_generic *
devm_irq_alloc_generic_chip(struct device *dev, const char *name, int num_ct,
       unsigned int irq_base, void *reg_base,
       irq_flow_handler_t handler);
int devm_irq_setup_generic_chip(struct device *dev, struct irq_chip_generic *gc,
    u32 msk, enum irq_gc_flags flags,
    unsigned int clr, unsigned int set);

struct irq_chip_generic *irq_get_domain_generic_chip(struct irq_domain *d, unsigned int hw_irq);

int __irq_alloc_domain_generic_chips(struct irq_domain *d, int irqs_per_chip,
         int num_ct, const char *name,
         irq_flow_handler_t handler,
         unsigned int clr, unsigned int set,
         enum irq_gc_flags flags);
# 1175 "./include/linux/irq.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_free_generic_chip(struct irq_chip_generic *gc)
{
 kfree(gc);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_destroy_generic_chip(struct irq_chip_generic *gc,
         u32 msk, unsigned int clr,
         unsigned int set)
{
 irq_remove_generic_chip(gc, msk, clr, set);
 irq_free_generic_chip(gc);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct irq_chip_type *irq_data_get_chip_type(struct irq_data *d)
{
 return ({ void *__mptr = (void *)(d->chip); _Static_assert(__builtin_types_compatible_p(typeof(*(d->chip)), typeof(((struct irq_chip_type *)0)->chip)) || __builtin_types_compatible_p(typeof(*(d->chip)), typeof(void)), "pointer type mismatch in container_of()"); ((struct irq_chip_type *)(__mptr - __builtin_offsetof(struct irq_chip_type, chip))); });
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_gc_lock(struct irq_chip_generic *gc)
{
 _raw_spin_lock(&gc->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_gc_unlock(struct irq_chip_generic *gc)
{
 _raw_spin_unlock(&gc->lock);
}





/*
 * The irqsave variants are for usage in non interrupt code. Do not use
 * them in irq_chip callbacks. Use irq_gc_lock() instead.
 */






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irq_reg_writel(struct irq_chip_generic *gc,
      u32 val, int reg_offset)
{
 if (gc->reg_writel)
  gc->reg_writel(val, gc->reg_base + reg_offset);
 else
  writel(val, gc->reg_base + reg_offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u32 irq_reg_readl(struct irq_chip_generic *gc,
    int reg_offset)
{
 if (gc->reg_readl)
  return gc->reg_readl(gc->reg_base + reg_offset);
 else
  return readl(gc->reg_base + reg_offset);
}

struct irq_matrix;
struct irq_matrix *irq_alloc_matrix(unsigned int matrix_bits,
        unsigned int alloc_start,
        unsigned int alloc_end);
void irq_matrix_online(struct irq_matrix *m);
void irq_matrix_offline(struct irq_matrix *m);
void irq_matrix_assign_system(struct irq_matrix *m, unsigned int bit, bool replace);
int irq_matrix_reserve_managed(struct irq_matrix *m, const struct cpumask *msk);
void irq_matrix_remove_managed(struct irq_matrix *m, const struct cpumask *msk);
int irq_matrix_alloc_managed(struct irq_matrix *m, const struct cpumask *msk,
    unsigned int *mapped_cpu);
void irq_matrix_reserve(struct irq_matrix *m);
void irq_matrix_remove_reserved(struct irq_matrix *m);
int irq_matrix_alloc(struct irq_matrix *m, const struct cpumask *msk,
       bool reserved, unsigned int *mapped_cpu);
void irq_matrix_free(struct irq_matrix *m, unsigned int cpu,
       unsigned int bit, bool managed);
void irq_matrix_assign(struct irq_matrix *m, unsigned int bit);
unsigned int irq_matrix_available(struct irq_matrix *m, bool cpudown);
unsigned int irq_matrix_allocated(struct irq_matrix *m);
unsigned int irq_matrix_reserved(struct irq_matrix *m);
void irq_matrix_debug_show(struct seq_file *sf, struct irq_matrix *m, int ind);

/* Contrary to Linux irqs, for hardware irqs the irq number 0 is valid */

irq_hw_number_t ipi_get_hwirq(unsigned int irq, unsigned int cpu);
int __ipi_send_single(struct irq_desc *desc, unsigned int cpu);
int __ipi_send_mask(struct irq_desc *desc, const struct cpumask *dest);
int ipi_send_single(unsigned int virq, unsigned int cpu);
int ipi_send_mask(unsigned int virq, const struct cpumask *dest);
# 18 "./include/asm-generic/hardirq.h" 2
# 18 "./arch/arm64/include/asm/hardirq.h" 2



struct nmi_ctx {
 u64 hcr;
 unsigned int cnt;
};

extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct nmi_ctx) nmi_contexts;
# 88 "./arch/arm64/include/asm/hardirq.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ack_bad_irq(unsigned int irq)
{
 extern unsigned long irq_err_count;
 irq_err_count++;
}
# 12 "./include/linux/hardirq.h" 2

extern void synchronize_irq(unsigned int irq);
extern bool synchronize_hardirq(unsigned int irq);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __rcu_irq_enter_check_tick(void) { }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void rcu_irq_enter_check_tick(void)
{
 if (context_tracking_enabled())
  __rcu_irq_enter_check_tick();
}

/*
 * It is safe to do non-atomic ops on ->hardirq_context,
 * because NMI handlers may not preempt and the ops are
 * always balanced, so the interrupted value of ->hardirq_context
 * will always be restored.
 */







/*
 * Like __irq_enter() without time accounting for fast
 * interrupts, e.g. reschedule IPI where time accounting
 * is more expensive than the actual interrupt.
 */






/*
 * Enter irq context (on NO_HZ, update jiffies):
 */
void irq_enter(void);
/*
 * Like irq_enter(), but RCU is already watching.
 */
void irq_enter_rcu(void);

/*
 * Exit irq context without processing softirqs:
 */







/*
 * Like __irq_exit() without time accounting
 */






/*
 * Exit irq context and process softirqs if needed:
 */
void irq_exit(void);

/*
 * Like irq_exit(), but return with RCU watching.
 */
void irq_exit_rcu(void);






/*
 * NMI vs Tracing
 * --------------
 *
 * We must not land in a tracer until (or after) we've changed preempt_count
 * such that in_nmi() becomes true. To that effect all NMI C entry points must
 * be marked 'notrace' and call nmi_enter() as soon as possible.
 */

/*
 * nmi_enter() can nest up to 15 times; see NMI_BITS.
 */
# 12 "./include/linux/interrupt.h" 2
# 23 "./include/linux/interrupt.h"
/*
 * These correspond to the IORESOURCE_IRQ_* defines in
 * linux/ioport.h to select the interrupt line behaviour.  When
 * requesting an interrupt without specifying a IRQF_TRIGGER, the
 * setting should be assumed to be "as already configured", which
 * may be as per machine or firmware initialisation.
 */
# 39 "./include/linux/interrupt.h"
/*
 * These flags used only by the kernel as part of the
 * irq handling routines.
 *
 * IRQF_SHARED - allow sharing the irq among several devices
 * IRQF_PROBE_SHARED - set by callers when they expect sharing mismatches to occur
 * IRQF_TIMER - Flag to mark this interrupt as timer interrupt
 * IRQF_PERCPU - Interrupt is per cpu
 * IRQF_NOBALANCING - Flag to exclude this interrupt from irq balancing
 * IRQF_IRQPOLL - Interrupt is used for polling (only the interrupt that is
 *                registered first in a shared interrupt is considered for
 *                performance reasons)
 * IRQF_ONESHOT - Interrupt is not reenabled after the hardirq handler finished.
 *                Used by threaded interrupts which need to keep the
 *                irq line disabled until the threaded handler has been run.
 * IRQF_NO_SUSPEND - Do not disable this IRQ during suspend.  Does not guarantee
 *                   that this interrupt will wake the system from a suspended
 *                   state.  See Documentation/power/suspend-and-interrupts.rst
 * IRQF_FORCE_RESUME - Force enable it on resume even if IRQF_NO_SUSPEND is set
 * IRQF_NO_THREAD - Interrupt cannot be threaded
 * IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device
 *                resume time.
 * IRQF_COND_SUSPEND - If the IRQ is shared with a NO_SUSPEND user, execute this
 *                interrupt handler after suspending interrupts. For system
 *                wakeup devices users need to implement wakeup detection in
 *                their interrupt handlers.
 * IRQF_NO_AUTOEN - Don't enable IRQ or NMI automatically when users request it.
 *                Users will enable it explicitly by enable_irq() or enable_nmi()
 *                later.
 * IRQF_NO_DEBUG - Exclude from runnaway detection for IPI and similar handlers,
 *		   depends on IRQF_PERCPU.
 */
# 88 "./include/linux/interrupt.h"
/*
 * These values can be returned by request_any_context_irq() and
 * describe the context the interrupt will be run in.
 *
 * IRQC_IS_HARDIRQ - interrupt runs in hardirq context
 * IRQC_IS_NESTED - interrupt runs in a nested threaded context
 */
enum {
 IRQC_IS_HARDIRQ = 0,
 IRQC_IS_NESTED,
};

typedef irqreturn_t (*irq_handler_t)(int, void *);

/**
 * struct irqaction - per interrupt action descriptor
 * @handler:	interrupt handler function
 * @name:	name of the device
 * @dev_id:	cookie to identify the device
 * @percpu_dev_id:	cookie to identify the device
 * @next:	pointer to the next irqaction for shared interrupts
 * @irq:	interrupt number
 * @flags:	flags (see IRQF_* above)
 * @thread_fn:	interrupt handler function for threaded interrupts
 * @thread:	thread pointer for threaded interrupts
 * @secondary:	pointer to secondary irqaction (force threading)
 * @thread_flags:	flags related to @thread
 * @thread_mask:	bitmask for keeping track of @thread activity
 * @dir:	pointer to the proc/irq/NN/name entry
 */
struct irqaction {
 irq_handler_t handler;
 void *dev_id;
 void /* nothing */ *percpu_dev_id;
 struct irqaction *next;
 irq_handler_t thread_fn;
 struct task_struct *thread;
 struct irqaction *secondary;
 unsigned int irq;
 unsigned int flags;
 unsigned long thread_flags;
 unsigned long thread_mask;
 const char *name;
 struct proc_dir_entry *dir;
} __attribute__((__aligned__(1 << ((6)))));

extern irqreturn_t no_action(int cpl, void *dev_id);

/*
 * If a (PCI) device interrupt is not connected we set dev->irq to
 * IRQ_NOTCONNECTED. This causes request_irq() to fail with -ENOTCONN, so we
 * can distingiush that case from other error returns.
 *
 * 0x80000000 is guaranteed to be outside the available range of interrupts
 * and easy to distinguish from other possible incorrect values.
 */


extern int __attribute__((__warn_unused_result__))
request_threaded_irq(unsigned int irq, irq_handler_t handler,
       irq_handler_t thread_fn,
       unsigned long flags, const char *name, void *dev);

/**
 * request_irq - Add a handler for an interrupt line
 * @irq:	The interrupt line to allocate
 * @handler:	Function to be called when the IRQ occurs.
 *		Primary handler for threaded interrupts
 *		If NULL, the default primary handler is installed
 * @flags:	Handling flags
 * @name:	Name of the device generating this interrupt
 * @dev:	A cookie passed to the handler function
 *
 * This call allocates an interrupt and establishes a handler; see
 * the documentation for request_threaded_irq() for details.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__))
request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,
     const char *name, void *dev)
{
 return request_threaded_irq(irq, handler, ((void *)0), flags, name, dev);
}

extern int __attribute__((__warn_unused_result__))
request_any_context_irq(unsigned int irq, irq_handler_t handler,
   unsigned long flags, const char *name, void *dev_id);

extern int __attribute__((__warn_unused_result__))
__request_percpu_irq(unsigned int irq, irq_handler_t handler,
       unsigned long flags, const char *devname,
       void /* nothing */ *percpu_dev_id);

extern int __attribute__((__warn_unused_result__))
request_nmi(unsigned int irq, irq_handler_t handler, unsigned long flags,
     const char *name, void *dev);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__))
request_percpu_irq(unsigned int irq, irq_handler_t handler,
     const char *devname, void /* nothing */ *percpu_dev_id)
{
 return __request_percpu_irq(irq, handler, 0,
        devname, percpu_dev_id);
}

extern int __attribute__((__warn_unused_result__))
request_percpu_nmi(unsigned int irq, irq_handler_t handler,
     const char *devname, void /* nothing */ *dev);

extern const void *free_irq(unsigned int, void *);
extern void free_percpu_irq(unsigned int, void /* nothing */ *);

extern const void *free_nmi(unsigned int irq, void *dev_id);
extern void free_percpu_nmi(unsigned int irq, void /* nothing */ *percpu_dev_id);

struct device;

extern int __attribute__((__warn_unused_result__))
devm_request_threaded_irq(struct device *dev, unsigned int irq,
     irq_handler_t handler, irq_handler_t thread_fn,
     unsigned long irqflags, const char *devname,
     void *dev_id);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__))
devm_request_irq(struct device *dev, unsigned int irq, irq_handler_t handler,
   unsigned long irqflags, const char *devname, void *dev_id)
{
 return devm_request_threaded_irq(dev, irq, handler, ((void *)0), irqflags,
      devname, dev_id);
}

extern int __attribute__((__warn_unused_result__))
devm_request_any_context_irq(struct device *dev, unsigned int irq,
   irq_handler_t handler, unsigned long irqflags,
   const char *devname, void *dev_id);

extern void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id);

bool irq_has_action(unsigned int irq);
extern void disable_irq_nosync(unsigned int irq);
extern bool disable_hardirq(unsigned int irq);
extern void disable_irq(unsigned int irq);
extern void disable_percpu_irq(unsigned int irq);
extern void enable_irq(unsigned int irq);
extern void enable_percpu_irq(unsigned int irq, unsigned int type);
extern bool irq_percpu_is_enabled(unsigned int irq);
extern void irq_wake_thread(unsigned int irq, void *dev_id);

extern void disable_nmi_nosync(unsigned int irq);
extern void disable_percpu_nmi(unsigned int irq);
extern void enable_nmi(unsigned int irq);
extern void enable_percpu_nmi(unsigned int irq, unsigned int type);
extern int prepare_percpu_nmi(unsigned int irq);
extern void teardown_percpu_nmi(unsigned int irq);

extern int irq_inject_interrupt(unsigned int irq);

/* The following three functions are for the core kernel use only. */
extern void suspend_device_irqs(void);
extern void resume_device_irqs(void);
extern void rearm_wake_irq(unsigned int irq);

/**
 * struct irq_affinity_notify - context for notification of IRQ affinity changes
 * @irq:		Interrupt to which notification applies
 * @kref:		Reference count, for internal use
 * @work:		Work item, for internal use
 * @notify:		Function to be called on change.  This will be
 *			called in process context.
 * @release:		Function to be called on release.  This will be
 *			called in process context.  Once registered, the
 *			structure must only be freed when this function is
 *			called or later.
 */
struct irq_affinity_notify {
 unsigned int irq;
 struct kref kref;
 struct work_struct work;
 void (*notify)(struct irq_affinity_notify *, const cpumask_t *mask);
 void (*release)(struct kref *ref);
};



/**
 * struct irq_affinity - Description for automatic irq affinity assignements
 * @pre_vectors:	Don't apply affinity to @pre_vectors at beginning of
 *			the MSI(-X) vector space
 * @post_vectors:	Don't apply affinity to @post_vectors at end of
 *			the MSI(-X) vector space
 * @nr_sets:		The number of interrupt sets for which affinity
 *			spreading is required
 * @set_size:		Array holding the size of each interrupt set
 * @calc_sets:		Callback for calculating the number and size
 *			of interrupt sets
 * @priv:		Private data for usage by @calc_sets, usually a
 *			pointer to driver/device specific data.
 */
struct irq_affinity {
 unsigned int pre_vectors;
 unsigned int post_vectors;
 unsigned int nr_sets;
 unsigned int set_size[4];
 void (*calc_sets)(struct irq_affinity *, unsigned int nvecs);
 void *priv;
};

/**
 * struct irq_affinity_desc - Interrupt affinity descriptor
 * @mask:	cpumask to hold the affinity assignment
 * @is_managed: 1 if the interrupt is managed internally
 */
struct irq_affinity_desc {
 struct cpumask mask;
 unsigned int is_managed : 1;
};



extern cpumask_var_t irq_default_affinity;

extern int irq_set_affinity(unsigned int irq, const struct cpumask *cpumask);
extern int irq_force_affinity(unsigned int irq, const struct cpumask *cpumask);

extern int irq_can_set_affinity(unsigned int irq);
extern int irq_select_affinity(unsigned int irq);

extern int __irq_apply_affinity_hint(unsigned int irq, const struct cpumask *m,
         bool setaffinity);

/**
 * irq_update_affinity_hint - Update the affinity hint
 * @irq:	Interrupt to update
 * @m:		cpumask pointer (NULL to clear the hint)
 *
 * Updates the affinity hint, but does not change the affinity of the interrupt.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
irq_update_affinity_hint(unsigned int irq, const struct cpumask *m)
{
 return __irq_apply_affinity_hint(irq, m, false);
}

/**
 * irq_set_affinity_and_hint - Update the affinity hint and apply the provided
 *			     cpumask to the interrupt
 * @irq:	Interrupt to update
 * @m:		cpumask pointer (NULL to clear the hint)
 *
 * Updates the affinity hint and if @m is not NULL it applies it as the
 * affinity of that interrupt.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
irq_set_affinity_and_hint(unsigned int irq, const struct cpumask *m)
{
 return __irq_apply_affinity_hint(irq, m, true);
}

/*
 * Deprecated. Use irq_update_affinity_hint() or irq_set_affinity_and_hint()
 * instead.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m)
{
 return irq_set_affinity_and_hint(irq, m);
}

extern int irq_update_affinity_desc(unsigned int irq,
        struct irq_affinity_desc *affinity);

extern int
irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify);

struct irq_affinity_desc *
irq_create_affinity_masks(unsigned int nvec, struct irq_affinity *affd);

unsigned int irq_calc_affinity_vectors(unsigned int minvec, unsigned int maxvec,
           const struct irq_affinity *affd);
# 430 "./include/linux/interrupt.h"
/*
 * Special lockdep variants of irq disabling/enabling.
 * These should be used for locking constructs that
 * know that a particular irq context which is disabled,
 * and which is the only irq-context user of a lock,
 * that it's safe to take the lock in the irq-disabled
 * section without disabling hardirqs.
 *
 * On !CONFIG_LOCKDEP they are equivalent to the normal
 * irq disable/enable methods.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void disable_irq_nosync_lockdep(unsigned int irq)
{
 disable_irq_nosync(irq);



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void disable_irq_nosync_lockdep_irqsave(unsigned int irq, unsigned long *flags)
{
 disable_irq_nosync(irq);



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void disable_irq_lockdep(unsigned int irq)
{
 disable_irq(irq);



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void enable_irq_lockdep(unsigned int irq)
{



 enable_irq(irq);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void enable_irq_lockdep_irqrestore(unsigned int irq, unsigned long *flags)
{



 enable_irq(irq);
}

/* IRQ wakeup (PM) control: */
extern int irq_set_irq_wake(unsigned int irq, unsigned int on);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int enable_irq_wake(unsigned int irq)
{
 return irq_set_irq_wake(irq, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int disable_irq_wake(unsigned int irq)
{
 return irq_set_irq_wake(irq, 0);
}

/*
 * irq_get_irqchip_state/irq_set_irqchip_state specific flags
 */
enum irqchip_irq_state {
 IRQCHIP_STATE_PENDING, /* Is interrupt pending? */
 IRQCHIP_STATE_ACTIVE, /* Is interrupt in progress? */
 IRQCHIP_STATE_MASKED, /* Is interrupt masked? */
 IRQCHIP_STATE_LINE_LEVEL, /* Is IRQ line high? */
};

extern int irq_get_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
     bool *state);
extern int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
     bool state);





extern struct static_key_false force_irqthreads_key;
# 532 "./include/linux/interrupt.h"
/* Some architectures might implement lazy enabling/disabling of
 * interrupts. In some cases, such as stop_machine, we might want
 * to ensure that after a local_irq_disable(), interrupts have
 * really been disabled in hardware. Such architectures need to
 * implement the following hook.
 */




/* PLEASE, avoid to allocate new softirqs, if you need not _really_ high
   frequency threaded job scheduling. For almost all the purposes
   tasklets are more than enough. F.e. all serial device BHs et
   al. should be converted to tasklets, not to softirqs.
 */

enum
{
 HI_SOFTIRQ=0,
 TIMER_SOFTIRQ,
 NET_TX_SOFTIRQ,
 NET_RX_SOFTIRQ,
 BLOCK_SOFTIRQ,
 IRQ_POLL_SOFTIRQ,
 TASKLET_SOFTIRQ,
 SCHED_SOFTIRQ,
 HRTIMER_SOFTIRQ,
 RCU_SOFTIRQ, /* Preferable RCU should always be the last softirq */

 NR_SOFTIRQS
};

/*
 * The following vectors can be safely ignored after ksoftirqd is parked:
 *
 * _ RCU:
 * 	1) rcutree_migrate_callbacks() migrates the queue.
 * 	2) rcu_report_dead() reports the final quiescent states.
 *
 * _ IRQ_POLL: irq_poll_cpu_dead() migrates the queue
 */


/* map softirq index to softirq name. update 'softirq_to_name' in
 * kernel/softirq.c when adding a new softirq.
 */
extern const char * const softirq_to_name[NR_SOFTIRQS];

/* softirq mask and active fields moved to irq_cpustat_t in
 * asm/hardirq.h to get better cache usage.  KAO
 */

struct softirq_action
{
 void (*action)(struct softirq_action *);
};

           void do_softirq(void);
           void __do_softirq(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void do_softirq_post_smp_call_flush(unsigned int unused)
{
 do_softirq();
}


extern void open_softirq(int nr, void (*action)(struct softirq_action *));
extern void softirq_init(void);
extern void __raise_softirq_irqoff(unsigned int nr);

extern void raise_softirq_irqoff(unsigned int nr);
extern void raise_softirq(unsigned int nr);

extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct task_struct *) ksoftirqd;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct task_struct *this_cpu_ksoftirqd(void)
{
 return ({ typeof(ksoftirqd) pscr_ret__; do { const void /* nothing */ *__vpp_verify = (typeof((&(ksoftirqd)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(ksoftirqd)) { case 1: pscr_ret__ = ({ typeof(ksoftirqd) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(ksoftirqd))__percpu_read_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(ksoftirqd)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd))); (typeof((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; case 2: pscr_ret__ = ({ typeof(ksoftirqd) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(ksoftirqd))__percpu_read_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(ksoftirqd)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd))); (typeof((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; case 4: pscr_ret__ = ({ typeof(ksoftirqd) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(ksoftirqd))__percpu_read_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(ksoftirqd)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd))); (typeof((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; case 8: pscr_ret__ = ({ typeof(ksoftirqd) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(ksoftirqd))__percpu_read_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(ksoftirqd)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd))); (typeof((typeof(*(&(ksoftirqd))) *)(&(ksoftirqd)))) (__ptr + ((__kern_my_cpu_offset()))); }); })); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); __retval; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; });
}

/* Tasklets --- multithreaded analogue of BHs.

   This API is deprecated. Please consider using threaded IRQs instead:
   https://lore.kernel.org/lkml/20200716081538.2sivhkj4hcyrusem@linutronix.de

   Main feature differing them of generic softirqs: tasklet
   is running only on one CPU simultaneously.

   Main feature differing them of BHs: different tasklets
   may be run simultaneously on different CPUs.

   Properties:
   * If tasklet_schedule() is called, then tasklet is guaranteed
     to be executed on some cpu at least once after this.
   * If the tasklet is already scheduled, but its execution is still not
     started, it will be executed only once.
   * If this tasklet is already running on another CPU (or schedule is called
     from tasklet itself), it is rescheduled for later.
   * Tasklet is strictly serialized wrt itself, but not
     wrt another tasklets. If client needs some intertask synchronization,
     he makes it with spinlocks.
 */

struct tasklet_struct
{
 struct tasklet_struct *next;
 unsigned long state;
 atomic_t count;
 bool use_callback;
 union {
  void (*func)(unsigned long data);
  void (*callback)(struct tasklet_struct *t);
 };
 unsigned long data;
};
# 680 "./include/linux/interrupt.h"
enum
{
 TASKLET_STATE_SCHED, /* Tasklet is scheduled for execution */
 TASKLET_STATE_RUN /* Tasklet is running (SMP only) */
};


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int tasklet_trylock(struct tasklet_struct *t)
{
 return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
}

void tasklet_unlock(struct tasklet_struct *t);
void tasklet_unlock_wait(struct tasklet_struct *t);
void tasklet_unlock_spin_wait(struct tasklet_struct *t);
# 703 "./include/linux/interrupt.h"
extern void __tasklet_schedule(struct tasklet_struct *t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tasklet_schedule(struct tasklet_struct *t)
{
 if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
  __tasklet_schedule(t);
}

extern void __tasklet_hi_schedule(struct tasklet_struct *t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tasklet_hi_schedule(struct tasklet_struct *t)
{
 if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
  __tasklet_hi_schedule(t);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tasklet_disable_nosync(struct tasklet_struct *t)
{
 atomic_inc(&t->count);
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
}

/*
 * Do not use in new code. Disabling tasklets from atomic contexts is
 * error prone and should be avoided.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tasklet_disable_in_atomic(struct tasklet_struct *t)
{
 tasklet_disable_nosync(t);
 tasklet_unlock_spin_wait(t);
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tasklet_disable(struct tasklet_struct *t)
{
 tasklet_disable_nosync(t);
 tasklet_unlock_wait(t);
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tasklet_enable(struct tasklet_struct *t)
{
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 atomic_dec(&t->count);
}

extern void tasklet_kill(struct tasklet_struct *t);
extern void tasklet_init(struct tasklet_struct *t,
    void (*func)(unsigned long), unsigned long data);
extern void tasklet_setup(struct tasklet_struct *t,
     void (*callback)(struct tasklet_struct *));

/*
 * Autoprobing for irqs:
 *
 * probe_irq_on() and probe_irq_off() provide robust primitives
 * for accurate IRQ probing during kernel initialization.  They are
 * reasonably simple to use, are not "fooled" by spurious interrupts,
 * and, unlike other attempts at IRQ probing, they do not get hung on
 * stuck interrupts (such as unused PS2 mouse interfaces on ASUS boards).
 *
 * For reasonably foolproof probing, use them as follows:
 *
 * 1. clear and/or mask the device's internal interrupt.
 * 2. sti();
 * 3. irqs = probe_irq_on();      // "take over" all unassigned idle IRQs
 * 4. enable the device and cause it to trigger an interrupt.
 * 5. wait for the device to interrupt, using non-intrusive polling or a delay.
 * 6. irq = probe_irq_off(irqs);  // get IRQ number, 0=none, negative=multiple
 * 7. service the device to clear its pending interrupt.
 * 8. loop again if paranoia is required.
 *
 * probe_irq_on() returns a mask of allocated irq's.
 *
 * probe_irq_off() takes the mask as a parameter,
 * and returns the irq number which occurred,
 * or zero if none occurred, or a negative irq number
 * if more than one irq occurred.
 */
# 797 "./include/linux/interrupt.h"
extern unsigned long probe_irq_on(void); /* returns 0 on failure */
extern int probe_irq_off(unsigned long); /* returns 0 or negative on failure */
extern unsigned int probe_irq_mask(unsigned long); /* returns mask of ISA interrupts */



/* Initialize /proc/irq/ */
extern void init_irq_proc(void);
# 817 "./include/linux/interrupt.h"
struct seq_file;
int show_interrupts(struct seq_file *p, void *v);
int arch_show_interrupts(struct seq_file *p, int prec);

extern int early_irq_init(void);
extern int arch_probe_nr_irqs(void);
extern int arch_early_irq_init(void);

/*
 * We want to know which function is an entrypoint of a hardirq or a softirq.
 */
# 6 "./include/linux/trace_recursion.h" 2
# 11 "./include/linux/ftrace.h" 2
# 1 "./include/linux/trace_clock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * 3 trace clock variants, with differing scalability/precision
 * tradeoffs:
 *
 *  -   local: CPU-local trace clock
 *  -  medium: scalable global clock with some jitter
 *  -  global: globally monotonic, serialized clock
 */



# 1 "./arch/arm64/include/generated/asm/trace_clock.h" 1
# 1 "./include/asm-generic/trace_clock.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/*
 * Arch-specific trace clocks.
 */

/*
 * Additional trace clocks added to the trace_clocks
 * array in kernel/trace/trace.c
 * None if the architecture has not defined it.
 */
# 2 "./arch/arm64/include/generated/asm/trace_clock.h" 2
# 17 "./include/linux/trace_clock.h" 2

extern u64 __attribute__((__no_instrument_function__)) trace_clock_local(void);
extern u64 __attribute__((__no_instrument_function__)) trace_clock(void);
extern u64 __attribute__((__no_instrument_function__)) trace_clock_jiffies(void);
extern u64 __attribute__((__no_instrument_function__)) trace_clock_global(void);
extern u64 __attribute__((__no_instrument_function__)) trace_clock_counter(void);
# 12 "./include/linux/ftrace.h" 2

# 1 "./include/linux/kallsyms.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/* Rewritten and vastly simplified by Rusty Russell for in-kernel
 * module loader:
 *   Copyright 2002 Rusty Russell <rusty@rustcorp.com.au> IBM Corporation
 */




# 1 "./include/linux/buildid.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







int build_id_parse(struct vm_area_struct *vma, unsigned char *build_id,
     __u32 *size);
int build_id_parse_buf(const void *buf, unsigned char *build_id, u32 buf_size);


extern unsigned char vmlinux_build_id[20];
void init_vmlinux_build_id(void);
# 11 "./include/linux/kallsyms.h" 2


# 1 "./include/linux/mm.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 15 "./include/linux/mm.h"
# 1 "./include/linux/mmap_lock.h" 1







# 1 "./include/linux/tracepoint-defs.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * File can be included directly by headers who only want to access
 * tracepoint->key to guard out of line trace calls, or the definition of
 * trace_print_flags{_u64}. Otherwise linux/tracepoint.h should be used.
 */


# 1 "./include/linux/static_key.h" 1
# 13 "./include/linux/tracepoint-defs.h" 2

struct static_call_key;

struct trace_print_flags {
 unsigned long mask;
 const char *name;
};

struct trace_print_flags_u64 {
 unsigned long long mask;
 const char *name;
};

struct tracepoint_func {
 void *func;
 void *data;
 int prio;
};

struct tracepoint {
 const char *name; /* Tracepoint name */
 struct static_key key;
 struct static_call_key *static_call_key;
 void *static_call_tramp;
 void *iterator;
 int (*regfunc)(void);
 void (*unregfunc)(void);
 struct tracepoint_func /* nothing */ *funcs;
};


typedef const int tracepoint_ptr_t;




struct bpf_raw_event_map {
 struct tracepoint *tp;
 void *bpf_func;
 u32 num_args;
 u32 writable_size;
} __attribute__((__aligned__(32)));

/*
 * If a tracepoint needs to be called from a header file, it is not
 * recommended to call it directly, as tracepoints in header files
 * may cause side-effects and bloat the kernel. Instead, use
 * tracepoint_enabled() to test if the tracepoint is enabled, then if
 * it is, call a wrapper function defined in a C file that will then
 * call the tracepoint.
 *
 * For "trace_foo_bar()", you would need to create a wrapper function
 * in a C file to call trace_foo_bar():
 *   void do_trace_foo_bar(args) { trace_foo_bar(args); }
 * Then in the header file, declare the tracepoint:
 *   DECLARE_TRACEPOINT(foo_bar);
 * And call your wrapper:
 *   static inline void some_inlined_function() {
 *            [..]
 *            if (tracepoint_enabled(foo_bar))
 *                    do_trace_foo_bar(args);
 *            [..]
 *   }
 *
 * Note: tracepoint_enabled(foo_bar) is equivalent to trace_foo_bar_enabled()
 *   but is safe to have in headers, where trace_foo_bar_enabled() is not.
 */
# 9 "./include/linux/mmap_lock.h" 2





extern struct tracepoint __tracepoint_mmap_lock_start_locking;
extern struct tracepoint __tracepoint_mmap_lock_acquire_returned;
extern struct tracepoint __tracepoint_mmap_lock_released;
# 47 "./include/linux/mmap_lock.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __mmap_lock_trace_start_locking(struct mm_struct *mm,
         bool write)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __mmap_lock_trace_acquire_returned(struct mm_struct *mm,
            bool write, bool success)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __mmap_lock_trace_released(struct mm_struct *mm, bool write)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_init_lock(struct mm_struct *mm)
{
 do { static struct lock_class_key __key; __init_rwsem((&mm->mmap_lock), "&mm->mmap_lock", &__key); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_write_lock(struct mm_struct *mm)
{
 __mmap_lock_trace_start_locking(mm, true);
 down_write(&mm->mmap_lock);
 __mmap_lock_trace_acquire_returned(mm, true, true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_write_lock_nested(struct mm_struct *mm, int subclass)
{
 __mmap_lock_trace_start_locking(mm, true);
 down_write(&mm->mmap_lock);
 __mmap_lock_trace_acquire_returned(mm, true, true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mmap_write_lock_killable(struct mm_struct *mm)
{
 int ret;

 __mmap_lock_trace_start_locking(mm, true);
 ret = down_write_killable(&mm->mmap_lock);
 __mmap_lock_trace_acquire_returned(mm, true, ret == 0);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mmap_write_trylock(struct mm_struct *mm)
{
 bool ret;

 __mmap_lock_trace_start_locking(mm, true);
 ret = down_write_trylock(&mm->mmap_lock) != 0;
 __mmap_lock_trace_acquire_returned(mm, true, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_write_unlock(struct mm_struct *mm)
{
 __mmap_lock_trace_released(mm, true);
 up_write(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_write_downgrade(struct mm_struct *mm)
{
 __mmap_lock_trace_acquire_returned(mm, false, true);
 downgrade_write(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_read_lock(struct mm_struct *mm)
{
 __mmap_lock_trace_start_locking(mm, false);
 down_read(&mm->mmap_lock);
 __mmap_lock_trace_acquire_returned(mm, false, true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mmap_read_lock_killable(struct mm_struct *mm)
{
 int ret;

 __mmap_lock_trace_start_locking(mm, false);
 ret = down_read_killable(&mm->mmap_lock);
 __mmap_lock_trace_acquire_returned(mm, false, ret == 0);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mmap_read_trylock(struct mm_struct *mm)
{
 bool ret;

 __mmap_lock_trace_start_locking(mm, false);
 ret = down_read_trylock(&mm->mmap_lock) != 0;
 __mmap_lock_trace_acquire_returned(mm, false, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_read_unlock(struct mm_struct *mm)
{
 __mmap_lock_trace_released(mm, false);
 up_read(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_read_unlock_non_owner(struct mm_struct *mm)
{
 __mmap_lock_trace_released(mm, false);
 up_read(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_assert_locked(struct mm_struct *mm)
{
 do { (void)(&mm->mmap_lock); } while (0);
 ((void)(sizeof(( long)(!rwsem_is_locked(&mm->mmap_lock)))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mmap_assert_write_locked(struct mm_struct *mm)
{
 do { (void)(&mm->mmap_lock); } while (0);
 ((void)(sizeof(( long)(!rwsem_is_locked(&mm->mmap_lock)))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mmap_lock_is_contended(struct mm_struct *mm)
{
 return rwsem_is_contended(&mm->mmap_lock);
}
# 16 "./include/linux/mm.h" 2
# 1 "./include/linux/range.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




struct range {
 u64 start;
 u64 end;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 range_len(const struct range *range)
{
 return range->end - range->start + 1;
}

int add_range(struct range *range, int az, int nr_range,
  u64 start, u64 end);


int add_range_with_merge(struct range *range, int az, int nr_range,
    u64 start, u64 end);

void subtract_range(struct range *range, int az, u64 start, u64 end);

int clean_sort_range(struct range *range, int az);

void sort_range(struct range *range, int nr_range);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) resource_size_t cap_resource(u64 val)
{
 if (val > ((resource_size_t)~0))
  return ((resource_size_t)~0);

 return val;
}
# 17 "./include/linux/mm.h" 2





# 1 "./include/linux/page_ext.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/stacktrace.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/generated/uapi/asm/errno.h" 1
# 7 "./include/linux/stacktrace.h" 2

struct task_struct;
struct pt_regs;



/**
 * stack_trace_consume_fn - Callback for arch_stack_walk()
 * @cookie:	Caller supplied pointer handed back by arch_stack_walk()
 * @addr:	The stack entry address to consume
 *
 * Return:	True, if the entry was consumed or skipped
 *		False, if there is no space left to store
 */
typedef bool (*stack_trace_consume_fn)(void *cookie, unsigned long addr);
/**
 * arch_stack_walk - Architecture specific function to walk the stack
 * @consume_entry:	Callback which is invoked by the architecture code for
 *			each entry.
 * @cookie:		Caller supplied pointer which is handed back to
 *			@consume_entry
 * @task:		Pointer to a task struct, can be NULL
 * @regs:		Pointer to registers, can be NULL
 *
 * ============ ======= ============================================
 * task	        regs
 * ============ ======= ============================================
 * task		NULL	Stack trace from task (can be current)
 * current	regs	Stack trace starting on regs->stackpointer
 * ============ ======= ============================================
 */
void arch_stack_walk(stack_trace_consume_fn consume_entry, void *cookie,
       struct task_struct *task, struct pt_regs *regs);

/**
 * arch_stack_walk_reliable - Architecture specific function to walk the
 *			      stack reliably
 *
 * @consume_entry:	Callback which is invoked by the architecture code for
 *			each entry.
 * @cookie:		Caller supplied pointer which is handed back to
 *			@consume_entry
 * @task:		Pointer to a task struct, can be NULL
 *
 * This function returns an error if it detects any unreliable
 * features of the stack. Otherwise it guarantees that the stack
 * trace is reliable.
 *
 * If the task is not 'current', the caller *must* ensure the task is
 * inactive and its stack is pinned.
 */
int arch_stack_walk_reliable(stack_trace_consume_fn consume_entry, void *cookie,
        struct task_struct *task);

void arch_stack_walk_user(stack_trace_consume_fn consume_entry, void *cookie,
     const struct pt_regs *regs);



void stack_trace_print(const unsigned long *trace, unsigned int nr_entries,
         int spaces);
int stack_trace_snprint(char *buf, size_t size, const unsigned long *entries,
   unsigned int nr_entries, int spaces);
unsigned int stack_trace_save(unsigned long *store, unsigned int size,
         unsigned int skipnr);
unsigned int stack_trace_save_tsk(struct task_struct *task,
      unsigned long *store, unsigned int size,
      unsigned int skipnr);
unsigned int stack_trace_save_regs(struct pt_regs *regs, unsigned long *store,
       unsigned int size, unsigned int skipnr);
unsigned int stack_trace_save_user(unsigned long *store, unsigned int size);
unsigned int filter_irq_stacks(unsigned long *entries, unsigned int nr_entries);
# 103 "./include/linux/stacktrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int stack_trace_save_tsk_reliable(struct task_struct *tsk,
      unsigned long *store,
      unsigned int size)
{
 return -38 /* Invalid system call number */;
}
# 7 "./include/linux/page_ext.h" 2
# 1 "./include/linux/stackdepot.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 * A generic stack depot implementation
 *
 * Author: Alexander Potapenko <glider@google.com>
 * Copyright (C) 2016 Google, Inc.
 *
 * Based on code by Dmitry Chernenkov.
 */






typedef u32 depot_stack_handle_t;
/*
 * Number of bits in the handle that stack depot doesn't use. Users may store
 * information in them.
 */


depot_stack_handle_t __stack_depot_save(unsigned long *entries,
     unsigned int nr_entries,
     unsigned int extra_bits,
     gfp_t gfp_flags, bool can_alloc);

/*
 * Every user of stack depot has to call stack_depot_init() during its own init
 * when it's decided that it will be calling stack_depot_save() later. This is
 * recommended for e.g. modules initialized later in the boot process, when
 * slab_is_available() is true.
 *
 * The alternative is to select STACKDEPOT_ALWAYS_INIT to have stack depot
 * enabled as part of mm_init(), for subsystems where it's known at compile time
 * that stack depot will be used.
 *
 * Another alternative is to call stack_depot_want_early_init(), when the
 * decision to use stack depot is taken e.g. when evaluating kernel boot
 * parameters, which precedes the enablement point in mm_init().
 *
 * stack_depot_init() and stack_depot_want_early_init() can be called regardless
 * of CONFIG_STACKDEPOT and are no-op when disabled. The actual save/fetch/print
 * functions should only be called from code that makes sure CONFIG_STACKDEPOT
 * is enabled.
 */

int stack_depot_init(void);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) stack_depot_want_early_init(void);

/* This is supposed to be called only from mm_init() */
int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) stack_depot_early_init(void);
# 62 "./include/linux/stackdepot.h"
depot_stack_handle_t stack_depot_save(unsigned long *entries,
          unsigned int nr_entries, gfp_t gfp_flags);

unsigned int stack_depot_fetch(depot_stack_handle_t handle,
          unsigned long **entries);

unsigned int stack_depot_get_extra_bits(depot_stack_handle_t handle);

int stack_depot_snprint(depot_stack_handle_t handle, char *buf, size_t size,
         int spaces);

void stack_depot_print(depot_stack_handle_t stack);
# 8 "./include/linux/page_ext.h" 2

struct pglist_data;
struct page_ext_operations {
 size_t offset;
 size_t size;
 bool (*need)(void);
 void (*init)(void);
};
# 75 "./include/linux/page_ext.h"
struct page_ext;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool early_page_ext_enabled(void)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgdat_page_ext_init(struct pglist_data *pgdat)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ext_init(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ext_init_flatmem_late(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ext_init_flatmem(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page_ext *page_ext_get(struct page *page)
{
 return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ext_put(struct page_ext *page_ext)
{
}
# 23 "./include/linux/mm.h" 2


# 1 "./include/linux/page_ref.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/page_ref.h"
extern struct tracepoint __tracepoint_page_ref_set;
extern struct tracepoint __tracepoint_page_ref_mod;
extern struct tracepoint __tracepoint_page_ref_mod_and_test;
extern struct tracepoint __tracepoint_page_ref_mod_and_return;
extern struct tracepoint __tracepoint_page_ref_mod_unless;
extern struct tracepoint __tracepoint_page_ref_freeze;
extern struct tracepoint __tracepoint_page_ref_unfreeze;
# 41 "./include/linux/page_ref.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __page_ref_set(struct page *page, int v)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __page_ref_mod(struct page *page, int v)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __page_ref_mod_and_test(struct page *page, int v, int ret)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __page_ref_mod_and_return(struct page *page, int v, int ret)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __page_ref_mod_unless(struct page *page, int v, int u)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __page_ref_freeze(struct page *page, int v, int ret)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __page_ref_unfreeze(struct page *page, int v)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_ref_count(const struct page *page)
{
 return atomic_read(&page->_refcount);
}

/**
 * folio_ref_count - The reference count on this folio.
 * @folio: The folio.
 *
 * The refcount is usually incremented by calls to folio_get() and
 * decremented by calls to folio_put().  Some typical users of the
 * folio refcount:
 *
 * - Each reference from a page table
 * - The page cache
 * - Filesystem private data
 * - The LRU list
 * - Pipes
 * - Direct IO which references this page in the process address space
 *
 * Return: The number of references to this folio.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_ref_count(const struct folio *folio)
{
 return page_ref_count(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_count(const struct page *page)
{
 return folio_ref_count((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_page_count(struct page *page, int v)
{
 atomic_set(&page->_refcount, v);
 if (false)
  __page_ref_set(page, v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_set_count(struct folio *folio, int v)
{
 set_page_count(&folio->page, v);
}

/*
 * Setup the page count before being freed into the page allocator for
 * the first time (boot or memory hotplug)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void init_page_count(struct page *page)
{
 set_page_count(page, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ref_add(struct page *page, int nr)
{
 atomic_add(nr, &page->_refcount);
 if (false)
  __page_ref_mod(page, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_ref_add(struct folio *folio, int nr)
{
 page_ref_add(&folio->page, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ref_sub(struct page *page, int nr)
{
 atomic_sub(nr, &page->_refcount);
 if (false)
  __page_ref_mod(page, -nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_ref_sub(struct folio *folio, int nr)
{
 page_ref_sub(&folio->page, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_ref_sub_return(struct page *page, int nr)
{
 int ret = atomic_sub_return(nr, &page->_refcount);

 if (false)
  __page_ref_mod_and_return(page, -nr, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_ref_sub_return(struct folio *folio, int nr)
{
 return page_ref_sub_return(&folio->page, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ref_inc(struct page *page)
{
 atomic_inc(&page->_refcount);
 if (false)
  __page_ref_mod(page, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_ref_inc(struct folio *folio)
{
 page_ref_inc(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ref_dec(struct page *page)
{
 atomic_dec(&page->_refcount);
 if (false)
  __page_ref_mod(page, -1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_ref_dec(struct folio *folio)
{
 page_ref_dec(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_ref_sub_and_test(struct page *page, int nr)
{
 int ret = atomic_sub_and_test(nr, &page->_refcount);

 if (false)
  __page_ref_mod_and_test(page, -nr, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_ref_sub_and_test(struct folio *folio, int nr)
{
 return page_ref_sub_and_test(&folio->page, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_ref_inc_return(struct page *page)
{
 int ret = atomic_inc_return(&page->_refcount);

 if (false)
  __page_ref_mod_and_return(page, 1, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_ref_inc_return(struct folio *folio)
{
 return page_ref_inc_return(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_ref_dec_and_test(struct page *page)
{
 int ret = atomic_dec_and_test(&page->_refcount);

 if (false)
  __page_ref_mod_and_test(page, -1, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_ref_dec_and_test(struct folio *folio)
{
 return page_ref_dec_and_test(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_ref_dec_return(struct page *page)
{
 int ret = atomic_dec_return(&page->_refcount);

 if (false)
  __page_ref_mod_and_return(page, -1, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_ref_dec_return(struct folio *folio)
{
 return page_ref_dec_return(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_ref_add_unless(struct page *page, int nr, int u)
{
 bool ret = atomic_add_unless(&page->_refcount, nr, u);

 if (false)
  __page_ref_mod_unless(page, nr, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_ref_add_unless(struct folio *folio, int nr, int u)
{
 return page_ref_add_unless(&folio->page, nr, u);
}

/**
 * folio_try_get - Attempt to increase the refcount on a folio.
 * @folio: The folio.
 *
 * If you do not already have a reference to a folio, you can attempt to
 * get one using this function.  It may fail if, for example, the folio
 * has been freed since you found a pointer to it, or it is frozen for
 * the purposes of splitting or migration.
 *
 * Return: True if the reference count was successfully incremented.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_try_get(struct folio *folio)
{
 return folio_ref_add_unless(folio, 1, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_ref_try_add_rcu(struct folio *folio, int count)
{
# 280 "./include/linux/page_ref.h"
 if (__builtin_expect(!!(!folio_ref_add_unless(folio, count, 0)), 0)) {
  /* Either the folio has been freed, or will be freed. */
  return false;
 }

 return true;
}

/**
 * folio_try_get_rcu - Attempt to increase the refcount on a folio.
 * @folio: The folio.
 *
 * This is a version of folio_try_get() optimised for non-SMP kernels.
 * If you are still holding the rcu_read_lock() after looking up the
 * page and know that the page cannot have its refcount decreased to
 * zero in interrupt context, you can use this instead of folio_try_get().
 *
 * Example users include get_user_pages_fast() (as pages are not unmapped
 * from interrupt context) and the page cache lookups (as pages are not
 * truncated from interrupt context).  We also know that pages are not
 * frozen in interrupt context for the purposes of splitting or migration.
 *
 * You can also use this function if you're holding a lock that prevents
 * pages being frozen & removed; eg the i_pages lock for the page cache
 * or the mmap_sem or page table lock for page tables.  In this case,
 * it will always succeed, and you could have used a plain folio_get(),
 * but it's sometimes more convenient to have a common function called
 * from both locked and RCU-protected contexts.
 *
 * Return: True if the reference count was successfully incremented.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_try_get_rcu(struct folio *folio)
{
 return folio_ref_try_add_rcu(folio, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_ref_freeze(struct page *page, int count)
{
 int ret = __builtin_expect(!!(atomic_cmpxchg(&page->_refcount, count, 0) == count), 1);

 if (false)
  __page_ref_freeze(page, count, ret);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_ref_freeze(struct folio *folio, int count)
{
 return page_ref_freeze(&folio->page, count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_ref_unfreeze(struct page *page, int count)
{
 ((void)(sizeof(( long)(page_count(page) != 0))));
 ((void)(sizeof(( long)(count == 0))));

 atomic_set_release(&page->_refcount, count);
 if (false)
  __page_ref_unfreeze(page, count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_ref_unfreeze(struct folio *folio, int count)
{
 page_ref_unfreeze(&folio->page, count);
}
# 26 "./include/linux/mm.h" 2





# 1 "./include/linux/memremap.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/ioport.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * ioport.h	Definitions of routines for detecting, reserving and
 *		allocating system resources.
 *
 * Authors:	Linus Torvalds
 */
# 17 "./include/linux/ioport.h"
/*
 * Resources are tree-like, allowing
 * nesting etc..
 */
struct resource {
 resource_size_t start;
 resource_size_t end;
 const char *name;
 unsigned long flags;
 unsigned long desc;
 struct resource *parent, *sibling, *child;
};

/*
 * IO resources have these defined flags.
 *
 * PCI devices expose these flags to userspace in the "resource" sysfs file,
 * so don't move them.
 */
# 62 "./include/linux/ioport.h"
/* IORESOURCE_SYSRAM specific bits. */
# 73 "./include/linux/ioport.h"
/* I/O resource extended types */


/* PnP IRQ specific bits (IORESOURCE_BITS) */
# 85 "./include/linux/ioport.h"
/* PnP DMA specific bits (IORESOURCE_BITS) */
# 101 "./include/linux/ioport.h"
/* PnP memory I/O specific bits (IORESOURCE_BITS) */
# 114 "./include/linux/ioport.h"
/* PnP I/O specific bits (IORESOURCE_BITS) */




/* PCI ROM control bits (IORESOURCE_BITS) */



/* PCI control bits.  Shares IORESOURCE_BITS with above PCI ROM.  */



/*
 * I/O Resource Descriptors
 *
 * Descriptors are used by walk_iomem_res_desc() and region_intersects()
 * for searching a specific resource range in the iomem table.  Assign
 * a new descriptor when a resource range supports the search interfaces.
 * Otherwise, resource.desc must be set to IORES_DESC_NONE (0).
 */
enum {
 IORES_DESC_NONE = 0,
 IORES_DESC_CRASH_KERNEL = 1,
 IORES_DESC_ACPI_TABLES = 2,
 IORES_DESC_ACPI_NV_STORAGE = 3,
 IORES_DESC_PERSISTENT_MEMORY = 4,
 IORES_DESC_PERSISTENT_MEMORY_LEGACY = 5,
 IORES_DESC_DEVICE_PRIVATE_MEMORY = 6,
 IORES_DESC_RESERVED = 7,
 IORES_DESC_SOFT_RESERVED = 8,
 IORES_DESC_CXL = 9,
};

/*
 * Flags controlling ioremap() behavior.
 */
enum {
 IORES_MAP_SYSTEM_RAM = ((((1UL))) << (0)),
 IORES_MAP_ENCRYPTED = ((((1UL))) << (1)),
};

/* helpers to define resources */
# 191 "./include/linux/ioport.h"
/* PC/ISA/whatever - the normal PC address spaces: IO and memory */
extern struct resource ioport_resource;
extern struct resource iomem_resource;

extern struct resource *request_resource_conflict(struct resource *root, struct resource *new);
extern int request_resource(struct resource *root, struct resource *new);
extern int release_resource(struct resource *new);
void release_child_resources(struct resource *new);
extern void reserve_region_with_split(struct resource *root,
        resource_size_t start, resource_size_t end,
        const char *name);
extern struct resource *insert_resource_conflict(struct resource *parent, struct resource *new);
extern int insert_resource(struct resource *parent, struct resource *new);
extern void insert_resource_expand_to_fit(struct resource *root, struct resource *new);
extern int remove_resource(struct resource *old);
extern void arch_remove_reservations(struct resource *avail);
extern int allocate_resource(struct resource *root, struct resource *new,
        resource_size_t size, resource_size_t min,
        resource_size_t max, resource_size_t align,
        resource_size_t (*alignf)(void *,
             const struct resource *,
             resource_size_t,
             resource_size_t),
        void *alignf_data);
struct resource *lookup_resource(struct resource *root, resource_size_t start);
int adjust_resource(struct resource *res, resource_size_t start,
      resource_size_t size);
resource_size_t resource_alignment(struct resource *res);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) resource_size_t resource_size(const struct resource *res)
{
 return res->end - res->start + 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long resource_type(const struct resource *res)
{
 return res->flags & 0x00001f00 /* Resource type */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long resource_ext_type(const struct resource *res)
{
 return res->flags & 0x01000000 /* Resource extended types */;
}
/* True iff r1 completely contains r2 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool resource_contains(struct resource *r1, struct resource *r2)
{
 if (resource_type(r1) != resource_type(r2))
  return false;
 if (r1->flags & 0x20000000 /* No address assigned yet */ || r2->flags & 0x20000000 /* No address assigned yet */)
  return false;
 return r1->start <= r2->start && r1->end >= r2->end;
}

/* True if any part of r1 overlaps r2 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool resource_overlaps(struct resource *r1, struct resource *r2)
{
       return r1->start <= r2->end && r1->end >= r2->start;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
resource_intersection(struct resource *r1, struct resource *r2, struct resource *r)
{
 if (!resource_overlaps(r1, r2))
  return false;
 r->start = __builtin_choose_expr(((!!(sizeof((typeof(r1->start) *)1 == (typeof(r2->start) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(r1->start) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(r2->start) * 0l)) : (int *)8))))), ((r1->start) > (r2->start) ? (r1->start) : (r2->start)), ({ typeof(r1->start) __UNIQUE_ID___x352 = (r1->start); typeof(r2->start) __UNIQUE_ID___y353 = (r2->start); ((__UNIQUE_ID___x352) > (__UNIQUE_ID___y353) ? (__UNIQUE_ID___x352) : (__UNIQUE_ID___y353)); }));
 r->end = __builtin_choose_expr(((!!(sizeof((typeof(r1->end) *)1 == (typeof(r2->end) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(r1->end) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(r2->end) * 0l)) : (int *)8))))), ((r1->end) < (r2->end) ? (r1->end) : (r2->end)), ({ typeof(r1->end) __UNIQUE_ID___x354 = (r1->end); typeof(r2->end) __UNIQUE_ID___y355 = (r2->end); ((__UNIQUE_ID___x354) < (__UNIQUE_ID___y355) ? (__UNIQUE_ID___x354) : (__UNIQUE_ID___y355)); }));
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool
resource_union(struct resource *r1, struct resource *r2, struct resource *r)
{
 if (!resource_overlaps(r1, r2))
  return false;
 r->start = __builtin_choose_expr(((!!(sizeof((typeof(r1->start) *)1 == (typeof(r2->start) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(r1->start) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(r2->start) * 0l)) : (int *)8))))), ((r1->start) < (r2->start) ? (r1->start) : (r2->start)), ({ typeof(r1->start) __UNIQUE_ID___x356 = (r1->start); typeof(r2->start) __UNIQUE_ID___y357 = (r2->start); ((__UNIQUE_ID___x356) < (__UNIQUE_ID___y357) ? (__UNIQUE_ID___x356) : (__UNIQUE_ID___y357)); }));
 r->end = __builtin_choose_expr(((!!(sizeof((typeof(r1->end) *)1 == (typeof(r2->end) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(r1->end) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(r2->end) * 0l)) : (int *)8))))), ((r1->end) > (r2->end) ? (r1->end) : (r2->end)), ({ typeof(r1->end) __UNIQUE_ID___x358 = (r1->end); typeof(r2->end) __UNIQUE_ID___y359 = (r2->end); ((__UNIQUE_ID___x358) > (__UNIQUE_ID___y359) ? (__UNIQUE_ID___x358) : (__UNIQUE_ID___y359)); }));
 return true;
}

/* Convenience shorthand with allocation */
# 278 "./include/linux/ioport.h"
extern struct resource * __request_region(struct resource *,
     resource_size_t start,
     resource_size_t n,
     const char *name, int flags);

/* Compatibility cruft */



extern void __release_region(struct resource *, resource_size_t,
    resource_size_t);

extern void release_mem_region_adjustable(resource_size_t, resource_size_t);


extern void merge_system_ram_resource(struct resource *res);


/* Wrappers for managed devices */
struct device;

extern int devm_request_resource(struct device *dev, struct resource *root,
     struct resource *new);
extern void devm_release_resource(struct device *dev, struct resource *new);






extern struct resource * __devm_request_region(struct device *dev,
    struct resource *parent, resource_size_t start,
    resource_size_t n, const char *name);






extern void __devm_release_region(struct device *dev, struct resource *parent,
      resource_size_t start, resource_size_t n);
extern int iomem_map_sanity_check(resource_size_t addr, unsigned long size);
extern bool iomem_is_exclusive(u64 addr);
extern bool resource_is_exclusive(struct resource *resource, u64 addr,
      resource_size_t size);

extern int
walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
  void *arg, int (*func)(unsigned long, unsigned long, void *));
extern int
walk_mem_res(u64 start, u64 end, void *arg,
      int (*func)(struct resource *, void *));
extern int
walk_system_ram_res(u64 start, u64 end, void *arg,
      int (*func)(struct resource *, void *));
extern int
walk_iomem_res_desc(unsigned long desc, unsigned long flags, u64 start, u64 end,
      void *arg, int (*func)(struct resource *, void *));

struct resource *devm_request_free_mem_region(struct device *dev,
  struct resource *base, unsigned long size);
struct resource *request_free_mem_region(struct resource *base,
  unsigned long size, const char *name);
struct resource *alloc_free_mem_region(struct resource *base,
  unsigned long size, unsigned long align, const char *name);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void irqresource_disabled(struct resource *res, u32 irq)
{
 res->start = irq;
 res->end = irq;
 res->flags |= 0x00000400 | 0x10000000 | 0x20000000 /* No address assigned yet */;
}

extern struct address_space *iomem_get_mapping(void);
# 8 "./include/linux/memremap.h" 2


struct resource;
struct device;

/**
 * struct vmem_altmap - pre-allocated storage for vmemmap_populate
 * @base_pfn: base of the entire dev_pagemap mapping
 * @reserve: pages mapped, but reserved for driver use (relative to @base)
 * @free: free pages set aside in the mapping for memmap storage
 * @align: pages reserved to meet allocation alignments
 * @alloc: track pages consumed, private to vmemmap_populate()
 */
struct vmem_altmap {
 unsigned long base_pfn;
 const unsigned long end_pfn;
 const unsigned long reserve;
 unsigned long free;
 unsigned long align;
 unsigned long alloc;
};

/*
 * Specialize ZONE_DEVICE memory into multiple types each has a different
 * usage.
 *
 * MEMORY_DEVICE_PRIVATE:
 * Device memory that is not directly addressable by the CPU: CPU can neither
 * read nor write private memory. In this case, we do still have struct pages
 * backing the device memory. Doing so simplifies the implementation, but it is
 * important to remember that there are certain points at which the struct page
 * must be treated as an opaque object, rather than a "normal" struct page.
 *
 * A more complete discussion of unaddressable memory may be found in
 * include/linux/hmm.h and Documentation/mm/hmm.rst.
 *
 * MEMORY_DEVICE_COHERENT:
 * Device memory that is cache coherent from device and CPU point of view. This
 * is used on platforms that have an advanced system bus (like CAPI or CXL). A
 * driver can hotplug the device memory using ZONE_DEVICE and with that memory
 * type. Any page of a process can be migrated to such memory. However no one
 * should be allowed to pin such memory so that it can always be evicted.
 *
 * MEMORY_DEVICE_FS_DAX:
 * Host memory that has similar access semantics as System RAM i.e. DMA
 * coherent and supports page pinning. In support of coordinating page
 * pinning vs other operations MEMORY_DEVICE_FS_DAX arranges for a
 * wakeup event whenever a page is unpinned and becomes idle. This
 * wakeup is used to coordinate physical address space management (ex:
 * fs truncate/hole punch) vs pinned pages (ex: device dma).
 *
 * MEMORY_DEVICE_GENERIC:
 * Host memory that has similar access semantics as System RAM i.e. DMA
 * coherent and supports page pinning. This is for example used by DAX devices
 * that expose memory using a character device.
 *
 * MEMORY_DEVICE_PCI_P2PDMA:
 * Device memory residing in a PCI BAR intended for use with Peer-to-Peer
 * transactions.
 */
enum memory_type {
 /* 0 is reserved to catch uninitialized type fields */
 MEMORY_DEVICE_PRIVATE = 1,
 MEMORY_DEVICE_COHERENT,
 MEMORY_DEVICE_FS_DAX,
 MEMORY_DEVICE_GENERIC,
 MEMORY_DEVICE_PCI_P2PDMA,
};

struct dev_pagemap_ops {
 /*
	 * Called once the page refcount reaches 0.  The reference count will be
	 * reset to one by the core code after the method is called to prepare
	 * for handing out the page again.
	 */
 void (*page_free)(struct page *page);

 /*
	 * Used for private (un-addressable) device memory only.  Must migrate
	 * the page back to a CPU accessible page.
	 */
 vm_fault_t (*migrate_to_ram)(struct vm_fault *vmf);

 /*
	 * Handle the memory failure happens on a range of pfns.  Notify the
	 * processes who are using these pfns, and try to recover the data on
	 * them if necessary.  The mf_flags is finally passed to the recover
	 * function through the whole notify routine.
	 *
	 * When this is not implemented, or it returns -EOPNOTSUPP, the caller
	 * will fall back to a common handler called mf_generic_kill_procs().
	 */
 int (*memory_failure)(struct dev_pagemap *pgmap, unsigned long pfn,
         unsigned long nr_pages, int mf_flags);
};



/**
 * struct dev_pagemap - metadata for ZONE_DEVICE mappings
 * @altmap: pre-allocated/reserved memory for vmemmap allocations
 * @ref: reference count that pins the devm_memremap_pages() mapping
 * @done: completion for @ref
 * @type: memory type: see MEMORY_* in memory_hotplug.h
 * @flags: PGMAP_* flags to specify defailed behavior
 * @vmemmap_shift: structural definition of how the vmemmap page metadata
 *      is populated, specifically the metadata page order.
 *	A zero value (default) uses base pages as the vmemmap metadata
 *	representation. A bigger value will set up compound struct pages
 *	of the requested order value.
 * @ops: method table
 * @owner: an opaque pointer identifying the entity that manages this
 *	instance.  Used by various helpers to make sure that no
 *	foreign ZONE_DEVICE memory is accessed.
 * @nr_range: number of ranges to be mapped
 * @range: range to be mapped when nr_range == 1
 * @ranges: array of ranges to be mapped when nr_range > 1
 */
struct dev_pagemap {
 struct vmem_altmap altmap;
 struct percpu_ref ref;
 struct completion done;
 enum memory_type type;
 unsigned int flags;
 unsigned long vmemmap_shift;
 const struct dev_pagemap_ops *ops;
 void *owner;
 int nr_range;
 union {
  struct range range;
  struct range ranges[0];
 };
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pgmap_has_memory_failure(struct dev_pagemap *pgmap)
{
 return pgmap->ops && pgmap->ops->memory_failure;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct vmem_altmap *pgmap_altmap(struct dev_pagemap *pgmap)
{
 if (pgmap->flags & (1 << 0))
  return &pgmap->altmap;
 return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long pgmap_vmemmap_nr(struct dev_pagemap *pgmap)
{
 return 1 << pgmap->vmemmap_shift;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_device_private_page(const struct page *page)
{
 return 0 &&
  is_zone_device_page(page) &&
  page->pgmap->type == MEMORY_DEVICE_PRIVATE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_is_device_private(const struct folio *folio)
{
 return is_device_private_page(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_pci_p2pdma_page(const struct page *page)
{
 return 0 &&
  is_zone_device_page(page) &&
  page->pgmap->type == MEMORY_DEVICE_PCI_P2PDMA;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_device_coherent_page(const struct page *page)
{
 return is_zone_device_page(page) &&
  page->pgmap->type == MEMORY_DEVICE_COHERENT;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_is_device_coherent(const struct folio *folio)
{
 return is_device_coherent_page(&folio->page);
}
# 203 "./include/linux/memremap.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *devm_memremap_pages(struct device *dev,
  struct dev_pagemap *pgmap)
{
 /*
	 * Fail attempts to call devm_memremap_pages() without
	 * ZONE_DEVICE support enabled, this requires callers to fall
	 * back to plain devm_memremap() based on config
	 */
 ({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/memremap.h\"; .popsection; .long 14472b - .; .short 211; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 return ERR_PTR(-6 /* No such device or address */);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void devm_memunmap_pages(struct device *dev,
  struct dev_pagemap *pgmap)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct dev_pagemap *get_dev_pagemap(unsigned long pfn,
  struct dev_pagemap *pgmap)
{
 return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pgmap_pfn_valid(struct dev_pagemap *pgmap, unsigned long pfn)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long vmem_altmap_offset(struct vmem_altmap *altmap)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vmem_altmap_free(struct vmem_altmap *altmap,
  unsigned long nr_pfns)
{
}

/* when memremap_pages() is disabled all archs can remap a single page */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long memremap_compat_align(void)
{
 return ((1UL) << 12);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void put_dev_pagemap(struct dev_pagemap *pgmap)
{
 if (pgmap)
  percpu_ref_put(&pgmap->ref);
}
# 32 "./include/linux/mm.h" 2

struct mempolicy;
struct anon_vma;
struct anon_vma_chain;
struct user_struct;
struct pt_regs;

extern int sysctl_page_lock_unfairness;

void init_mm_internals(void);
# 51 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_max_mapnr(unsigned long limit) { }


extern atomic_long_t _totalram_pages;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long totalram_pages(void)
{
 return (unsigned long)atomic_long_read(&_totalram_pages);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void totalram_pages_inc(void)
{
 atomic_long_inc(&_totalram_pages);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void totalram_pages_dec(void)
{
 atomic_long_dec(&_totalram_pages);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void totalram_pages_add(long count)
{
 atomic_long_add(count, &_totalram_pages);
}

extern void * high_memory;
extern int page_cluster;
extern const int page_cluster_max;


extern int sysctl_legacy_va_layout;





extern const int mmap_rnd_bits_min;
extern const int mmap_rnd_bits_max;
extern int mmap_rnd_bits __attribute__((__section__(".data..read_mostly")));


extern const int mmap_rnd_compat_bits_min;
extern const int mmap_rnd_compat_bits_max;
extern int mmap_rnd_compat_bits __attribute__((__section__(".data..read_mostly")));





/*
 * Architectures that support memory tagging (assigning tags to memory regions,
 * embedding these tags into addresses that point to these memory regions, and
 * checking that the memory and the pointer tags match on memory accesses)
 * redefine this macro to strip tags from pointers.
 * It's defined as noop for architectures that don't support memory tagging.
 */
# 122 "./include/linux/mm.h"
/*
 * To prevent common memory management code establishing
 * a zero page mapping on a read fault.
 * This macro should be defined within <asm/pgtable.h>.
 * s390 does this to prevent multiplexing of hardware bits
 * related to the physical page in case of virtualization.
 */




/*
 * On some architectures it is expensive to call memset() for small sizes.
 * If an architecture decides to implement their own version of
 * mm_zero_struct_page they should wrap the defines below in a #ifndef and
 * define their own version of this macro in <asm/pgtable.h>
 */

/* This function must be updated when the size of struct page grows above 80
 * or reduces below 56. The idea that compiler optimizes out switch()
 * statement, and only leaves move/store instructions. Also the compiler can
 * combine write statements if they are both assignments and can be reordered,
 * this can result in several of the writes here being dropped.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __mm_zero_struct_page(struct page *page)
{
 unsigned long *_pp = (void *)page;

  /* Check that struct page is either 56, 64, 72, or 80 bytes */
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_360(void) __attribute__((__error__("BUILD_BUG_ON failed: " "sizeof(struct page) & 7"))); if (!(!(sizeof(struct page) & 7))) __compiletime_assert_360(); } while (0);
# 153 "./include/linux/mm.h"
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_361(void) __attribute__((__error__("BUILD_BUG_ON failed: " "sizeof(struct page) < 56"))); if (!(!(sizeof(struct page) < 56))) __compiletime_assert_361(); } while (0);
# 154 "./include/linux/mm.h"
 do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_362(void) __attribute__((__error__("BUILD_BUG_ON failed: " "sizeof(struct page) > 80"))); if (!(!(sizeof(struct page) > 80))) __compiletime_assert_362(); } while (0);
# 156 "./include/linux/mm.h"
 switch (sizeof(struct page)) {
 case 80:
  _pp[9] = 0;
  __attribute__((__fallthrough__));
 case 72:
  _pp[8] = 0;
  __attribute__((__fallthrough__));
 case 64:
  _pp[7] = 0;
  __attribute__((__fallthrough__));
 case 56:
  _pp[6] = 0;
  _pp[5] = 0;
  _pp[4] = 0;
  _pp[3] = 0;
  _pp[2] = 0;
  _pp[1] = 0;
  _pp[0] = 0;
 }
}




/*
 * Default maximum number of active map areas, this limits the number of vmas
 * per mm struct. Users can overwrite this number by sysctl but there is a
 * problem.
 *
 * When a program's coredump is generated as ELF format, a section is created
 * per a vma. In ELF, the number of sections is represented in unsigned short.
 * This means the number of sections should be smaller than 65535 at coredump.
 * Because the kernel adds some informative sections to a image of program at
 * generating coredump, we need some margin. The number of extra sections is
 * 1-3 now and depends on arch. We use "5" as safe margin, here.
 *
 * ELF extended numbering allows more than 65535 sections, so 16-bit bound is
 * not a hard limit any more. Although some userspace tools can be surprised by
 * that.
 */



extern int sysctl_max_map_count;

extern unsigned long sysctl_user_reserve_kbytes;
extern unsigned long sysctl_admin_reserve_kbytes;

extern int sysctl_overcommit_memory;
extern int sysctl_overcommit_ratio;
extern unsigned long sysctl_overcommit_kbytes;

int overcommit_ratio_handler(struct ctl_table *, int, void *, size_t *,
  loff_t *);
int overcommit_kbytes_handler(struct ctl_table *, int, void *, size_t *,
  loff_t *);
int overcommit_policy_handler(struct ctl_table *, int, void *, size_t *,
  loff_t *);
# 223 "./include/linux/mm.h"
/* to align the pointer to the (next) page boundary */


/* to align the pointer to the (prev) page boundary */


/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct folio *lru_to_folio(struct list_head *head)
{
 return ({ void *__mptr = (void *)((head)->prev); _Static_assert(__builtin_types_compatible_p(typeof(*((head)->prev)), typeof(((struct folio *)0)->lru)) || __builtin_types_compatible_p(typeof(*((head)->prev)), typeof(void)), "pointer type mismatch in container_of()"); ((struct folio *)(__mptr - __builtin_offsetof(struct folio, lru))); });
}

void setup_initial_init_mm(void *start_code, void *end_code,
      void *end_data, void *brk);

/*
 * Linux kernel virtual memory manager primitives.
 * The idea being to have a "virtual" mm in the same way
 * we have a virtual fs - giving a cleaner interface to the
 * mm details, and allowing different kinds of memory mappings
 * (from shared memory to executable loading to arbitrary
 * mmap() functions).
 */

struct vm_area_struct *vm_area_alloc(struct mm_struct *);
struct vm_area_struct *vm_area_dup(struct vm_area_struct *);
void vm_area_free(struct vm_area_struct *);
# 261 "./include/linux/mm.h"
/*
 * vm_flags in vm_area_struct, see mm_types.h.
 * When changing, update also include/trace/events/mmflags.h
 */







/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */
# 286 "./include/linux/mm.h"
     /* Used by sys_madvise() */
# 375 "./include/linux/mm.h"
/* Bits set in the VMA until the stack is in its final location */




/* Common data flag combinations */
# 404 "./include/linux/mm.h"
/* VMA basic access permission flags */



/*
 * Special vmas that are non-mergable, non-mlock()able.
 */


/* This mask prevents VMA from being scanned with khugepaged */


/* This mask defines which mm->def_flags a process can inherit its parent */


/* This mask is used to clear all the VMA flags used by mlock */


/* Arch-specific flags to clear when updating VM flags on protection change */





/*
 * mapping from the currently active vm_flags protection bits (the
 * low four bits) to a page protection mask..
 */

/*
 * The default fault flags that should be used by most of the
 * arch-specific page fault handlers.
 */




/**
 * fault_flag_allow_retry_first - check ALLOW_RETRY the first time
 * @flags: Fault flags.
 *
 * This is mostly used for places where we want to try to avoid taking
 * the mmap_lock for too long a time when waiting for another condition
 * to change, in which case we can try to be polite to release the
 * mmap_lock in the first round to avoid potential starvation of other
 * processes that would also want the mmap_lock.
 *
 * Return: true if the page fault allows retry and this is the first
 * attempt of the fault handling; false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool fault_flag_allow_retry_first(enum fault_flag flags)
{
 return (flags & FAULT_FLAG_ALLOW_RETRY) &&
     (!(flags & FAULT_FLAG_TRIED));
}
# 472 "./include/linux/mm.h"
/*
 * vm_fault is filled by the pagefault handler and passed to the vma's
 * ->fault function. The vma's ->fault is responsible for returning a bitmask
 * of VM_FAULT_xxx flags that give details about how the fault was handled.
 *
 * MM layer fills up gfp_mask for page allocations but fault handler might
 * alter it if its implementation requires a different allocation context.
 *
 * pgoff should be used in favour of virtual_address, if possible.
 */
struct vm_fault {
 const struct {
  struct vm_area_struct *vma; /* Target VMA */
  gfp_t gfp_mask; /* gfp mask to be used for allocations */
  unsigned long pgoff; /* Logical page offset based on vma */
  unsigned long address; /* Faulting virtual address - masked */
  unsigned long real_address; /* Faulting virtual address - unmasked */
 };
 enum fault_flag flags; /* FAULT_FLAG_xxx flags
					 * XXX: should really be 'const' */
 pmd_t *pmd; /* Pointer to pmd entry matching
					 * the 'address' */
 pud_t *pud; /* Pointer to pud entry matching
					 * the 'address'
					 */
 union {
  pte_t orig_pte; /* Value of PTE at the time of fault */
  pmd_t orig_pmd; /* Value of PMD at the time of fault,
					 * used by PMD fault only.
					 */
 };

 struct page *cow_page; /* Page handler may use for COW fault */
 struct page *page; /* ->fault handlers should return a
					 * page here, unless VM_FAULT_NOPAGE
					 * is set (which is also implied by
					 * VM_FAULT_ERROR).
					 */
 /* These three entries are valid only while holding ptl lock */
 pte_t *pte; /* Pointer to pte entry matching
					 * the 'address'. NULL if the page
					 * table hasn't been allocated.
					 */
 spinlock_t *ptl; /* Page table lock.
					 * Protects pte page table if 'pte'
					 * is not NULL, otherwise pmd.
					 */
 pgtable_t prealloc_pte; /* Pre-allocated pte page table.
					 * vm_ops->map_pages() sets up a page
					 * table from atomic context.
					 * do_fault_around() pre-allocates
					 * page table to avoid allocation from
					 * atomic context.
					 */
};

/* page entry size for vm->huge_fault() */
enum page_entry_size {
 PE_SIZE_PTE = 0,
 PE_SIZE_PMD,
 PE_SIZE_PUD,
};

/*
 * These are the virtual MM functions - opening of an area, closing and
 * unmapping it (needed to keep files on disk up-to-date etc), pointer
 * to the functions called when a no-page or a wp-page exception occurs.
 */
struct vm_operations_struct {
 void (*open)(struct vm_area_struct * area);
 /**
	 * @close: Called when the VMA is being removed from the MM.
	 * Context: User context.  May sleep.  Caller holds mmap_lock.
	 */
 void (*close)(struct vm_area_struct * area);
 /* Called any time before splitting to check if it's allowed */
 int (*may_split)(struct vm_area_struct *area, unsigned long addr);
 int (*mremap)(struct vm_area_struct *area);
 /*
	 * Called by mprotect() to make driver-specific permission
	 * checks before mprotect() is finalised.   The VMA must not
	 * be modified.  Returns 0 if mprotect() can proceed.
	 */
 int (*mprotect)(struct vm_area_struct *vma, unsigned long start,
   unsigned long end, unsigned long newflags);
 vm_fault_t (*fault)(struct vm_fault *vmf);
 vm_fault_t (*huge_fault)(struct vm_fault *vmf,
   enum page_entry_size pe_size);
 vm_fault_t (*map_pages)(struct vm_fault *vmf,
   unsigned long start_pgoff, unsigned long end_pgoff);
 unsigned long (*pagesize)(struct vm_area_struct * area);

 /* notification that a previously read-only page is about to become
	 * writable, if an error is returned it will cause a SIGBUS */
 vm_fault_t (*page_mkwrite)(struct vm_fault *vmf);

 /* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */
 vm_fault_t (*pfn_mkwrite)(struct vm_fault *vmf);

 /* called by access_process_vm when get_user_pages() fails, typically
	 * for use by special VMAs. See also generic_access_phys() for a generic
	 * implementation useful for any iomem mapping.
	 */
 int (*access)(struct vm_area_struct *vma, unsigned long addr,
        void *buf, int len, int write);

 /* Called by the /proc/PID/maps code to ask the vma whether it
	 * has a special name.  Returning non-NULL will also cause this
	 * vma to be dumped unconditionally. */
 const char *(*name)(struct vm_area_struct *vma);


 /*
	 * set_policy() op must add a reference to any non-NULL @new mempolicy
	 * to hold the policy upon return.  Caller should pass NULL @new to
	 * remove a policy and fall back to surrounding context--i.e. do not
	 * install a MPOL_DEFAULT policy, nor the task or system default
	 * mempolicy.
	 */
 int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);

 /*
	 * get_policy() op must add reference [mpol_get()] to any policy at
	 * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
	 * in mm/mempolicy.c will do this automatically.
	 * get_policy() must NOT add a ref if the policy at (vma,addr) is not
	 * marked as MPOL_SHARED. vma policies are protected by the mmap_lock.
	 * If no [shared/vma] mempolicy exists at the addr, get_policy() op
	 * must return NULL--i.e., do not "fallback" to task or system default
	 * policy.
	 */
 struct mempolicy *(*get_policy)(struct vm_area_struct *vma,
     unsigned long addr);

 /*
	 * Called by vm_normal_page() for special PTEs to find the
	 * page for @addr.  This is useful if the default behavior
	 * (using pte_page()) would not find the correct page.
	 */
 struct page *(*find_special_page)(struct vm_area_struct *vma,
       unsigned long addr);
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vma_init(struct vm_area_struct *vma, struct mm_struct *mm)
{
 static const struct vm_operations_struct dummy_vm_ops = {};

 memset(vma, 0, sizeof(*vma));
 vma->vm_mm = mm;
 vma->vm_ops = &dummy_vm_ops;
 INIT_LIST_HEAD(&vma->anon_vma_chain);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void vma_set_anonymous(struct vm_area_struct *vma)
{
 vma->vm_ops = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_is_anonymous(struct vm_area_struct *vma)
{
 return !vma->vm_ops;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_is_temporary_stack(struct vm_area_struct *vma)
{
 int maybe_stack = vma->vm_flags & (0x00000100 /* general info on the segment */ | 0x00000000);

 if (!maybe_stack)
  return false;

 if ((vma->vm_flags & (0x00010000 /* App will not benefit from clustered reads */ | 0x00008000 /* App will access data sequentially */)) ==
      (0x00010000 /* App will not benefit from clustered reads */ | 0x00008000 /* App will access data sequentially */))
  return true;

 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_is_foreign(struct vm_area_struct *vma)
{
 if (!get_current()->mm)
  return true;

 if (get_current()->mm != vma->vm_mm)
  return true;

 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_is_accessible(struct vm_area_struct *vma)
{
 return vma->vm_flags & (0x00000001 /* currently active flags */ | 0x00000002 | 0x00000004);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
struct vm_area_struct *vma_find(struct vma_iterator *vmi, unsigned long max)
{
 return mas_find(&vmi->mas, max);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct vm_area_struct *vma_next(struct vma_iterator *vmi)
{
 /*
	 * Uses vma_find() to get the first VMA when the iterator starts.
	 * Calling mas_next() could skip the first entry.
	 */
 return vma_find(vmi, (~0UL));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct vm_area_struct *vma_prev(struct vma_iterator *vmi)
{
 return mas_prev(&vmi->mas, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long vma_iter_addr(struct vma_iterator *vmi)
{
 return vmi->mas.index;
}




/* The MM code likes to work with exclusive end addresses */




/*
 * The vma_is_shmem is not inline because it is used only by slow
 * paths in userfault.
 */
bool vma_is_shmem(struct vm_area_struct *vma);
bool vma_is_anon_shmem(struct vm_area_struct *vma);





int vma_is_stack_for_current(struct vm_area_struct *vma);

/* flush_tlb_range() takes a vma, not a mm, and can care about flags */


struct mmu_gather;
struct inode;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int compound_order(struct page *page)
{
 if (!PageHead(page))
  return 0;
 return page[1].compound_order;
}

/**
 * folio_order - The allocation order of a folio.
 * @folio: The folio.
 *
 * A folio is composed of 2^order pages.  See get_order() for the definition
 * of order.
 *
 * Return: The order of the folio.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int folio_order(struct folio *folio)
{
 if (!folio_test_large(folio))
  return 0;
 return folio->_folio_order;
}

# 1 "./include/linux/huge_mm.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/sched/coredump.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./include/linux/sched/coredump.h"
/* mm flags */

/* for SUID_DUMP_* above */



extern void set_dumpable(struct mm_struct *mm, int value);
/*
 * This returns the actual value of the suid_dumpable flag. For things
 * that are using this for checking for privilege transitions, it must
 * test against SUID_DUMP_USER rather than treating it as a boolean
 * value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __get_dumpable(unsigned long mm_flags)
{
 return mm_flags & ((1 << 2) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_dumpable(struct mm_struct *mm)
{
 return __get_dumpable(mm->flags);
}

/* coredump filter bits */
# 58 "./include/linux/sched/coredump.h"
     /* leave room for more dump flags */



/*
 * This one-shot flag is dropped due to necessity of changing exe once again
 * on NFS restore
 */
//#define MMF_EXE_FILE_CHANGED	18	/* see prctl_set_mm_exe_file() */
# 76 "./include/linux/sched/coredump.h"
/*
 * MMF_HAS_PINNED: Whether this mm has pinned any pages.  This can be either
 * replaced in the future by mm.pinned_vm when it becomes stable, or grow into
 * a counter on its own. We're aggresive on this bit for now: even if the
 * pinned pages were unpinned later on, we'll still keep this bit set for the
 * lifecycle of this mm, just for simplicity.
 */
# 6 "./include/linux/huge_mm.h" 2




vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf);
int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
    pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
    struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma);
void huge_pmd_set_accessed(struct vm_fault *vmf);
int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,
    pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
    struct vm_area_struct *vma);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud)
{
}


vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf);
struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
       unsigned long addr, pmd_t *pmd,
       unsigned int flags);
bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
      pmd_t *pmd, unsigned long addr, unsigned long next);
int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma, pmd_t *pmd,
   unsigned long addr);
int zap_huge_pud(struct mmu_gather *tlb, struct vm_area_struct *vma, pud_t *pud,
   unsigned long addr);
bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,
     unsigned long new_addr, pmd_t *old_pmd, pmd_t *new_pmd);
int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
      pmd_t *pmd, unsigned long addr, pgprot_t newprot,
      unsigned long cp_flags);
vm_fault_t vmf_insert_pfn_pmd_prot(struct vm_fault *vmf, pfn_t pfn,
       pgprot_t pgprot, bool write);

/**
 * vmf_insert_pfn_pmd - insert a pmd size pfn
 * @vmf: Structure describing the fault
 * @pfn: pfn to insert
 * @pgprot: page protection to use
 * @write: whether it's a write fault
 *
 * Insert a pmd size pfn. See vmf_insert_pfn() for additional info.
 *
 * Return: vm_fault_t value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vm_fault_t vmf_insert_pfn_pmd(struct vm_fault *vmf, pfn_t pfn,
         bool write)
{
 return vmf_insert_pfn_pmd_prot(vmf, pfn, vmf->vma->vm_page_prot, write);
}
vm_fault_t vmf_insert_pfn_pud_prot(struct vm_fault *vmf, pfn_t pfn,
       pgprot_t pgprot, bool write);

/**
 * vmf_insert_pfn_pud - insert a pud size pfn
 * @vmf: Structure describing the fault
 * @pfn: pfn to insert
 * @pgprot: page protection to use
 * @write: whether it's a write fault
 *
 * Insert a pud size pfn. See vmf_insert_pfn() for additional info.
 *
 * Return: vm_fault_t value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vm_fault_t vmf_insert_pfn_pud(struct vm_fault *vmf, pfn_t pfn,
         bool write)
{
 return vmf_insert_pfn_pud_prot(vmf, pfn, vmf->vma->vm_page_prot, write);
}

enum transparent_hugepage_flag {
 TRANSPARENT_HUGEPAGE_NEVER_DAX,
 TRANSPARENT_HUGEPAGE_FLAG,
 TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,
 TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG,
 TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG,
 TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG,
 TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG,
 TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG,
 TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG,
};

struct kobject;
struct kobj_attribute;

ssize_t single_hugepage_flag_store(struct kobject *kobj,
       struct kobj_attribute *attr,
       const char *buf, size_t count,
       enum transparent_hugepage_flag flag);
ssize_t single_hugepage_flag_show(struct kobject *kobj,
      struct kobj_attribute *attr, char *buf,
      enum transparent_hugepage_flag flag);
extern struct kobj_attribute shmem_enabled_attr;
# 117 "./include/linux/huge_mm.h"
extern unsigned long transparent_hugepage_flags;
# 127 "./include/linux/huge_mm.h"
/*
 * Do the below checks:
 *   - For file vma, check if the linear page offset of vma is
 *     HPAGE_PMD_NR aligned within the file.  The hugepage is
 *     guaranteed to be hugepage-aligned within the file, but we must
 *     check that the PMD-aligned addresses in the VMA map to
 *     PMD-aligned offsets within the file, else the hugepage will
 *     not be PMD-mappable.
 *   - For all vmas, check if the haddr is in an aligned HPAGE_PMD_SIZE
 *     area.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool transhuge_vma_suitable(struct vm_area_struct *vma,
  unsigned long addr)
{
 unsigned long haddr;

 /* Don't have to check pgoff for anonymous vma */
 if (!vma_is_anonymous(vma)) {
  if (!((((vma->vm_start >> 12) - vma->vm_pgoff) & ((typeof((vma->vm_start >> 12) - vma->vm_pgoff))((1<<(((12 - 3) * (4 - (2)) + 3)-12))) - 1)) == 0))

   return false;
 }

 haddr = addr & (~(((1UL) << ((12 - 3) * (4 - (2)) + 3)) - 1));

 if (haddr < vma->vm_start || haddr + ((1UL) << ((12 - 3) * (4 - (2)) + 3)) > vma->vm_end)
  return false;
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool file_thp_enabled(struct vm_area_struct *vma)
{
 struct inode *inode;

 if (!vma->vm_file)
  return false;

 inode = vma->vm_file->f_inode;

 return (0) &&
        (vma->vm_flags & 0x00000004) &&
        !inode_is_open_for_write(inode) && (((inode->i_mode) & 00170000) == 0100000);
}

bool hugepage_vma_check(struct vm_area_struct *vma, unsigned long vm_flags,
   bool smaps, bool in_pf, bool enforce_sysfs);





unsigned long thp_get_unmapped_area(struct file *filp, unsigned long addr,
  unsigned long len, unsigned long pgoff, unsigned long flags);

void prep_transhuge_page(struct page *page);
void free_transhuge_page(struct page *page);

bool can_split_folio(struct folio *folio, int *pextra_pins);
int split_huge_page_to_list(struct page *page, struct list_head *list);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int split_huge_page(struct page *page)
{
 return split_huge_page_to_list(page, ((void *)0));
}
void deferred_split_huge_page(struct page *page);

void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
  unsigned long address, bool freeze, struct folio *folio);
# 205 "./include/linux/huge_mm.h"
void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,
  bool freeze, struct folio *folio);

void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,
  unsigned long address);
# 219 "./include/linux/huge_mm.h"
int hugepage_madvise(struct vm_area_struct *vma, unsigned long *vm_flags,
       int advice);
int madvise_collapse(struct vm_area_struct *vma,
       struct vm_area_struct **prev,
       unsigned long start, unsigned long end);
void vma_adjust_trans_huge(struct vm_area_struct *vma, unsigned long start,
      unsigned long end, long adjust_next);
spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma);
spinlock_t *__pud_trans_huge_lock(pud_t *pud, struct vm_area_struct *vma);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_swap_pmd(pmd_t pmd)
{
 return !(!((pmd).pmd)) && !pmd_present(pmd);
}

/* mmap_lock must be held on entry */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,
  struct vm_area_struct *vma)
{
 if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || (!!(((pmd_pte(*pmd)).pte) & (((pteval_t)(1)) << 57))))
  return __pmd_trans_huge_lock(pmd, vma);
 else
  return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *pud_trans_huge_lock(pud_t *pud,
  struct vm_area_struct *vma)
{
 if (pud_trans_huge(*pud) || pud_devmap(*pud))
  return __pud_trans_huge_lock(pud, vma);
 else
  return ((void *)0);
}

/**
 * folio_test_pmd_mappable - Can we map this folio with a PMD?
 * @folio: The folio to test
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_test_pmd_mappable(struct folio *folio)
{
 return folio_order(folio) >= (((12 - 3) * (4 - (2)) + 3)-12);
}

struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,
  pmd_t *pmd, int flags, struct dev_pagemap **pgmap);
struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,
  pud_t *pud, int flags, struct dev_pagemap **pgmap);

vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf);

extern struct page *huge_zero_page;
extern unsigned long huge_zero_pfn;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_huge_zero_page(struct page *page)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_363(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(huge_zero_page) == sizeof(char) || sizeof(huge_zero_page) == sizeof(short) || sizeof(huge_zero_page) == sizeof(int) || sizeof(huge_zero_page) == sizeof(long)) || sizeof(huge_zero_page) == sizeof(long long))) __compiletime_assert_363(); } while (0); (*(const volatile typeof( _Generic((huge_zero_page), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (huge_zero_page))) *)&(huge_zero_page)); }) == page;
# 274 "./include/linux/huge_mm.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_huge_zero_pmd(pmd_t pmd)
{
 return pmd_present(pmd) && ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_364(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(huge_zero_pfn) == sizeof(char) || sizeof(huge_zero_pfn) == sizeof(short) || sizeof(huge_zero_pfn) == sizeof(int) || sizeof(huge_zero_pfn) == sizeof(long)) || sizeof(huge_zero_pfn) == sizeof(long long))) __compiletime_assert_364(); } while (0); (*(const volatile typeof( _Generic((huge_zero_pfn), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (huge_zero_pfn))) *)&(huge_zero_pfn)); }) == (((((pmd_pte(pmd)).pte) & (((((pteval_t)(1)) << (48 - 12)) - 1) << 12)) & (~(((1UL) << ((12 - 3) * (4 - (2)) + 3))-1))) >> 12);
# 279 "./include/linux/huge_mm.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_huge_zero_pud(pud_t pud)
{
 return false;
}

struct page *mm_get_huge_zero_page(struct mm_struct *mm);
void mm_put_huge_zero_page(struct mm_struct *mm);



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool thp_migration_supported(void)
{
 return 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct list_head *page_deferred_list(struct page *page)
{
 /*
	 * See organization of tail pages of compound page in
	 * "struct page" definition.
	 */
 return &page[2].deferred_list;
}
# 441 "./include/linux/huge_mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int split_folio_to_list(struct folio *folio,
  struct list_head *list)
{
 return split_huge_page_to_list(&folio->page, list);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int split_folio(struct folio *folio)
{
 return split_folio_to_list(folio, ((void *)0));
}

/*
 * archs that select ARCH_WANTS_THP_SWAP but don't support THP_SWP due to
 * limitations in the implementation like arm64 MTE can override this to
 * false
 */
# 741 "./include/linux/mm.h" 2

/*
 * Methods to modify the page usage count.
 *
 * What counts for a page usage:
 * - cache mapping   (page->mapping)
 * - private data    (page->private)
 * - page mapped in a task's page tables, each mapping
 *   is counted separately
 *
 * Also, many kernel routines increase the page count before a critical
 * routine so they can be sure the page doesn't go away from under them.
 */

/*
 * Drop a ref, return true if the refcount fell to zero (the page has no users)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int put_page_testzero(struct page *page)
{
 ((void)(sizeof(( long)(page_ref_count(page) == 0))));
 return page_ref_dec_and_test(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_put_testzero(struct folio *folio)
{
 return put_page_testzero(&folio->page);
}

/*
 * Try to grab a ref unless the page has a refcount of zero, return false if
 * that is the case.
 * This can be called when MMU is off so it must not access
 * any of the virtual mappings.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool get_page_unless_zero(struct page *page)
{
 return page_ref_add_unless(page, 1, 0);
}

extern int page_is_ram(unsigned long pfn);

enum {
 REGION_INTERSECTS,
 REGION_DISJOINT,
 REGION_MIXED,
};

int region_intersects(resource_size_t offset, size_t size, unsigned long flags,
        unsigned long desc);

/* Support for virtually mapped pages */
struct page *vmalloc_to_page(const void *addr);
unsigned long vmalloc_to_pfn(const void *addr);

/*
 * Determine if an address is within the vmalloc range
 *
 * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there
 * is no special casing required.
 */






extern bool is_vmalloc_addr(const void *x);
extern int is_vmalloc_or_module_addr(const void *x);
# 820 "./include/linux/mm.h"
/*
 * How many times the entire folio is mapped as a single unit (eg by a
 * PMD or PUD entry).  This is probably not what you want, except for
 * debugging purposes - it does not include PTE-mapped sub-pages; look
 * at folio_mapcount() or page_mapcount() or total_mapcount() instead.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_entire_mapcount(struct folio *folio)
{
 ((void)(sizeof(( long)(!folio_test_large(folio)))));
 return atomic_read(folio_mapcount_ptr(folio)) + 1;
}

/*
 * Mapcount of compound page as a whole, does not include mapped sub-pages.
 * Must be called only on head of compound page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int head_compound_mapcount(struct page *head)
{
 return atomic_read(compound_mapcount_ptr(head)) + 1;
}

/*
 * If a 16GB hugetlb page were mapped by PTEs of all of its 4kB sub-pages,
 * its subpages_mapcount would be 0x400000: choose the COMPOUND_MAPPED bit
 * above that range, instead of 2*(PMD_SIZE/PAGE_SIZE).  Hugetlb currently
 * leaves subpages_mapcount at 0, but avoid surprise if it participates later.
 */



/*
 * Number of sub-pages mapped by PTE, does not include compound mapcount.
 * Must be called only on head of compound page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int head_subpages_mapcount(struct page *head)
{
 return atomic_read(subpages_mapcount_ptr(head)) & (0x800000 - 1);
}

/*
 * The atomic page->_mapcount, starts from -1: so that transitions
 * both from it and to it can be tracked, using atomic_inc_and_test
 * and atomic_add_negative(-1).
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_mapcount_reset(struct page *page)
{
 atomic_set(&(page)->_mapcount, -1);
}

/*
 * Mapcount of 0-order page; when compound sub-page, includes
 * compound_mapcount of compound_head of page.
 *
 * Result is undefined for pages which cannot be mapped into userspace.
 * For example SLAB or special types of pages. See function page_has_type().
 * They use this place in struct page differently.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_mapcount(struct page *page)
{
 int mapcount = atomic_read(&page->_mapcount) + 1;

 if (__builtin_expect(!!(!PageCompound(page)), 1))
  return mapcount;
 page = ((typeof(page))_compound_head(page));
 return head_compound_mapcount(page) + mapcount;
}

int total_compound_mapcount(struct page *head);

/**
 * folio_mapcount() - Calculate the number of mappings of this folio.
 * @folio: The folio.
 *
 * A large folio tracks both how many times the entire folio is mapped,
 * and how many times each individual page in the folio is mapped.
 * This function calculates the total number of times the folio is
 * mapped.
 *
 * Return: The number of times this folio is mapped.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_mapcount(struct folio *folio)
{
 if (__builtin_expect(!!(!folio_test_large(folio)), 1))
  return atomic_read(&folio->_mapcount) + 1;
 return total_compound_mapcount(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int total_mapcount(struct page *page)
{
 if (__builtin_expect(!!(!PageCompound(page)), 1))
  return atomic_read(&page->_mapcount) + 1;
 return total_compound_mapcount(((typeof(page))_compound_head(page)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_large_is_mapped(struct folio *folio)
{
 /*
	 * Reading folio_mapcount_ptr() below could be omitted if hugetlb
	 * participated in incrementing subpages_mapcount when compound mapped.
	 */
 return atomic_read(folio_subpages_mapcount_ptr(folio)) > 0 ||
  atomic_read(folio_mapcount_ptr(folio)) >= 0;
}

/**
 * folio_mapped - Is this folio mapped into userspace?
 * @folio: The folio.
 *
 * Return: True if any page in this folio is referenced by user page tables.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_mapped(struct folio *folio)
{
 if (__builtin_expect(!!(!folio_test_large(folio)), 1))
  return atomic_read(&folio->_mapcount) >= 0;
 return folio_large_is_mapped(folio);
}

/*
 * Return true if this page is mapped into pagetables.
 * For compound page it returns true if any sub-page of compound page is mapped,
 * even if this particular sub-page is not itself mapped by any PTE or PMD.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_mapped(struct page *page)
{
 if (__builtin_expect(!!(!PageCompound(page)), 1))
  return atomic_read(&page->_mapcount) >= 0;
 return folio_large_is_mapped((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *virt_to_head_page(const void *x)
{
 struct page *page = ({ u64 __idx = (((u64)x) - ((-((((1UL))) << ((48)))))) / ((1UL) << 12); u64 __addr = (-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) + (__idx * sizeof(struct page)); (struct page *)__addr; });

 return ((typeof(page))_compound_head(page));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct folio *virt_to_folio(const void *x)
{
 struct page *page = ({ u64 __idx = (((u64)x) - ((-((((1UL))) << ((48)))))) / ((1UL) << 12); u64 __addr = (-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) + (__idx * sizeof(struct page)); (struct page *)__addr; });

 return (_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page)));
}

void __folio_put(struct folio *folio);

void put_pages_list(struct list_head *pages);

void split_page(struct page *page, unsigned int order);
void folio_copy(struct folio *dst, struct folio *src);

unsigned long nr_free_buffer_pages(void);

/*
 * Compound pages have a destructor function.  Provide a
 * prototype for that function and accessor functions.
 * These are _only_ valid on the head of a compound page.
 */
typedef void compound_page_dtor(struct page *);

/* Keep the enum in sync with compound_page_dtors array in mm/page_alloc.c */
enum compound_dtor_id {
 NULL_COMPOUND_DTOR,
 COMPOUND_PAGE_DTOR,

 HUGETLB_PAGE_DTOR,


 TRANSHUGE_PAGE_DTOR,

 NR_COMPOUND_DTORS,
};
extern compound_page_dtor * const compound_page_dtors[NR_COMPOUND_DTORS];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_compound_page_dtor(struct page *page,
  enum compound_dtor_id compound_dtor)
{
 ((void)(sizeof(( long)(compound_dtor >= NR_COMPOUND_DTORS))));
 page[1].compound_dtor = compound_dtor;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_set_compound_dtor(struct folio *folio,
  enum compound_dtor_id compound_dtor)
{
 ((void)(sizeof(( long)(compound_dtor >= NR_COMPOUND_DTORS))));
 folio->_folio_dtor = compound_dtor;
}

void destroy_large_folio(struct folio *folio);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int head_compound_pincount(struct page *head)
{
 return atomic_read(compound_pincount_ptr(head));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_compound_order(struct page *page, unsigned int order)
{
 page[1].compound_order = order;

 page[1].compound_nr = 1U << order;

}

/*
 * folio_set_compound_order is generally passed a non-zero order to
 * initialize a large folio.  However, hugetlb code abuses this by
 * passing in zero when 'dissolving' a large folio.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_set_compound_order(struct folio *folio,
  unsigned int order)
{
 ((void)(sizeof(( long)(!folio_test_large(folio)))));

 folio->_folio_order = order;

 folio->_folio_nr_pages = order ? 1U << order : 0;

}

/* Returns the number of pages in this potentially compound page. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long compound_nr(struct page *page)
{
 if (!PageHead(page))
  return 1;

 return page[1].compound_nr;



}

/* Returns the number of bytes in this potentially compound page. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long page_size(struct page *page)
{
 return ((1UL) << 12) << compound_order(page);
}

/* Returns the number of bits needed for the number of bytes in a page */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int page_shift(struct page *page)
{
 return 12 + compound_order(page);
}

/**
 * thp_order - Order of a transparent huge page.
 * @page: Head page of a transparent huge page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int thp_order(struct page *page)
{
 ((void)(sizeof(( long)(PageTail(page)))));
 return compound_order(page);
}

/**
 * thp_nr_pages - The number of regular pages in this huge page.
 * @page: The head page of a huge page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int thp_nr_pages(struct page *page)
{
 ((void)(sizeof(( long)(PageTail(page)))));
 return compound_nr(page);
}

/**
 * thp_size - Size of a transparent huge page.
 * @page: Head page of a transparent huge page.
 *
 * Return: Number of bytes in this page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long thp_size(struct page *page)
{
 return ((1UL) << 12) << thp_order(page);
}

void free_compound_page(struct page *page);


/*
 * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when
 * servicing faults for write access.  In the normal case, do always want
 * pte_mkwrite.  But get_user_pages can cause write faults for mappings
 * that do not have writing enabled, when used by access_process_vm.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
{
 if (__builtin_expect(!!(vma->vm_flags & 0x00000002), 1))
  pte = pte_mkwrite(pte);
 return pte;
}

vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page);
void do_set_pte(struct vm_fault *vmf, struct page *page, unsigned long addr);

vm_fault_t finish_fault(struct vm_fault *vmf);
vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);


/*
 * Multiple processes may "see" the same page. E.g. for untouched
 * mappings of /dev/null, all processes see the same page full of
 * zeroes, and text pages of executables and shared libraries have
 * only one copy in memory, at most, normally.
 *
 * For the non-reserved pages, page_count(page) denotes a reference count.
 *   page_count() == 0 means the page is free. page->lru is then used for
 *   freelist management in the buddy allocator.
 *   page_count() > 0  means the page has been allocated.
 *
 * Pages are allocated by the slab allocator in order to provide memory
 * to kmalloc and kmem_cache_alloc. In this case, the management of the
 * page, and the fields in 'struct page' are the responsibility of mm/slab.c
 * unless a particular usage is carefully commented. (the responsibility of
 * freeing the kmalloc memory is the caller's, of course).
 *
 * A page may be used by anyone else who does a __get_free_page().
 * In this case, page_count still tracks the references, and should only
 * be used through the normal accessor functions. The top bits of page->flags
 * and page->virtual store page management information, but all other fields
 * are unused and could be used privately, carefully. The management of this
 * page is the responsibility of the one who allocated it, and those who have
 * subsequently been given references to it.
 *
 * The other pages (we may call them "pagecache pages") are completely
 * managed by the Linux memory manager: I/O, buffers, swapping etc.
 * The following discussion applies only to them.
 *
 * A pagecache page contains an opaque `private' member, which belongs to the
 * page's address_space. Usually, this is the address of a circular list of
 * the page's disk buffers. PG_private must be set to tell the VM to call
 * into the filesystem to release these pages.
 *
 * A page may belong to an inode's memory mapping. In this case, page->mapping
 * is the pointer to the inode, and page->index is the file offset of the page,
 * in units of PAGE_SIZE.
 *
 * If pagecache pages are not associated with an inode, they are said to be
 * anonymous pages. These may become associated with the swapcache, and in that
 * case PG_swapcache is set, and page->private is an offset into the swapcache.
 *
 * In either case (swapcache or inode backed), the pagecache itself holds one
 * reference to the page. Setting PG_private should also increment the
 * refcount. The each user mapping also has a reference to the page.
 *
 * The pagecache pages are stored in a per-mapping radix tree, which is
 * rooted at mapping->i_pages, and indexed by offset.
 * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space
 * lists, we instead now tag pages as dirty/writeback in the radix tree.
 *
 * All pagecache pages may be subject to I/O:
 * - inode pages may need to be read from disk,
 * - inode pages which have been modified and are MAP_SHARED may need
 *   to be written back to the inode on disk,
 * - anonymous pages (including MAP_PRIVATE file mappings) which have been
 *   modified may need to be swapped out to swap space and (later) to be read
 *   back into memory.
 */
# 1189 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool put_devmap_managed_page_refs(struct page *page, int refs)
{
 return false;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool put_devmap_managed_page(struct page *page)
{
 return put_devmap_managed_page_refs(page, 1);
}

/* 127: arbitrary random number, small enough to assemble well */



/**
 * folio_get - Increment the reference count on a folio.
 * @folio: The folio.
 *
 * Context: May be called in any context, as long as you know that
 * you have a refcount on the folio.  If you do not already have one,
 * folio_try_get() may be the right interface for you to use.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_get(struct folio *folio)
{
 ((void)(sizeof(( long)(((unsigned int) folio_ref_count(folio) + 127u <= 127u)))));
 folio_ref_inc(folio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void get_page(struct page *page)
{
 folio_get((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

int __attribute__((__warn_unused_result__)) try_grab_page(struct page *page, unsigned int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) bool try_get_page(struct page *page)
{
 page = ((typeof(page))_compound_head(page));
 if (({ int __ret_warn_on = !!(page_ref_count(page) <= 0); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/mm.h\"; .popsection; .long 14472b - .; .short 1228; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); }))
  return false;
 page_ref_inc(page);
 return true;
}

/**
 * folio_put - Decrement the reference count on a folio.
 * @folio: The folio.
 *
 * If the folio's reference count reaches zero, the memory will be
 * released back to the page allocator and may be used by another
 * allocation immediately.  Do not access the memory or the struct folio
 * after calling folio_put() unless you can be sure that it wasn't the
 * last reference.
 *
 * Context: May be called in process or interrupt context, but not in NMI
 * context.  May be called while holding a spinlock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_put(struct folio *folio)
{
 if (folio_put_testzero(folio))
  __folio_put(folio);
}

/**
 * folio_put_refs - Reduce the reference count on a folio.
 * @folio: The folio.
 * @refs: The amount to subtract from the folio's reference count.
 *
 * If the folio's reference count reaches zero, the memory will be
 * released back to the page allocator and may be used by another
 * allocation immediately.  Do not access the memory or the struct folio
 * after calling folio_put_refs() unless you can be sure that these weren't
 * the last references.
 *
 * Context: May be called in process or interrupt context, but not in NMI
 * context.  May be called while holding a spinlock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_put_refs(struct folio *folio, int refs)
{
 if (folio_ref_sub_and_test(folio, refs))
  __folio_put(folio);
}

/**
 * release_pages - release an array of pages or folios
 *
 * This just releases a simple array of multiple pages, and
 * accepts various different forms of said page array: either
 * a regular old boring array of pages, an array of folios, or
 * an array of encoded page pointers.
 *
 * The transparent union syntax for this kind of "any of these
 * argument types" is all kinds of ugly, so look away.
 */
typedef union {
 struct page **pages;
 struct folio **folios;
 struct encoded_page **encoded_pages;
} release_pages_arg __attribute__ ((__transparent_union__));

void release_pages(release_pages_arg, int nr);

/**
 * folios_put - Decrement the reference count on an array of folios.
 * @folios: The folios.
 * @nr: How many folios there are.
 *
 * Like folio_put(), but for an array of folios.  This is more efficient
 * than writing the loop yourself as it will optimise the locks which
 * need to be taken if the folios are freed.
 *
 * Context: May be called in process or interrupt context, but not in NMI
 * context.  May be called while holding a spinlock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folios_put(struct folio **folios, unsigned int nr)
{
 release_pages(folios, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void put_page(struct page *page)
{
 struct folio *folio = (_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page)));

 /*
	 * For some devmap managed pages we need to catch refcount transition
	 * from 2 to 1:
	 */
 if (put_devmap_managed_page(&folio->page))
  return;
 folio_put(folio);
}

/*
 * GUP_PIN_COUNTING_BIAS, and the associated functions that use it, overload
 * the page's refcount so that two separate items are tracked: the original page
 * reference count, and also a new count of how many pin_user_pages() calls were
 * made against the page. ("gup-pinned" is another term for the latter).
 *
 * With this scheme, pin_user_pages() becomes special: such pages are marked as
 * distinct from normal pages. As such, the unpin_user_page() call (and its
 * variants) must be used in order to release gup-pinned pages.
 *
 * Choice of value:
 *
 * By making GUP_PIN_COUNTING_BIAS a power of two, debugging of page reference
 * counts with respect to pin_user_pages() and unpin_user_page() becomes
 * simpler, due to the fact that adding an even power of two to the page
 * refcount has the effect of using only the upper N bits, for the code that
 * counts up using the bias value. This means that the lower bits are left for
 * the exclusive use of the original code that increments and decrements by one
 * (or at least, by much smaller values than the bias value).
 *
 * Of course, once the lower bits overflow into the upper bits (and this is
 * OK, because subtraction recovers the original values), then visual inspection
 * no longer suffices to directly view the separate counts. However, for normal
 * applications that don't have huge page reference counts, this won't be an
 * issue.
 *
 * Locking: the lockless algorithm described in folio_try_get_rcu()
 * provides safe operation for get_user_pages(), page_mkclean() and
 * other calls that race to set up page table entries.
 */


void unpin_user_page(struct page *page);
void unpin_user_pages_dirty_lock(struct page **pages, unsigned long npages,
     bool make_dirty);
void unpin_user_page_range_dirty_lock(struct page *page, unsigned long npages,
          bool make_dirty);
void unpin_user_pages(struct page **pages, unsigned long npages);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_cow_mapping(vm_flags_t flags)
{
 return (flags & (0x00000008 | 0x00000020)) == 0x00000020;
}





/*
 * The identification function is mainly used by the buddy allocator for
 * determining if two pages could be buddies. We are not really identifying
 * the zone since we could be using the section number id if we do not have
 * node id available in page flags.
 * We only guarantee that it will return the same value for two combinable
 * pages in a zone.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_zone_id(struct page *page)
{
 return (page->flags >> ((((((sizeof(unsigned long)*8) - 0) - 4) < ((((sizeof(unsigned long)*8) - 0) - 4) - 2)) ? (((sizeof(unsigned long)*8) - 0) - 4) : ((((sizeof(unsigned long)*8) - 0) - 4) - 2)) * ((4 + 2) != 0))) & ((1UL << (4 + 2)) - 1);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_to_nid(const struct page *page)
{
 struct page *p = (struct page *)page;

 return (({ ((void)(sizeof(( long)(PagePoisoned(p))))); p; })->flags >> ((((sizeof(unsigned long)*8) - 0) - 4) * (4 != 0))) & ((1UL << 4) - 1);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int folio_nid(const struct folio *folio)
{
 return page_to_nid(&folio->page);
}


/* page access time bits needs to hold at least 4 seconds */
# 1412 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpu_pid_to_cpupid(int cpu, int pid)
{
 return ((cpu & ((1 << 8 /* ilog2(CONFIG_NR_CPUS) */)-1)) << 8) | (pid & ((1 << 8)-1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpupid_to_pid(int cpupid)
{
 return cpupid & ((1 << 8)-1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpupid_to_cpu(int cpupid)
{
 return (cpupid >> 8) & ((1 << 8 /* ilog2(CONFIG_NR_CPUS) */)-1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpupid_to_nid(int cpupid)
{
 return cpu_to_node(cpupid_to_cpu(cpupid));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpupid_pid_unset(int cpupid)
{
 return cpupid_to_pid(cpupid) == (-1 & ((1 << 8)-1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cpupid_cpu_unset(int cpupid)
{
 return cpupid_to_cpu(cpupid) == (-1 & ((1 << 8 /* ilog2(CONFIG_NR_CPUS) */)-1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __cpupid_match_pid(pid_t task_pid, int cpupid)
{
 return (task_pid & ((1 << 8)-1)) == cpupid_to_pid(cpupid);
}
# 1463 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int page_cpupid_last(struct page *page)
{
 return (page->flags >> ((((((sizeof(unsigned long)*8) - 0) - 4) - 2) - (8 +8 /* ilog2(CONFIG_NR_CPUS) */)) * ((8 +8 /* ilog2(CONFIG_NR_CPUS) */) != 0))) & ((1UL << (8 +8 /* ilog2(CONFIG_NR_CPUS) */)) - 1);
}

extern int page_cpupid_xchg_last(struct page *page, int cpupid);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_cpupid_reset_last(struct page *page)
{
 page->flags |= ((1UL << (8 +8 /* ilog2(CONFIG_NR_CPUS) */)) - 1) << ((((((sizeof(unsigned long)*8) - 0) - 4) - 2) - (8 +8 /* ilog2(CONFIG_NR_CPUS) */)) * ((8 +8 /* ilog2(CONFIG_NR_CPUS) */) != 0));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int xchg_page_access_time(struct page *page, int time)
{
 int last_time;

 last_time = page_cpupid_xchg_last(page, time >> 0);
 return last_time << 0;
}
# 1578 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u8 page_kasan_tag(const struct page *page)
{
 return 0xff;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_kasan_tag_set(struct page *page, u8 tag) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void page_kasan_tag_reset(struct page *page) { }



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct zone *page_zone(const struct page *page)
{
 return &(node_data[(page_to_nid(page))])->node_zones[page_zonenum(page)];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pg_data_t *page_pgdat(const struct page *page)
{
 return (node_data[(page_to_nid(page))]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct zone *folio_zone(const struct folio *folio)
{
 return page_zone(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pg_data_t *folio_pgdat(const struct folio *folio)
{
 return page_pgdat(&folio->page);
}
# 1621 "./include/linux/mm.h"
/**
 * folio_pfn - Return the Page Frame Number of a folio.
 * @folio: The folio.
 *
 * A folio may contain multiple pages.  The pages have consecutive
 * Page Frame Numbers.
 *
 * Return: The Page Frame Number of the first page in the folio.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long folio_pfn(struct folio *folio)
{
 return (unsigned long)((&folio->page) - ((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct folio *pfn_folio(unsigned long pfn)
{
 return (_Generic(((((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)) + (pfn))), const struct page *: (const struct folio *)_compound_head((((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)) + (pfn))), struct page *: (struct folio *)_compound_head((((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)) + (pfn)))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) atomic_t *folio_pincount_ptr(struct folio *folio)
{
 return &((&(folio)->page) + (1))->compound_pincount;
}

/**
 * folio_maybe_dma_pinned - Report if a folio may be pinned for DMA.
 * @folio: The folio.
 *
 * This function checks if a folio has been pinned via a call to
 * a function in the pin_user_pages() family.
 *
 * For small folios, the return value is partially fuzzy: false is not fuzzy,
 * because it means "definitely not pinned for DMA", but true means "probably
 * pinned for DMA, but possibly a false positive due to having at least
 * GUP_PIN_COUNTING_BIAS worth of normal folio references".
 *
 * False positives are OK, because: a) it's unlikely for a folio to
 * get that many refcounts, and b) all the callers of this routine are
 * expected to be able to deal gracefully with a false positive.
 *
 * For large folios, the result will be exactly correct. That's because
 * we have more tracking data available: the compound_pincount is used
 * instead of the GUP_PIN_COUNTING_BIAS scheme.
 *
 * For more information, please see Documentation/core-api/pin_user_pages.rst.
 *
 * Return: True, if it is likely that the page has been "dma-pinned".
 * False, if the page is definitely not dma-pinned.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_maybe_dma_pinned(struct folio *folio)
{
 if (folio_test_large(folio))
  return atomic_read(folio_pincount_ptr(folio)) > 0;

 /*
	 * folio_ref_count() is signed. If that refcount overflows, then
	 * folio_ref_count() returns a negative value, and callers will avoid
	 * further incrementing the refcount.
	 *
	 * Here, for that overflow case, use the sign bit to count a little
	 * bit higher via unsigned math, and thus still get an accurate result.
	 */
 return ((unsigned int)folio_ref_count(folio)) >=
  (1U << 10);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_maybe_dma_pinned(struct page *page)
{
 return folio_maybe_dma_pinned((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

/*
 * This should most likely only be called during fork() to see whether we
 * should break the cow immediately for an anon page on the src mm.
 *
 * The caller has to hold the PT lock and the vma->vm_mm->->write_protect_seq.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_needs_cow_for_dma(struct vm_area_struct *vma,
       struct page *page)
{
 ((void)(sizeof(( long)(!(({ unsigned __seq = _Generic(*(&vma->vm_mm->write_protect_seq), seqcount_t: __seqprop_sequence((void *)(&vma->vm_mm->write_protect_seq)), seqcount_raw_spinlock_t: __seqprop_raw_spinlock_sequence((void *)((&vma->vm_mm->write_protect_seq))), seqcount_spinlock_t: __seqprop_spinlock_sequence((void *)((&vma->vm_mm->write_protect_seq))), seqcount_rwlock_t: __seqprop_rwlock_sequence((void *)((&vma->vm_mm->write_protect_seq))), seqcount_mutex_t: __seqprop_mutex_sequence((void *)((&vma->vm_mm->write_protect_seq)))); do { do { } while (0); asm volatile("dmb " "ishld" : : : "memory"); } while (0); kcsan_atomic_next(1000); __seq; }) & 1)))));

 if (!((__builtin_constant_p(27 /* FOLL_PIN has run, never cleared */) && __builtin_constant_p((uintptr_t)(&vma->vm_mm->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&vma->vm_mm->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&vma->vm_mm->flags))) ? const_test_bit(27 /* FOLL_PIN has run, never cleared */, &vma->vm_mm->flags) : generic_test_bit(27 /* FOLL_PIN has run, never cleared */, &vma->vm_mm->flags)))
  return false;

 return page_maybe_dma_pinned(page);
}

/* MIGRATE_CMA and ZONE_MOVABLE do not allow pin pages */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_longterm_pinnable_page(struct page *page)
{

 int mt = get_pfnblock_flags_mask(page, (unsigned long)((page) - ((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12))), ((1UL << 3) - 1));

 if (mt == MIGRATE_CMA || mt == MIGRATE_ISOLATE)
  return false;

 /* The zero page may always be pinned */
 if (is_zero_pfn((unsigned long)((page) - ((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)))))
  return true;

 /* Coherent device memory must always allow eviction. */
 if (is_device_coherent_page(page))
  return false;

 /* Otherwise, non-movable zone pages can be pinned. */
 return !is_zone_movable_page(page);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool folio_is_longterm_pinnable(struct folio *folio)
{
 return is_longterm_pinnable_page(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_page_zone(struct page *page, enum zone_type zone)
{
 page->flags &= ~(((1UL << 2) - 1) << (((((sizeof(unsigned long)*8) - 0) - 4) - 2) * (2 != 0)));
 page->flags |= (zone & ((1UL << 2) - 1)) << (((((sizeof(unsigned long)*8) - 0) - 4) - 2) * (2 != 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_page_node(struct page *page, unsigned long node)
{
 page->flags &= ~(((1UL << 4) - 1) << ((((sizeof(unsigned long)*8) - 0) - 4) * (4 != 0)));
 page->flags |= (node & ((1UL << 4) - 1)) << ((((sizeof(unsigned long)*8) - 0) - 4) * (4 != 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_page_links(struct page *page, enum zone_type zone,
 unsigned long node, unsigned long pfn)
{
 set_page_zone(page, zone);
 set_page_node(page, node);



}

/**
 * folio_nr_pages - The number of pages in the folio.
 * @folio: The folio.
 *
 * Return: A positive power of two.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) long folio_nr_pages(struct folio *folio)
{
 if (!folio_test_large(folio))
  return 1;

 return folio->_folio_nr_pages;



}

/**
 * folio_next - Move to the next physical folio.
 * @folio: The folio we're currently operating on.
 *
 * If you have physically contiguous memory which may span more than
 * one folio (eg a &struct bio_vec), use this function to move from one
 * folio to the next.  Do not use it if the memory is only virtually
 * contiguous as the folios are almost certainly not adjacent to each
 * other.  This is the folio equivalent to writing ``page++``.
 *
 * Context: We assume that the folios are refcounted and/or locked at a
 * higher level and do not adjust the reference counts.
 * Return: The next struct folio.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct folio *folio_next(struct folio *folio)
{
 return (struct folio *)((&(folio)->page) + (folio_nr_pages(folio)));
}

/**
 * folio_shift - The size of the memory described by this folio.
 * @folio: The folio.
 *
 * A folio represents a number of bytes which is a power-of-two in size.
 * This function tells you which power-of-two the folio is.  See also
 * folio_size() and folio_order().
 *
 * Context: The caller should have a reference on the folio to prevent
 * it from being split.  It is not necessary for the folio to be locked.
 * Return: The base-2 logarithm of the size of this folio.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int folio_shift(struct folio *folio)
{
 return 12 + folio_order(folio);
}

/**
 * folio_size - The number of bytes in a folio.
 * @folio: The folio.
 *
 * Context: The caller should have a reference on the folio to prevent
 * it from being split.  It is not necessary for the folio to be locked.
 * Return: The number of bytes in this folio.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t folio_size(struct folio *folio)
{
 return ((1UL) << 12) << folio_order(folio);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_make_page_accessible(struct page *page)
{
 return 0;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_make_folio_accessible(struct folio *folio)
{
 int ret;
 long i, nr = folio_nr_pages(folio);

 for (i = 0; i < nr; i++) {
  ret = arch_make_page_accessible(((&(folio)->page) + (i)));
  if (ret)
   break;
 }

 return ret;
}


/*
 * Some inline functions in vmstat.h depend on page_zone()
 */
# 1 "./include/linux/vmstat.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






# 1 "./include/linux/vm_event_item.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 32 "./include/linux/vm_event_item.h"
enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
  PGALLOC_DMA, PGALLOC_DMA32, PGALLOC_NORMAL, PGALLOC_MOVABLE,
  ALLOCSTALL_DMA, ALLOCSTALL_DMA32, ALLOCSTALL_NORMAL, ALLOCSTALL_MOVABLE,
  PGSCAN_SKIP_DMA, PGSCAN_SKIP_DMA32, PGSCAN_SKIP_NORMAL, PGSCAN_SKIP_MOVABLE,
  PGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,
  PGFAULT, PGMAJFAULT,
  PGLAZYFREED,
  PGREFILL,
  PGREUSE,
  PGSTEAL_KSWAPD,
  PGSTEAL_DIRECT,
  PGSTEAL_KHUGEPAGED,
  PGDEMOTE_KSWAPD,
  PGDEMOTE_DIRECT,
  PGDEMOTE_KHUGEPAGED,
  PGSCAN_KSWAPD,
  PGSCAN_DIRECT,
  PGSCAN_KHUGEPAGED,
  PGSCAN_DIRECT_THROTTLE,
  PGSCAN_ANON,
  PGSCAN_FILE,
  PGSTEAL_ANON,
  PGSTEAL_FILE,

  PGSCAN_ZONE_RECLAIM_FAILED,

  PGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,
  KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
  PAGEOUTRUN, PGROTATED,
  DROP_PAGECACHE, DROP_SLAB,
  OOM_KILL,

  NUMA_PTE_UPDATES,
  NUMA_HUGE_PTE_UPDATES,
  NUMA_HINT_FAULTS,
  NUMA_HINT_FAULTS_LOCAL,
  NUMA_PAGE_MIGRATE,


  PGMIGRATE_SUCCESS, PGMIGRATE_FAIL,
  THP_MIGRATION_SUCCESS,
  THP_MIGRATION_FAIL,
  THP_MIGRATION_SPLIT,


  COMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,
  COMPACTISOLATED,
  COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
  KCOMPACTD_WAKE,
  KCOMPACTD_MIGRATE_SCANNED, KCOMPACTD_FREE_SCANNED,


  HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,


  CMA_ALLOC_SUCCESS,
  CMA_ALLOC_FAIL,

  UNEVICTABLE_PGCULLED, /* culled to noreclaim list */
  UNEVICTABLE_PGSCANNED, /* scanned for reclaimability */
  UNEVICTABLE_PGRESCUED, /* rescued from noreclaim list */
  UNEVICTABLE_PGMLOCKED,
  UNEVICTABLE_PGMUNLOCKED,
  UNEVICTABLE_PGCLEARED, /* on COW, page truncate */
  UNEVICTABLE_PGSTRANDED, /* unable to isolate on unlock */

  THP_FAULT_ALLOC,
  THP_FAULT_FALLBACK,
  THP_FAULT_FALLBACK_CHARGE,
  THP_COLLAPSE_ALLOC,
  THP_COLLAPSE_ALLOC_FAILED,
  THP_FILE_ALLOC,
  THP_FILE_FALLBACK,
  THP_FILE_FALLBACK_CHARGE,
  THP_FILE_MAPPED,
  THP_SPLIT_PAGE,
  THP_SPLIT_PAGE_FAILED,
  THP_DEFERRED_SPLIT_PAGE,
  THP_SPLIT_PMD,
  THP_SCAN_EXCEED_NONE_PTE,
  THP_SCAN_EXCEED_SWAP_PTE,
  THP_SCAN_EXCEED_SHARED_PTE,



  THP_ZERO_PAGE_ALLOC,
  THP_ZERO_PAGE_ALLOC_FAILED,
  THP_SWPOUT,
  THP_SWPOUT_FALLBACK,


  BALLOON_INFLATE,
  BALLOON_DEFLATE,

  BALLOON_MIGRATE,
# 136 "./include/linux/vm_event_item.h"
  SWAP_RA,
  SWAP_RA_HIT,

  KSM_SWPIN_COPY,



  COW_KSM,
# 153 "./include/linux/vm_event_item.h"
  NR_VM_EVENT_ITEMS
};
# 9 "./include/linux/vmstat.h" 2

# 1 "./include/linux/static_key.h" 1
# 11 "./include/linux/vmstat.h" 2


extern int sysctl_stat_interval;




extern int sysctl_vm_numa_stat;
extern struct static_key_true vm_numa_stat_key;
int sysctl_vm_numa_stat_handler(struct ctl_table *table, int write,
  void *buffer, size_t *length, loff_t *ppos);


struct reclaim_stat {
 unsigned nr_dirty;
 unsigned nr_unqueued_dirty;
 unsigned nr_congested;
 unsigned nr_writeback;
 unsigned nr_immediate;
 unsigned nr_pageout;
 unsigned nr_activate[2];
 unsigned nr_ref_keep;
 unsigned nr_unmap_fail;
 unsigned nr_lazyfree_fail;
};

enum writeback_stat_item {
 NR_DIRTY_THRESHOLD,
 NR_DIRTY_BG_THRESHOLD,
 NR_VM_WRITEBACK_STAT_ITEMS,
};


/*
 * Light weight per cpu counter implementation.
 *
 * Counters should only be incremented and no critical kernel component
 * should rely on the counter values.
 *
 * Counters are handled completely inline. On many platforms the code
 * generated will simply be the increment of a global address.
 */

struct vm_event_state {
 unsigned long event[NR_VM_EVENT_ITEMS];
};

extern /* nothing */ __attribute__((section(".data..percpu" ""))) __typeof__(struct vm_event_state) vm_event_states;

/*
 * vm counters are allowed to be racy. Use raw_cpu_ops to avoid the
 * local_irq_disable overhead.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __count_vm_event(enum vm_event_item item)
{
 do { do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(vm_event_states.event[item])) { case 1: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; case 2: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; case 4: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; case 8: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; default: __bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void count_vm_event(enum vm_event_item item)
{
 do { do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(vm_event_states.event[item])) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), 1); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __count_vm_events(enum vm_event_item item, long delta)
{
 do { do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(vm_event_states.event[item])) { case 1: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; case 2: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; case 4: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; case 8: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; default: __bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void count_vm_events(enum vm_event_item item, long delta)
{
 do { do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(vm_event_states.event[item])) { case 1: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_8(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), delta); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 2: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_16(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), delta); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 4: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_32(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), delta); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; case 8: ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __percpu_add_case_64(({ do { const void /* nothing */ *__vpp_verify = (typeof((&(vm_event_states.event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item]))); (typeof((typeof(*(&(vm_event_states.event[item]))) *)(&(vm_event_states.event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }), delta); do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule_notrace(); } while (0); });break; default: __bad_size_call_parameter();break; } } while (0);
}

extern void all_vm_events(unsigned long *);

extern void vm_events_fold_cpu(int cpu);
# 131 "./include/linux/vmstat.h"
/*
 * Zone and node-based page accounting with per cpu differentials.
 */
extern atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS];
extern atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS];
extern atomic_long_t vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_numa_event_add(long x, struct zone *zone,
    enum numa_stat_item item)
{
 atomic_long_add(x, &zone->vm_numa_event[item]);
 atomic_long_add(x, &vm_numa_event[item]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long zone_numa_event_state(struct zone *zone,
     enum numa_stat_item item)
{
 return atomic_long_read(&zone->vm_numa_event[item]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
global_numa_event_state(enum numa_stat_item item)
{
 return atomic_long_read(&vm_numa_event[item]);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_page_state_add(long x, struct zone *zone,
     enum zone_stat_item item)
{
 atomic_long_add(x, &zone->vm_stat[item]);
 atomic_long_add(x, &vm_zone_stat[item]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_page_state_add(long x, struct pglist_data *pgdat,
     enum node_stat_item item)
{
 atomic_long_add(x, &pgdat->vm_stat[item]);
 atomic_long_add(x, &vm_node_stat[item]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long global_zone_page_state(enum zone_stat_item item)
{
 long x = atomic_long_read(&vm_zone_stat[item]);

 if (x < 0)
  x = 0;

 return x;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long global_node_page_state_pages(enum node_stat_item item)
{
 long x = atomic_long_read(&vm_node_stat[item]);

 if (x < 0)
  x = 0;

 return x;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long global_node_page_state(enum node_stat_item item)
{
 ((void)(sizeof(( long)(vmstat_item_in_bytes(item)))));

 return global_node_page_state_pages(item);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long zone_page_state(struct zone *zone,
     enum zone_stat_item item)
{
 long x = atomic_long_read(&zone->vm_stat[item]);

 if (x < 0)
  x = 0;

 return x;
}

/*
 * More accurate version that also considers the currently pending
 * deltas. For that we need to loop over all cpus to find the current
 * deltas. There is no synchronization so the result cannot be
 * exactly accurate either.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long zone_page_state_snapshot(struct zone *zone,
     enum zone_stat_item item)
{
 long x = atomic_long_read(&zone->vm_stat[item]);


 int cpu;
 for (((cpu)) = 0; ((cpu)) = find_next_bit((((((const struct cpumask *)&__cpu_online_mask))->bits)), (nr_cpu_ids), ((cpu))), ((cpu)) < (nr_cpu_ids); ((cpu))++)
  x += ({ do { const void /* nothing */ *__vpp_verify = (typeof((zone->per_cpu_zonestats) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((zone->per_cpu_zonestats))) *)((zone->per_cpu_zonestats))); (typeof((typeof(*((zone->per_cpu_zonestats))) *)((zone->per_cpu_zonestats)))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); })->vm_stat_diff[item];

 if (x < 0)
  x = 0;

 return x;
}


/* See __count_vm_event comment on why raw_cpu_inc is used. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
__count_numa_event(struct zone *zone, enum numa_stat_item item)
{
 struct per_cpu_zonestat /* nothing */ *pzstats = zone->per_cpu_zonestats;

 do { do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(pzstats->vm_numa_event[item])) { case 1: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; case 2: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; case 4: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; case 8: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += 1; } while (0);break; default: __bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void
__count_numa_events(struct zone *zone, enum numa_stat_item item, long delta)
{
 struct per_cpu_zonestat /* nothing */ *pzstats = zone->per_cpu_zonestats;

 do { do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(pzstats->vm_numa_event[item])) { case 1: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; case 2: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; case 4: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; case 8: do { *({ do { const void /* nothing */ *__vpp_verify = (typeof((&(pzstats->vm_numa_event[item])) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item]))); (typeof((typeof(*(&(pzstats->vm_numa_event[item]))) *)(&(pzstats->vm_numa_event[item])))) (__ptr + ((__kern_my_cpu_offset()))); }); }) += delta; } while (0);break; default: __bad_size_call_parameter();break; } } while (0);
}

extern unsigned long sum_zone_node_page_state(int node,
           enum zone_stat_item item);
extern unsigned long sum_zone_numa_event_state(int node, enum numa_stat_item item);
extern unsigned long node_page_state(struct pglist_data *pgdat,
      enum node_stat_item item);
extern unsigned long node_page_state_pages(struct pglist_data *pgdat,
        enum node_stat_item item);
extern void fold_vm_numa_events(void);
# 270 "./include/linux/vmstat.h"
void __mod_zone_page_state(struct zone *, enum zone_stat_item item, long);
void __inc_zone_page_state(struct page *, enum zone_stat_item);
void __dec_zone_page_state(struct page *, enum zone_stat_item);

void __mod_node_page_state(struct pglist_data *, enum node_stat_item item, long);
void __inc_node_page_state(struct page *, enum node_stat_item);
void __dec_node_page_state(struct page *, enum node_stat_item);

void mod_zone_page_state(struct zone *, enum zone_stat_item, long);
void inc_zone_page_state(struct page *, enum zone_stat_item);
void dec_zone_page_state(struct page *, enum zone_stat_item);

void mod_node_page_state(struct pglist_data *, enum node_stat_item, long);
void inc_node_page_state(struct page *, enum node_stat_item);
void dec_node_page_state(struct page *, enum node_stat_item);

extern void inc_node_state(struct pglist_data *, enum node_stat_item);
extern void __inc_zone_state(struct zone *, enum zone_stat_item);
extern void __inc_node_state(struct pglist_data *, enum node_stat_item);
extern void dec_zone_state(struct zone *, enum zone_stat_item);
extern void __dec_zone_state(struct zone *, enum zone_stat_item);
extern void __dec_node_state(struct pglist_data *, enum node_stat_item);

void quiet_vmstat(void);
void cpu_vm_stats_fold(int cpu);
void refresh_zone_stat_thresholds(void);

struct ctl_table;
int vmstat_refresh(struct ctl_table *, int write, void *buffer, size_t *lenp,
  loff_t *ppos);

void drain_zonestat(struct zone *zone, struct per_cpu_zonestat *);

int calculate_pressure_threshold(struct zone *zone);
int calculate_normal_threshold(struct zone *zone);
void set_pgdat_percpu_threshold(pg_data_t *pgdat,
    int (*calculate_pressure)(struct zone *));
# 412 "./include/linux/vmstat.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __zone_stat_mod_folio(struct folio *folio,
  enum zone_stat_item item, long nr)
{
 __mod_zone_page_state(folio_zone(folio), item, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __zone_stat_add_folio(struct folio *folio,
  enum zone_stat_item item)
{
 __mod_zone_page_state(folio_zone(folio), item, folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __zone_stat_sub_folio(struct folio *folio,
  enum zone_stat_item item)
{
 __mod_zone_page_state(folio_zone(folio), item, -folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_stat_mod_folio(struct folio *folio,
  enum zone_stat_item item, long nr)
{
 mod_zone_page_state(folio_zone(folio), item, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_stat_add_folio(struct folio *folio,
  enum zone_stat_item item)
{
 mod_zone_page_state(folio_zone(folio), item, folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zone_stat_sub_folio(struct folio *folio,
  enum zone_stat_item item)
{
 mod_zone_page_state(folio_zone(folio), item, -folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __node_stat_mod_folio(struct folio *folio,
  enum node_stat_item item, long nr)
{
 __mod_node_page_state(folio_pgdat(folio), item, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __node_stat_add_folio(struct folio *folio,
  enum node_stat_item item)
{
 __mod_node_page_state(folio_pgdat(folio), item, folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __node_stat_sub_folio(struct folio *folio,
  enum node_stat_item item)
{
 __mod_node_page_state(folio_pgdat(folio), item, -folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_stat_mod_folio(struct folio *folio,
  enum node_stat_item item, long nr)
{
 mod_node_page_state(folio_pgdat(folio), item, nr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_stat_add_folio(struct folio *folio,
  enum node_stat_item item)
{
 mod_node_page_state(folio_pgdat(folio), item, folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void node_stat_sub_folio(struct folio *folio,
  enum node_stat_item item)
{
 mod_node_page_state(folio_pgdat(folio), item, -folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __mod_zone_freepage_state(struct zone *zone, int nr_pages,
          int migratetype)
{
 __mod_zone_page_state(zone, NR_FREE_PAGES, nr_pages);
 if (__builtin_expect(!!((migratetype) == MIGRATE_CMA), 0))
  __mod_zone_page_state(zone, NR_FREE_CMA_PAGES, nr_pages);
}

extern const char * const vmstat_text[];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *zone_stat_name(enum zone_stat_item item)
{
 return vmstat_text[item];
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *numa_stat_name(enum numa_stat_item item)
{
 return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
      item];
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *node_stat_name(enum node_stat_item item)
{
 return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
      NR_VM_NUMA_EVENT_ITEMS +
      item];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *lru_list_name(enum lru_list lru)
{
 return node_stat_name(NR_LRU_BASE + lru) + 3; // skip "nr_"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *writeback_stat_name(enum writeback_stat_item item)
{
 return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
      NR_VM_NUMA_EVENT_ITEMS +
      NR_VM_NODE_STAT_ITEMS +
      item];
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *vm_event_name(enum vm_event_item item)
{
 return vmstat_text[NR_VM_ZONE_STAT_ITEMS +
      NR_VM_NUMA_EVENT_ITEMS +
      NR_VM_NODE_STAT_ITEMS +
      NR_VM_WRITEBACK_STAT_ITEMS +
      item];
}




void __mod_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,
   int val);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mod_lruvec_state(struct lruvec *lruvec,
        enum node_stat_item idx, int val)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); } while (0);
 __mod_lruvec_state(lruvec, idx, val);
 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
}

void __mod_lruvec_page_state(struct page *page,
        enum node_stat_item idx, int val);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mod_lruvec_page_state(struct page *page,
      enum node_stat_item idx, int val)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); } while (0);
 __mod_lruvec_page_state(page, idx, val);
 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
}
# 594 "./include/linux/vmstat.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __inc_lruvec_page_state(struct page *page,
        enum node_stat_item idx)
{
 __mod_lruvec_page_state(page, idx, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __dec_lruvec_page_state(struct page *page,
        enum node_stat_item idx)
{
 __mod_lruvec_page_state(page, idx, -1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __lruvec_stat_mod_folio(struct folio *folio,
        enum node_stat_item idx, int val)
{
 __mod_lruvec_page_state(&folio->page, idx, val);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __lruvec_stat_add_folio(struct folio *folio,
        enum node_stat_item idx)
{
 __lruvec_stat_mod_folio(folio, idx, folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __lruvec_stat_sub_folio(struct folio *folio,
        enum node_stat_item idx)
{
 __lruvec_stat_mod_folio(folio, idx, -folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inc_lruvec_page_state(struct page *page,
      enum node_stat_item idx)
{
 mod_lruvec_page_state(page, idx, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dec_lruvec_page_state(struct page *page,
      enum node_stat_item idx)
{
 mod_lruvec_page_state(page, idx, -1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lruvec_stat_mod_folio(struct folio *folio,
      enum node_stat_item idx, int val)
{
 mod_lruvec_page_state(&folio->page, idx, val);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lruvec_stat_add_folio(struct folio *folio,
      enum node_stat_item idx)
{
 lruvec_stat_mod_folio(folio, idx, folio_nr_pages(folio));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void lruvec_stat_sub_folio(struct folio *folio,
      enum node_stat_item idx)
{
 lruvec_stat_mod_folio(folio, idx, -folio_nr_pages(folio));
}
# 1857 "./include/linux/mm.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void *lowmem_page_address(const struct page *page)
{
 return ({ __typeof__(page) __page = page; u64 __idx = ((u64)__page - (-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) ))))))) / sizeof(struct page); u64 __addr = ((-((((1UL))) << ((48))))) + (__idx * ((1UL) << 12)); (void *)__tag_set((const void *)__addr, page_kasan_tag(__page));});
}
# 1891 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *folio_address(const struct folio *folio)
{
 return lowmem_page_address(&folio->page);
}

extern void *page_rmapping(struct page *page);
extern unsigned long __page_file_index(struct page *page);

/*
 * Return the pagecache index of the passed page.  Regular pagecache pages
 * use ->index whereas swapcache pages use swp_offset(->private)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long page_index(struct page *page)
{
 if (__builtin_expect(!!(PageSwapCache(page)), 0))
  return __page_file_index(page);
 return page->index;
}

/*
 * Return true only if the page has been allocated with
 * ALLOC_NO_WATERMARKS and the low watermark was not
 * met implying that the system is under some pressure.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_is_pfmemalloc(const struct page *page)
{
 /*
	 * lru.next has bit 1 set if the page is allocated from the
	 * pfmemalloc reserves.  Callers may simply overwrite it if
	 * they do not need to preserve that information.
	 */
 return (uintptr_t)page->lru.next & ((((1UL))) << (1));
}

/*
 * Only to be called by the page allocator on a freshly allocated
 * page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_page_pfmemalloc(struct page *page)
{
 page->lru.next = (void *)((((1UL))) << (1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_page_pfmemalloc(struct page *page)
{
 page->lru.next = ((void *)0);
}

/*
 * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.
 */
extern void pagefault_out_of_memory(void);





/*
 * Flags passed to show_mem() and show_free_areas() to suppress output in
 * various contexts.
 */


extern void __show_free_areas(unsigned int flags, nodemask_t *nodemask, int max_zone_idx);
static void __attribute__((__unused__)) show_free_areas(unsigned int flags, nodemask_t *nodemask)
{
 __show_free_areas(flags, nodemask, 4 /* __MAX_NR_ZONES */ - 1);
}

/*
 * Parameter block passed down to zap_pte_range in exceptional cases.
 */
struct zap_details {
 struct folio *single_folio; /* Locked folio to be unmapped */
 bool even_cows; /* Zap COWed private pages too? */
 zap_flags_t zap_flags; /* Extra flags for zapping */
};

/*
 * Whether to drop the pte markers, for example, the uffd-wp information for
 * file-backed memory.  This should only be specified when we will completely
 * drop the page in the mm, either by truncation or unmapping of the vma.  By
 * default, the flag is not set.
 */

/* Set in unmap_vmas() to indicate a final unmap call.  Only used by hugetlb */



extern bool can_do_mlock(void);



extern int user_shm_lock(size_t, struct ucounts *);
extern void user_shm_unlock(size_t, struct ucounts *);

struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
        pte_t pte);
struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,
    pmd_t pmd);

void zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
    unsigned long size);
void zap_page_range(struct vm_area_struct *vma, unsigned long address,
      unsigned long size);
void zap_page_range_single(struct vm_area_struct *vma, unsigned long address,
      unsigned long size, struct zap_details *details);
void unmap_vmas(struct mmu_gather *tlb, struct maple_tree *mt,
  struct vm_area_struct *start_vma, unsigned long start,
  unsigned long end);

struct mmu_notifier_range;

void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
  unsigned long end, unsigned long floor, unsigned long ceiling);
int
copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma);
int follow_pte(struct mm_struct *mm, unsigned long address,
        pte_t **ptepp, spinlock_t **ptlp);
int follow_pfn(struct vm_area_struct *vma, unsigned long address,
 unsigned long *pfn);
int follow_phys(struct vm_area_struct *vma, unsigned long address,
  unsigned int flags, unsigned long *prot, resource_size_t *phys);
int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
   void *buf, int len, int write);

extern void truncate_pagecache(struct inode *inode, loff_t new);
extern void truncate_setsize(struct inode *inode, loff_t newsize);
void pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to);
void truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);
int generic_error_remove_page(struct address_space *mapping, struct page *page);


extern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,
      unsigned long address, unsigned int flags,
      struct pt_regs *regs);
extern int fixup_user_fault(struct mm_struct *mm,
       unsigned long address, unsigned int fault_flags,
       bool *unlocked);
void unmap_mapping_pages(struct address_space *mapping,
  unsigned long start, unsigned long nr, bool even_cows);
void unmap_mapping_range(struct address_space *mapping,
  loff_t const holebegin, loff_t const holelen, int even_cows);
# 2056 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unmap_shared_mapping_range(struct address_space *mapping,
  loff_t const holebegin, loff_t const holelen)
{
 unmap_mapping_range(mapping, holebegin, holelen, 0);
}

extern int access_process_vm(struct task_struct *tsk, unsigned long addr,
  void *buf, int len, unsigned int gup_flags);
extern int access_remote_vm(struct mm_struct *mm, unsigned long addr,
  void *buf, int len, unsigned int gup_flags);
extern int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
         void *buf, int len, unsigned int gup_flags);

long get_user_pages_remote(struct mm_struct *mm,
       unsigned long start, unsigned long nr_pages,
       unsigned int gup_flags, struct page **pages,
       struct vm_area_struct **vmas, int *locked);
long pin_user_pages_remote(struct mm_struct *mm,
      unsigned long start, unsigned long nr_pages,
      unsigned int gup_flags, struct page **pages,
      struct vm_area_struct **vmas, int *locked);
long get_user_pages(unsigned long start, unsigned long nr_pages,
       unsigned int gup_flags, struct page **pages,
       struct vm_area_struct **vmas);
long pin_user_pages(unsigned long start, unsigned long nr_pages,
      unsigned int gup_flags, struct page **pages,
      struct vm_area_struct **vmas);
long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
      struct page **pages, unsigned int gup_flags);
long pin_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
      struct page **pages, unsigned int gup_flags);

int get_user_pages_fast(unsigned long start, int nr_pages,
   unsigned int gup_flags, struct page **pages);
int pin_user_pages_fast(unsigned long start, int nr_pages,
   unsigned int gup_flags, struct page **pages);

int account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc);
int __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,
   struct task_struct *task, bool bypass_rlim);

struct kvec;
int get_kernel_pages(const struct kvec *iov, int nr_pages, int write,
   struct page **pages);
struct page *get_dump_page(unsigned long addr);

bool folio_mark_dirty(struct folio *folio);
bool set_page_dirty(struct page *page);
int set_page_dirty_lock(struct page *page);

int get_cmdline(struct task_struct *task, char *buffer, int buflen);

extern unsigned long move_page_tables(struct vm_area_struct *vma,
  unsigned long old_addr, struct vm_area_struct *new_vma,
  unsigned long new_addr, unsigned long len,
  bool need_rmap_locks);

/*
 * Flags used by change_protection().  For now we make it a bitmap so
 * that we can pass in multiple flags just like parameters.  However
 * for now all the callers are only use one of the flags at the same
 * time.
 */
/*
 * Whether we should manually check if we can map individual PTEs writable,
 * because something (e.g., COW, uffd-wp) blocks that from happening for all
 * PTEs automatically in a writable mapping.
 */

/* Whether this protection change is for NUMA hints */

/* Whether this change is for write protecting */





int vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_wants_manual_pte_write_upgrade(struct vm_area_struct *vma)
{
 /*
	 * We want to check manually if we can change individual PTEs writable
	 * if we can't do that automatically for all PTEs in a mapping. For
	 * private mappings, that's always the case when we have write
	 * permissions as we properly have to handle COW.
	 */
 if (vma->vm_flags & 0x00000008)
  return vma_wants_writenotify(vma, vma->vm_page_prot);
 return !!(vma->vm_flags & 0x00000002);

}
bool can_change_pte_writable(struct vm_area_struct *vma, unsigned long addr,
        pte_t pte);
extern unsigned long change_protection(struct mmu_gather *tlb,
         struct vm_area_struct *vma, unsigned long start,
         unsigned long end, pgprot_t newprot,
         unsigned long cp_flags);
extern int mprotect_fixup(struct mmu_gather *tlb, struct vm_area_struct *vma,
     struct vm_area_struct **pprev, unsigned long start,
     unsigned long end, unsigned long newflags);

/*
 * doesn't attempt to fault and will return short.
 */
int get_user_pages_fast_only(unsigned long start, int nr_pages,
        unsigned int gup_flags, struct page **pages);
int pin_user_pages_fast_only(unsigned long start, int nr_pages,
        unsigned int gup_flags, struct page **pages);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool get_user_page_fast_only(unsigned long addr,
   unsigned int gup_flags, struct page **pagep)
{
 return get_user_pages_fast_only(addr, 1, gup_flags, pagep) == 1;
}
/*
 * per-process(per-mm_struct) statistics.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_mm_counter(struct mm_struct *mm, int member)
{
 return percpu_counter_read_positive(&mm->rss_stat[member]);
}

void mm_trace_rss_stat(struct mm_struct *mm, int member);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void add_mm_counter(struct mm_struct *mm, int member, long value)
{
 percpu_counter_add(&mm->rss_stat[member], value);

 mm_trace_rss_stat(mm, member);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void inc_mm_counter(struct mm_struct *mm, int member)
{
 percpu_counter_inc(&mm->rss_stat[member]);

 mm_trace_rss_stat(mm, member);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dec_mm_counter(struct mm_struct *mm, int member)
{
 percpu_counter_dec(&mm->rss_stat[member]);

 mm_trace_rss_stat(mm, member);
}

/* Optimized variant when page is already known not to be PageAnon */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mm_counter_file(struct page *page)
{
 if (PageSwapBacked(page))
  return MM_SHMEMPAGES;
 return MM_FILEPAGES;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mm_counter(struct page *page)
{
 if (PageAnon(page))
  return MM_ANONPAGES;
 return mm_counter_file(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_mm_rss(struct mm_struct *mm)
{
 return get_mm_counter(mm, MM_FILEPAGES) +
  get_mm_counter(mm, MM_ANONPAGES) +
  get_mm_counter(mm, MM_SHMEMPAGES);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_mm_hiwater_rss(struct mm_struct *mm)
{
 return __builtin_choose_expr(((!!(sizeof((typeof(mm->hiwater_rss) *)1 == (typeof(get_mm_rss(mm)) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(mm->hiwater_rss) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(get_mm_rss(mm)) * 0l)) : (int *)8))))), ((mm->hiwater_rss) > (get_mm_rss(mm)) ? (mm->hiwater_rss) : (get_mm_rss(mm))), ({ typeof(mm->hiwater_rss) __UNIQUE_ID___x365 = (mm->hiwater_rss); typeof(get_mm_rss(mm)) __UNIQUE_ID___y366 = (get_mm_rss(mm)); ((__UNIQUE_ID___x365) > (__UNIQUE_ID___y366) ? (__UNIQUE_ID___x365) : (__UNIQUE_ID___y366)); }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_mm_hiwater_vm(struct mm_struct *mm)
{
 return __builtin_choose_expr(((!!(sizeof((typeof(mm->hiwater_vm) *)1 == (typeof(mm->total_vm) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(mm->hiwater_vm) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(mm->total_vm) * 0l)) : (int *)8))))), ((mm->hiwater_vm) > (mm->total_vm) ? (mm->hiwater_vm) : (mm->total_vm)), ({ typeof(mm->hiwater_vm) __UNIQUE_ID___x367 = (mm->hiwater_vm); typeof(mm->total_vm) __UNIQUE_ID___y368 = (mm->total_vm); ((__UNIQUE_ID___x367) > (__UNIQUE_ID___y368) ? (__UNIQUE_ID___x367) : (__UNIQUE_ID___y368)); }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void update_hiwater_rss(struct mm_struct *mm)
{
 unsigned long _rss = get_mm_rss(mm);

 if ((mm)->hiwater_rss < _rss)
  (mm)->hiwater_rss = _rss;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void update_hiwater_vm(struct mm_struct *mm)
{
 if (mm->hiwater_vm < mm->total_vm)
  mm->hiwater_vm = mm->total_vm;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void reset_mm_hiwater_rss(struct mm_struct *mm)
{
 mm->hiwater_rss = get_mm_rss(mm);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void setmax_mm_hiwater_rss(unsigned long *maxrss,
      struct mm_struct *mm)
{
 unsigned long hiwater_rss = get_mm_hiwater_rss(mm);

 if (*maxrss < hiwater_rss)
  *maxrss = hiwater_rss;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sync_mm_rss(struct mm_struct *mm)
{
}
# 2288 "./include/linux/mm.h"
extern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,
          spinlock_t **ptl);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,
        spinlock_t **ptl)
{
 pte_t *ptep;
 (ptep = __get_locked_pte(mm, addr, ptl));
 return ptep;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd,
      unsigned long address)
{
 return 0;
}
# 2318 "./include/linux/mm.h"
int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_inc_nr_puds(struct mm_struct *mm)
{
 if (0)
  return;
 atomic_long_add((1 << (12 - 3)) * sizeof(pud_t), &mm->pgtables_bytes);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_dec_nr_puds(struct mm_struct *mm)
{
 if (0)
  return;
 atomic_long_sub((1 << (12 - 3)) * sizeof(pud_t), &mm->pgtables_bytes);
}
# 2346 "./include/linux/mm.h"
int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_inc_nr_pmds(struct mm_struct *mm)
{
 if (0)
  return;
 atomic_long_add((1 << (12 - 3)) * sizeof(pmd_t), &mm->pgtables_bytes);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_dec_nr_pmds(struct mm_struct *mm)
{
 if (0)
  return;
 atomic_long_sub((1 << (12 - 3)) * sizeof(pmd_t), &mm->pgtables_bytes);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_pgtables_bytes_init(struct mm_struct *mm)
{
 atomic_long_set(&mm->pgtables_bytes, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long mm_pgtables_bytes(const struct mm_struct *mm)
{
 return atomic_long_read(&mm->pgtables_bytes);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_inc_nr_ptes(struct mm_struct *mm)
{
 atomic_long_add((1 << (12 - 3)) * sizeof(pte_t), &mm->pgtables_bytes);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_dec_nr_ptes(struct mm_struct *mm)
{
 atomic_long_sub((1 << (12 - 3)) * sizeof(pte_t), &mm->pgtables_bytes);
}
# 2395 "./include/linux/mm.h"
int __pte_alloc(struct mm_struct *mm, pmd_t *pmd);
int __pte_alloc_kernel(pmd_t *pmd);



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) p4d_t *p4d_alloc(struct mm_struct *mm, pgd_t *pgd,
  unsigned long address)
{
 return (__builtin_expect(!!(pgd_none(*pgd)), 0) && __p4d_alloc(mm, pgd, address)) ?
  ((void *)0) : p4d_offset(pgd, address);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pud_t *pud_alloc(struct mm_struct *mm, p4d_t *p4d,
  unsigned long address)
{
 return (__builtin_expect(!!((!((((*p4d).pgd).pgd)))), 0) && __pud_alloc(mm, p4d, address)) ?
  ((void *)0) : pud_offset(p4d, address);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
 return (__builtin_expect(!!((!((*pud).pud))), 0) && __pmd_alloc(mm, pud, address))?
  ((void *)0): pmd_offset(pud, address);
}
# 2432 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptlock_cache_init(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool ptlock_alloc(struct page *page)
{
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptlock_free(struct page *page)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *ptlock_ptr(struct page *page)
{
 return &page->ptl;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
 return ptlock_ptr(((((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)) + (((unsigned long)(((((pmd_pte(*pmd)).pte) & (((((pteval_t)(1)) << (48 - 12)) - 1) << 12))) >> 12))))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool ptlock_init(struct page *page)
{
 /*
	 * prep_new_page() initialize page->private (and therefore page->ptl)
	 * with 0. Make sure nobody took it in use in between.
	 *
	 * It can happen if arch try to use slab for page table allocation:
	 * slab code uses page->slab_cache, which share storage with page->ptl.
	 */
 ((void)(sizeof(( long)(*(unsigned long *)&page->ptl))));
 if (!ptlock_alloc(page))
  return false;
 do { spinlock_check(ptlock_ptr(page)); *(ptlock_ptr(page)) = (spinlock_t) { { .rlock = { .raw_lock = { { .val = { (0) } } }, } } }; } while (0);
 return true;
}
# 2485 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgtable_init(void)
{
 ptlock_cache_init();
 pgtable_cache_init();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pgtable_pte_page_ctor(struct page *page)
{
 if (!ptlock_init(page))
  return false;
 __SetPageTable(page);
 inc_lruvec_page_state(page, NR_PAGETABLE);
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgtable_pte_page_dtor(struct page *page)
{
 ptlock_free(page);
 __ClearPageTable(page);
 dec_lruvec_page_state(page, NR_PAGETABLE);
}
# 2536 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *pmd_pgtable_page(pmd_t *pmd)
{
 unsigned long mask = ~((1 << (12 - 3)) * sizeof(pmd_t) - 1);
 return ({ u64 __idx = (((u64)(void *)((unsigned long) pmd & mask)) - ((-((((1UL))) << ((48)))))) / ((1UL) << 12); u64 __addr = (-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) + (__idx * sizeof(struct page)); (struct page *)__addr; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
 return ptlock_ptr(pmd_pgtable_page(pmd));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pmd_ptlock_init(struct page *page)
{

 page->pmd_huge_pte = ((void *)0);

 return ptlock_init(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pmd_ptlock_free(struct page *page)
{

 ((void)(sizeof(( long)(page->pmd_huge_pte))));

 ptlock_free(page);
}
# 2579 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)
{
 spinlock_t *ptl = pmd_lockptr(mm, pmd);
 spin_lock(ptl);
 return ptl;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool pgtable_pmd_page_ctor(struct page *page)
{
 if (!pmd_ptlock_init(page))
  return false;
 __SetPageTable(page);
 inc_lruvec_page_state(page, NR_PAGETABLE);
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pgtable_pmd_page_dtor(struct page *page)
{
 pmd_ptlock_free(page);
 __ClearPageTable(page);
 dec_lruvec_page_state(page, NR_PAGETABLE);
}

/*
 * No scalability reason to split PUD locks yet, but follow the same pattern
 * as the PMD locks to make it easier if we decide to.  The VM should not be
 * considered ready to switch to split PUD locks yet; there may be places
 * which need to be converted from page_table_lock.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)
{
 return &mm->page_table_lock;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) spinlock_t *pud_lock(struct mm_struct *mm, pud_t *pud)
{
 spinlock_t *ptl = pud_lockptr(mm, pud);

 spin_lock(ptl);
 return ptl;
}

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pagecache_init(void);
extern void free_initmem(void);

/*
 * Free reserved pages within range [PAGE_ALIGN(start), end & PAGE_MASK)
 * into the buddy system. The freed pages will be poisoned with pattern
 * "poison" if it's within range [0, UCHAR_MAX].
 * Return pages freed into the buddy system.
 */
extern unsigned long free_reserved_area(void *start, void *end,
     int poison, const char *s);

extern void adjust_managed_page_count(struct page *page, long count);
extern void mem_init_print_info(void);

extern void reserve_bootmem_region(phys_addr_t start, phys_addr_t end);

/* Free the reserved page into the buddy system, so it gets managed. */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void free_reserved_page(struct page *page)
{
 ClearPageReserved(page);
 init_page_count(page);
 __free_pages((page), 0);
 adjust_managed_page_count(page, 1);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mark_page_reserved(struct page *page)
{
 SetPageReserved(page);
 adjust_managed_page_count(page, -1);
}

/*
 * Default method to free all the __init memory into the buddy system.
 * The freed pages will be poisoned with pattern "poison" if it's within
 * range [0, UCHAR_MAX].
 * Return pages freed into the buddy system.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long free_initmem_default(int poison)
{
 extern char __init_begin[], __init_end[];

 return free_reserved_area(&__init_begin, &__init_end,
      poison, "unused kernel image (initmem)");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_num_physpages(void)
{
 int nid;
 unsigned long phys_pages = 0;

 for (((nid)) = __first_node(&(node_states[N_ONLINE])); ((nid) >= 0) && ((nid)) < (1 << 4); ((nid)) = __next_node((((nid))), &((node_states[N_ONLINE]))))
  phys_pages += ((node_data[(nid)])->node_present_pages);

 return phys_pages;
}

/*
 * Using memblock node mappings, an architecture may initialise its
 * zones, allocate the backing mem_map and account for memory holes in an
 * architecture independent manner.
 *
 * An architecture is expected to register range of page frames backed by
 * physical memory with memblock_add[_node]() before calling
 * free_area_init() passing in the PFN each zone ends at. At a basic
 * usage, an architecture is expected to do something like
 *
 * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,
 * 							 max_highmem_pfn};
 * for_each_valid_physical_page_range()
 *	memblock_add_node(base, size, nid, MEMBLOCK_NONE)
 * free_area_init(max_zone_pfns);
 */
void free_area_init(unsigned long *max_zone_pfn);
unsigned long node_map_pfn_alignment(void);
unsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,
      unsigned long end_pfn);
extern unsigned long absent_pages_in_range(unsigned long start_pfn,
      unsigned long end_pfn);
extern void get_pfn_range_for_nid(unsigned int nid,
   unsigned long *start_pfn, unsigned long *end_pfn);







/* please see mm/page_alloc.c */
extern int __attribute__((__section__(".meminit.text"))) __attribute__((__cold__)) __attribute__((__no_instrument_function__)) early_pfn_to_nid(unsigned long pfn);


extern void set_dma_reserve(unsigned long new_dma_reserve);
extern void memmap_init_range(unsigned long, int, unsigned long,
  unsigned long, unsigned long, enum meminit_context,
  struct vmem_altmap *, int migratetype);
extern void setup_per_zone_wmarks(void);
extern void calculate_min_free_kbytes(void);
extern int __attribute__((__section__(".meminit.text"))) __attribute__((__cold__)) __attribute__((__no_instrument_function__)) init_per_zone_wmark_min(void);
extern void mem_init(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) mmap_init(void);

extern void __show_mem(unsigned int flags, nodemask_t *nodemask, int max_zone_idx);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void show_mem(unsigned int flags, nodemask_t *nodemask)
{
 __show_mem(flags, nodemask, 4 /* __MAX_NR_ZONES */ - 1);
}
extern long si_mem_available(void);
extern void si_meminfo(struct sysinfo * val);
extern void si_meminfo_node(struct sysinfo *val, int nid);




extern __attribute__((__format__(printf, 3, 4)))
void warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...);

extern void setup_per_cpu_pageset(void);

/* page_alloc.c */
extern int min_free_kbytes;
extern int watermark_boost_factor;
extern int watermark_scale_factor;
extern bool arch_has_descending_max_zone_pfns(void);

/* nommu.c */
extern atomic_long_t mmap_pages_allocated;
extern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);

/* interval_tree.c */
void vma_interval_tree_insert(struct vm_area_struct *node,
         struct rb_root_cached *root);
void vma_interval_tree_insert_after(struct vm_area_struct *node,
        struct vm_area_struct *prev,
        struct rb_root_cached *root);
void vma_interval_tree_remove(struct vm_area_struct *node,
         struct rb_root_cached *root);
struct vm_area_struct *vma_interval_tree_iter_first(struct rb_root_cached *root,
    unsigned long start, unsigned long last);
struct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,
    unsigned long start, unsigned long last);





void anon_vma_interval_tree_insert(struct anon_vma_chain *node,
       struct rb_root_cached *root);
void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
       struct rb_root_cached *root);
struct anon_vma_chain *
anon_vma_interval_tree_iter_first(struct rb_root_cached *root,
      unsigned long start, unsigned long last);
struct anon_vma_chain *anon_vma_interval_tree_iter_next(
 struct anon_vma_chain *node, unsigned long start, unsigned long last);
# 2785 "./include/linux/mm.h"
/* mmap.c */
extern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);
extern int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 unsigned long end, unsigned long pgoff, struct vm_area_struct *insert,
 struct vm_area_struct *expand);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vma_adjust(struct vm_area_struct *vma, unsigned long start,
 unsigned long end, unsigned long pgoff, struct vm_area_struct *insert)
{
 return __vma_adjust(vma, start, end, pgoff, insert, ((void *)0));
}
extern struct vm_area_struct *vma_merge(struct mm_struct *,
 struct vm_area_struct *prev, unsigned long addr, unsigned long end,
 unsigned long vm_flags, struct anon_vma *, struct file *, unsigned long,
 struct mempolicy *, struct vm_userfaultfd_ctx, struct anon_vma_name *);
extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);
extern int __split_vma(struct mm_struct *, struct vm_area_struct *,
 unsigned long addr, int new_below);
extern int split_vma(struct mm_struct *, struct vm_area_struct *,
 unsigned long addr, int new_below);
extern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);
extern void unlink_file_vma(struct vm_area_struct *);
extern struct vm_area_struct *copy_vma(struct vm_area_struct **,
 unsigned long addr, unsigned long len, unsigned long pgoff,
 bool *need_rmap_locks);
extern void exit_mmap(struct mm_struct *);

void vma_mas_store(struct vm_area_struct *vma, struct ma_state *mas);
void vma_mas_remove(struct vm_area_struct *vma, struct ma_state *mas);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int check_data_rlimit(unsigned long rlim,
        unsigned long new,
        unsigned long start,
        unsigned long end_data,
        unsigned long start_data)
{
 if (rlim < (~0UL)) {
  if (((new - start) + (end_data - start_data)) > rlim)
   return -28 /* No space left on device */;
 }

 return 0;
}

extern int mm_take_all_locks(struct mm_struct *mm);
extern void mm_drop_all_locks(struct mm_struct *mm);

extern int set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);
extern int replace_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);
extern struct file *get_mm_exe_file(struct mm_struct *mm);
extern struct file *get_task_exe_file(struct task_struct *task);

extern bool may_expand_vm(struct mm_struct *, vm_flags_t, unsigned long npages);
extern void vm_stat_account(struct mm_struct *, vm_flags_t, long npages);

extern bool vma_is_special_mapping(const struct vm_area_struct *vma,
       const struct vm_special_mapping *sm);
extern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,
       unsigned long addr, unsigned long len,
       unsigned long flags,
       const struct vm_special_mapping *spec);
/* This is an obsolete alternative to _install_special_mapping. */
extern int install_special_mapping(struct mm_struct *mm,
       unsigned long addr, unsigned long len,
       unsigned long flags, struct page **pages);

unsigned long randomize_stack_top(unsigned long stack_top);
unsigned long randomize_page(unsigned long start, unsigned long range);

extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);

extern unsigned long mmap_region(struct file *file, unsigned long addr,
 unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
 struct list_head *uf);
extern unsigned long do_mmap(struct file *file, unsigned long addr,
 unsigned long len, unsigned long prot, unsigned long flags,
 unsigned long pgoff, unsigned long *populate, struct list_head *uf);
extern int do_mas_munmap(struct ma_state *mas, struct mm_struct *mm,
    unsigned long start, size_t len, struct list_head *uf,
    bool downgrade);
extern int do_munmap(struct mm_struct *, unsigned long, size_t,
       struct list_head *uf);
extern int do_madvise(struct mm_struct *mm, unsigned long start, size_t len_in, int behavior);


extern int __mm_populate(unsigned long addr, unsigned long len,
    int ignore_errors);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void mm_populate(unsigned long addr, unsigned long len)
{
 /* Ignore errors */
 (void) __mm_populate(addr, len, 1);
}




/* These take the mm semaphore themselves */
extern int __attribute__((__warn_unused_result__)) vm_brk(unsigned long, unsigned long);
extern int __attribute__((__warn_unused_result__)) vm_brk_flags(unsigned long, unsigned long, unsigned long);
extern int vm_munmap(unsigned long, size_t);
extern unsigned long __attribute__((__warn_unused_result__)) vm_mmap(struct file *, unsigned long,
        unsigned long, unsigned long,
        unsigned long, unsigned long);

struct vm_unmapped_area_info {

 unsigned long flags;
 unsigned long length;
 unsigned long low_limit;
 unsigned long high_limit;
 unsigned long align_mask;
 unsigned long align_offset;
};

extern unsigned long vm_unmapped_area(struct vm_unmapped_area_info *info);

/* truncate.c */
extern void truncate_inode_pages(struct address_space *, loff_t);
extern void truncate_inode_pages_range(struct address_space *,
           loff_t lstart, loff_t lend);
extern void truncate_inode_pages_final(struct address_space *);

/* generic vm_area_ops exported for stackable file systems */
extern vm_fault_t filemap_fault(struct vm_fault *vmf);
extern vm_fault_t filemap_map_pages(struct vm_fault *vmf,
  unsigned long start_pgoff, unsigned long end_pgoff);
extern vm_fault_t filemap_page_mkwrite(struct vm_fault *vmf);

extern unsigned long stack_guard_gap;
/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
extern int expand_stack(struct vm_area_struct *vma, unsigned long address);

/* CONFIG_STACK_GROWSUP still needs to grow downwards at some places */
extern int expand_downwards(struct vm_area_struct *vma,
  unsigned long address);






/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */
extern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);
extern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,
          struct vm_area_struct **pprev);

/*
 * Look up the first VMA which intersects the interval [start_addr, end_addr)
 * NULL if none.  Assume start_addr < end_addr.
 */
struct vm_area_struct *find_vma_intersection(struct mm_struct *mm,
   unsigned long start_addr, unsigned long end_addr);

/**
 * vma_lookup() - Find a VMA at a specific address
 * @mm: The process address space.
 * @addr: The user address.
 *
 * Return: The vm_area_struct at the given address, %NULL otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
struct vm_area_struct *vma_lookup(struct mm_struct *mm, unsigned long addr)
{
 return mtree_load(&mm->mm_mt, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long vm_start_gap(struct vm_area_struct *vma)
{
 unsigned long vm_start = vma->vm_start;

 if (vma->vm_flags & 0x00000100 /* general info on the segment */) {
  vm_start -= stack_guard_gap;
  if (vm_start > vma->vm_start)
   vm_start = 0;
 }
 return vm_start;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long vm_end_gap(struct vm_area_struct *vma)
{
 unsigned long vm_end = vma->vm_end;

 if (vma->vm_flags & 0x00000000) {
  vm_end += stack_guard_gap;
  if (vm_end < vma->vm_end)
   vm_end = -((1UL) << 12);
 }
 return vm_end;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long vma_pages(struct vm_area_struct *vma)
{
 return (vma->vm_end - vma->vm_start) >> 12;
}

/* Look up the first VMA which exactly match the interval vm_start ... vm_end */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct vm_area_struct *find_exact_vma(struct mm_struct *mm,
    unsigned long vm_start, unsigned long vm_end)
{
 struct vm_area_struct *vma = vma_lookup(mm, vm_start);

 if (vma && (vma->vm_start != vm_start || vma->vm_end != vm_end))
  vma = ((void *)0);

 return vma;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool range_in_vma(struct vm_area_struct *vma,
    unsigned long start, unsigned long end)
{
 return (vma && vma->vm_start <= start && end <= vma->vm_end);
}


pgprot_t vm_get_page_prot(unsigned long vm_flags);
void vma_set_page_prot(struct vm_area_struct *vma);
# 3011 "./include/linux/mm.h"
void vma_set_file(struct vm_area_struct *vma, struct file *file);


unsigned long change_prot_numa(struct vm_area_struct *vma,
   unsigned long start, unsigned long end);


struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);
int remap_pfn_range(struct vm_area_struct *, unsigned long addr,
   unsigned long pfn, unsigned long size, pgprot_t);
int remap_pfn_range_notrack(struct vm_area_struct *vma, unsigned long addr,
  unsigned long pfn, unsigned long size, pgprot_t prot);
int vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);
int vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,
   struct page **pages, unsigned long *num);
int vm_map_pages(struct vm_area_struct *vma, struct page **pages,
    unsigned long num);
int vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,
    unsigned long num);
vm_fault_t vmf_insert_pfn(struct vm_area_struct *vma, unsigned long addr,
   unsigned long pfn);
vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,
   unsigned long pfn, pgprot_t pgprot);
vm_fault_t vmf_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
   pfn_t pfn);
vm_fault_t vmf_insert_mixed_prot(struct vm_area_struct *vma, unsigned long addr,
   pfn_t pfn, pgprot_t pgprot);
vm_fault_t vmf_insert_mixed_mkwrite(struct vm_area_struct *vma,
  unsigned long addr, pfn_t pfn);
int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vm_fault_t vmf_insert_page(struct vm_area_struct *vma,
    unsigned long addr, struct page *page)
{
 int err = vm_insert_page(vma, addr, page);

 if (err == -12 /* Out of memory */)
  return VM_FAULT_OOM;
 if (err < 0 && err != -16 /* Device or resource busy */)
  return VM_FAULT_SIGBUS;

 return VM_FAULT_NOPAGE;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int io_remap_pfn_range(struct vm_area_struct *vma,
         unsigned long addr, unsigned long pfn,
         unsigned long size, pgprot_t prot)
{
 return remap_pfn_range(vma, addr, pfn, size, (prot));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) vm_fault_t vmf_error(int err)
{
 if (err == -12 /* Out of memory */)
  return VM_FAULT_OOM;
 return VM_FAULT_SIGBUS;
}

struct page *follow_page(struct vm_area_struct *vma, unsigned long address,
    unsigned int foll_flags);
# 3092 "./include/linux/mm.h"
/*
 * FOLL_PIN and FOLL_LONGTERM may be used in various combinations with each
 * other. Here is what they mean, and how to use them:
 *
 * FOLL_LONGTERM indicates that the page will be held for an indefinite time
 * period _often_ under userspace control.  This is in contrast to
 * iov_iter_get_pages(), whose usages are transient.
 *
 * FIXME: For pages which are part of a filesystem, mappings are subject to the
 * lifetime enforced by the filesystem and we need guarantees that longterm
 * users like RDMA and V4L2 only establish mappings which coordinate usage with
 * the filesystem.  Ideas for this coordination include revoking the longterm
 * pin, delaying writeback, bounce buffer page writeback, etc.  As FS DAX was
 * added after the problem with filesystems was found FS DAX VMAs are
 * specifically failed.  Filesystem pages are still subject to bugs and use of
 * FOLL_LONGTERM should be avoided on those pages.
 *
 * FIXME: Also NOTE that FOLL_LONGTERM is not supported in every GUP call.
 * Currently only get_user_pages() and get_user_pages_fast() support this flag
 * and calls to get_user_pages_[un]locked are specifically not allowed.  This
 * is due to an incompatibility with the FS DAX check and
 * FAULT_FLAG_ALLOW_RETRY.
 *
 * In the CMA case: long term pins in a CMA region would unnecessarily fragment
 * that region.  And so, CMA attempts to migrate the page before pinning, when
 * FOLL_LONGTERM is specified.
 *
 * FOLL_PIN indicates that a special kind of tracking (not just page->_refcount,
 * but an additional pin counting system) will be invoked. This is intended for
 * anything that gets a page reference and then touches page data (for example,
 * Direct IO). This lets the filesystem know that some non-file-system entity is
 * potentially changing the pages' data. In contrast to FOLL_GET (whose pages
 * are released via put_page()), FOLL_PIN pages must be released, ultimately, by
 * a call to unpin_user_page().
 *
 * FOLL_PIN is similar to FOLL_GET: both of these pin pages. They use different
 * and separate refcounting mechanisms, however, and that means that each has
 * its own acquire and release mechanisms:
 *
 *     FOLL_GET: get_user_pages*() to acquire, and put_page() to release.
 *
 *     FOLL_PIN: pin_user_pages*() to acquire, and unpin_user_pages to release.
 *
 * FOLL_PIN and FOLL_GET are mutually exclusive for a given function call.
 * (The underlying pages may experience both FOLL_GET-based and FOLL_PIN-based
 * calls applied to them, and that's perfectly OK. This is a constraint on the
 * callers, not on the pages.)
 *
 * FOLL_PIN should be set internally by the pin_user_pages*() APIs, never
 * directly by the caller. That's in order to help avoid mismatches when
 * releasing pages: get_user_pages*() pages must be released via put_page(),
 * while pin_user_pages*() pages must be released via unpin_user_page().
 *
 * Please see Documentation/core-api/pin_user_pages.rst for more information.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int vm_fault_to_errno(vm_fault_t vm_fault, int foll_flags)
{
 if (vm_fault & VM_FAULT_OOM)
  return -12 /* Out of memory */;
 if (vm_fault & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
  return (foll_flags & 0x100 /* check page is hwpoisoned */) ? -133 /* Memory page has hardware error */ : -14 /* Bad address */;
 if (vm_fault & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))
  return -14 /* Bad address */;
 return 0;
}

/*
 * Indicates for which pages that are write-protected in the page table,
 * whether GUP has to trigger unsharing via FAULT_FLAG_UNSHARE such that the
 * GUP pin will remain consistent with the pages mapped into the page tables
 * of the MM.
 *
 * Temporary unmapping of PageAnonExclusive() pages or clearing of
 * PageAnonExclusive() has to protect against concurrent GUP:
 * * Ordinary GUP: Using the PT lock
 * * GUP-fast and fork(): mm->write_protect_seq
 * * GUP-fast and KSM or temporary unmapping (swap, migration): see
 *    page_try_share_anon_rmap()
 *
 * Must be called with the (sub)page that's actually referenced via the
 * page table entry, which might not necessarily be the head page for a
 * PTE-mapped THP.
 *
 * If the vma is NULL, we're coming from the GUP-fast path and might have
 * to fallback to the slow path just to lookup the vma.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gup_must_unshare(struct vm_area_struct *vma,
        unsigned int flags, struct page *page)
{
 /*
	 * FOLL_WRITE is implicitly handled correctly as the page table entry
	 * has to be writable -- and if it references (part of) an anonymous
	 * folio, that part is required to be marked exclusive.
	 */
 if ((flags & (0x01 /* check pte is writable */ | 0x40000 /* pages must be released via unpin_user_page */)) != 0x40000 /* pages must be released via unpin_user_page */)
  return false;
 /*
	 * Note: PageAnon(page) is stable until the page is actually getting
	 * freed.
	 */
 if (!PageAnon(page)) {
  /*
		 * We only care about R/O long-term pining: R/O short-term
		 * pinning does not have the semantics to observe successive
		 * changes through the process page tables.
		 */
  if (!(flags & 0x10000 /* mapping lifetime is indefinite: see below */))
   return false;

  /* We really need the vma ... */
  if (!vma)
   return true;

  /*
		 * ... because we only care about writable private ("COW")
		 * mappings where we have to break COW early.
		 */
  return is_cow_mapping(vma->vm_flags);
 }

 /* Paired with a memory barrier in page_try_share_anon_rmap(). */
 if (1)
  do { do { } while (0); asm volatile("dmb " "ishld" : : : "memory"); } while (0);

 /*
	 * Note that PageKsm() pages cannot be exclusive, and consequently,
	 * cannot get pinned.
	 */
 return !PageAnonExclusive(page);
}

/*
 * Indicates whether GUP can follow a PROT_NONE mapped page, or whether
 * a (NUMA hinting) fault is required.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool gup_can_follow_protnone(unsigned int flags)
{
 /*
	 * FOLL_FORCE has to be able to make progress even if the VMA is
	 * inaccessible. Further, FOLL_FORCE access usually does not represent
	 * application behaviour and we should avoid triggering NUMA hinting
	 * faults.
	 */
 return flags & 0x10 /* get_user_pages read/write w/o permission */;
}

typedef int (*pte_fn_t)(pte_t *pte, unsigned long addr, void *data);
extern int apply_to_page_range(struct mm_struct *mm, unsigned long address,
          unsigned long size, pte_fn_t fn, void *data);
extern int apply_to_existing_page_range(struct mm_struct *mm,
       unsigned long address, unsigned long size,
       pte_fn_t fn, void *data);

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) init_mem_debugging_and_hardening(void);
# 3275 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_poisoning_enabled(void) { return false; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_poisoning_enabled_static(void) { return false; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kernel_poison_pages(struct page *page, int nunmpages) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kernel_poison_pages(struct page *page, int numpages) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kernel_unpoison_pages(struct page *page, int numpages) { }


extern struct static_key_false init_on_alloc;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool want_init_on_alloc(gfp_t flags)
{
 if ((0 ? ({ bool branch; if (__builtin_types_compatible_p(typeof(*&init_on_alloc), struct static_key_true)) branch = !arch_static_branch(&(&init_on_alloc)->key, true); else if (__builtin_types_compatible_p(typeof(*&init_on_alloc), struct static_key_false)) branch = !arch_static_branch_jump(&(&init_on_alloc)->key, true); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 1); }) : ({ bool branch; if (__builtin_types_compatible_p(typeof(*&init_on_alloc), struct static_key_true)) branch = arch_static_branch_jump(&(&init_on_alloc)->key, false); else if (__builtin_types_compatible_p(typeof(*&init_on_alloc), struct static_key_false)) branch = arch_static_branch(&(&init_on_alloc)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); })))

  return true;
 return flags & (( gfp_t)0x100u);
}

extern struct static_key_false init_on_free;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool want_init_on_free(void)
{
 return (0 ? ({ bool branch; if (__builtin_types_compatible_p(typeof(*&init_on_free), struct static_key_true)) branch = !arch_static_branch(&(&init_on_free)->key, true); else if (__builtin_types_compatible_p(typeof(*&init_on_free), struct static_key_false)) branch = !arch_static_branch_jump(&(&init_on_free)->key, true); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 1); }) : ({ bool branch; if (__builtin_types_compatible_p(typeof(*&init_on_free), struct static_key_true)) branch = arch_static_branch_jump(&(&init_on_free)->key, false); else if (__builtin_types_compatible_p(typeof(*&init_on_free), struct static_key_false)) branch = arch_static_branch(&(&init_on_free)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); }));

}

extern bool _debug_pagealloc_enabled_early;
extern struct static_key_false _debug_pagealloc_enabled;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool debug_pagealloc_enabled(void)
{
 return 0 &&
  _debug_pagealloc_enabled_early;
}

/*
 * For use in fast paths after init_debug_pagealloc() has run, or when a
 * false negative result is not harmful when called too early.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool debug_pagealloc_enabled_static(void)
{
 if (!0)
  return false;

 return ({ bool branch; if (__builtin_types_compatible_p(typeof(*&_debug_pagealloc_enabled), struct static_key_true)) branch = arch_static_branch_jump(&(&_debug_pagealloc_enabled)->key, false); else if (__builtin_types_compatible_p(typeof(*&_debug_pagealloc_enabled), struct static_key_false)) branch = arch_static_branch(&(&_debug_pagealloc_enabled)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); });
}
# 3338 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void debug_pagealloc_map_pages(struct page *page, int numpages) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void debug_pagealloc_unmap_pages(struct page *page, int numpages) {}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct vm_area_struct *get_gate_vma(struct mm_struct *mm)
{
 return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int in_gate_area_no_mm(unsigned long addr) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int in_gate_area(struct mm_struct *mm, unsigned long addr)
{
 return 0;
}


extern bool process_shares_mm(struct task_struct *p, struct mm_struct *mm);


extern int sysctl_drop_caches;
int drop_caches_sysctl_handler(struct ctl_table *, int, void *, size_t *,
  loff_t *);


void drop_slab(void);




extern int randomize_va_space;


const char * arch_vma_name(struct vm_area_struct *vma);

void print_vma_addr(char *prefix, unsigned long rip);






void *sparse_buffer_alloc(unsigned long size);
struct page * __populate_section_memmap(unsigned long pfn,
  unsigned long nr_pages, int nid, struct vmem_altmap *altmap,
  struct dev_pagemap *pgmap);
void pmd_init(void *addr);
void pud_init(void *addr);
pgd_t *vmemmap_pgd_populate(unsigned long addr, int node);
p4d_t *vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node);
pud_t *vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node);
pmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);
pte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node,
       struct vmem_altmap *altmap, struct page *reuse);
void *vmemmap_alloc_block(unsigned long size, int node);
struct vmem_altmap;
void *vmemmap_alloc_block_buf(unsigned long size, int node,
         struct vmem_altmap *altmap);
void vmemmap_verify(pte_t *, int, unsigned long, unsigned long);
void vmemmap_set_pmd(pmd_t *pmd, void *p, int node,
       unsigned long addr, unsigned long next);
int vmemmap_check_pmd(pmd_t *pmd, int node,
        unsigned long addr, unsigned long next);
int vmemmap_populate_basepages(unsigned long start, unsigned long end,
          int node, struct vmem_altmap *altmap);
int vmemmap_populate_hugepages(unsigned long start, unsigned long end,
          int node, struct vmem_altmap *altmap);
int vmemmap_populate(unsigned long start, unsigned long end, int node,
  struct vmem_altmap *altmap);
void vmemmap_populate_print_last(void);

void vmemmap_free(unsigned long start, unsigned long end,
  struct vmem_altmap *altmap);

void register_page_bootmem_memmap(unsigned long section_nr, struct page *map,
      unsigned long nr_pages);

enum mf_flags {
 MF_COUNT_INCREASED = 1 << 0,
 MF_ACTION_REQUIRED = 1 << 1,
 MF_MUST_KILL = 1 << 2,
 MF_SOFT_OFFLINE = 1 << 3,
 MF_UNPOISON = 1 << 4,
 MF_SW_SIMULATED = 1 << 5,
 MF_NO_RETRY = 1 << 6,
};
int mf_dax_kill_procs(struct address_space *mapping, unsigned long index,
        unsigned long count, int mf_flags);
extern int memory_failure(unsigned long pfn, int flags);
extern void memory_failure_queue_kick(int cpu);
extern int unpoison_memory(unsigned long pfn);
extern int sysctl_memory_failure_early_kill;
extern int sysctl_memory_failure_recovery;
extern void shake_page(struct page *p);
extern atomic_long_t num_poisoned_pages __attribute__((__section__(".data..read_mostly")));
extern int soft_offline_page(unsigned long pfn, int flags);

extern void memory_failure_queue(unsigned long pfn, int flags);
extern int __get_huge_page_for_hwpoison(unsigned long pfn, int flags,
     bool *migratable_cleared);
void num_poisoned_pages_inc(unsigned long pfn);
void num_poisoned_pages_sub(unsigned long pfn, long i);
# 3464 "./include/linux/mm.h"
extern void memblk_nr_poison_inc(unsigned long pfn);
extern void memblk_nr_poison_sub(unsigned long pfn, long i);
# 3477 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_memory_failure(unsigned long pfn, int flags)
{
 return -6 /* No such device or address */;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_is_platform_page(u64 paddr)
{
 return false;
}


/*
 * Error handlers for various types of pages.
 */
enum mf_result {
 MF_IGNORED, /* Error: cannot be handled */
 MF_FAILED, /* Error: handling failed */
 MF_DELAYED, /* Will be handled later */
 MF_RECOVERED, /* Successfully recovered */
};

enum mf_action_page_type {
 MF_MSG_KERNEL,
 MF_MSG_KERNEL_HIGH_ORDER,
 MF_MSG_SLAB,
 MF_MSG_DIFFERENT_COMPOUND,
 MF_MSG_HUGE,
 MF_MSG_FREE_HUGE,
 MF_MSG_UNMAP_FAILED,
 MF_MSG_DIRTY_SWAPCACHE,
 MF_MSG_CLEAN_SWAPCACHE,
 MF_MSG_DIRTY_MLOCKED_LRU,
 MF_MSG_CLEAN_MLOCKED_LRU,
 MF_MSG_DIRTY_UNEVICTABLE_LRU,
 MF_MSG_CLEAN_UNEVICTABLE_LRU,
 MF_MSG_DIRTY_LRU,
 MF_MSG_CLEAN_LRU,
 MF_MSG_TRUNCATED_LRU,
 MF_MSG_BUDDY,
 MF_MSG_DAX,
 MF_MSG_UNSPLIT_THP,
 MF_MSG_UNKNOWN,
};


extern void clear_huge_page(struct page *page,
       unsigned long addr_hint,
       unsigned int pages_per_huge_page);
extern void copy_user_huge_page(struct page *dst, struct page *src,
    unsigned long addr_hint,
    struct vm_area_struct *vma,
    unsigned int pages_per_huge_page);
extern long copy_huge_page_from_user(struct page *dst_page,
    const void /* nothing */ *usr_src,
    unsigned int pages_per_huge_page,
    bool allow_pagefault);

/**
 * vma_is_special_huge - Are transhuge page-table entries considered special?
 * @vma: Pointer to the struct vm_area_struct to consider
 *
 * Whether transhuge page-table entries are considered "special" following
 * the definition in vm_normal_page().
 *
 * Return: true if transhuge page-table entries should be considered special,
 * false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool vma_is_special_huge(const struct vm_area_struct *vma)
{
 return vma_is_dax(vma) || (vma->vm_file &&
       (vma->vm_flags & (0x00000400 /* Page-ranges managed without "struct page", just pure PFN */ | 0x10000000 /* Can contain "struct page" and pure PFN pages */)));
}
# 3576 "./include/linux/mm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int debug_guardpage_minorder(void) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool debug_guardpage_enabled(void) { return false; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool page_is_guard(struct page *page) { return false; }



void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) setup_nr_node_ids(void);




extern int memcmp_pages(struct page *page1, struct page *page2);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int pages_identical(struct page *page1, struct page *page2)
{
 return !memcmp_pages(page1, page2);
}
# 3606 "./include/linux/mm.h"
extern int sysctl_nr_trim_pages;


void mem_dump_obj(void *object);




/**
 * seal_check_future_write - Check for F_SEAL_FUTURE_WRITE flag and handle it
 * @seals: the seals to check
 * @vma: the vma to operate on
 *
 * Check whether F_SEAL_FUTURE_WRITE is set; if so, do proper check/handling on
 * the vma flags.  Return 0 if check pass, or <0 for errors.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int seal_check_future_write(int seals, struct vm_area_struct *vma)
{
 if (seals & 0x0010 /* prevent future writes while mapped */) {
  /*
		 * New PROT_WRITE and MAP_SHARED mmaps are not allowed when
		 * "future write" seal active.
		 */
  if ((vma->vm_flags & 0x00000008) && (vma->vm_flags & 0x00000002))
   return -1 /* Operation not permitted */;

  /*
		 * Since an F_SEAL_FUTURE_WRITE sealed memfd can be mapped as
		 * MAP_SHARED and read-only, take care to not allow mprotect to
		 * revert protections on such mappings. Do this only for shared
		 * mappings. For private mappings, don't need to mask
		 * VM_MAYWRITE as we still want them to be COW-writable.
		 */
  if (vma->vm_flags & 0x00000008)
   vma->vm_flags &= ~(0x00000020);
 }

 return 0;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
madvise_set_anon_name(struct mm_struct *mm, unsigned long start,
        unsigned long len_in, struct anon_vma_name *anon_name) {
 return 0;
}
# 14 "./include/linux/kallsyms.h" 2
# 1 "./include/linux/module.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Dynamic loading of modules into the kernel.
 *
 * Rewritten by Richard Henderson <rth@tamu.edu> Dec 1996
 * Rewritten again by Rusty Russell, 2002
 */
# 17 "./include/linux/module.h"
# 1 "./include/linux/kmod.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */



/*
 *	include/linux/kmod.h
 */

# 1 "./include/linux/umh.h" 1
# 11 "./include/linux/umh.h"
struct cred;
struct file;







struct subprocess_info {
 struct work_struct work;
 struct completion *complete;
 const char *path;
 char **argv;
 char **envp;
 int wait;
 int retval;
 int (*init)(struct subprocess_info *info, struct cred *new);
 void (*cleanup)(struct subprocess_info *info);
 void *data;
} ;

extern int
call_usermodehelper(const char *path, char **argv, char **envp, int wait);

extern struct subprocess_info *
call_usermodehelper_setup(const char *path, char **argv, char **envp,
     gfp_t gfp_mask,
     int (*init)(struct subprocess_info *info, struct cred *new),
     void (*cleanup)(struct subprocess_info *), void *data);

extern int
call_usermodehelper_exec(struct subprocess_info *info, int wait);

extern struct ctl_table usermodehelper_table[];

enum umh_disable_depth {
 UMH_ENABLED = 0,
 UMH_FREEZING,
 UMH_DISABLED,
};

extern int __usermodehelper_disable(enum umh_disable_depth depth);
extern void __usermodehelper_set_disable_depth(enum umh_disable_depth depth);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int usermodehelper_disable(void)
{
 return __usermodehelper_disable(UMH_DISABLED);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void usermodehelper_enable(void)
{
 __usermodehelper_set_disable_depth(UMH_ENABLED);
}

extern int usermodehelper_read_trylock(void);
extern long usermodehelper_read_lock_wait(long timeout);
extern void usermodehelper_read_unlock(void);
# 10 "./include/linux/kmod.h" 2
# 20 "./include/linux/kmod.h"
extern char modprobe_path[]; /* for sysctl */
/* modprobe exit status on success, -ve on error.  Return value
 * usually useless though. */
extern __attribute__((__format__(printf, 2, 3)))
int __request_module(bool wait, const char *name, ...);
# 18 "./include/linux/module.h" 2

# 1 "./include/linux/elf.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/asm/elf.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */





/*
 * ELF register definitions..
 */

# 1 "./arch/arm64/include/generated/asm/user.h" 1
# 1 "./include/asm-generic/user.h" 1


/*
 * This file may define a 'struct user' structure. However, it is only
 * used for a.out files, which are not supported on new architectures.
 */
# 2 "./arch/arm64/include/generated/asm/user.h" 2
# 15 "./arch/arm64/include/asm/elf.h" 2

/*
 * AArch64 static relocation types.
 */

/* Miscellaneous. */



/* Data. */







/* Instructions. */
# 71 "./arch/arm64/include/asm/elf.h"
/*
 * These are used to set parameters in the core dumps.
 */
# 82 "./arch/arm64/include/asm/elf.h"
/*
 * This yields a string that ld.so will use to load implementation
 * specific libraries for optimization.  This is more specific in
 * intent than poking at uname or /proc/cpuinfo.
 */







/*
 * This is used to ensure we don't load something for the wrong architecture.
 */


/*
 * An executable for which elf_read_implies_exec() returns TRUE will
 * have the READ_IMPLIES_EXEC personality flag set automatically.
 *
 * The decision process for determining the results are:
 *
 *                CPU*: | arm32      | arm64      |
 * ELF:                 |            |            |
 * ---------------------|------------|------------|
 * missing PT_GNU_STACK | exec-all   | exec-none  |
 * PT_GNU_STACK == RWX  | exec-stack | exec-stack |
 * PT_GNU_STACK == RW   | exec-none  | exec-none  |
 *
 *  exec-all  : all PROT_READ user mappings are executable, except when
 *              backed by files on a noexec-filesystem.
 *  exec-none : only PROT_EXEC user mappings are executable.
 *  exec-stack: only the stack and PROT_EXEC user mappings are executable.
 *
 *  *all arm64 CPUs support NX, so there is no "lacks NX" column.
 *
 */





/*
 * This is the base location for PIE (ET_DYN with INTERP) loads. On
 * 64-bit, this is above 4GB to leave the entire 32-bit address
 * space open for things that want to use the area for 32-bit pointers.
 */
# 138 "./arch/arm64/include/asm/elf.h"
# 1 "./include/uapi/linux/elf.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */




# 1 "./include/uapi/linux/elf-em.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/* These constants define the various ELF target machines */
# 15 "./include/uapi/linux/elf-em.h"
    /* Next two are historical and binaries and
				   modules of these types will be rejected by
				   Linux.  */
# 57 "./include/uapi/linux/elf-em.h"
/*
 * This is an interim value that we will use until the committee comes
 * up with a final number.
 */


/* Bogus old m32r magic number, used by old tools. */

/* This is the old interim value for S/390 architecture */

/* Also Panasonic/MEI MN10300, AM33 */
# 7 "./include/uapi/linux/elf.h" 2

/* 32-bit ELF base types. */
typedef __u32 Elf32_Addr;
typedef __u16 Elf32_Half;
typedef __u32 Elf32_Off;
typedef __s32 Elf32_Sword;
typedef __u32 Elf32_Word;

/* 64-bit ELF base types. */
typedef __u64 Elf64_Addr;
typedef __u16 Elf64_Half;
typedef __s16 Elf64_SHalf;
typedef __u64 Elf64_Off;
typedef __s32 Elf64_Sword;
typedef __u32 Elf64_Word;
typedef __u64 Elf64_Xword;
typedef __s64 Elf64_Sxword;

/* These constants are for the segment types stored in the image headers */
# 44 "./include/uapi/linux/elf.h"
/* ARM MTE memory tag segment type */


/*
 * Extended Numbering
 *
 * If the real number of program header table entries is larger than
 * or equal to PN_XNUM(0xffff), it is set to sh_info field of the
 * section header at index 0, and PN_XNUM is set to e_phnum
 * field. Otherwise, the section header at index 0 is zero
 * initialized, if it exists.
 *
 * Specifications are available in:
 *
 * - Oracle: Linker and Libraries.
 *   Part No: 817–1984–19, August 2011.
 *   https://docs.oracle.com/cd/E18752_01/pdf/817-1984.pdf
 *
 * - System V ABI AMD64 Architecture Processor Supplement
 *   Draft Version 0.99.4,
 *   January 13, 2010.
 *   http://www.cs.washington.edu/education/courses/cse351/12wi/supp-docs/abi.pdf
 */


/* These constants define the different elf file types */
# 78 "./include/uapi/linux/elf.h"
/* This is the info that is needed to parse the dynamic section of the file */
# 123 "./include/uapi/linux/elf.h"
/* This info is needed when parsing the symbol table */
# 143 "./include/uapi/linux/elf.h"
typedef struct dynamic {
  Elf32_Sword d_tag;
  union {
    Elf32_Sword d_val;
    Elf32_Addr d_ptr;
  } d_un;
} Elf32_Dyn;

typedef struct {
  Elf64_Sxword d_tag; /* entry tag value */
  union {
    Elf64_Xword d_val;
    Elf64_Addr d_ptr;
  } d_un;
} Elf64_Dyn;

/* The following are used with relocations */






typedef struct elf32_rel {
  Elf32_Addr r_offset;
  Elf32_Word r_info;
} Elf32_Rel;

typedef struct elf64_rel {
  Elf64_Addr r_offset; /* Location at which to apply the action */
  Elf64_Xword r_info; /* index and type of relocation */
} Elf64_Rel;

typedef struct elf32_rela {
  Elf32_Addr r_offset;
  Elf32_Word r_info;
  Elf32_Sword r_addend;
} Elf32_Rela;

typedef struct elf64_rela {
  Elf64_Addr r_offset; /* Location at which to apply the action */
  Elf64_Xword r_info; /* index and type of relocation */
  Elf64_Sxword r_addend; /* Constant addend used to compute value */
} Elf64_Rela;

typedef struct elf32_sym {
  Elf32_Word st_name;
  Elf32_Addr st_value;
  Elf32_Word st_size;
  unsigned char st_info;
  unsigned char st_other;
  Elf32_Half st_shndx;
} Elf32_Sym;

typedef struct elf64_sym {
  Elf64_Word st_name; /* Symbol name, index in string tbl */
  unsigned char st_info; /* Type and binding attributes */
  unsigned char st_other; /* No defined meaning, 0 */
  Elf64_Half st_shndx; /* Associated section index */
  Elf64_Addr st_value; /* Value of the symbol */
  Elf64_Xword st_size; /* Associated symbol size */
} Elf64_Sym;




typedef struct elf32_hdr {
  unsigned char e_ident[16];
  Elf32_Half e_type;
  Elf32_Half e_machine;
  Elf32_Word e_version;
  Elf32_Addr e_entry; /* Entry point */
  Elf32_Off e_phoff;
  Elf32_Off e_shoff;
  Elf32_Word e_flags;
  Elf32_Half e_ehsize;
  Elf32_Half e_phentsize;
  Elf32_Half e_phnum;
  Elf32_Half e_shentsize;
  Elf32_Half e_shnum;
  Elf32_Half e_shstrndx;
} Elf32_Ehdr;

typedef struct elf64_hdr {
  unsigned char e_ident[16]; /* ELF "magic number" */
  Elf64_Half e_type;
  Elf64_Half e_machine;
  Elf64_Word e_version;
  Elf64_Addr e_entry; /* Entry point virtual address */
  Elf64_Off e_phoff; /* Program header table file offset */
  Elf64_Off e_shoff; /* Section header table file offset */
  Elf64_Word e_flags;
  Elf64_Half e_ehsize;
  Elf64_Half e_phentsize;
  Elf64_Half e_phnum;
  Elf64_Half e_shentsize;
  Elf64_Half e_shnum;
  Elf64_Half e_shstrndx;
} Elf64_Ehdr;

/* These constants define the permissions on sections in the program
   header, p_flags. */




typedef struct elf32_phdr {
  Elf32_Word p_type;
  Elf32_Off p_offset;
  Elf32_Addr p_vaddr;
  Elf32_Addr p_paddr;
  Elf32_Word p_filesz;
  Elf32_Word p_memsz;
  Elf32_Word p_flags;
  Elf32_Word p_align;
} Elf32_Phdr;

typedef struct elf64_phdr {
  Elf64_Word p_type;
  Elf64_Word p_flags;
  Elf64_Off p_offset; /* Segment file offset */
  Elf64_Addr p_vaddr; /* Segment virtual address */
  Elf64_Addr p_paddr; /* Segment physical address */
  Elf64_Xword p_filesz; /* Segment size in file */
  Elf64_Xword p_memsz; /* Segment size in memory */
  Elf64_Xword p_align; /* Segment alignment, file & memory */
} Elf64_Phdr;

/* sh_type */
# 290 "./include/uapi/linux/elf.h"
/* sh_flags */







/* special section indexes */
# 308 "./include/uapi/linux/elf.h"
typedef struct elf32_shdr {
  Elf32_Word sh_name;
  Elf32_Word sh_type;
  Elf32_Word sh_flags;
  Elf32_Addr sh_addr;
  Elf32_Off sh_offset;
  Elf32_Word sh_size;
  Elf32_Word sh_link;
  Elf32_Word sh_info;
  Elf32_Word sh_addralign;
  Elf32_Word sh_entsize;
} Elf32_Shdr;

typedef struct elf64_shdr {
  Elf64_Word sh_name; /* Section name, index in string tbl */
  Elf64_Word sh_type; /* Type of section */
  Elf64_Xword sh_flags; /* Miscellaneous section attributes */
  Elf64_Addr sh_addr; /* Section virtual addr at execution */
  Elf64_Off sh_offset; /* Section file offset */
  Elf64_Xword sh_size; /* Size of section in bytes */
  Elf64_Word sh_link; /* Index of another section */
  Elf64_Word sh_info; /* Additional section information */
  Elf64_Xword sh_addralign; /* Section alignment */
  Elf64_Xword sh_entsize; /* Entry size if section holds table */
} Elf64_Shdr;
# 371 "./include/uapi/linux/elf.h"
/*
 * Notes used in ET_CORE. Architectures export some of the arch register sets
 * using the corresponding note types via the PTRACE_GETREGSET and
 * PTRACE_SETREGSET requests.
 * The note name for all these is "LINUX".
 */





/*
 * Note to userspace developers: size of NT_SIGINFO note may increase
 * in the future to accomodate more fields, don't assume it is fixed!
 */
# 448 "./include/uapi/linux/elf.h"
/* Note types with note name "GNU" */


/* Note header in a PT_NOTE section */
typedef struct elf32_note {
  Elf32_Word n_namesz; /* Name size */
  Elf32_Word n_descsz; /* Content size */
  Elf32_Word n_type; /* Content type */
} Elf32_Nhdr;

/* Note header in a PT_NOTE section */
typedef struct elf64_note {
  Elf64_Word n_namesz; /* Name size */
  Elf64_Word n_descsz; /* Content size */
  Elf64_Word n_type; /* Content type */
} Elf64_Nhdr;

/* .note.gnu.property types for EM_AARCH64: */


/* Bits for GNU_PROPERTY_AARCH64_FEATURE_1_BTI */
# 139 "./arch/arm64/include/asm/elf.h" 2






typedef unsigned long elf_greg_t;





typedef elf_greg_t elf_gregset_t[(sizeof(struct user_pt_regs) / sizeof(elf_greg_t))];
typedef struct user_fpsimd_state elf_fpregset_t;

/*
 * When the program starts, a1 contains a pointer to a function to be
 * registered with atexit, as per the SVR4 ABI.  A value of 0 means we have no
 * such handler.
 */
# 167 "./arch/arm64/include/asm/elf.h"
/* update AT_VECTOR_SIZE_ARCH if the number of NEW_AUX_ENT entries changes */
# 185 "./arch/arm64/include/asm/elf.h"
struct linux_binprm;
extern int arch_setup_additional_pages(struct linux_binprm *bprm,
           int uses_interp);

/* 1GB of VA */
# 206 "./arch/arm64/include/asm/elf.h"
/* PIE load location for compat arm. Must match ARM ELF_ET_DYN_BASE. */


/* AArch32 registers. */

typedef unsigned int compat_elf_greg_t;
typedef compat_elf_greg_t compat_elf_gregset_t[18];

/* AArch32 EABI. */

int compat_elf_check_arch(const struct elf32_hdr *);


/*
 * Unlike the native SET_PERSONALITY macro, the compat version maintains
 * READ_IMPLIES_EXEC across an execve() since this is the behaviour on
 * arch/arm/.
 */
# 243 "./arch/arm64/include/asm/elf.h"
extern int aarch32_setup_additional_pages(struct linux_binprm *bprm,
       int uses_interp);





struct arch_elf_state {
 int flags;
};







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_parse_elf_property(u32 type, const void *data,
       size_t datasz, bool compat,
       struct arch_elf_state *arch)
{
 /* No known properties for AArch32 yet */
 if (1 && compat)
  return 0;

 if (type == 0xc0000000) {
  const u32 *p = data;

  if (datasz != sizeof(*p))
   return -8 /* Exec format error */;

  if (system_supports_bti() &&
      (*p & (1U << 0)))
   arch->flags |= (1 << 0);
 }

 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_elf_pt_proc(void *ehdr, void *phdr,
       struct file *f, bool is_interp,
       struct arch_elf_state *state)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int arch_check_elf(void *ehdr, bool has_interp,
     void *interp_ehdr,
     struct arch_elf_state *state)
{
 return 0;
}
# 7 "./include/linux/elf.h" 2



  /* Executables for which elf_read_implies_exec() returns TRUE will
     have the READ_IMPLIES_EXEC personality flag set automatically.
     Override in asm/elf.h as needed.  */
# 52 "./include/linux/elf.h"
extern Elf64_Dyn _DYNAMIC [];
# 64 "./include/linux/elf.h"
/* Optional callbacks to write extra ELF notes. */
struct file;
struct coredump_params;


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int elf_coredump_extra_notes_size(void) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int elf_coredump_extra_notes_write(struct coredump_params *cprm) { return 0; }





/*
 * NT_GNU_PROPERTY_TYPE_0 header:
 * Keep this internal until/unless there is an agreed UAPI definition.
 * pr_type values (GNU_PROPERTY_*) are public and defined in the UAPI header.
 */
struct gnu_property {
 u32 pr_type;
 u32 pr_datasz;
};

struct arch_elf_state;
# 96 "./include/linux/elf.h"
extern int arch_parse_elf_property(u32 type, const void *data, size_t datasz,
       bool compat, struct arch_elf_state *arch);



int arch_elf_adjust_prot(int prot, const struct arch_elf_state *state,
    bool has_interp, bool is_interp);
# 20 "./include/linux/module.h" 2


# 1 "./include/linux/moduleparam.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/* (C) Copyright 2001, 2002 Rusty Russell IBM Corporation */




/* You can override this manually, but generally this should match the
   module name. */





/* We cannot use MODULE_PARAM_PREFIX because some modules override it. */



/* Chosen so that structs with an unsigned long line up. */
# 31 "./include/linux/moduleparam.h"
/* One for each parameter, describing how to use it.  Some files do
   multiple of these per line, so can't just use MODULE_INFO. */



struct kernel_param;

/*
 * Flags available for kernel_param_ops
 *
 * NOARG - the parameter allows for no argument (foo instead of foo=1)
 */
enum {
 KERNEL_PARAM_OPS_FL_NOARG = (1 << 0)
};

struct kernel_param_ops {
 /* How the ops should behave */
 unsigned int flags;
 /* Returns 0, or -errno.  arg is in kp->arg. */
 int (*set)(const char *val, const struct kernel_param *kp);
 /* Returns length written or -errno.  Buffer is 4k (ie. be short!) */
 int (*get)(char *buffer, const struct kernel_param *kp);
 /* Optional function to free kp->arg when module unloaded. */
 void (*free)(void *arg);
};

/*
 * Flags available for kernel_param
 *
 * UNSAFE - the parameter is dangerous and setting it will taint the kernel
 * HWPARAM - Hardware param not permitted in lockdown mode
 */
enum {
 KERNEL_PARAM_FL_UNSAFE = (1 << 0),
 KERNEL_PARAM_FL_HWPARAM = (1 << 1),
};

struct kernel_param {
 const char *name;
 struct module *mod;
 const struct kernel_param_ops *ops;
 const u16 perm;
 s8 level;
 u8 flags;
 union {
  void *arg;
  const struct kparam_string *str;
  const struct kparam_array *arr;
 };
};

extern const struct kernel_param __start___param[], __stop___param[];

/* Special one for strings we want to copy into */
struct kparam_string {
 unsigned int maxlen;
 char *string;
};

/* Special one for arrays */
struct kparam_array
{
 unsigned int max;
 unsigned int elemsize;
 unsigned int *num;
 const struct kernel_param_ops *ops;
 void *elem;
};

/**
 * module_param - typesafe helper for a module/cmdline parameter
 * @name: the variable to alter, and exposed parameter name.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 *
 * @name becomes the module parameter, or (prefixed by KBUILD_MODNAME and a
 * ".") the kernel commandline parameter.  Note that - is changed to _, so
 * the user can use "foo-bar=1" even for variable "foo_bar".
 *
 * @perm is 0 if the variable is not to appear in sysfs, or 0444
 * for world-readable, 0644 for root-writable, etc.  Note that if it
 * is writable, you may need to use kernel_param_lock() around
 * accesses (esp. charp, which can be kfreed when it changes).
 *
 * The @type is simply pasted to refer to a param_ops_##type and a
 * param_check_##type: for convenience many standard types are provided but
 * you can create your own by defining those variables.
 *
 * Standard types are:
 *	byte, hexint, short, ushort, int, uint, long, ulong
 *	charp: a character pointer
 *	bool: a bool, values 0/1, y/n, Y/N.
 *	invbool: the above, only sense-reversed (N = true).
 */



/**
 * module_param_unsafe - same as module_param but taints kernel
 * @name: the variable to alter, and exposed parameter name.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 */



/**
 * module_param_named - typesafe helper for a renamed module/cmdline parameter
 * @name: a valid C identifier which is the parameter name.
 * @value: the actual lvalue to alter.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 *
 * Usually it's a good idea to have variable names and user-exposed names the
 * same, but that's harder if the variable must be non-static or is inside a
 * structure.  This allows exposure under a different name.
 */





/**
 * module_param_named_unsafe - same as module_param_named but taints kernel
 * @name: a valid C identifier which is the parameter name.
 * @value: the actual lvalue to alter.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 */





/**
 * module_param_cb - general callback for a module/cmdline parameter
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */
# 184 "./include/linux/moduleparam.h"
/**
 * core_param_cb - general callback for a module/cmdline parameter
 *                 to be evaluated before core initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */



/**
 * postcore_param_cb - general callback for a module/cmdline parameter
 *                     to be evaluated before postcore initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */



/**
 * arch_param_cb - general callback for a module/cmdline parameter
 *                 to be evaluated before arch initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */



/**
 * subsys_param_cb - general callback for a module/cmdline parameter
 *                   to be evaluated before subsys initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */



/**
 * fs_param_cb - general callback for a module/cmdline parameter
 *               to be evaluated before fs initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */



/**
 * device_param_cb - general callback for a module/cmdline parameter
 *                   to be evaluated before device initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */



/**
 * late_param_cb - general callback for a module/cmdline parameter
 *                 to be evaluated before late initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @arg: args for @ops
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */



/* On alpha, ia64 and ppc64 relocations to global data cannot go into
   read-only sections (which is part of respective UNIX ABI on these
   platforms). So 'const' makes no sense and even causes compile failures
   with some compilers. */






/* This is the fundamental function for registering boot/module
   parameters. */
# 296 "./include/linux/moduleparam.h"
/* Obsolete - use module_param_cb() */







extern void kernel_param_lock(struct module *mod);
extern void kernel_param_unlock(struct module *mod);
# 316 "./include/linux/moduleparam.h"
/**
 * core_param - define a historical core kernel parameter.
 * @name: the name of the cmdline and sysfs parameter (often the same as var)
 * @var: the variable
 * @type: the type of the parameter
 * @perm: visibility in sysfs
 *
 * core_param is just like module_param(), but cannot be modular and
 * doesn't add a prefix (such as "printk.").  This is for compatibility
 * with __setup(), and it makes sense as truly core parameters aren't
 * tied to the particular file they're in.
 */




/**
 * core_param_unsafe - same as core_param but taints kernel
 * @name: the name of the cmdline and sysfs parameter (often the same as var)
 * @var: the variable
 * @type: the type of the parameter
 * @perm: visibility in sysfs
 */







/**
 * module_param_string - a char array parameter
 * @name: the name of the parameter
 * @string: the string variable
 * @len: the maximum length of the string, incl. terminator
 * @perm: visibility in sysfs.
 *
 * This actually copies the string when it's set (unlike type charp).
 * @len is usually just sizeof(string).
 */
# 364 "./include/linux/moduleparam.h"
/**
 * parameq - checks if two parameter names match
 * @name1: parameter name 1
 * @name2: parameter name 2
 *
 * Returns true if the two parameter names are equal.
 * Dashes (-) are considered equal to underscores (_).
 */
extern bool parameq(const char *name1, const char *name2);

/**
 * parameqn - checks if two parameter names match
 * @name1: parameter name 1
 * @name2: parameter name 2
 * @n: the length to compare
 *
 * Similar to parameq(), except it compares @n characters.
 */
extern bool parameqn(const char *name1, const char *name2, size_t n);

/* Called on module insert or kernel boot */
extern char *parse_args(const char *name,
        char *args,
        const struct kernel_param *params,
        unsigned num,
        s16 level_min,
        s16 level_max,
        void *arg,
        int (*unknown)(char *param, char *val,
         const char *doing, void *arg));

/* Called by module remove. */

extern void destroy_params(const struct kernel_param *params, unsigned num);







/* All the helper functions */
/* The macros to do compile-time type checking stolen from Jakub
   Jelinek, who IIRC came up with this idea for the 2.4 module init code. */



extern const struct kernel_param_ops param_ops_byte;
extern int param_set_byte(const char *val, const struct kernel_param *kp);
extern int param_get_byte(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_short;
extern int param_set_short(const char *val, const struct kernel_param *kp);
extern int param_get_short(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_ushort;
extern int param_set_ushort(const char *val, const struct kernel_param *kp);
extern int param_get_ushort(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_int;
extern int param_set_int(const char *val, const struct kernel_param *kp);
extern int param_get_int(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_uint;
extern int param_set_uint(const char *val, const struct kernel_param *kp);
extern int param_get_uint(char *buffer, const struct kernel_param *kp);
int param_set_uint_minmax(const char *val, const struct kernel_param *kp,
  unsigned int min, unsigned int max);


extern const struct kernel_param_ops param_ops_long;
extern int param_set_long(const char *val, const struct kernel_param *kp);
extern int param_get_long(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_ulong;
extern int param_set_ulong(const char *val, const struct kernel_param *kp);
extern int param_get_ulong(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_ullong;
extern int param_set_ullong(const char *val, const struct kernel_param *kp);
extern int param_get_ullong(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_hexint;
extern int param_set_hexint(const char *val, const struct kernel_param *kp);
extern int param_get_hexint(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_charp;
extern int param_set_charp(const char *val, const struct kernel_param *kp);
extern int param_get_charp(char *buffer, const struct kernel_param *kp);
extern void param_free_charp(void *arg);


/* We used to allow int as well as bool.  We're taking that away! */
extern const struct kernel_param_ops param_ops_bool;
extern int param_set_bool(const char *val, const struct kernel_param *kp);
extern int param_get_bool(char *buffer, const struct kernel_param *kp);


extern const struct kernel_param_ops param_ops_bool_enable_only;
extern int param_set_bool_enable_only(const char *val,
          const struct kernel_param *kp);
/* getter is the same as for the regular bool */


extern const struct kernel_param_ops param_ops_invbool;
extern int param_set_invbool(const char *val, const struct kernel_param *kp);
extern int param_get_invbool(char *buffer, const struct kernel_param *kp);


/* An int, which can only be set like a bool (though it shows as an int). */
extern const struct kernel_param_ops param_ops_bint;
extern int param_set_bint(const char *val, const struct kernel_param *kp);



/**
 * module_param_array - a parameter which is an array of some type
 * @name: the name of the array variable
 * @type: the type, as per module_param()
 * @nump: optional pointer filled in with the number written
 * @perm: visibility in sysfs
 *
 * Input and output are as comma-separated values.  Commas inside values
 * don't work properly (eg. an array of charp).
 *
 * ARRAY_SIZE(@name) is used to determine the number of elements in the
 * array, so the definition must be visible.
 */



/**
 * module_param_array_named - renamed parameter which is an array of some type
 * @name: a valid C identifier which is the parameter name
 * @array: the name of the array variable
 * @type: the type, as per module_param()
 * @nump: optional pointer filled in with the number written
 * @perm: visibility in sysfs
 *
 * This exposes a different name than the actual variable name.  See
 * module_param_named() for why this might be necessary.
 */
# 526 "./include/linux/moduleparam.h"
enum hwparam_type {
 hwparam_ioport, /* Module parameter configures an I/O port */
 hwparam_iomem, /* Module parameter configures an I/O mem address */
 hwparam_ioport_or_iomem, /* Module parameter could be either, depending on other option */
 hwparam_irq, /* Module parameter configures an IRQ */
 hwparam_dma, /* Module parameter configures a DMA channel */
 hwparam_dma_addr, /* Module parameter configures a DMA buffer address */
 hwparam_other, /* Module parameter configures some other value */
};

/**
 * module_param_hw_named - A parameter representing a hw parameters
 * @name: a valid C identifier which is the parameter name.
 * @value: the actual lvalue to alter.
 * @type: the type of the parameter
 * @hwtype: what the value represents (enum hwparam_type)
 * @perm: visibility in sysfs.
 *
 * Usually it's a good idea to have variable names and user-exposed names the
 * same, but that's harder if the variable must be non-static or is inside a
 * structure.  This allows exposure under a different name.
 */
# 559 "./include/linux/moduleparam.h"
/**
 * module_param_hw_array - A parameter representing an array of hw parameters
 * @name: the name of the array variable
 * @type: the type, as per module_param()
 * @hwtype: what the value represents (enum hwparam_type)
 * @nump: optional pointer filled in with the number written
 * @perm: visibility in sysfs
 *
 * Input and output are as comma-separated values.  Commas inside values
 * don't work properly (eg. an array of charp).
 *
 * ARRAY_SIZE(@name) is used to determine the number of elements in the
 * array, so the definition must be visible.
 */
# 587 "./include/linux/moduleparam.h"
extern const struct kernel_param_ops param_array_ops;

extern const struct kernel_param_ops param_ops_string;
extern int param_set_copystring(const char *val, const struct kernel_param *);
extern int param_get_string(char *buffer, const struct kernel_param *kp);

/* for exporting parameters in /sys/module/.../parameters */

struct module;


extern int module_param_sysfs_setup(struct module *mod,
        const struct kernel_param *kparam,
        unsigned int num_params);

extern void module_param_sysfs_remove(struct module *mod);
# 23 "./include/linux/module.h" 2


# 1 "./include/linux/rbtree_latch.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Latched RB-trees
 *
 * Copyright (C) 2015 Intel Corp., Peter Zijlstra <peterz@infradead.org>
 *
 * Since RB-trees have non-atomic modifications they're not immediately suited
 * for RCU/lockless queries. Even though we made RB-tree lookups non-fatal for
 * lockless lookups; we cannot guarantee they return a correct result.
 *
 * The simplest solution is a seqlock + RB-tree, this will allow lockless
 * lookups; but has the constraint (inherent to the seqlock) that read sides
 * cannot nest in write sides.
 *
 * If we need to allow unconditional lookups (say as required for NMI context
 * usage) we need a more complex setup; this data structure provides this by
 * employing the latch technique -- see @raw_write_seqcount_latch -- to
 * implement a latched RB-tree which does allow for unconditional lookups by
 * virtue of always having (at least) one stable copy of the tree.
 *
 * However, while we have the guarantee that there is at all times one stable
 * copy, this does not guarantee an iteration will not observe modifications.
 * What might have been a stable copy at the start of the iteration, need not
 * remain so for the duration of the iteration.
 *
 * Therefore, this does require a lockless RB-tree iteration to be non-fatal;
 * see the comment in lib/rbtree.c. Note however that we only require the first
 * condition -- not seeing partial stores -- because the latch thing isolates
 * us from loops. If we were to interrupt a modification the lookup would be
 * pointed at the stable tree and complete while the modification was halted.
 */
# 40 "./include/linux/rbtree_latch.h"
struct latch_tree_node {
 struct rb_node node[2];
};

struct latch_tree_root {
 seqcount_latch_t seq;
 struct rb_root tree[2];
};

/**
 * latch_tree_ops - operators to define the tree order
 * @less: used for insertion; provides the (partial) order between two elements.
 * @comp: used for lookups; provides the order between the search key and an element.
 *
 * The operators are related like:
 *
 *	comp(a->key,b) < 0  := less(a,b)
 *	comp(a->key,b) > 0  := less(b,a)
 *	comp(a->key,b) == 0 := !less(a,b) && !less(b,a)
 *
 * If these operators define a partial order on the elements we make no
 * guarantee on which of the elements matching the key is found. See
 * latch_tree_find().
 */
struct latch_tree_ops {
 bool (*less)(struct latch_tree_node *a, struct latch_tree_node *b);
 int (*comp)(void *key, struct latch_tree_node *b);
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct latch_tree_node *
__lt_from_rb(struct rb_node *node, int idx)
{
 return ({ void *__mptr = (void *)(node); _Static_assert(__builtin_types_compatible_p(typeof(*(node)), typeof(((struct latch_tree_node *)0)->node[idx])) || __builtin_types_compatible_p(typeof(*(node)), typeof(void)), "pointer type mismatch in container_of()"); ((struct latch_tree_node *)(__mptr - __builtin_offsetof(struct latch_tree_node, node[idx]))); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
__lt_insert(struct latch_tree_node *ltn, struct latch_tree_root *ltr, int idx,
     bool (*less)(struct latch_tree_node *a, struct latch_tree_node *b))
{
 struct rb_root *root = &ltr->tree[idx];
 struct rb_node **link = &root->rb_node;
 struct rb_node *node = &ltn->node[idx];
 struct rb_node *parent = ((void *)0);
 struct latch_tree_node *ltp;

 while (*link) {
  parent = *link;
  ltp = __lt_from_rb(parent, idx);

  if (less(ltn, ltp))
   link = &parent->rb_left;
  else
   link = &parent->rb_right;
 }

 rb_link_node_rcu(node, parent, link);
 rb_insert_color(node, root);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
__lt_erase(struct latch_tree_node *ltn, struct latch_tree_root *ltr, int idx)
{
 rb_erase(&ltn->node[idx], &ltr->tree[idx]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct latch_tree_node *
__lt_find(void *key, struct latch_tree_root *ltr, int idx,
   int (*comp)(void *key, struct latch_tree_node *node))
{
 struct rb_node *node = ({ /* Dependency order vs. p above. */ typeof(ltr->tree[idx].rb_node) __UNIQUE_ID_rcu369 = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_370(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ltr->tree[idx].rb_node) == sizeof(char) || sizeof(ltr->tree[idx].rb_node) == sizeof(short) || sizeof(ltr->tree[idx].rb_node) == sizeof(int) || sizeof(ltr->tree[idx].rb_node) == sizeof(long)) || sizeof(ltr->tree[idx].rb_node) == sizeof(long long))) __compiletime_assert_370(); } while (0); (*(const volatile typeof( _Generic((ltr->tree[idx].rb_node), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (ltr->tree[idx].rb_node))) *)&(ltr->tree[idx].rb_node)); }); ((typeof(*ltr->tree[idx].rb_node) *)(__UNIQUE_ID_rcu369)); });
# 110 "./include/linux/rbtree_latch.h"
 struct latch_tree_node *ltn;
 int c;

 while (node) {
  ltn = __lt_from_rb(node, idx);
  c = comp(key, ltn);

  if (c < 0)
   node = ({ /* Dependency order vs. p above. */ typeof(node->rb_left) __UNIQUE_ID_rcu371 = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_372(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(node->rb_left) == sizeof(char) || sizeof(node->rb_left) == sizeof(short) || sizeof(node->rb_left) == sizeof(int) || sizeof(node->rb_left) == sizeof(long)) || sizeof(node->rb_left) == sizeof(long long))) __compiletime_assert_372(); } while (0); (*(const volatile typeof( _Generic((node->rb_left), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (node->rb_left))) *)&(node->rb_left)); }); ((typeof(*node->rb_left) *)(__UNIQUE_ID_rcu371)); });
# 119 "./include/linux/rbtree_latch.h"
  else if (c > 0)
   node = ({ /* Dependency order vs. p above. */ typeof(node->rb_right) __UNIQUE_ID_rcu373 = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_374(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(node->rb_right) == sizeof(char) || sizeof(node->rb_right) == sizeof(short) || sizeof(node->rb_right) == sizeof(int) || sizeof(node->rb_right) == sizeof(long)) || sizeof(node->rb_right) == sizeof(long long))) __compiletime_assert_374(); } while (0); (*(const volatile typeof( _Generic((node->rb_right), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (node->rb_right))) *)&(node->rb_right)); }); ((typeof(*node->rb_right) *)(__UNIQUE_ID_rcu373)); });
# 121 "./include/linux/rbtree_latch.h"
  else
   return ltn;
 }

 return ((void *)0);
}

/**
 * latch_tree_insert() - insert @node into the trees @root
 * @node: nodes to insert
 * @root: trees to insert @node into
 * @ops: operators defining the node order
 *
 * It inserts @node into @root in an ordered fashion such that we can always
 * observe one complete tree. See the comment for raw_write_seqcount_latch().
 *
 * The inserts use rcu_assign_pointer() to publish the element such that the
 * tree structure is stored before we can observe the new @node.
 *
 * All modifications (latch_tree_insert, latch_tree_remove) are assumed to be
 * serialized.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
latch_tree_insert(struct latch_tree_node *node,
    struct latch_tree_root *root,
    const struct latch_tree_ops *ops)
{
 raw_write_seqcount_latch(&root->seq);
 __lt_insert(node, root, 0, ops->less);
 raw_write_seqcount_latch(&root->seq);
 __lt_insert(node, root, 1, ops->less);
}

/**
 * latch_tree_erase() - removes @node from the trees @root
 * @node: nodes to remote
 * @root: trees to remove @node from
 * @ops: operators defining the node order
 *
 * Removes @node from the trees @root in an ordered fashion such that we can
 * always observe one complete tree. See the comment for
 * raw_write_seqcount_latch().
 *
 * It is assumed that @node will observe one RCU quiescent state before being
 * reused of freed.
 *
 * All modifications (latch_tree_insert, latch_tree_remove) are assumed to be
 * serialized.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void
latch_tree_erase(struct latch_tree_node *node,
   struct latch_tree_root *root,
   const struct latch_tree_ops *ops)
{
 raw_write_seqcount_latch(&root->seq);
 __lt_erase(node, root, 0);
 raw_write_seqcount_latch(&root->seq);
 __lt_erase(node, root, 1);
}

/**
 * latch_tree_find() - find the node matching @key in the trees @root
 * @key: search key
 * @root: trees to search for @key
 * @ops: operators defining the node order
 *
 * Does a lockless lookup in the trees @root for the node matching @key.
 *
 * It is assumed that this is called while holding the appropriate RCU read
 * side lock.
 *
 * If the operators define a partial order on the elements (there are multiple
 * elements which have the same key value) it is undefined which of these
 * elements will be found. Nor is it possible to iterate the tree to find
 * further elements with the same key value.
 *
 * Returns: a pointer to the node matching @key or NULL.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) struct latch_tree_node *
latch_tree_find(void *key, struct latch_tree_root *root,
  const struct latch_tree_ops *ops)
{
 struct latch_tree_node *node;
 unsigned int seq;

 do {
  seq = raw_read_seqcount_latch(&root->seq);
  node = __lt_find(key, root, seq & 1, ops->comp);
 } while (read_seqcount_latch_retry(&root->seq, seq));

 return node;
}
# 26 "./include/linux/module.h" 2
# 1 "./include/linux/error-injection.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/asm-generic/error-injection.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




enum {
 EI_ETYPE_NONE, /* Dummy value for undefined case */
 EI_ETYPE_NULL, /* Return NULL if failure */
 EI_ETYPE_ERRNO, /* Return -ERRNO if failure */
 EI_ETYPE_ERRNO_NULL, /* Return -ERRNO or NULL if failure */
 EI_ETYPE_TRUE, /* Return true if failure */
};

struct error_injection_entry {
 unsigned long addr;
 int etype;
};

struct pt_regs;
# 38 "./include/asm-generic/error-injection.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void override_function_with_return(struct pt_regs *regs) { }
# 7 "./include/linux/error-injection.h" 2








static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool within_error_injection_list(unsigned long addr)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_injectable_error_type(unsigned long addr)
{
 return EI_ETYPE_NONE;
}
# 27 "./include/linux/module.h" 2





# 1 "./arch/arm64/include/asm/module.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



# 1 "./include/asm-generic/module.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Many architectures just need a simple module
 * loader without arch specific data.
 */
# 9 "./arch/arm64/include/asm/module.h" 2


struct mod_plt_sec {
 int plt_shndx;
 int plt_num_entries;
 int plt_max_entries;
};

struct mod_arch_specific {
 struct mod_plt_sec core;
 struct mod_plt_sec init;

 /* for CONFIG_DYNAMIC_FTRACE */
 struct plt_entry *ftrace_trampolines;
};


u64 module_emit_plt_entry(struct module *mod, Elf64_Shdr *sechdrs,
     void *loc, const Elf64_Rela *rela,
     Elf64_Sym *sym);

u64 module_emit_veneer_for_adrp(struct module *mod, Elf64_Shdr *sechdrs,
    void *loc, u64 val);


extern u64 module_alloc_base;




struct plt_entry {
 /*
	 * A program that conforms to the AArch64 Procedure Call Standard
	 * (AAPCS64) must assume that a veneer that alters IP0 (x16) and/or
	 * IP1 (x17) may be inserted at any branch instruction that is
	 * exposed to a relocation that supports long branches. Since that
	 * is exactly what we are dealing with here, we are free to use x16
	 * as a scratch register in the PLT veneers.
	 */
 __le32 adrp; /* adrp	x16, ....			*/
 __le32 add; /* add	x16, x16, #0x....		*/
 __le32 br; /* br	x16				*/
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_forbidden_offset_for_adrp(void *place)
{
 return 1 &&
        cpus_have_const_cap(55) &&
        ((u64)place & 0xfff) >= 0xff8;
}

struct plt_entry get_plt_entry(u64 dst, void *pc);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const Elf64_Shdr *find_section(const Elf64_Ehdr *hdr,
        const Elf64_Shdr *sechdrs,
        const char *name)
{
 const Elf64_Shdr *s, *se;
 const char *secstrs = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;

 for (s = sechdrs, se = sechdrs + hdr->e_shnum; s < se; s++) {
  if (strcmp(name, secstrs + s->sh_name) == 0)
   return s;
 }

 return ((void *)0);
}
# 33 "./include/linux/module.h" 2



struct modversion_info {
 unsigned long crc;
 char name[(64 - sizeof(unsigned long))];
};

struct module;
struct exception_table_entry;

struct module_kobject {
 struct kobject kobj;
 struct module *mod;
 struct kobject *drivers_dir;
 struct module_param_attrs *mp;
 struct completion *kobj_completion;
} ;

struct module_attribute {
 struct attribute attr;
 ssize_t (*show)(struct module_attribute *, struct module_kobject *,
   char *);
 ssize_t (*store)(struct module_attribute *, struct module_kobject *,
    const char *, size_t count);
 void (*setup)(struct module *, const char *);
 int (*test)(struct module *);
 void (*free)(struct module *);
};

struct module_version_attribute {
 struct module_attribute mattr;
 const char *module_name;
 const char *version;
};

extern ssize_t __modver_version_show(struct module_attribute *,
         struct module_kobject *, char *);

extern struct module_attribute module_uevent;

/* These are either module local, or the kernel's dummy ones. */
extern int init_module(void);
extern void cleanup_module(void);


/**
 * module_init() - driver initialization entry point
 * @x: function to be run at kernel boot time or module insertion
 *
 * module_init() will either be called during do_initcalls() (if
 * builtin) or at module insertion time (if a module).  There can only
 * be one per module.
 */


/**
 * module_exit() - driver exit entry point
 * @x: function to be run when driver is removed
 *
 * module_exit() will wrap the driver clean-up code
 * with cleanup_module() when used with rmmod when
 * the driver is a module.  If the driver is statically
 * compiled into the kernel, module_exit() has no effect.
 * There can only be one per module.
 */
# 146 "./include/linux/module.h"
/* This means "can be init if no module support, otherwise module load
   may call it." */
# 164 "./include/linux/module.h"
/* Generic info of form tag = "info" */


/* For userspace: you can also call me... */


/* Soft module dependencies. See man modprobe.d for details.
 * Example: MODULE_SOFTDEP("pre: module-foo module-bar post: module-baz")
 */


/*
 * MODULE_FILE is used for generating modules.builtin
 * So, make it no-op when this is being built as a module
 */






/*
 * The following license idents are currently accepted as indicating free
 * software modules
 *
 *	"GPL"				[GNU Public License v2]
 *	"GPL v2"			[GNU Public License v2]
 *	"GPL and additional rights"	[GNU Public License v2 rights and more]
 *	"Dual BSD/GPL"			[GNU Public License v2
 *					 or BSD license choice]
 *	"Dual MIT/GPL"			[GNU Public License v2
 *					 or MIT license choice]
 *	"Dual MPL/GPL"			[GNU Public License v2
 *					 or Mozilla license choice]
 *
 * The following other idents are available
 *
 *	"Proprietary"			[Non free products]
 *
 * Both "GPL v2" and "GPL" (the latter also in dual licensed strings) are
 * merely stating that the module is licensed under the GPL v2, but are not
 * telling whether "GPL v2 only" or "GPL v2 or later". The reason why there
 * are two variants is a historic and failed attempt to convey more
 * information in the MODULE_LICENSE string. For module loading the
 * "only/or later" distinction is completely irrelevant and does neither
 * replace the proper license identifiers in the corresponding source file
 * nor amends them in any way. The sole purpose is to make the
 * 'Proprietary' flagging work and to refuse to bind symbols which are
 * exported with EXPORT_SYMBOL_GPL when a non free module is loaded.
 *
 * In the same way "BSD" is not a clear license information. It merely
 * states, that the module is licensed under one of the compatible BSD
 * license variants. The detailed and correct license information is again
 * to be found in the corresponding source files.
 *
 * There are dual licensed components, but when running with Linux it is the
 * GPL that is relevant so this is a non issue. Similarly LGPL linked with GPL
 * is a GPL combined work.
 *
 * This exists for several reasons
 * 1.	So modinfo can show license info for users wanting to vet their setup
 *	is free
 * 2.	So the community can ignore bug reports including proprietary modules
 * 3.	So vendors can do likewise based on their own policies
 */


/*
 * Author(s), use "Name <email>" or just "Name", for multiple
 * authors use multiple MODULE_AUTHOR() statements/lines.
 */


/* What your module does. */
# 249 "./include/linux/module.h"
/* Version of form [<epoch>:]<version>[-<extra-version>].
 * Or for CVS/RCS ID version, everything but the number is stripped.
 * <epoch>: A (small) unsigned integer which allows you to start versions
 * anew. If not mentioned, it's zero.  eg. "2:1.0" is after
 * "1:2.0".

 * <version>: The <version> may contain only alphanumerics and the
 * character `.'.  Ordered by numeric sort for numeric parts,
 * ascii sort for ascii parts (as per RPM or DEB algorithm).

 * <extraversion>: Like <version>, but inserted for local
 * customizations, eg "rh3" or "rusty1".

 * Using this automatically adds a checksum of the .c files and the
 * local headers in "srcversion".
 */
# 287 "./include/linux/module.h"
/* Optional firmware file (or files) needed by the module
 * format is simply firmware file name.  Multiple firmware
 * files require multiple MODULE_FIRMWARE() specifiers */




struct notifier_block;



extern int modules_disabled; /* for sysctl */
/* Get/put a kernel symbol (calls must be symmetric) */
void *__symbol_get(const char *symbol);
void *__symbol_get_gpl(const char *symbol);


/* modules using other modules: kdb wants to see this. */
struct module_use {
 struct list_head source_list;
 struct list_head target_list;
 struct module *source, *target;
};

enum module_state {
 MODULE_STATE_LIVE, /* Normal state. */
 MODULE_STATE_COMING, /* Full formed, running module_init. */
 MODULE_STATE_GOING, /* Going away. */
 MODULE_STATE_UNFORMED, /* Still setting it up. */
};

struct mod_tree_node {
 struct module *mod;
 struct latch_tree_node node;
};

struct module_layout {
 /* The actual code + data. */
 void *base;
 /* Total size. */
 unsigned int size;
 /* The size of the executable code.  */
 unsigned int text_size;
 /* Size of RO section of the module (text+rodata) */
 unsigned int ro_size;
 /* Size of RO after init section */
 unsigned int ro_after_init_size;


 struct mod_tree_node mtn;

};


/* Only touch one cacheline for common rbtree-for-core-layout case. */





struct mod_kallsyms {
 Elf64_Sym *symtab;
 unsigned int num_symtab;
 char *strtab;
 char *typetab;
};
# 363 "./include/linux/module.h"
struct module {
 enum module_state state;

 /* Member of list of modules */
 struct list_head list;

 /* Unique handle for this module */
 char name[(64 - sizeof(unsigned long))];






 /* Sysfs stuff. */
 struct module_kobject mkobj;
 struct module_attribute *modinfo_attrs;
 const char *version;
 const char *srcversion;
 struct kobject *holders_dir;

 /* Exported symbols */
 const struct kernel_symbol *syms;
 const s32 *crcs;
 unsigned int num_syms;






 /* Kernel parameters. */

 struct mutex param_lock;

 struct kernel_param *kp;
 unsigned int num_kp;

 /* GPL-only exported symbols. */
 unsigned int num_gpl_syms;
 const struct kernel_symbol *gpl_syms;
 const s32 *gpl_crcs;
 bool using_gplonly_symbols;






 bool async_probe_requested;

 /* Exception table */
 unsigned int num_exentries;
 struct exception_table_entry *extable;

 /* Startup function. */
 int (*init)(void);

 /* Core layout: rbtree is accessed frequently, so keep together. */
 struct module_layout core_layout __attribute__((__aligned__((1 << (6)))));
 struct module_layout init_layout;




 /* Arch-specific module values */
 struct mod_arch_specific arch;

 unsigned long taints; /* same bits as kernel:taint_flags */


 /* Support for BUG */
 unsigned num_bugs;
 struct list_head bug_list;
 struct bug_entry *bug_table;



 /* Protected by RCU and/or module_mutex: use rcu_dereference() */
 struct mod_kallsyms /* nothing */ *kallsyms;
 struct mod_kallsyms core_kallsyms;

 /* Section attributes */
 struct module_sect_attrs *sect_attrs;

 /* Notes attributes */
 struct module_notes_attrs *notes_attrs;


 /* The command line arguments (may be mangled).  People like
	   keeping pointers to this stuff */
 char *args;


 /* Per-cpu data. */
 void /* nothing */ *percpu;
 unsigned int percpu_size;

 void *noinstr_text_start;
 unsigned int noinstr_text_size;






 unsigned int num_srcu_structs;
 struct srcu_struct **srcu_struct_ptrs;
# 481 "./include/linux/module.h"
 struct jump_entry *jump_entries;
 unsigned int num_jump_entries;
# 528 "./include/linux/module.h"
 /* What modules depend on me? */
 struct list_head source_list;
 /* What modules do I depend on? */
 struct list_head target_list;

 /* Destruction function. */
 void (*exit)(void);

 atomic_t refcnt;
# 549 "./include/linux/module.h"
} __attribute__((__aligned__((1 << (6))))) ;





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long kallsyms_symbol_value(const Elf64_Sym *sym)
{
 return sym->st_value;
}


/* FIXME: It'd be nice to isolate modules during init, too, so they
   aren't used before they (may) fail.  But presently too much code
   (IDE & SCSI) require entry into the module during init.*/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool module_is_live(struct module *mod)
{
 return mod->state != MODULE_STATE_GOING;
}

struct module *__module_text_address(unsigned long addr);
struct module *__module_address(unsigned long addr);
bool is_module_address(unsigned long addr);
bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr);
bool is_module_percpu_address(unsigned long addr);
bool is_module_text_address(unsigned long addr);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool within_module_core(unsigned long addr,
          const struct module *mod)
{





 return (unsigned long)mod->core_layout.base <= addr &&
        addr < (unsigned long)mod->core_layout.base + mod->core_layout.size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool within_module_init(unsigned long addr,
          const struct module *mod)
{
 return (unsigned long)mod->init_layout.base <= addr &&
        addr < (unsigned long)mod->init_layout.base + mod->init_layout.size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool within_module(unsigned long addr, const struct module *mod)
{
 return within_module_init(addr, mod) || within_module_core(addr, mod);
}

/* Search for module by name: must be in a RCU-sched critical section. */
struct module *find_module(const char *name);

/* Returns 0 and fills in value, defined and namebuf, or -ERANGE if
   symnum out of range. */
int module_get_kallsym(unsigned int symnum, unsigned long *value, char *type,
   char *name, char *module_name, int *exported);

/* Look for this name: can be of form module:name. */
unsigned long module_kallsyms_lookup_name(const char *name);

extern void __attribute__((__noreturn__)) __module_put_and_kthread_exit(struct module *mod,
   long code);



int module_refcount(struct module *mod);
void __symbol_put(const char *symbol);

void symbol_put_addr(void *addr);

/* Sometimes we know we already have a refcount, and it's easier not
   to handle the error case (which only happens with rmmod --wait). */
extern void __module_get(struct module *module);

/* This is the Right Way to get a module: if it fails, it's being removed,
 * so pretend it's not there. */
extern bool try_module_get(struct module *module);

extern void module_put(struct module *module);
# 647 "./include/linux/module.h"
/* This is a #define so the string doesn't get put in every .o file */






/* Dereference module function descriptor */
void *dereference_module_function_descriptor(struct module *mod, void *ptr);

/* For kallsyms to ask for address resolution.  namebuf should be at
 * least KSYM_NAME_LEN long: a pointer to namebuf is returned if
 * found, otherwise NULL. */
const char *module_address_lookup(unsigned long addr,
       unsigned long *symbolsize,
       unsigned long *offset,
       char **modname, const unsigned char **modbuildid,
       char *namebuf);
int lookup_module_symbol_name(unsigned long addr, char *symname);
int lookup_module_symbol_attrs(unsigned long addr, unsigned long *size, unsigned long *offset, char *modname, char *name);

int register_module_notifier(struct notifier_block *nb);
int unregister_module_notifier(struct notifier_block *nb);

extern void print_modules(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool module_requested_async_probing(struct module *module)
{
 return module && module->async_probe_requested;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_livepatch_module(struct module *mod)
{



 return false;

}

void set_module_sig_enforced(void);
# 828 "./include/linux/module.h"
extern struct kset *module_kset;
extern struct kobj_type module_ktype;




/* BELOW HERE ALL THESE ARE OBSOLETE AND WILL VANISH */




void module_bug_finalize(const Elf64_Ehdr *, const Elf64_Shdr *,
    struct module *);
void module_bug_cleanup(struct module *);
# 856 "./include/linux/module.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool retpoline_module_ok(bool has_retpoline)
{
 return true;
}
# 870 "./include/linux/module.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_module_sig_enforced(void)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool module_sig_ok(struct module *module)
{
 return true;
}



int module_kallsyms_on_each_symbol(int (*fn)(void *, const char *,
          struct module *, unsigned long),
       void *data);
# 15 "./include/linux/kallsyms.h" 2
# 24 "./include/linux/kallsyms.h"
struct cred;
struct module;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_kernel_text(unsigned long addr)
{
 if (__is_kernel_text(addr))
  return 1;
 return in_gate_area_no_mm(addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_kernel(unsigned long addr)
{
 if (__is_kernel(addr))
  return 1;
 return in_gate_area_no_mm(addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int is_ksym_addr(unsigned long addr)
{
 if (1)
  return is_kernel(addr);

 return is_kernel_text(addr) || is_kernel_inittext(addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *dereference_symbol_descriptor(void *ptr)
{
# 65 "./include/linux/kallsyms.h"
 return ptr;
}


unsigned long kallsyms_sym_address(int idx);
int kallsyms_on_each_symbol(int (*fn)(void *, const char *, struct module *,
          unsigned long),
       void *data);
int kallsyms_on_each_match_symbol(int (*fn)(void *, unsigned long),
      const char *name, void *data);

/* Lookup the address for a symbol. Returns 0 if not found. */
unsigned long kallsyms_lookup_name(const char *name);

extern int kallsyms_lookup_size_offset(unsigned long addr,
      unsigned long *symbolsize,
      unsigned long *offset);

/* Lookup an address.  modname is set to NULL if it's in the kernel. */
const char *kallsyms_lookup(unsigned long addr,
       unsigned long *symbolsize,
       unsigned long *offset,
       char **modname, char *namebuf);

/* Look up a kernel symbol and return it in a text buffer. */
extern int sprint_symbol(char *buffer, unsigned long address);
extern int sprint_symbol_build_id(char *buffer, unsigned long address);
extern int sprint_symbol_no_offset(char *buffer, unsigned long address);
extern int sprint_backtrace(char *buffer, unsigned long address);
extern int sprint_backtrace_build_id(char *buffer, unsigned long address);

int lookup_symbol_name(unsigned long addr, char *symname);
int lookup_symbol_attrs(unsigned long addr, unsigned long *size, unsigned long *offset, char *modname, char *name);

/* How and when do we show kallsyms values? */
extern bool kallsyms_show_value(const struct cred *cred);
# 182 "./include/linux/kallsyms.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void print_ip_sym(const char *loglvl, unsigned long ip)
{
 ({ do {} while (0); _printk("%s[<%px>] %pS\n", loglvl, (void *) ip, (void *) ip); });
}
# 14 "./include/linux/ftrace.h" 2


# 1 "./include/linux/ptrace.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/ptrace.h"
# 1 "./include/linux/pid_namespace.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 10 "./include/linux/pid_namespace.h"
# 1 "./include/linux/nsproxy.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






struct mnt_namespace;
struct uts_namespace;
struct ipc_namespace;
struct pid_namespace;
struct cgroup_namespace;
struct fs_struct;

/*
 * A structure to contain pointers to all per-process
 * namespaces - fs (mount), uts, network, sysvipc, etc.
 *
 * The pid namespace is an exception -- it's accessed using
 * task_active_pid_ns.  The pid namespace here is the
 * namespace that children will use.
 *
 * 'count' is the number of tasks holding a reference.
 * The count for each namespace, then, will be the number
 * of nsproxies pointing to it, not the number of tasks.
 *
 * The nsproxy is shared by tasks which share all namespaces.
 * As soon as a single namespace is cloned or unshared, the
 * nsproxy is copied.
 */
struct nsproxy {
 atomic_t count;
 struct uts_namespace *uts_ns;
 struct ipc_namespace *ipc_ns;
 struct mnt_namespace *mnt_ns;
 struct pid_namespace *pid_ns_for_children;
 struct net *net_ns;
 struct time_namespace *time_ns;
 struct time_namespace *time_ns_for_children;
 struct cgroup_namespace *cgroup_ns;
};
extern struct nsproxy init_nsproxy;

/*
 * A structure to encompass all bits needed to install
 * a partial or complete new set of namespaces.
 *
 * If a new user namespace is requested cred will
 * point to a modifiable set of credentials. If a pointer
 * to a modifiable set is needed nsset_cred() must be
 * used and tested.
 */
struct nsset {
 unsigned flags;
 struct nsproxy *nsproxy;
 struct fs_struct *fs;
 const struct cred *cred;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct cred *nsset_cred(struct nsset *set)
{
 if (set->flags & 0x10000000 /* New user namespace */)
  return (struct cred *)set->cred;

 return ((void *)0);
}

/*
 * the namespaces access rules are:
 *
 *  1. only current task is allowed to change tsk->nsproxy pointer or
 *     any pointer on the nsproxy itself.  Current must hold the task_lock
 *     when changing tsk->nsproxy.
 *
 *  2. when accessing (i.e. reading) current task's namespaces - no
 *     precautions should be taken - just dereference the pointers
 *
 *  3. the access to other task namespaces is performed like this
 *     task_lock(task);
 *     nsproxy = task->nsproxy;
 *     if (nsproxy != NULL) {
 *             / *
 *               * work with the namespaces here
 *               * e.g. get the reference on one of them
 *               * /
 *     } / *
 *         * NULL task->nsproxy means that this task is
 *         * almost dead (zombie)
 *         * /
 *     task_unlock(task);
 *
 */

int copy_namespaces(unsigned long flags, struct task_struct *tsk);
void exit_task_namespaces(struct task_struct *tsk);
void switch_task_namespaces(struct task_struct *tsk, struct nsproxy *new);
int exec_task_namespaces(void);
void free_nsproxy(struct nsproxy *ns);
int unshare_nsproxy_namespaces(unsigned long, struct nsproxy **,
 struct cred *, struct fs_struct *);
int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) nsproxy_cache_init(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void put_nsproxy(struct nsproxy *ns)
{
 if (atomic_dec_and_test(&ns->count)) {
  free_nsproxy(ns);
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void get_nsproxy(struct nsproxy *ns)
{
 atomic_inc(&ns->count);
}
# 11 "./include/linux/pid_namespace.h" 2
# 1 "./include/linux/ns_common.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





struct proc_ns_operations;

struct ns_common {
 atomic_long_t stashed;
 const struct proc_ns_operations *ops;
 unsigned int inum;
 refcount_t count;
};
# 12 "./include/linux/pid_namespace.h" 2


/* MAX_PID_NS_LEVEL is needed for limiting size of 'struct pid' */


struct fs_pin;

struct pid_namespace {
 struct idr idr;
 struct callback_head rcu;
 unsigned int pid_allocated;
 struct task_struct *child_reaper;
 struct kmem_cache *pid_cachep;
 unsigned int level;
 struct pid_namespace *parent;

 struct fs_pin *bacct;

 struct user_namespace *user_ns;
 struct ucounts *ucounts;
 int reboot; /* group exit code if this pidns was rebooted */
 struct ns_common ns;
} ;

extern struct pid_namespace init_pid_ns;




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid_namespace *get_pid_ns(struct pid_namespace *ns)
{
 if (ns != &init_pid_ns)
  refcount_inc(&ns->ns.count);
 return ns;
}

extern struct pid_namespace *copy_pid_ns(unsigned long flags,
 struct user_namespace *user_ns, struct pid_namespace *ns);
extern void zap_pid_ns_processes(struct pid_namespace *pid_ns);
extern int reboot_pid_ns(struct pid_namespace *pid_ns, int cmd);
extern void put_pid_ns(struct pid_namespace *ns);
# 85 "./include/linux/pid_namespace.h"
extern struct pid_namespace *task_active_pid_ns(struct task_struct *tsk);
void pidhash_init(void);
void pid_idr_init(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool task_is_in_init_pid_ns(struct task_struct *tsk)
{
 return task_active_pid_ns(tsk) == &init_pid_ns;
}
# 11 "./include/linux/ptrace.h" 2
# 1 "./include/uapi/linux/ptrace.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */


/* ptrace.h */
/* structs and defines to help the user use the ptrace system call. */

/* has the defines to get at the registers. */
# 27 "./include/uapi/linux/ptrace.h"
/* 0x4200-0x4300 are reserved for architecture-independent additions.  */





/*
 * Generic ptrace interface that exports the architecture specific regsets
 * using the corresponding NT_* types (which are also used in the core dump).
 * Please note that the NT_PRSTATUS note type in a core dump contains a full
 * 'struct elf_prstatus'. But the user_regset for NT_PRSTATUS contains just the
 * elf_gregset_t that is the pr_reg field of 'struct elf_prstatus'. For all the
 * other user_regset flavors, the user_regset layout and the ELF core dump note
 * payload are exactly the same layout.
 *
 * This interface usage is as follows:
 *	struct iovec iov = { buf, len};
 *
 *	ret = ptrace(PTRACE_GETREGSET/PTRACE_SETREGSET, pid, NT_XXX_TYPE, &iov);
 *
 * On the successful completion, iov.len will be updated by the kernel,
 * specifying how much the kernel has written/read to/from the user's iov.buf.
 */
# 59 "./include/uapi/linux/ptrace.h"
struct ptrace_peeksiginfo_args {
 __u64 off; /* from which siginfo to start */
 __u32 flags;
 __s32 nr; /* how may siginfos to take */
};







struct seccomp_metadata {
 __u64 filter_off; /* Input: which filter */
 __u64 flags; /* Output: filter's flags */
};







struct ptrace_syscall_info {
 __u8 op; /* PTRACE_SYSCALL_INFO_* */
 __u8 pad[3];
 __u32 arch;
 __u64 instruction_pointer;
 __u64 stack_pointer;
 union {
  struct {
   __u64 nr;
   __u64 args[6];
  } entry;
  struct {
   __s64 rval;
   __u8 is_error;
  } exit;
  struct {
   __u64 nr;
   __u64 args[6];
   __u32 ret_data;
  } seccomp;
 };
};



struct ptrace_rseq_configuration {
 __u64 rseq_abi_pointer;
 __u32 rseq_abi_size;
 __u32 signature;
 __u32 flags;
 __u32 pad;
};

/*
 * These values are stored in task->ptrace_message
 * by ptrace_stop to describe the current syscall-stop.
 */



/* Read signals from a shared (process wide) queue */


/* Wait extended result codes for the above trace options.  */







/* Extended result codes which enabled by means other than options.  */


/* Options set using PTRACE_SETOPTIONS or using PTRACE_SEIZE @data param */
# 146 "./include/uapi/linux/ptrace.h"
/* eventless options */
# 12 "./include/linux/ptrace.h" 2


/* Add sp to seccomp_data, as seccomp is user API, we don't want to modify it */
struct syscall_info {
 __u64 sp;
 struct seccomp_data data;
};

extern int ptrace_access_vm(struct task_struct *tsk, unsigned long addr,
       void *buf, int len, unsigned int gup_flags);

/*
 * Ptrace flags
 *
 * The owner ship rules for task->ptrace which holds the ptrace
 * flags is simple.  When a task is running it owns it's task->ptrace
 * flags.  When the a task is stopped the ptracer owns task->ptrace.
 */





/* PT_TRACE_* event enable flags */
# 49 "./include/linux/ptrace.h"
extern long arch_ptrace(struct task_struct *child, long request,
   unsigned long addr, unsigned long data);
extern int ptrace_readdata(struct task_struct *tsk, unsigned long src, char /* nothing */ *dst, int len);
extern int ptrace_writedata(struct task_struct *tsk, char /* nothing */ *src, unsigned long dst, int len);
extern void ptrace_disable(struct task_struct *);
extern int ptrace_request(struct task_struct *child, long request,
     unsigned long addr, unsigned long data);
extern int ptrace_notify(int exit_code, unsigned long message);
extern void __ptrace_link(struct task_struct *child,
     struct task_struct *new_parent,
     const struct cred *ptracer_cred);
extern void __ptrace_unlink(struct task_struct *child);
extern void exit_ptrace(struct task_struct *tracer, struct list_head *dead);






/* shorthands for READ/ATTACH and FSCREDS/REALCREDS combinations */





/**
 * ptrace_may_access - check whether the caller is permitted to access
 * a target task.
 * @task: target task
 * @mode: selects type of access and caller credentials
 *
 * Returns true on success, false on denial.
 *
 * One of the flags PTRACE_MODE_FSCREDS and PTRACE_MODE_REALCREDS must
 * be set in @mode to specify whether the access was requested through
 * a filesystem syscall (should use effective capabilities and fsuid
 * of the caller) or through an explicit syscall such as
 * process_vm_writev or ptrace (and should use the real credentials).
 */
extern bool ptrace_may_access(struct task_struct *task, unsigned int mode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ptrace_reparented(struct task_struct *child)
{
 return !same_thread_group(child->real_parent, child->parent);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrace_unlink(struct task_struct *child)
{
 if (__builtin_expect(!!(child->ptrace), 0))
  __ptrace_unlink(child);
}

int generic_ptrace_peekdata(struct task_struct *tsk, unsigned long addr,
       unsigned long data);
int generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,
       unsigned long data);

/**
 * ptrace_parent - return the task that is tracing the given task
 * @task: task to consider
 *
 * Returns %NULL if no one is tracing @task, or the &struct task_struct
 * pointer to its tracer.
 *
 * Must called under rcu_read_lock().  The pointer returned might be kept
 * live only by RCU.  During exec, this may be called with task_lock() held
 * on @task, still held from when check_unsafe_exec() was called.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct task_struct *ptrace_parent(struct task_struct *task)
{
 if (__builtin_expect(!!(task->ptrace), 0))
  return ({ /* Dependency order vs. p above. */ typeof(*(task->parent)) *__UNIQUE_ID_rcu375 = (typeof(*(task->parent)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_376(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((task->parent)) == sizeof(char) || sizeof((task->parent)) == sizeof(short) || sizeof((task->parent)) == sizeof(int) || sizeof((task->parent)) == sizeof(long)) || sizeof((task->parent)) == sizeof(long long))) __compiletime_assert_376(); } while (0); (*(const volatile typeof( _Generic(((task->parent)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((task->parent)))) *)&((task->parent))); }); do { } while (0 && (!((0) || rcu_read_lock_held()))); ; ((typeof(*(task->parent)) *)(__UNIQUE_ID_rcu375)); });
# 121 "./include/linux/ptrace.h"
 return ((void *)0);
}

/**
 * ptrace_event_enabled - test whether a ptrace event is enabled
 * @task: ptracee of interest
 * @event: %PTRACE_EVENT_* to test
 *
 * Test whether @event is enabled for ptracee @task.
 *
 * Returns %true if @event is enabled, %false otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool ptrace_event_enabled(struct task_struct *task, int event)
{
 return task->ptrace & (1 << (3 + (event)));
}

/**
 * ptrace_event - possibly stop for a ptrace event notification
 * @event:	%PTRACE_EVENT_* value to report
 * @message:	value for %PTRACE_GETEVENTMSG to return
 *
 * Check whether @event is enabled and, if so, report @event and @message
 * to the ptrace parent.
 *
 * Called without locks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrace_event(int event, unsigned long message)
{
 if (__builtin_expect(!!(ptrace_event_enabled(get_current(), event)), 0)) {
  ptrace_notify((event << 8) | 5, message);
 } else if (event == 4) {
  /* legacy EXEC report via SIGTRAP */
  if ((get_current()->ptrace & (0x00000001|0x00010000 /* SEIZE used, enable new behavior */)) == 0x00000001)
   send_sig(5, get_current(), 0);
 }
}

/**
 * ptrace_event_pid - possibly stop for a ptrace event notification
 * @event:	%PTRACE_EVENT_* value to report
 * @pid:	process identifier for %PTRACE_GETEVENTMSG to return
 *
 * Check whether @event is enabled and, if so, report @event and @pid
 * to the ptrace parent.  @pid is reported as the pid_t seen from the
 * ptrace parent's pid namespace.
 *
 * Called without locks.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrace_event_pid(int event, struct pid *pid)
{
 /*
	 * FIXME: There's a potential race if a ptracer in a different pid
	 * namespace than parent attaches between computing message below and
	 * when we acquire tasklist_lock in ptrace_stop().  If this happens,
	 * the ptracer will get a bogus pid from PTRACE_GETEVENTMSG.
	 */
 unsigned long message = 0;
 struct pid_namespace *ns;

 rcu_read_lock();
 ns = task_active_pid_ns(({ /* Dependency order vs. p above. */ typeof(*(get_current()->parent)) *__UNIQUE_ID_rcu377 = (typeof(*(get_current()->parent)) *)({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_378(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((get_current()->parent)) == sizeof(char) || sizeof((get_current()->parent)) == sizeof(short) || sizeof((get_current()->parent)) == sizeof(int) || sizeof((get_current()->parent)) == sizeof(long)) || sizeof((get_current()->parent)) == sizeof(long long))) __compiletime_assert_378(); } while (0); (*(const volatile typeof( _Generic(((get_current()->parent)), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((get_current()->parent)))) *)&((get_current()->parent))); }); do { } while (0 && (!((0) || rcu_read_lock_held()))); ; ((typeof(*(get_current()->parent)) *)(__UNIQUE_ID_rcu377)); }));
# 183 "./include/linux/ptrace.h"
 if (ns)
  message = pid_nr_ns(pid, ns);
 rcu_read_unlock();

 ptrace_event(event, message);
}

/**
 * ptrace_init_task - initialize ptrace state for a new child
 * @child:		new child task
 * @ptrace:		true if child should be ptrace'd by parent's tracer
 *
 * This is called immediately after adding @child to its parent's children
 * list.  @ptrace is false in the normal case, and true to ptrace @child.
 *
 * Called with current's siglock and write_lock_irq(&tasklist_lock) held.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrace_init_task(struct task_struct *child, bool ptrace)
{
 INIT_LIST_HEAD(&child->ptrace_entry);
 INIT_LIST_HEAD(&child->ptraced);
 child->jobctl = 0;
 child->ptrace = 0;
 child->parent = child->real_parent;

 if (__builtin_expect(!!(ptrace), 0) && get_current()->ptrace) {
  child->ptrace = get_current()->ptrace;
  __ptrace_link(child, get_current()->parent, get_current()->ptracer_cred);

  if (child->ptrace & 0x00010000 /* SEIZE used, enable new behavior */)
   task_set_jobctl_pending(child, (1UL << 19 /* trap for STOP */));
  else
   sigaddset(&child->pending.signal, 19);
 }
 else
  child->ptracer_cred = ((void *)0);
}

/**
 * ptrace_release_task - final ptrace-related cleanup of a zombie being reaped
 * @task:	task in %EXIT_DEAD state
 *
 * Called with write_lock(&tasklist_lock) held.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrace_release_task(struct task_struct *task)
{
 do { if (__builtin_expect(!!(!list_empty(&task->ptraced)), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/ptrace.h\"; .popsection; .long 14472b - .; .short 229; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 ptrace_unlink(task);
 do { if (__builtin_expect(!!(!list_empty(&task->ptrace_entry)), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/ptrace.h\"; .popsection; .long 14472b - .; .short 231; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
}


/*
 * System call handlers that, upon successful completion, need to return a
 * negative value should call force_successful_syscall_return() right before
 * returning.  On architectures where the syscall convention provides for a
 * separate error flag (e.g., alpha, ia64, ppc{,64}, sparc{,64}, possibly
 * others), this macro can be used to ensure that the error flag will not get
 * set.  On architectures which do not support a separate error flag, the macro
 * is a no-op and the spurious error condition needs to be filtered out by some
 * other means (e.g., in user-level, by passing an extra argument to the
 * syscall handler, or something along those lines).
 */




/*
 * On most systems we can tell if a syscall is a success based on if the retval
 * is an error value.  On some systems like ia64 and powerpc they have different
 * indicators of success/failure and must define their own.
 */



/*
 * <asm/ptrace.h> should define the following things inside #ifdef __KERNEL__.
 *
 * These do-nothing inlines are used when the arch does not
 * implement single-step.  The kerneldoc comments are here
 * to document the interface for all arch definitions.
 */
# 305 "./include/linux/ptrace.h"
extern void user_enable_single_step(struct task_struct *);
extern void user_disable_single_step(struct task_struct *);



/**
 * arch_has_block_step - does this CPU support user-mode block-step?
 *
 * If this is defined, then there must be a function declaration or inline
 * for user_enable_block_step(), and arch_has_single_step() must be defined
 * too.  arch_has_block_step() should evaluate to nonzero iff the machine
 * supports step-until-branch for user mode.  It can be a constant or it
 * can test a CPU feature bit.
 */


/**
 * user_enable_block_step - step until branch in user-mode task
 * @task: either current or a task stopped in %TASK_TRACED
 *
 * This can only be called when arch_has_block_step() has returned nonzero,
 * and will never be called when single-instruction stepping is being used.
 * Set @task so that when it returns to user mode, it will trap after the
 * next branch or trap taken.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void user_enable_block_step(struct task_struct *task)
{
 do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/ptrace.h\"; .popsection; .long 14472b - .; .short 332; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); /* This can never be called.  */
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void user_single_step_report(struct pt_regs *regs)
{
 kernel_siginfo_t info;
 clear_siginfo(&info);
 info.si_signo = 5;
 info.si_errno = 0;
 info.si_code = 0 /* sent by kill, sigsend, raise */;
 info._sifields._kill._pid = 0;
 info._sifields._kill._uid = 0;
 force_sig_info(&info);
}



/**
 * arch_ptrace_stop_needed - Decide whether arch_ptrace_stop() should be called
 *
 * This is called with the siglock held, to decide whether or not it's
 * necessary to release the siglock and call arch_ptrace_stop().  It can be
 * defined to a constant if arch_ptrace_stop() is never required, or always
 * is.  On machines where this makes sense, it should be defined to a quick
 * test to optimize out calling arch_ptrace_stop() when it would be
 * superfluous.  For example, if the thread has not been back to user mode
 * since the last stop, the thread state might indicate that nothing needs
 * to be done.
 *
 * This is guaranteed to be invoked once before a task stops for ptrace and
 * may include arch-specific operations necessary prior to a ptrace stop.
 */




/**
 * arch_ptrace_stop - Do machine-specific work before stopping for ptrace
 *
 * This is called with no locks held when arch_ptrace_stop_needed() has
 * just returned nonzero.  It is allowed to block, e.g. for user memory
 * access.  The arch can have machine-specific work to be done before
 * ptrace stops.  On ia64, register backing store gets written back to user
 * memory here.  Since this can be costly (requires dropping the siglock),
 * we only do it when the arch requires it for this particular stop, as
 * indicated by arch_ptrace_stop_needed().
 */
# 396 "./include/linux/ptrace.h"
extern int task_current_syscall(struct task_struct *target, struct syscall_info *info);

extern void sigaction_compat_abi(struct k_sigaction *act, struct k_sigaction *oact);

/*
 * ptrace report for syscall entry and exit looks identical.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ptrace_report_syscall(unsigned long message)
{
 int ptrace = get_current()->ptrace;
 int signr;

 if (!(ptrace & 0x00000001))
  return 0;

 signr = ptrace_notify(5 | ((ptrace & (1 << (3 + (0)))) ? 0x80 : 0),
         message);

 /*
	 * this isn't the same as continuing with a signal, but it will do
	 * for normal use.  strace only continues with a signal if the
	 * stopping signal is not SIGTRAP.  -brl
	 */
 if (signr)
  send_sig(signr, get_current(), 1);

 return fatal_signal_pending(get_current());
}

/**
 * ptrace_report_syscall_entry - task is about to attempt a system call
 * @regs:		user register state of current task
 *
 * This will be called if %SYSCALL_WORK_SYSCALL_TRACE or
 * %SYSCALL_WORK_SYSCALL_EMU have been set, when the current task has just
 * entered the kernel for a system call.  Full user register state is
 * available here.  Changing the values in @regs can affect the system
 * call number and arguments to be tried.  It is safe to block here,
 * preventing the system call from beginning.
 *
 * Returns zero normally, or nonzero if the calling arch code should abort
 * the system call.  That must prevent normal entry so no system call is
 * made.  If @task ever returns to user mode after this, its register state
 * is unspecified, but should be something harmless like an %ENOSYS error
 * return.  It should preserve enough information so that syscall_rollback()
 * can work (see asm-generic/syscall.h).
 *
 * Called without locks, just after entering kernel mode.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__warn_unused_result__)) int ptrace_report_syscall_entry(
 struct pt_regs *regs)
{
 return ptrace_report_syscall(1);
}

/**
 * ptrace_report_syscall_exit - task has just finished a system call
 * @regs:		user register state of current task
 * @step:		nonzero if simulating single-step or block-step
 *
 * This will be called if %SYSCALL_WORK_SYSCALL_TRACE has been set, when
 * the current task has just finished an attempted system call.  Full
 * user register state is available here.  It is safe to block here,
 * preventing signals from being processed.
 *
 * If @step is nonzero, this report is also in lieu of the normal
 * trap that would follow the system call instruction because
 * user_enable_block_step() or user_enable_single_step() was used.
 * In this case, %SYSCALL_WORK_SYSCALL_TRACE might not be set.
 *
 * Called without locks, just before checking for pending signals.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ptrace_report_syscall_exit(struct pt_regs *regs, int step)
{
 if (step)
  user_single_step_report(regs);
 else
  ptrace_report_syscall(2);
}
# 17 "./include/linux/ftrace.h" 2






# 1 "./arch/arm64/include/asm/ftrace.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * arch/arm64/include/asm/ftrace.h
 *
 * Copyright (C) 2013 Linaro Limited
 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
 */







/*
 * HAVE_FUNCTION_GRAPH_RET_ADDR_PTR means that the architecture can provide a
 * "return address pointer" which can be used to uniquely identify a return
 * address which has been overwritten.
 *
 * On arm64 we use the address of the caller's frame record, which remains the
 * same for the lifetime of the instrumented function, unlike the return
 * address in the LR.
 */
# 32 "./arch/arm64/include/asm/ftrace.h"
/* The BL at the callsite's adjusted rec->ip */





/*
 * Currently, gcc tends to save the link register after the local variables
 * on the stack. This causes the max stack tracer to report the function
 * frame sizes for the wrong functions. By defining
 * ARCH_FTRACE_SHIFT_STACK_TRACER, it will tell the stack tracer to expect
 * to find the return address on the stack after the local variables have
 * been set up.
 *
 * Note, this may change in the future, and we will need to deal with that
 * if it were to happen.
 */



# 1 "./include/linux/compat.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/*
 * These are the type definitions for the architecture specific
 * syscall compatibility layer.
 */







# 1 "./include/linux/socket.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./arch/arm64/include/generated/uapi/asm/socket.h" 1
# 1 "./include/uapi/asm-generic/socket.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */




# 1 "./arch/arm64/include/generated/uapi/asm/sockios.h" 1
# 1 "./include/uapi/asm-generic/sockios.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/* Socket-level I/O control calls. */
# 2 "./arch/arm64/include/generated/uapi/asm/sockios.h" 2
# 7 "./include/uapi/asm-generic/socket.h" 2

/* For setsockopt(2) */
# 37 "./include/uapi/asm-generic/socket.h"
/* Security levels - as per NRL IPv6 - don't actually do anything */






/* Socket filtering */
# 67 "./include/uapi/asm-generic/socket.h"
/* Instruct lower device to use last 4-bytes of skb data as FCS */
# 2 "./arch/arm64/include/generated/uapi/asm/socket.h" 2
# 7 "./include/linux/socket.h" 2
# 1 "./include/uapi/linux/sockios.h" 1
/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions of the socket-level I/O control calls.
 *
 * Version:	@(#)sockios.h	1.0.2	03/09/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */




# 1 "./arch/arm64/include/generated/uapi/asm/sockios.h" 1
# 24 "./include/uapi/linux/sockios.h" 2

/* Linux-specific socket ioctls */





/*
 * the timeval/timespec data structure layout is defined by libc,
 * so we need to cover both possible versions on 32-bit.
 */
/* Get stamp (timeval) */

/* Get stamp (timespec) */



/* on 64-bit and x32, avoid the ?: operator */
# 51 "./include/uapi/linux/sockios.h"
/* Routing table calls. */




/* Socket configuration controls. */
# 99 "./include/uapi/linux/sockios.h"
/* SIOCGIFDIVERT was:	0x8944		Frame diversion support */
/* SIOCSIFDIVERT was:	0x8945		Set frame diversion options */
# 113 "./include/uapi/linux/sockios.h"
/* ARP cache control calls. */
      /*  0x8950 - 0x8952  * obsolete calls, don't re-use */




/* RARP cache control calls. */




/* Driver configuration calls */




/* DLCI configuration calls */







/* bonding calls */
# 146 "./include/uapi/linux/sockios.h"
/* bridge calls */





/* hardware time stamping: parameters in linux/net_tstamp.h */



/* Device private ioctl calls */

/*
 *	These 16 ioctls are available to devices via the do_ioctl() device
 *	vector. Each device should include this file and redefine these names
 *	as their own. Because these are device dependent it is a good idea
 *	_NOT_ to issue them to random objects and hope.
 *
 *	THESE IOCTLS ARE _DEPRECATED_ AND WILL DISAPPEAR IN 2.5.X -DaveM
 */



/*
 *	These 16 ioctl calls are protocol private
 */
# 8 "./include/linux/socket.h" 2
# 1 "./include/linux/uio.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 *	Berkeley style UIO structures	-	Alan Cox 1994.
 */






# 1 "./include/uapi/linux/uio.h" 1
/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
/*
 *	Berkeley style UIO structures	-	Alan Cox 1994.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







struct iovec
{
 void /* nothing */ *iov_base; /* BSD uses caddr_t (1003.1g requires void *) */
 __kernel_size_t iov_len; /* Must be size_t (1003.1g) */
};

/*
 *	UIO_MAXIOV shall be at least 16 1003.1g (5.4.1.1)
 */
# 12 "./include/linux/uio.h" 2

struct page;
struct pipe_inode_info;

struct kvec {
 void *iov_base; /* and that should *never* hold a userland pointer */
 size_t iov_len;
};

enum iter_type {
 /* iter types */
 ITER_IOVEC,
 ITER_KVEC,
 ITER_BVEC,
 ITER_PIPE,
 ITER_XARRAY,
 ITER_DISCARD,
 ITER_UBUF,
};




struct iov_iter_state {
 size_t iov_offset;
 size_t count;
 unsigned long nr_segs;
};

struct iov_iter {
 u8 iter_type;
 bool nofault;
 bool data_source;
 bool user_backed;
 union {
  size_t iov_offset;
  int last_offset;
 };
 size_t count;
 union {
  const struct iovec *iov;
  const struct kvec *kvec;
  const struct bio_vec *bvec;
  struct xarray *xarray;
  struct pipe_inode_info *pipe;
  void /* nothing */ *ubuf;
 };
 union {
  unsigned long nr_segs;
  struct {
   unsigned int head;
   unsigned int start_head;
  };
  loff_t xarray_start;
 };
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum iter_type iov_iter_type(const struct iov_iter *i)
{
 return i->iter_type;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iov_iter_save_state(struct iov_iter *iter,
           struct iov_iter_state *state)
{
 state->iov_offset = iter->iov_offset;
 state->count = iter->count;
 state->nr_segs = iter->nr_segs;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iter_is_ubuf(const struct iov_iter *i)
{
 return iov_iter_type(i) == ITER_UBUF;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iter_is_iovec(const struct iov_iter *i)
{
 return iov_iter_type(i) == ITER_IOVEC;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iov_iter_is_kvec(const struct iov_iter *i)
{
 return iov_iter_type(i) == ITER_KVEC;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iov_iter_is_bvec(const struct iov_iter *i)
{
 return iov_iter_type(i) == ITER_BVEC;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iov_iter_is_pipe(const struct iov_iter *i)
{
 return iov_iter_type(i) == ITER_PIPE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iov_iter_is_discard(const struct iov_iter *i)
{
 return iov_iter_type(i) == ITER_DISCARD;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool iov_iter_is_xarray(const struct iov_iter *i)
{
 return iov_iter_type(i) == ITER_XARRAY;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned char iov_iter_rw(const struct iov_iter *i)
{
 return i->data_source ? 1 : 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool user_backed_iter(const struct iov_iter *i)
{
 return i->user_backed;
}

/*
 * Total number of bytes covered by an iovec.
 *
 * NOTE that it is not safe to use this function until all the iovec's
 * segment lengths have been validated.  Because the individual lengths can
 * overflow a size_t when added together.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t iov_length(const struct iovec *iov, unsigned long nr_segs)
{
 unsigned long seg;
 size_t ret = 0;

 for (seg = 0; seg < nr_segs; seg++)
  ret += iov[seg].iov_len;
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct iovec iov_iter_iovec(const struct iov_iter *iter)
{
 return (struct iovec) {
  .iov_base = iter->iov->iov_base + iter->iov_offset,
  .iov_len = __builtin_choose_expr(((!!(sizeof((typeof(iter->count) *)1 == (typeof(iter->iov->iov_len - iter->iov_offset) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(iter->count) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(iter->iov->iov_len - iter->iov_offset) * 0l)) : (int *)8))))), ((iter->count) < (iter->iov->iov_len - iter->iov_offset) ? (iter->count) : (iter->iov->iov_len - iter->iov_offset)), ({ typeof(iter->count) __UNIQUE_ID___x379 = (iter->count); typeof(iter->iov->iov_len - iter->iov_offset) __UNIQUE_ID___y380 = (iter->iov->iov_len - iter->iov_offset); ((__UNIQUE_ID___x379) < (__UNIQUE_ID___y380) ? (__UNIQUE_ID___x379) : (__UNIQUE_ID___y380)); })),

 };
}

size_t copy_page_from_iter_atomic(struct page *page, unsigned offset,
      size_t bytes, struct iov_iter *i);
void iov_iter_advance(struct iov_iter *i, size_t bytes);
void iov_iter_revert(struct iov_iter *i, size_t bytes);
size_t fault_in_iov_iter_readable(const struct iov_iter *i, size_t bytes);
size_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t bytes);
size_t iov_iter_single_seg_count(const struct iov_iter *i);
size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
    struct iov_iter *i);
size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
    struct iov_iter *i);

size_t _copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i);
size_t _copy_from_iter(void *addr, size_t bytes, struct iov_iter *i);
size_t _copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t copy_folio_to_iter(struct folio *folio, size_t offset,
  size_t bytes, struct iov_iter *i)
{
 return copy_page_to_iter(&folio->page, offset, bytes, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__))
size_t copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)
{
 if (check_copy_size(addr, bytes, true))
  return _copy_to_iter(addr, bytes, i);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__))
size_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)
{
 if (check_copy_size(addr, bytes, false))
  return _copy_from_iter(addr, bytes, i);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__))
bool copy_from_iter_full(void *addr, size_t bytes, struct iov_iter *i)
{
 size_t copied = copy_from_iter(addr, bytes, i);
 if (__builtin_expect(!!(copied == bytes), 1))
  return true;
 iov_iter_revert(i, copied);
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__))
size_t copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i)
{
 if (check_copy_size(addr, bytes, false))
  return _copy_from_iter_nocache(addr, bytes, i);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__))
bool copy_from_iter_full_nocache(void *addr, size_t bytes, struct iov_iter *i)
{
 size_t copied = copy_from_iter_nocache(addr, bytes, i);
 if (__builtin_expect(!!(copied == bytes), 1))
  return true;
 iov_iter_revert(i, copied);
 return false;
}
# 237 "./include/linux/uio.h"
size_t iov_iter_zero(size_t bytes, struct iov_iter *);
bool iov_iter_is_aligned(const struct iov_iter *i, unsigned addr_mask,
   unsigned len_mask);
unsigned long iov_iter_alignment(const struct iov_iter *i);
unsigned long iov_iter_gap_alignment(const struct iov_iter *i);
void iov_iter_init(struct iov_iter *i, unsigned int direction, const struct iovec *iov,
   unsigned long nr_segs, size_t count);
void iov_iter_kvec(struct iov_iter *i, unsigned int direction, const struct kvec *kvec,
   unsigned long nr_segs, size_t count);
void iov_iter_bvec(struct iov_iter *i, unsigned int direction, const struct bio_vec *bvec,
   unsigned long nr_segs, size_t count);
void iov_iter_pipe(struct iov_iter *i, unsigned int direction, struct pipe_inode_info *pipe,
   size_t count);
void iov_iter_discard(struct iov_iter *i, unsigned int direction, size_t count);
void iov_iter_xarray(struct iov_iter *i, unsigned int direction, struct xarray *xarray,
       loff_t start, size_t count);
ssize_t iov_iter_get_pages(struct iov_iter *i, struct page **pages,
  size_t maxsize, unsigned maxpages, size_t *start,
  unsigned gup_flags);
ssize_t iov_iter_get_pages2(struct iov_iter *i, struct page **pages,
   size_t maxsize, unsigned maxpages, size_t *start);
ssize_t iov_iter_get_pages_alloc(struct iov_iter *i,
  struct page ***pages, size_t maxsize, size_t *start,
  unsigned gup_flags);
ssize_t iov_iter_get_pages_alloc2(struct iov_iter *i, struct page ***pages,
   size_t maxsize, size_t *start);
int iov_iter_npages(const struct iov_iter *i, int maxpages);
void iov_iter_restore(struct iov_iter *i, struct iov_iter_state *state);

const void *dup_iter(struct iov_iter *new, struct iov_iter *old, gfp_t flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t iov_iter_count(const struct iov_iter *i)
{
 return i->count;
}

/*
 * Cap the iov_iter by given limit; note that the second argument is
 * *not* the new size - it's upper limit for such.  Passing it a value
 * greater than the amount of data in iov_iter is fine - it'll just do
 * nothing in that case.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iov_iter_truncate(struct iov_iter *i, u64 count)
{
 /*
	 * count doesn't have to fit in size_t - comparison extends both
	 * operands to u64 here and any value that would be truncated by
	 * conversion in assignement is by definition greater than all
	 * values of size_t, including old i->count.
	 */
 if (i->count > count)
  i->count = count;
}

/*
 * reexpand a previously truncated iterator; count must be no more than how much
 * we had shrunk it.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iov_iter_reexpand(struct iov_iter *i, size_t count)
{
 i->count = count;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
iov_iter_npages_cap(struct iov_iter *i, int maxpages, size_t max_bytes)
{
 size_t shorted = 0;
 int npages;

 if (iov_iter_count(i) > max_bytes) {
  shorted = iov_iter_count(i) - max_bytes;
  iov_iter_truncate(i, max_bytes);
 }
 npages = iov_iter_npages(i, maxpages);
 if (shorted)
  iov_iter_reexpand(i, iov_iter_count(i) + shorted);

 return npages;
}

struct csum_state {
 __wsum csum;
 size_t off;
};

size_t csum_and_copy_to_iter(const void *addr, size_t bytes, void *csstate, struct iov_iter *i);
size_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum, struct iov_iter *i);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__))
bool csum_and_copy_from_iter_full(void *addr, size_t bytes,
      __wsum *csum, struct iov_iter *i)
{
 size_t copied = csum_and_copy_from_iter(addr, bytes, csum, i);
 if (__builtin_expect(!!(copied == bytes), 1))
  return true;
 iov_iter_revert(i, copied);
 return false;
}
size_t hash_and_copy_to_iter(const void *addr, size_t bytes, void *hashp,
  struct iov_iter *i);

struct iovec *iovec_from_user(const struct iovec /* nothing */ *uvector,
  unsigned long nr_segs, unsigned long fast_segs,
  struct iovec *fast_iov, bool compat);
ssize_t import_iovec(int type, const struct iovec /* nothing */ *uvec,
   unsigned nr_segs, unsigned fast_segs, struct iovec **iovp,
   struct iov_iter *i);
ssize_t __import_iovec(int type, const struct iovec /* nothing */ *uvec,
   unsigned nr_segs, unsigned fast_segs, struct iovec **iovp,
   struct iov_iter *i, bool compat);
int import_single_range(int type, void /* nothing */ *buf, size_t len,
   struct iovec *iov, struct iov_iter *i);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void iov_iter_ubuf(struct iov_iter *i, unsigned int direction,
   void /* nothing */ *buf, size_t count)
{
 ({ int __ret_warn_on = !!(direction & ~(0 | 1)); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/uio.h\"; .popsection; .long 14472b - .; .short 353; .short (1 << 0)|(((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 *i = (struct iov_iter) {
  .iter_type = ITER_UBUF,
  .user_backed = true,
  .data_source = direction,
  .ubuf = buf,
  .count = count
 };
}
# 9 "./include/linux/socket.h" 2


# 1 "./include/uapi/linux/socket.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */



/*
 * Desired design of maximum size and alignment (see RFC2553)
 */


typedef unsigned short __kernel_sa_family_t;

/*
 * The definition uses anonymous union and struct in order to control the
 * default alignment.
 */
struct __kernel_sockaddr_storage {
 union {
  struct {
   __kernel_sa_family_t ss_family; /* address family */
   /* Following field(s) are implementation specific */
   char __data[128 /* Implementation specific max size */ - sizeof(unsigned short)];
    /* space to achieve desired size, */
    /* _SS_MAXSIZE value minus size of ss_family */
  };
  void *__align; /* implementation specific desired alignment */
 };
};
# 12 "./include/linux/socket.h" 2

struct file;
struct pid;
struct cred;
struct socket;
struct sock;
struct sk_buff;





struct seq_file;
extern void socket_seq_show(struct seq_file *seq);


typedef __kernel_sa_family_t sa_family_t;

/*
 *	1003.1g requires sa_family_t and that sa_data is char.
 */

struct sockaddr {
 sa_family_t sa_family; /* address family, AF_xxx	*/
 union {
  char sa_data_min[14]; /* Minimum 14 bytes of protocol address	*/
  struct { struct { } __empty_sa_data; char sa_data[]; };
 };
};

struct linger {
 int l_onoff; /* Linger active		*/
 int l_linger; /* How long to linger for	*/
};



/*
 *	As we do 4.4BSD message passing we use a 4.4BSD message passing
 *	system, not 4.3. Thus msg_accrights(len) are now missing. They
 *	belong in an obscure libc emulation or the bin.
 */

struct msghdr {
 void *msg_name; /* ptr to socket address structure */
 int msg_namelen; /* size of socket address structure */

 int msg_inq; /* output, data left in socket */

 struct iov_iter msg_iter; /* data */

 /*
	 * Ancillary data. msg_control_user is the user buffer used for the
	 * recv* side when msg_control_is_user is set, msg_control is the kernel
	 * buffer used for all other cases.
	 */
 union {
  void *msg_control;
  void /* nothing */ *msg_control_user;
 };
 bool msg_control_is_user : 1;
 bool msg_get_inq : 1;/* return INQ after receive */
 unsigned int msg_flags; /* flags on received message */
 __kernel_size_t msg_controllen; /* ancillary data buffer length */
 struct kiocb *msg_iocb; /* ptr to iocb for async requests */
 struct ubuf_info *msg_ubuf;
 int (*sg_from_iter)(struct sock *sk, struct sk_buff *skb,
       struct iov_iter *from, size_t length);
};

struct user_msghdr {
 void /* nothing */ *msg_name; /* ptr to socket address structure */
 int msg_namelen; /* size of socket address structure */
 struct iovec /* nothing */ *msg_iov; /* scatter/gather array */
 __kernel_size_t msg_iovlen; /* # elements in msg_iov */
 void /* nothing */ *msg_control; /* ancillary data */
 __kernel_size_t msg_controllen; /* ancillary data buffer length */
 unsigned int msg_flags; /* flags on received message */
};

/* For recvmmsg/sendmmsg */
struct mmsghdr {
 struct user_msghdr msg_hdr;
 unsigned int msg_len;
};

/*
 *	POSIX 1003.1g - ancillary data object information
 *	Ancillary data consists of a sequence of pairs of
 *	(cmsghdr, cmsg_data[])
 */

struct cmsghdr {
 __kernel_size_t cmsg_len; /* data byte count, including hdr */
        int cmsg_level; /* originating protocol */
        int cmsg_type; /* protocol-specific type */
};

/*
 *	Ancillary data object information MACROS
 *	Table 5-14 of POSIX 1003.1g
 */
# 140 "./include/linux/socket.h"
/*
 *	Get the next cmsg header
 *
 *	PLEASE, do not touch this function. If you think, that it is
 *	incorrect, grep kernel sources and think about consequences
 *	before trying to improve it.
 *
 *	Now it always returns valid, not truncated ancillary object
 *	HEADER. But caller still MUST check, that cmsg->cmsg_len is
 *	inside range, given by msg->msg_controllen before using
 *	ancillary object DATA.				--ANK (980731)
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct cmsghdr * __cmsg_nxthdr(void *__ctl, __kernel_size_t __size,
            struct cmsghdr *__cmsg)
{
 struct cmsghdr * __ptr;

 __ptr = (struct cmsghdr*)(((unsigned char *) __cmsg) + ( ((__cmsg->cmsg_len)+sizeof(long)-1) & ~(sizeof(long)-1) ));
 if ((unsigned long)((char*)(__ptr+1) - (char *) __ctl) > __size)
  return (struct cmsghdr *)0;

 return __ptr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct cmsghdr * cmsg_nxthdr (struct msghdr *__msg, struct cmsghdr *__cmsg)
{
 return __cmsg_nxthdr(__msg->msg_control, __msg->msg_controllen, __cmsg);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t msg_data_left(struct msghdr *msg)
{
 return iov_iter_count(&msg->msg_iter);
}

/* "Socket"-level control message types: */





struct ucred {
 __u32 pid;
 __u32 uid;
 __u32 gid;
};

/* Supported address families. */
# 244 "./include/linux/socket.h"
/* Protocol families, same as address families. */
# 295 "./include/linux/socket.h"
/* Maximum queue length specifiable by listen.  */


/* Flags we can use with send/ and recv.
   Added those for 1003.1g not all are supported yet
 */
# 341 "./include/linux/socket.h"
/* Setsockoptions(2) level. Thanks to BSD these must match IPPROTO_xxx */

/* #define SOL_ICMP	1	No-no-no! Due to Linux :-) we cannot use SOL_ICMP=1 */
# 383 "./include/linux/socket.h"
/* IPX options */


extern int move_addr_to_kernel(void /* nothing */ *uaddr, int ulen, struct __kernel_sockaddr_storage *kaddr);
extern int put_cmsg(struct msghdr*, int level, int type, int len, void *data);

struct timespec64;
struct __kernel_timespec;
struct old_timespec32;

struct scm_timestamping_internal {
 struct timespec64 ts[3];
};

extern void put_cmsg_scm_timestamping64(struct msghdr *msg, struct scm_timestamping_internal *tss);
extern void put_cmsg_scm_timestamping(struct msghdr *msg, struct scm_timestamping_internal *tss);

/* The __sys_...msg variants allow MSG_CMSG_COMPAT iff
 * forbid_cmsg_compat==false
 */
extern long __sys_recvmsg(int fd, struct user_msghdr /* nothing */ *msg,
     unsigned int flags, bool forbid_cmsg_compat);
extern long __sys_sendmsg(int fd, struct user_msghdr /* nothing */ *msg,
     unsigned int flags, bool forbid_cmsg_compat);
extern int __sys_recvmmsg(int fd, struct mmsghdr /* nothing */ *mmsg,
     unsigned int vlen, unsigned int flags,
     struct __kernel_timespec /* nothing */ *timeout,
     struct old_timespec32 /* nothing */ *timeout32);
extern int __sys_sendmmsg(int fd, struct mmsghdr /* nothing */ *mmsg,
     unsigned int vlen, unsigned int flags,
     bool forbid_cmsg_compat);
extern long __sys_sendmsg_sock(struct socket *sock, struct msghdr *msg,
          unsigned int flags);
extern long __sys_recvmsg_sock(struct socket *sock, struct msghdr *msg,
          struct user_msghdr /* nothing */ *umsg,
          struct sockaddr /* nothing */ *uaddr,
          unsigned int flags);
extern int sendmsg_copy_msghdr(struct msghdr *msg,
          struct user_msghdr /* nothing */ *umsg, unsigned flags,
          struct iovec **iov);
extern int recvmsg_copy_msghdr(struct msghdr *msg,
          struct user_msghdr /* nothing */ *umsg, unsigned flags,
          struct sockaddr /* nothing */ **uaddr,
          struct iovec **iov);
extern int __copy_msghdr(struct msghdr *kmsg,
    struct user_msghdr *umsg,
    struct sockaddr /* nothing */ **save_addr);

/* helpers which do the actual work for syscalls */
extern int __sys_recvfrom(int fd, void /* nothing */ *ubuf, size_t size,
     unsigned int flags, struct sockaddr /* nothing */ *addr,
     int /* nothing */ *addr_len);
extern int __sys_sendto(int fd, void /* nothing */ *buff, size_t len,
   unsigned int flags, struct sockaddr /* nothing */ *addr,
   int addr_len);
extern struct file *do_accept(struct file *file, unsigned file_flags,
         struct sockaddr /* nothing */ *upeer_sockaddr,
         int /* nothing */ *upeer_addrlen, int flags);
extern int __sys_accept4(int fd, struct sockaddr /* nothing */ *upeer_sockaddr,
    int /* nothing */ *upeer_addrlen, int flags);
extern int __sys_socket(int family, int type, int protocol);
extern struct file *__sys_socket_file(int family, int type, int protocol);
extern int __sys_bind(int fd, struct sockaddr /* nothing */ *umyaddr, int addrlen);
extern int __sys_connect_file(struct file *file, struct __kernel_sockaddr_storage *addr,
         int addrlen, int file_flags);
extern int __sys_connect(int fd, struct sockaddr /* nothing */ *uservaddr,
    int addrlen);
extern int __sys_listen(int fd, int backlog);
extern int __sys_getsockname(int fd, struct sockaddr /* nothing */ *usockaddr,
        int /* nothing */ *usockaddr_len);
extern int __sys_getpeername(int fd, struct sockaddr /* nothing */ *usockaddr,
        int /* nothing */ *usockaddr_len);
extern int __sys_socketpair(int family, int type, int protocol,
       int /* nothing */ *usockvec);
extern int __sys_shutdown_sock(struct socket *sock, int how);
extern int __sys_shutdown(int fd, int how);
# 16 "./include/linux/compat.h" 2
# 1 "./include/uapi/linux/if.h" 1
/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the INET interface module.
 *
 * Version:	@(#)if.h	1.0.2	04/18/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1982-1988
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */



# 1 "./include/uapi/linux/libc-compat.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Compatibility interface for userspace libc header coordination:
 *
 * Define compatibility macros that are used to control the inclusion or
 * exclusion of UAPI structures and definitions in coordination with another
 * userspace C library.
 *
 * This header is intended to solve the problem of UAPI definitions that
 * conflict with userspace definitions. If a UAPI header has such conflicting
 * definitions then the solution is as follows:
 *
 * * Synchronize the UAPI header and the libc headers so either one can be
 *   used and such that the ABI is preserved. If this is not possible then
 *   no simple compatibility interface exists (you need to write translating
 *   wrappers and rename things) and you can't use this interface.
 *
 * Then follow this process:
 *
 * (a) Include libc-compat.h in the UAPI header.
 *      e.g. #include <linux/libc-compat.h>
 *     This include must be as early as possible.
 *
 * (b) In libc-compat.h add enough code to detect that the comflicting
 *     userspace libc header has been included first.
 *
 * (c) If the userspace libc header has been included first define a set of
 *     guard macros of the form __UAPI_DEF_FOO and set their values to 1, else
 *     set their values to 0.
 *
 * (d) Back in the UAPI header with the conflicting definitions, guard the
 *     definitions with:
 *     #if __UAPI_DEF_FOO
 *       ...
 *     #endif
 *
 * This fixes the situation where the linux headers are included *after* the
 * libc headers. To fix the problem with the inclusion in the other order the
 * userspace libc headers must be fixed like this:
 *
 * * For all definitions that conflict with kernel definitions wrap those
 *   defines in the following:
 *   #if !__UAPI_DEF_FOO
 *     ...
 *   #endif
 *
 * This prevents the redefinition of a construct already defined by the kernel.
 */



/* We have included glibc headers... */
# 175 "./include/uapi/linux/libc-compat.h"
/* Definitions for if.h */
# 188 "./include/uapi/linux/libc-compat.h"
/* Everything up to IFF_DYNAMIC, matches net/if.h until glibc 2.23 */



/* For the future if glibc adds IFF_LOWER_UP, IFF_DORMANT and IFF_ECHO */




/* Definitions for in.h */
# 217 "./include/uapi/linux/libc-compat.h"
/* Definitions for in6.h */
# 243 "./include/uapi/linux/libc-compat.h"
/* Definitions for ipx.h */
# 260 "./include/uapi/linux/libc-compat.h"
/* Definitions for xattr.h */
# 24 "./include/uapi/linux/if.h" 2
# 37 "./include/uapi/linux/if.h"
# 1 "./include/uapi/linux/hdlc/ioctl.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
# 40 "./include/uapi/linux/hdlc/ioctl.h"
typedef struct {
 unsigned int clock_rate; /* bits per second */
 unsigned int clock_type; /* internal, external, TX-internal etc. */
 unsigned short loopback;
} sync_serial_settings; /* V.35, V.24, X.21 */

typedef struct {
 unsigned int clock_rate; /* bits per second */
 unsigned int clock_type; /* internal, external, TX-internal etc. */
 unsigned short loopback;
 unsigned int slot_map;
} te1_settings; /* T1, E1 */

typedef struct {
 unsigned short encoding;
 unsigned short parity;
} raw_hdlc_proto;

typedef struct {
 unsigned int t391;
 unsigned int t392;
 unsigned int n391;
 unsigned int n392;
 unsigned int n393;
 unsigned short lmi;
 unsigned short dce; /* 1 for DCE (network side) operation */
} fr_proto;

typedef struct {
 unsigned int dlci;
} fr_proto_pvc; /* for creating/deleting FR PVCs */

typedef struct {
 unsigned int dlci;
 char master[16]; /* Name of master FRAD device */
}fr_proto_pvc_info; /* for returning PVC information only */

typedef struct {
    unsigned int interval;
    unsigned int timeout;
} cisco_proto;

typedef struct {
 unsigned short dce; /* 1 for DCE (network side) operation */
 unsigned int modulo; /* modulo (8 = basic / 128 = extended) */
 unsigned int window; /* frame window size */
 unsigned int t1; /* timeout t1 */
 unsigned int t2; /* timeout t2 */
 unsigned int n2; /* frame retry counter */
} x25_hdlc_proto;

/* PPP doesn't need any info now - supply length = 0 to ioctl */
# 38 "./include/uapi/linux/if.h" 2

/* For glibc compatibility. An empty enum does not compile. */


/**
 * enum net_device_flags - &struct net_device flags
 *
 * These are the &struct net_device flags, they can be set by drivers, the
 * kernel and some can be triggered by userspace. Userspace can query and
 * set these flags using userspace utilities but there is also a sysfs
 * entry available for all dev flags which can be queried and set. These flags
 * are shared for all types of net_devices. The sysfs entries are available
 * via /sys/class/net/<dev>/flags. Flags which can be toggled through sysfs
 * are annotated below, note that only a few flags can be toggled and some
 * other flags are always preserved from the original net_device flags
 * even if you try to set them via sysfs. Flags which are always preserved
 * are kept under the flag grouping @IFF_VOLATILE. Flags which are volatile
 * are annotated below as such.
 *
 * You should have a pretty good reason to be extending these flags.
 *
 * @IFF_UP: interface is up. Can be toggled through sysfs.
 * @IFF_BROADCAST: broadcast address valid. Volatile.
 * @IFF_DEBUG: turn on debugging. Can be toggled through sysfs.
 * @IFF_LOOPBACK: is a loopback net. Volatile.
 * @IFF_POINTOPOINT: interface is has p-p link. Volatile.
 * @IFF_NOTRAILERS: avoid use of trailers. Can be toggled through sysfs.
 *	Volatile.
 * @IFF_RUNNING: interface RFC2863 OPER_UP. Volatile.
 * @IFF_NOARP: no ARP protocol. Can be toggled through sysfs. Volatile.
 * @IFF_PROMISC: receive all packets. Can be toggled through sysfs.
 * @IFF_ALLMULTI: receive all multicast packets. Can be toggled through
 *	sysfs.
 * @IFF_MASTER: master of a load balancer. Volatile.
 * @IFF_SLAVE: slave of a load balancer. Volatile.
 * @IFF_MULTICAST: Supports multicast. Can be toggled through sysfs.
 * @IFF_PORTSEL: can set media type. Can be toggled through sysfs.
 * @IFF_AUTOMEDIA: auto media select active. Can be toggled through sysfs.
 * @IFF_DYNAMIC: dialup device with changing addresses. Can be toggled
 *	through sysfs.
 * @IFF_LOWER_UP: driver signals L1 up. Volatile.
 * @IFF_DORMANT: driver signals dormant. Volatile.
 * @IFF_ECHO: echo sent packets. Volatile.
 */
enum net_device_flags {
/* for compatibility with glibc net/if.h */

 IFF_UP = 1<<0, /* sysfs */
 IFF_BROADCAST = 1<<1, /* volatile */
 IFF_DEBUG = 1<<2, /* sysfs */
 IFF_LOOPBACK = 1<<3, /* volatile */
 IFF_POINTOPOINT = 1<<4, /* volatile */
 IFF_NOTRAILERS = 1<<5, /* sysfs */
 IFF_RUNNING = 1<<6, /* volatile */
 IFF_NOARP = 1<<7, /* sysfs */
 IFF_PROMISC = 1<<8, /* sysfs */
 IFF_ALLMULTI = 1<<9, /* sysfs */
 IFF_MASTER = 1<<10, /* volatile */
 IFF_SLAVE = 1<<11, /* volatile */
 IFF_MULTICAST = 1<<12, /* sysfs */
 IFF_PORTSEL = 1<<13, /* sysfs */
 IFF_AUTOMEDIA = 1<<14, /* sysfs */
 IFF_DYNAMIC = 1<<15, /* sysfs */


 IFF_LOWER_UP = 1<<16, /* volatile */
 IFF_DORMANT = 1<<17, /* volatile */
 IFF_ECHO = 1<<18, /* volatile */

};


/* for compatibility with glibc net/if.h */
# 142 "./include/uapi/linux/if.h"
/* For definitions see hdlc.h */
# 151 "./include/uapi/linux/if.h"
/* For definitions see hdlc.h */
# 166 "./include/uapi/linux/if.h"
/* RFC 2863 operational status */
enum {
 IF_OPER_UNKNOWN,
 IF_OPER_NOTPRESENT,
 IF_OPER_DOWN,
 IF_OPER_LOWERLAYERDOWN,
 IF_OPER_TESTING,
 IF_OPER_DORMANT,
 IF_OPER_UP,
};

/* link modes */
enum {
 IF_LINK_MODE_DEFAULT,
 IF_LINK_MODE_DORMANT, /* limit upward transition to dormant */
 IF_LINK_MODE_TESTING, /* limit upward transition to testing */
};

/*
 *	Device mapping structure. I'd just gone off and designed a
 *	beautiful scheme using only loadable modules with arguments
 *	for driver options and along come the PCMCIA people 8)
 *
 *	Ah well. The get() side of this is good for WDSETUP, and it'll
 *	be handy for debugging things. The set side is fine for now and
 *	being very small might be worth keeping for clean configuration.
 */

/* for compatibility with glibc net/if.h */

struct ifmap {
 unsigned long mem_start;
 unsigned long mem_end;
 unsigned short base_addr;
 unsigned char irq;
 unsigned char dma;
 unsigned char port;
 /* 3 bytes spare */
};


struct if_settings {
 unsigned int type; /* Type of physical device or protocol */
 unsigned int size; /* Size of the data allocated by the caller */
 union {
  /* {atm/eth/dsl}_settings anyone ? */
  raw_hdlc_proto /* nothing */ *raw_hdlc;
  cisco_proto /* nothing */ *cisco;
  fr_proto /* nothing */ *fr;
  fr_proto_pvc /* nothing */ *fr_pvc;
  fr_proto_pvc_info /* nothing */ *fr_pvc_info;
  x25_hdlc_proto /* nothing */ *x25;

  /* interface settings */
  sync_serial_settings /* nothing */ *sync;
  te1_settings /* nothing */ *te1;
 } ifs_ifsu;
};

/*
 * Interface request structure used for socket
 * ioctl's.  All interface ioctl's must have parameter
 * definitions which begin with ifr_name.  The
 * remainder may be interface specific.
 */

/* for compatibility with glibc net/if.h */

struct ifreq {

 union
 {
  char ifrn_name[16]; /* if name, e.g. "en0" */
 } ifr_ifrn;

 union {
  struct sockaddr ifru_addr;
  struct sockaddr ifru_dstaddr;
  struct sockaddr ifru_broadaddr;
  struct sockaddr ifru_netmask;
  struct sockaddr ifru_hwaddr;
  short ifru_flags;
  int ifru_ivalue;
  int ifru_mtu;
  struct ifmap ifru_map;
  char ifru_slave[16]; /* Just fits the size */
  char ifru_newname[16];
  void /* nothing */ * ifru_data;
  struct if_settings ifru_settings;
 } ifr_ifru;
};
# 277 "./include/uapi/linux/if.h"
/*
 * Structure used in SIOCGIFCONF request.
 * Used to retrieve interface configuration
 * for machine (useful for programs which
 * must know all networks accessible).
 */

/* for compatibility with glibc net/if.h */

struct ifconf {
 int ifc_len; /* size of buffer	*/
 union {
  char /* nothing */ *ifcu_buf;
  struct ifreq /* nothing */ *ifcu_req;
 } ifc_ifcu;
};
# 17 "./include/linux/compat.h" 2

# 1 "./include/uapi/linux/aio_abi.h" 1
/* include/linux/aio_abi.h
 *
 * Copyright 2000,2001,2002 Red Hat.
 *
 * Written by Benjamin LaHaise <bcrl@kvack.org>
 *
 * Distribute under the terms of the GPLv2 (see ../../COPYING) or under
 * the following terms.
 *
 * Permission to use, copy, modify, and distribute this software and its
 * documentation is hereby granted, provided that the above copyright
 * notice appears in all copies.  This software is provided without any
 * warranty, express or implied.  Red Hat makes no representations about
 * the suitability of this software for any purpose.
 *
 * IN NO EVENT SHALL RED HAT BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,
 * SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF
 * THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF RED HAT HAS BEEN ADVISED
 * OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * RED HAT DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS ON AN "AS IS" BASIS, AND
 * RED HAT HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES,
 * ENHANCEMENTS, OR MODIFICATIONS.
 */







typedef __kernel_ulong_t aio_context_t;

enum {
 IOCB_CMD_PREAD = 0,
 IOCB_CMD_PWRITE = 1,
 IOCB_CMD_FSYNC = 2,
 IOCB_CMD_FDSYNC = 3,
 /* 4 was the experimental IOCB_CMD_PREADX */
 IOCB_CMD_POLL = 5,
 IOCB_CMD_NOOP = 6,
 IOCB_CMD_PREADV = 7,
 IOCB_CMD_PWRITEV = 8,
};

/*
 * Valid flags for the "aio_flags" member of the "struct iocb".
 *
 * IOCB_FLAG_RESFD - Set if the "aio_resfd" member of the "struct iocb"
 *                   is valid.
 * IOCB_FLAG_IOPRIO - Set if the "aio_reqprio" member of the "struct iocb"
 *                    is valid.
 */



/* read() from /dev/aio returns these structures. */
struct io_event {
 __u64 data; /* the data field from the iocb */
 __u64 obj; /* what iocb this event came from */
 __s64 res; /* result code for this event */
 __s64 res2; /* secondary result */
};

/*
 * we always use a 64bit off_t when communicating
 * with userland.  its up to libraries to do the
 * proper padding and aio_error abstraction
 */

struct iocb {
 /* these are internal to the kernel/libc. */
 __u64 aio_data; /* data to be returned in event's data */


 __u32 aio_key; /* the kernel sets aio_key to the req # */
 __kernel_rwf_t aio_rw_flags; /* RWF_* flags */







 /* common fields */
 __u16 aio_lio_opcode; /* see IOCB_CMD_ above */
 __s16 aio_reqprio;
 __u32 aio_fildes;

 __u64 aio_buf;
 __u64 aio_nbytes;
 __s64 aio_offset;

 /* extra parameters */
 __u64 aio_reserved2; /* TODO: use this for a (struct sigevent *) */

 /* flags for the "struct iocb" */
 __u32 aio_flags;

 /*
	 * if the IOCB_FLAG_RESFD flag of "aio_flags" is set, this is an
	 * eventfd to signal AIO readiness to
	 */
 __u32 aio_resfd;
}; /* 64 bytes */
# 19 "./include/linux/compat.h" 2




# 1 "./arch/arm64/include/generated/uapi/asm/siginfo.h" 1
# 24 "./include/linux/compat.h" 2



/*
 * It may be useful for an architecture to override the definitions of the
 * COMPAT_SYSCALL_DEFINE0 and COMPAT_SYSCALL_DEFINEx() macros, in particular
 * to use a different calling convention for syscalls. To allow for that,
 + the prototypes for the compat_sys_*() functions below will *not* be included
 * if CONFIG_ARCH_HAS_SYSCALL_WRAPPER is enabled.
 */
# 1 "./arch/arm64/include/asm/syscall_wrapper.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * syscall_wrapper.h - arm64 specific wrappers to syscall definitions
 *
 * Based on arch/x86/include/asm_syscall_wrapper.h
 */
# 35 "./include/linux/compat.h" 2
# 65 "./include/linux/compat.h"
/*
 * The asmlinkage stub is aliased to a function named __se_compat_sys_*() which
 * sign-extends 32-bit ints to longs whenever needed. The actual work is
 * done within __do_compat_sys_*().
 */
# 90 "./include/linux/compat.h"
struct compat_iovec {
 compat_uptr_t iov_base;
 compat_size_t iov_len;
};





typedef struct compat_sigaltstack {
 compat_uptr_t ss_sp;
 int ss_flags;
 compat_size_t ss_size;
} compat_stack_t;
# 112 "./include/linux/compat.h"
typedef __compat_uid32_t compat_uid_t;
typedef __compat_gid32_t compat_gid_t;

struct compat_sel_arg_struct;
struct rusage;

struct old_itimerval32;

struct compat_tms {
 compat_clock_t tms_utime;
 compat_clock_t tms_stime;
 compat_clock_t tms_cutime;
 compat_clock_t tms_cstime;
};



typedef struct {
 compat_sigset_word sig[(64 / 32)];
} compat_sigset_t;

int set_compat_user_sigmask(const compat_sigset_t /* nothing */ *umask,
       size_t sigsetsize);

struct compat_sigaction {

 compat_uptr_t sa_handler;
 compat_ulong_t sa_flags;





 compat_uptr_t sa_restorer;

 compat_sigset_t sa_mask __attribute__((__packed__));
};

typedef union compat_sigval {
 compat_int_t sival_int;
 compat_uptr_t sival_ptr;
} compat_sigval_t;

typedef struct compat_siginfo {
 int si_signo;

 int si_errno;
 int si_code;





 union {
  int _pad[128/sizeof(int) - 3];

  /* kill() */
  struct {
   compat_pid_t _pid; /* sender's pid */
   __compat_uid32_t _uid; /* sender's uid */
  } _kill;

  /* POSIX.1b timers */
  struct {
   compat_timer_t _tid; /* timer id */
   int _overrun; /* overrun count */
   compat_sigval_t _sigval; /* same as below */
  } _timer;

  /* POSIX.1b signals */
  struct {
   compat_pid_t _pid; /* sender's pid */
   __compat_uid32_t _uid; /* sender's uid */
   compat_sigval_t _sigval;
  } _rt;

  /* SIGCHLD */
  struct {
   compat_pid_t _pid; /* which child */
   __compat_uid32_t _uid; /* sender's uid */
   int _status; /* exit code */
   compat_clock_t _utime;
   compat_clock_t _stime;
  } _sigchld;
# 208 "./include/linux/compat.h"
  /* SIGILL, SIGFPE, SIGSEGV, SIGBUS, SIGTRAP, SIGEMT */
  struct {
   compat_uptr_t _addr; /* faulting insn/memory ref. */


   union {
    /* used on alpha and sparc */
    int _trapno; /* TRAP # which caused the signal */
    /*
				 * used when si_code=BUS_MCEERR_AR or
				 * used when si_code=BUS_MCEERR_AO
				 */
    short int _addr_lsb; /* Valid LSB of the reported address. */
    /* used when si_code=SEGV_BNDERR */
    struct {
     char _dummy_bnd[(__alignof__(compat_uptr_t) < sizeof(short) ? sizeof(short) : __alignof__(compat_uptr_t))];
     compat_uptr_t _lower;
     compat_uptr_t _upper;
    } _addr_bnd;
    /* used when si_code=SEGV_PKUERR */
    struct {
     char _dummy_pkey[(__alignof__(compat_uptr_t) < sizeof(short) ? sizeof(short) : __alignof__(compat_uptr_t))];
     u32 _pkey;
    } _addr_pkey;
    /* used when si_code=TRAP_PERF */
    struct {
     compat_ulong_t _data;
     u32 _type;
     u32 _flags;
    } _perf;
   };
  } _sigfault;

  /* SIGPOLL */
  struct {
   compat_long_t _band; /* POLL_IN, POLL_OUT, POLL_MSG */
   int _fd;
  } _sigpoll;

  struct {
   compat_uptr_t _call_addr; /* calling user insn */
   int _syscall; /* triggering system call number */
   unsigned int _arch; /* AUDIT_ARCH_* of syscall */
  } _sigsys;
 } _sifields;
} compat_siginfo_t;

struct compat_rlimit {
 compat_ulong_t rlim_cur;
 compat_ulong_t rlim_max;
};







struct compat_flock {
 short l_type;
 short l_whence;
 compat_off_t l_start;
 compat_off_t l_len;



 compat_pid_t l_pid;



};

struct compat_flock64 {
 short l_type;
 short l_whence;
 compat_loff_t l_start;
 compat_loff_t l_len;
 compat_pid_t l_pid;



} ;

struct compat_rusage {
 struct old_timeval32 ru_utime;
 struct old_timeval32 ru_stime;
 compat_long_t ru_maxrss;
 compat_long_t ru_ixrss;
 compat_long_t ru_idrss;
 compat_long_t ru_isrss;
 compat_long_t ru_minflt;
 compat_long_t ru_majflt;
 compat_long_t ru_nswap;
 compat_long_t ru_inblock;
 compat_long_t ru_oublock;
 compat_long_t ru_msgsnd;
 compat_long_t ru_msgrcv;
 compat_long_t ru_nsignals;
 compat_long_t ru_nvcsw;
 compat_long_t ru_nivcsw;
};

extern int put_compat_rusage(const struct rusage *,
        struct compat_rusage /* nothing */ *);

struct compat_siginfo;
struct __compat_aio_sigset;

struct compat_dirent {
 u32 d_ino;
 compat_off_t d_off;
 u16 d_reclen;
 char d_name[256];
};

struct compat_ustat {
 compat_daddr_t f_tfree;
 compat_ino_t f_tinode;
 char f_fname[6];
 char f_fpack[6];
};



typedef struct compat_sigevent {
 compat_sigval_t sigev_value;
 compat_int_t sigev_signo;
 compat_int_t sigev_notify;
 union {
  compat_int_t _pad[((64/sizeof(int)) - 3)];
  compat_int_t _tid;

  struct {
   compat_uptr_t _function;
   compat_uptr_t _attribute;
  } _sigev_thread;
 } _sigev_un;
} compat_sigevent_t;

struct compat_ifmap {
 compat_ulong_t mem_start;
 compat_ulong_t mem_end;
 unsigned short base_addr;
 unsigned char irq;
 unsigned char dma;
 unsigned char port;
};

struct compat_if_settings {
 unsigned int type; /* Type of physical device or protocol */
 unsigned int size; /* Size of the data allocated by the caller */
 compat_uptr_t ifs_ifsu; /* union of pointers */
};

struct compat_ifreq {
 union {
  char ifrn_name[16]; /* if name, e.g. "en0" */
 } ifr_ifrn;
 union {
  struct sockaddr ifru_addr;
  struct sockaddr ifru_dstaddr;
  struct sockaddr ifru_broadaddr;
  struct sockaddr ifru_netmask;
  struct sockaddr ifru_hwaddr;
  short ifru_flags;
  compat_int_t ifru_ivalue;
  compat_int_t ifru_mtu;
  struct compat_ifmap ifru_map;
  char ifru_slave[16]; /* Just fits the size */
  char ifru_newname[16];
  compat_caddr_t ifru_data;
  struct compat_if_settings ifru_settings;
 } ifr_ifru;
};

struct compat_ifconf {
 compat_int_t ifc_len; /* size of buffer */
 compat_caddr_t ifcbuf;
};

struct compat_robust_list {
 compat_uptr_t next;
};

struct compat_robust_list_head {
 struct compat_robust_list list;
 compat_long_t futex_offset;
 compat_uptr_t list_op_pending;
};


struct compat_old_sigaction {
 compat_uptr_t sa_handler;
 compat_old_sigset_t sa_mask;
 compat_ulong_t sa_flags;
 compat_uptr_t sa_restorer;
};


struct compat_keyctl_kdf_params {
 compat_uptr_t hashname;
 compat_uptr_t otherinfo;
 __u32 otherinfolen;
 __u32 __spare[8];
};

struct compat_stat;
struct compat_statfs;
struct compat_statfs64;
struct compat_old_linux_dirent;
struct compat_linux_dirent;
struct linux_dirent64;
struct compat_msghdr;
struct compat_mmsghdr;
struct compat_sysinfo;
struct compat_sysctl_args;
struct compat_kexec_segment;
struct compat_mq_attr;
struct compat_msgbuf;

void copy_siginfo_to_external32(struct compat_siginfo *to,
  const struct kernel_siginfo *from);
int copy_siginfo_from_user32(kernel_siginfo_t *to,
  const struct compat_siginfo /* nothing */ *from);
int __copy_siginfo_to_user32(struct compat_siginfo /* nothing */ *to,
  const kernel_siginfo_t *from);



int get_compat_sigevent(struct sigevent *event,
  const struct compat_sigevent /* nothing */ *u_event);

extern int get_compat_sigset(sigset_t *set, const compat_sigset_t /* nothing */ *compat);

/*
 * Defined inline such that size can be compile time constant, which avoids
 * CONFIG_HARDENED_USERCOPY complaining about copies from task_struct
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
put_compat_sigset(compat_sigset_t /* nothing */ *compat, const sigset_t *set,
    unsigned int size)
{
 /* size <= sizeof(compat_sigset_t) <= sizeof(sigset_t) */
# 464 "./include/linux/compat.h"
 return copy_to_user(compat, set, size) ? -14 /* Bad address */ : 0;

}
# 535 "./include/linux/compat.h"
extern int compat_ptrace_request(struct task_struct *child,
     compat_long_t request,
     compat_ulong_t addr, compat_ulong_t data);

extern long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
          compat_ulong_t addr, compat_ulong_t data);

struct epoll_event; /* fortunately, this one is fixed-layout */

int compat_restore_altstack(const compat_stack_t /* nothing */ *uss);
int __compat_save_altstack(compat_stack_t /* nothing */ *, unsigned long);
# 555 "./include/linux/compat.h"
/*
 * These syscall function prototypes are kept in the same order as
 * include/uapi/asm-generic/unistd.h. Deprecated or obsolete system calls
 * go below.
 *
 * Please note that these prototypes here are only provided for information
 * purposes, for static analysis, and for linking from the syscall table.
 * These functions should not be called elsewhere from kernel code.
 *
 * As the syscall calling convention may be different from the default
 * for architectures overriding the syscall calling convention, do not
 * include the prototypes if CONFIG_ARCH_HAS_SYSCALL_WRAPPER is enabled.
 */
# 967 "./include/linux/compat.h"
/**
 * ns_to_old_timeval32 - Compat version of ns_to_timeval
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the old_timeval32 representation of the nsec parameter.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct old_timeval32 ns_to_old_timeval32(s64 nsec)
{
 struct __kernel_old_timeval tv;
 struct old_timeval32 ctv;

 tv = ns_to_kernel_old_timeval(nsec);
 ctv.tv_sec = tv.tv_sec;
 ctv.tv_usec = tv.tv_usec;

 return ctv;
}

/*
 * Kernel code should not call compat syscalls (i.e., compat_sys_xyzyyz())
 * directly.  Instead, use one of the functions which work equivalently, such
 * as the kcompat_sys_xyzyyz() functions prototyped below.
 */

int kcompat_sys_statfs64(const char /* nothing */ * pathname, compat_size_t sz,
       struct compat_statfs64 /* nothing */ * buf);
int kcompat_sys_fstatfs64(unsigned int fd, compat_size_t sz,
     struct compat_statfs64 /* nothing */ * buf);



/*
 * For most but not all architectures, "am I in a compat syscall?" and
 * "am I a compat task?" are the same question.  For architectures on which
 * they aren't the same question, arch code can override in_compat_syscall.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool in_compat_syscall(void) { return is_compat_task(); }
# 1020 "./include/linux/compat.h"
long compat_get_bitmap(unsigned long *mask, const compat_ulong_t /* nothing */ *umask,
         unsigned long bitmap_size);
long compat_put_bitmap(compat_ulong_t /* nothing */ *umask, unsigned long *mask,
         unsigned long bitmap_size);

/*
 * Some legacy ABIs like the i386 one use less than natural alignment for 64-bit
 * types, and will need special compat treatment for that.  Most architectures
 * don't need that special handling even for compat syscalls.
 */




/*
 * A pointer passed in from user mode. This should not
 * be used for syscall parameters, just declare them
 * as pointers because the syscall entry code will have
 * appropriately converted them already.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void /* nothing */ *compat_ptr(compat_uptr_t uptr)
{
 return (void /* nothing */ *)(unsigned long)uptr;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) compat_uptr_t ptr_to_compat(void /* nothing */ *uptr)
{
 return (u32)(unsigned long)uptr;
}
# 53 "./arch/arm64/include/asm/ftrace.h" 2

extern void _mcount(unsigned long);
extern void *return_address(unsigned int);

struct dyn_arch_ftrace {
 /* No extra data needed for arm64 */
};

extern unsigned long ftrace_graph_call;

extern void return_to_handler(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long ftrace_call_adjust(unsigned long addr)
{
 /*
	 * Adjust addr to point at the BL in the callsite.
	 * See ftrace_init_nop() for the callsite sequence.
	 */
 if (0)
  return addr + 4;
 /*
	 * addr is the address of the mcount call instruction.
	 * recordmcount does the necessary offset calculation.
	 */
 return addr;
}
# 156 "./arch/arm64/include/asm/ftrace.h"
/*
 * Because AArch32 mode does not share the same syscall table with AArch64,
 * tracing compat syscalls may result in reporting bogus syscalls or even
 * hang-up, so just do not trace them.
 * See kernel/trace/trace_syscalls.c
 *
 * x86 code says:
 * If the user really wants these, then they should use the
 * raw syscall tracepoints with filtering.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_trace_is_compat_syscall(struct pt_regs *regs)
{
 return is_compat_task();
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool arch_syscall_match_sym_name(const char *sym,
            const char *name)
{
 /*
	 * Since all syscall functions have __arm64_ prefix, we must skip it.
	 * However, as we described above, we decided to ignore compat
	 * syscalls, so we don't care about __arm64_compat_ prefix here.
	 */
 return !strcmp(sym + 8, name);
}
# 24 "./include/linux/ftrace.h" 2

/*
 * If the arch supports passing the variable contents of
 * function_trace_op as the third parameter back from the
 * mcount call, then the arch should define this as 1.
 */







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_boot_snapshot(void) { }


struct ftrace_ops;
struct ftrace_regs;
# 62 "./include/linux/ftrace.h"
/* Main tracing buffer and events set up */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void trace_init(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void early_trace_init(void) { }


struct module;
struct ftrace_hash;
struct ftrace_direct_func;







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *
ftrace_mod_address_lookup(unsigned long addr, unsigned long *size,
     unsigned long *off, char **modname, char *sym)
{
 return ((void *)0);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ftrace_mod_get_kallsym(unsigned int symnum, unsigned long *value,
      char *type, char *name,
      char *module_name, int *exported)
{
 return -1;
}
# 373 "./include/linux/ftrace.h"
/*
 * (un)register_ftrace_function must be a macro since the ops parameter
 * must not be evaluated.
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_kill(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_free_init_mem(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_free_mem(struct module *mod, void *start, void *end) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ftrace_lookup_symbols(const char **sorted_syms, size_t cnt, unsigned long *addrs)
{
 return -95 /* Operation not supported on transport endpoint */;
}


struct ftrace_func_entry {
 struct hlist_node hlist;
 unsigned long ip;
 unsigned long direct; /* for direct lookup only */
};

struct dyn_ftrace;
# 413 "./include/linux/ftrace.h"
struct ftrace_ops;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_ftrace_direct(unsigned long ip, unsigned long addr)
{
 return -524 /* Operation is not supported */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int unregister_ftrace_direct(unsigned long ip, unsigned long addr)
{
 return -524 /* Operation is not supported */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int modify_ftrace_direct(unsigned long ip,
           unsigned long old_addr, unsigned long new_addr)
{
 return -524 /* Operation is not supported */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct ftrace_direct_func *ftrace_find_direct_func(unsigned long addr)
{
 return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ftrace_modify_direct_caller(struct ftrace_func_entry *entry,
           struct dyn_ftrace *rec,
           unsigned long old_addr,
           unsigned long new_addr)
{
 return -19 /* No such device */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long ftrace_find_rec_direct(unsigned long ip)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_ftrace_direct_multi(struct ftrace_ops *ops, unsigned long addr)
{
 return -19 /* No such device */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int unregister_ftrace_direct_multi(struct ftrace_ops *ops, unsigned long addr)
{
 return -19 /* No such device */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int modify_ftrace_direct_multi(struct ftrace_ops *ops, unsigned long addr)
{
 return -19 /* No such device */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int modify_ftrace_direct_multi_nolock(struct ftrace_ops *ops, unsigned long addr)
{
 return -19 /* No such device */;
}

/*
 * This must be implemented by the architecture.
 * It is the way the ftrace direct_ops helper, when called
 * via ftrace (because there's other callbacks besides the
 * direct call), can inform the architecture's trampoline that this
 * routine has a direct caller, and what the caller is.
 *
 * For example, in x86, it returns the direct caller
 * callback function via the regs->orig_ax parameter.
 * Then in the ftrace trampoline, if this is set, it makes
 * the return from the trampoline jump to the direct caller
 * instead of going back to the function it just traced.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_ftrace_set_direct_caller(struct ftrace_regs *fregs,
       unsigned long addr) { }
# 519 "./include/linux/ftrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void stack_tracer_disable(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void stack_tracer_enable(void) { }
# 867 "./include/linux/ftrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int skip_trace(unsigned long ip) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_disable_daemon(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_enable_daemon(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_module_init(struct module *mod) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_module_enable(struct module *mod) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_release_mod(struct module *mod) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int ftrace_text_reserved(const void *start, const void *end)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long ftrace_location(unsigned long ip)
{
 return 0;
}

/*
 * Again users of functions that have ftrace_ops may not
 * have them defined when ftrace is not enabled, but these
 * functions may still be called. Use a macro instead of inline.
 */
# 896 "./include/linux/ftrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t ftrace_filter_write(struct file *file, const char /* nothing */ *ubuf,
       size_t cnt, loff_t *ppos) { return -19 /* No such device */; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) ssize_t ftrace_notrace_write(struct file *file, const char /* nothing */ *ubuf,
        size_t cnt, loff_t *ppos) { return -19 /* No such device */; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
ftrace_regex_release(struct inode *inode, struct file *file) { return -19 /* No such device */; }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_ftrace_trampoline(unsigned long addr)
{
 return false;
}
# 918 "./include/linux/ftrace.h"
/* totally disable ftrace - can not re-enable after this */
void ftrace_kill(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void tracer_disable(void)
{



}

/*
 * Ftrace disable/restore without lock. Some synchronization mechanism
 * must be used to prevent ftrace_enabled to be changed between
 * disable/restore.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __ftrace_enabled_save(void)
{





 return 0;

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __ftrace_enabled_restore(int enabled)
{



}

/* All archs should have this, but we define it for consistency */




/* Archs may use other ways for ADDR1 and beyond */
# 973 "./include/linux/ftrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long get_lock_parent_ip(void)
{
 unsigned long addr = ((unsigned long)(void *)((((unsigned long)__builtin_return_address(0) & ((((1ULL))) << (55))) ? ((unsigned long)__builtin_return_address(0) | ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (63)) * 0l)) : (int *)8))), (((u64)(48))) > (63), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (63)))))) : ((unsigned long)__builtin_return_address(0) & ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((((u64)(48))) > (54)) * 0l)) : (int *)8))), (((u64)(48))) > (54), 0))); })))) + (((~(((0ULL)))) - ((((1ULL))) << (((u64)(48)))) + 1) & (~(((0ULL))) >> (64 - 1 - (54)))))))));

 if (!in_lock_functions(addr))
  return addr;
 addr = ((unsigned long)return_address(1));
 if (!in_lock_functions(addr))
  return addr;
 return ((unsigned long)return_address(2));
}





/*
 * Use defines instead of static inlines because some arches will make code out
 * of the CALLER_ADDR, when we really want these to be a real nop.
 */
# 1005 "./include/linux/ftrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_init(void) { }


/*
 * Structure that defines an entry function trace.
 * It's already packed but the attribute "packed" is needed
 * to remove extra padding at the end.
 */
struct ftrace_graph_ent {
 unsigned long func; /* Current function */
 int depth;
} __attribute__((__packed__));

/*
 * Structure that defines a return function trace.
 * It's already packed but the attribute "packed" is needed
 * to remove extra padding at the end.
 */
struct ftrace_graph_ret {
 unsigned long func; /* Current function */
 int depth;
 /* Number of functions that overran the depth limit for current task */
 unsigned int overrun;
 unsigned long long calltime;
 unsigned long long rettime;
} __attribute__((__packed__));

/* Type of the callback handlers for tracing function graph*/
typedef void (*trace_func_graph_ret_t)(struct ftrace_graph_ret *); /* return */
typedef int (*trace_func_graph_ent_t)(struct ftrace_graph_ent *); /* entry */

extern int ftrace_graph_entry_stub(struct ftrace_graph_ent *trace);
# 1132 "./include/linux/ftrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_graph_init_task(struct task_struct *t) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_graph_exit_task(struct task_struct *t) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void ftrace_graph_init_idle_task(struct task_struct *t, int cpu) { }

/* Define as macros as fgraph_ops may not be defined */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long
ftrace_graph_ret_addr(struct task_struct *task, int *idx, unsigned long ret,
        unsigned long *retp)
{
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pause_graph_tracing(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unpause_graph_tracing(void) { }
# 1164 "./include/linux/ftrace.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void disable_trace_on_warning(void) { }
# 29 "./include/linux/kprobes.h" 2

# 1 "./include/linux/freelist.h" 1
/* SPDX-License-Identifier: GPL-2.0-only OR BSD-2-Clause */





/*
 * Copyright: cameron@moodycamel.com
 *
 * A simple CAS-based lock-free free list. Not the fastest thing in the world
 * under heavy contention, but simple and correct (assuming nodes are never
 * freed until after the free list is destroyed), and fairly speedy under low
 * contention.
 *
 * Adapted from: https://moodycamel.com/blog/2014/solving-the-aba-problem-for-lock-free-free-lists
 */

struct freelist_node {
 atomic_t refs;
 struct freelist_node *next;
};

struct freelist_head {
 struct freelist_node *head;
};




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __freelist_add(struct freelist_node *node, struct freelist_head *list)
{
 /*
	 * Since the refcount is zero, and nobody can increase it once it's
	 * zero (except us, and we run only one copy of this method per node at
	 * a time, i.e. the single thread case), then we know we can safely
	 * change the next pointer of the node; however, once the refcount is
	 * back above zero, then other threads could increase it (happens under
	 * heavy contention, when the refcount goes to zero in between a load
	 * and a refcount increment of a node in try_get, then back up to
	 * something non-zero, then the refcount increment is done by the other
	 * thread) -- so if the CAS to add the node to the actual list fails,
	 * decrese the refcount and leave the add operation to the next thread
	 * who puts the refcount back to zero (which could be us, hence the
	 * loop).
	 */
 struct freelist_node *head = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_381(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(list->head) == sizeof(char) || sizeof(list->head) == sizeof(short) || sizeof(list->head) == sizeof(int) || sizeof(list->head) == sizeof(long)) || sizeof(list->head) == sizeof(long long))) __compiletime_assert_381(); } while (0); (*(const volatile typeof( _Generic((list->head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (list->head))) *)&(list->head)); });
# 48 "./include/linux/freelist.h"
 for (;;) {
  do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_382(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(node->next) == sizeof(char) || sizeof(node->next) == sizeof(short) || sizeof(node->next) == sizeof(int) || sizeof(node->next) == sizeof(long)) || sizeof(node->next) == sizeof(long long))) __compiletime_assert_382(); } while (0); do { *(volatile typeof(node->next) *)&(node->next) = (head); } while (0); } while (0);
# 50 "./include/linux/freelist.h"
  atomic_set_release(&node->refs, 1);

  if (!({ typeof(&list->head) __ai_ptr = (&list->head); typeof(&head) __ai_oldp = (&head); do { } while (0); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); instrument_atomic_write(__ai_oldp, sizeof(*__ai_oldp)); ({ typeof(*(__ai_ptr)) *___op = (__ai_oldp), ___o = *___op, ___r; ___r = ({ __typeof__(*((__ai_ptr))) __ret; __ret = (__typeof__(*((__ai_ptr)))) __cmpxchg_rel(((__ai_ptr)), (unsigned long)(___o), (unsigned long)((node)), sizeof(*((__ai_ptr)))); __ret; }); if (__builtin_expect(!!(___r != ___o), 0)) *___op = ___r; __builtin_expect(!!(___r == ___o), 1); }); })) {
   /*
			 * Hmm, the add failed, but we can only try again when
			 * the refcount goes back to zero.
			 */
   if (atomic_fetch_add_release(0x80000000 - 1, &node->refs) == 1)
    continue;
  }
  return;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void freelist_add(struct freelist_node *node, struct freelist_head *list)
{
 /*
	 * We know that the should-be-on-freelist bit is 0 at this point, so
	 * it's safe to set it using a fetch_add.
	 */
 if (!atomic_fetch_add_release(0x80000000, &node->refs)) {
  /*
		 * Oh look! We were the last ones referencing this node, and we
		 * know we want to add it to the free list, so let's do it!
		 */
  __freelist_add(node, list);
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct freelist_node *freelist_try_get(struct freelist_head *list)
{
 struct freelist_node *prev, *next, *head = ({ union { typeof( _Generic((*&list->head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&list->head))) __val; char __c[1]; } __u; typeof(&list->head) __p = (&list->head); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_383(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&list->head) == sizeof(char) || sizeof(*&list->head) == sizeof(short) || sizeof(*&list->head) == sizeof(int) || sizeof(*&list->head) == sizeof(long)))) __compiletime_assert_383(); } while (0); kasan_check_read(__p, sizeof(*&list->head)); switch (sizeof(*&list->head)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&list->head))__u.__val; });
# 82 "./include/linux/freelist.h"
 unsigned int refs;

 while (head) {
  prev = head;
  refs = atomic_read(&head->refs);
  if ((refs & 0x7FFFFFFF) == 0 ||
      !atomic_try_cmpxchg_acquire(&head->refs, &refs, refs+1)) {
   head = ({ union { typeof( _Generic((*&list->head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&list->head))) __val; char __c[1]; } __u; typeof(&list->head) __p = (&list->head); do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_384(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&list->head) == sizeof(char) || sizeof(*&list->head) == sizeof(short) || sizeof(*&list->head) == sizeof(int) || sizeof(*&list->head) == sizeof(long)))) __compiletime_assert_384(); } while (0); kasan_check_read(__p, sizeof(*&list->head)); switch (sizeof(*&list->head)) { case 1: asm volatile ("ldarb %w0, %1" : "=r" (*(__u8 *)__u.__c) : "Q" (*__p) : "memory"); break; case 2: asm volatile ("ldarh %w0, %1" : "=r" (*(__u16 *)__u.__c) : "Q" (*__p) : "memory"); break; case 4: asm volatile ("ldar %w0, %1" : "=r" (*(__u32 *)__u.__c) : "Q" (*__p) : "memory"); break; case 8: asm volatile ("ldar %0, %1" : "=r" (*(__u64 *)__u.__c) : "Q" (*__p) : "memory"); break; } (typeof(*&list->head))__u.__val; });
# 90 "./include/linux/freelist.h"
   continue;
  }

  /*
		 * Good, reference count has been incremented (it wasn't at
		 * zero), which means we can read the next and not worry about
		 * it changing between now and the time we do the CAS.
		 */
  next = ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_385(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head->next) == sizeof(char) || sizeof(head->next) == sizeof(short) || sizeof(head->next) == sizeof(int) || sizeof(head->next) == sizeof(long)) || sizeof(head->next) == sizeof(long long))) __compiletime_assert_385(); } while (0); (*(const volatile typeof( _Generic((head->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head->next))) *)&(head->next)); });
# 99 "./include/linux/freelist.h"
  if (({ typeof(&list->head) __ai_ptr = (&list->head); typeof(&head) __ai_oldp = (&head); instrument_atomic_write(__ai_ptr, sizeof(*__ai_ptr)); instrument_atomic_write(__ai_oldp, sizeof(*__ai_oldp)); ({ typeof(*(__ai_ptr)) *___op = (__ai_oldp), ___o = *___op, ___r; ___r = ({ __typeof__(*((__ai_ptr))) __ret; __ret = (__typeof__(*((__ai_ptr)))) __cmpxchg_acq(((__ai_ptr)), (unsigned long)(___o), (unsigned long)((next)), sizeof(*((__ai_ptr)))); __ret; }); if (__builtin_expect(!!(___r != ___o), 0)) *___op = ___r; __builtin_expect(!!(___r == ___o), 1); }); })) {
   /*
			 * Yay, got the node. This means it was on the list,
			 * which means should-be-on-freelist must be false no
			 * matter the refcount (because nobody else knows it's
			 * been taken off yet, it can't have been put back on).
			 */
   ({ int __ret_warn_on = !!(atomic_read(&head->refs) & 0x80000000); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/freelist.h\"; .popsection; .long 14472b - .; .short 106; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });

   /*
			 * Decrease refcount twice, once for our ref, and once
			 * for the list's ref.
			 */
   atomic_fetch_add(-2, &head->refs);

   return head;
  }

  /*
		 * OK, the head must have changed on us, but we still need to decrement
		 * the refcount we increased.
		 */
  refs = atomic_fetch_add(-1, &prev->refs);
  if (refs == 0x80000000 + 1)
   __freelist_add(prev, list);
 }

 return ((void *)0);
}
# 31 "./include/linux/kprobes.h" 2
# 1 "./include/linux/rethook.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Return hooking with list-based shadow stack.
 */
# 15 "./include/linux/rethook.h"
struct rethook_node;

typedef void (*rethook_handler_t) (struct rethook_node *, void *, struct pt_regs *);

/**
 * struct rethook - The rethook management data structure.
 * @data: The user-defined data storage.
 * @handler: The user-defined return hook handler.
 * @pool: The pool of struct rethook_node.
 * @ref: The reference counter.
 * @rcu: The rcu_head for deferred freeing.
 *
 * Don't embed to another data structure, because this is a self-destructive
 * data structure when all rethook_node are freed.
 */
struct rethook {
 void *data;
 rethook_handler_t handler;
 struct freelist_head pool;
 refcount_t ref;
 struct callback_head rcu;
};

/**
 * struct rethook_node - The rethook shadow-stack entry node.
 * @freelist: The freelist, linked to struct rethook::pool.
 * @rcu: The rcu_head for deferred freeing.
 * @llist: The llist, linked to a struct task_struct::rethooks.
 * @rethook: The pointer to the struct rethook.
 * @ret_addr: The storage for the real return address.
 * @frame: The storage for the frame pointer.
 *
 * You can embed this to your extended data structure to store any data
 * on each entry of the shadow stack.
 */
struct rethook_node {
 union {
  struct freelist_node freelist;
  struct callback_head rcu;
 };
 struct llist_node llist;
 struct rethook *rethook;
 unsigned long ret_addr;
 unsigned long frame;
};

struct rethook *rethook_alloc(void *data, rethook_handler_t handler);
void rethook_free(struct rethook *rh);
void rethook_add_node(struct rethook *rh, struct rethook_node *node);
struct rethook_node *rethook_try_get(struct rethook *rh);
void rethook_recycle(struct rethook_node *node);
void rethook_hook(struct rethook_node *node, struct pt_regs *regs, bool mcount);
unsigned long rethook_find_ret_addr(struct task_struct *tsk, unsigned long frame,
        struct llist_node **cur);

/* Arch dependent code must implement arch_* and trampoline code */
void arch_rethook_prepare(struct rethook_node *node, struct pt_regs *regs, bool mcount);
void arch_rethook_trampoline(void);

/**
 * is_rethook_trampoline() - Check whether the address is rethook trampoline
 * @addr: The address to be checked
 *
 * Return true if the @addr is the rethook trampoline address.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_rethook_trampoline(unsigned long addr)
{
 return addr == (unsigned long)dereference_symbol_descriptor(arch_rethook_trampoline);
}

/* If the architecture needs to fixup the return address, implement it. */
void arch_rethook_fixup_return(struct pt_regs *regs,
          unsigned long correct_ret_addr);

/* Generic trampoline handler, arch code must prepare asm stub */
unsigned long rethook_trampoline_handler(struct pt_regs *regs,
      unsigned long frame);
# 32 "./include/linux/kprobes.h" 2
# 1 "./arch/arm64/include/asm/kprobes.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * arch/arm64/include/asm/kprobes.h
 *
 * Copyright (C) 2013 Linaro Limited
 */




# 1 "./include/asm-generic/kprobes.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 12 "./arch/arm64/include/asm/kprobes.h" 2
# 33 "./include/linux/kprobes.h" 2
# 44 "./include/linux/kprobes.h"
typedef int kprobe_opcode_t;
struct arch_specific_insn {
 int dummy;
};


struct kprobe;
struct pt_regs;
struct kretprobe;
struct kretprobe_instance;
typedef int (*kprobe_pre_handler_t) (struct kprobe *, struct pt_regs *);
typedef void (*kprobe_post_handler_t) (struct kprobe *, struct pt_regs *,
           unsigned long flags);
typedef int (*kretprobe_handler_t) (struct kretprobe_instance *,
        struct pt_regs *);

struct kprobe {
 struct hlist_node hlist;

 /* list of kprobes for multi-handler support */
 struct list_head list;

 /*count the number of times this probe was temporarily disarmed */
 unsigned long nmissed;

 /* location of the probe point */
 kprobe_opcode_t *addr;

 /* Allow user to indicate symbol name of the probe point */
 const char *symbol_name;

 /* Offset into the symbol */
 unsigned int offset;

 /* Called before addr is executed. */
 kprobe_pre_handler_t pre_handler;

 /* Called after addr is executed, unless... */
 kprobe_post_handler_t post_handler;

 /* Saved opcode (which has been replaced with breakpoint) */
 kprobe_opcode_t opcode;

 /* copy of the original instruction */
 struct arch_specific_insn ainsn;

 /*
	 * Indicates various status flags.
	 * Protected by kprobe_mutex after this kprobe is registered.
	 */
 u32 flags;
};

/* Kprobe status flags */
# 108 "./include/linux/kprobes.h"
/* Has this kprobe gone ? */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kprobe_gone(struct kprobe *p)
{
 return p->flags & 1 /* breakpoint has already gone */;
}

/* Is this kprobe disabled ? */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kprobe_disabled(struct kprobe *p)
{
 return p->flags & (2 /* probe is temporarily disabled */ | 1 /* breakpoint has already gone */);
}

/* Is this kprobe really running optimized path ? */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kprobe_optimized(struct kprobe *p)
{
 return p->flags & 4 /*
				   * probe is really optimized.
				   * NOTE:
				   * this flag is only for optimized_kprobe.
				   */;
# 124 "./include/linux/kprobes.h"
}

/* Is this kprobe uses ftrace ? */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kprobe_ftrace(struct kprobe *p)
{
 return p->flags & 8 /* probe is using ftrace */;
}

/*
 * Function-return probe -
 * Note:
 * User needs to provide a handler function, and initialize maxactive.
 * maxactive - The maximum number of instances of the probed function that
 * can be active concurrently.
 * nmissed - tracks the number of times the probed function's return was
 * ignored, due to maxactive being too low.
 *
 */
struct kretprobe_holder {
 struct kretprobe *rp;
 refcount_t ref;
};

struct kretprobe {
 struct kprobe kp;
 kretprobe_handler_t handler;
 kretprobe_handler_t entry_handler;
 int maxactive;
 int nmissed;
 size_t data_size;



 struct freelist_head freelist;
 struct kretprobe_holder *rph;

};



struct kretprobe_instance {



 union {
  struct freelist_node freelist;
  struct callback_head rcu;
 };
 struct llist_node llist;
 struct kretprobe_holder *rph;
 kprobe_opcode_t *ret_addr;
 void *fp;

 char data[];
};

struct kretprobe_blackpoint {
 const char *name;
 void *addr;
};

struct kprobe_blacklist_entry {
 struct list_head list;
 unsigned long start_addr;
 unsigned long end_addr;
};
# 453 "./include/linux/kprobes.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kprobe_fault_handler(struct pt_regs *regs, int trapnr)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kprobe *get_kprobe(void *addr)
{
 return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct kprobe *kprobe_running(void)
{
 return ((void *)0);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_kprobe(struct kprobe *p)
{
 return -95 /* Operation not supported on transport endpoint */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_kprobes(struct kprobe **kps, int num)
{
 return -95 /* Operation not supported on transport endpoint */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unregister_kprobe(struct kprobe *p)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unregister_kprobes(struct kprobe **kps, int num)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_kretprobe(struct kretprobe *rp)
{
 return -95 /* Operation not supported on transport endpoint */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int register_kretprobes(struct kretprobe **rps, int num)
{
 return -95 /* Operation not supported on transport endpoint */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unregister_kretprobe(struct kretprobe *rp)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void unregister_kretprobes(struct kretprobe **rps, int num)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kprobe_flush_task(struct task_struct *tk)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kprobe_free_init_mem(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int disable_kprobe(struct kprobe *kp)
{
 return -95 /* Operation not supported on transport endpoint */;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int enable_kprobe(struct kprobe *kp)
{
 return -95 /* Operation not supported on transport endpoint */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool within_kprobe_blacklist(unsigned long addr)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kprobe_get_kallsym(unsigned int symnum, unsigned long *value,
         char *type, char *sym)
{
 return -34 /* Math result not representable */;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int disable_kretprobe(struct kretprobe *rp)
{
 return disable_kprobe(&rp->kp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int enable_kretprobe(struct kretprobe *rp)
{
 return enable_kprobe(&rp->kp);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kprobe_insn_slot(unsigned long addr)
{
 return false;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kprobe_optinsn_slot(unsigned long addr)
{
 return false;
}
# 568 "./include/linux/kprobes.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kretprobe_trampoline(unsigned long addr)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
unsigned long kretprobe_find_ret_addr(struct task_struct *tsk, void *fp,
          struct llist_node **cur)
{
 return 0;
}


/* Returns true if kprobes handled the fault */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kprobe_page_fault(struct pt_regs *regs,
           unsigned int trap)
{
 if (!0)
  return false;
 if ((((regs)->pstate & 0x0000000f) == 0x00000000))
  return false;
 /*
	 * To be potentially processing a kprobe fault and to be allowed
	 * to call kprobe_running(), we have to be non-preemptible.
	 */
 if ((preempt_count() == 0 && !({ unsigned long _flags; do { ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _flags = arch_local_save_flags(); } while (0); ({ ({ unsigned long __dummy; typeof(_flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(_flags); }); })))
  return false;
 if (!kprobe_running())
  return false;
 return kprobe_fault_handler(regs, trap);
}
# 20 "./include/linux/kgdb.h" 2

# 1 "./arch/arm64/include/asm/kgdb.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * AArch64 KGDB support
 *
 * Based on arch/arm/include/kgdb.h
 *
 * Copyright (C) 2013 Cavium Inc.
 * Author: Vijaya Kumar K <vijaya.kumar@caviumnetworks.com>
 */





# 1 "./arch/arm64/include/asm/debug-monitors.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */
# 15 "./arch/arm64/include/asm/debug-monitors.h"
/* Low-level stepping controls. */



/* MDSCR_EL1 enabling bits */






/* AArch64 */





/*
 * Break point instruction encoding
 */







/* kprobes BRK opcodes with ESR encoding  */


/* uprobes BRK opcodes with ESR encoding  */


/* AArch32 */
# 58 "./arch/arm64/include/asm/debug-monitors.h"
struct task_struct;






struct step_hook {
 struct list_head node;
 int (*fn)(struct pt_regs *regs, unsigned long esr);
};

void register_user_step_hook(struct step_hook *hook);
void unregister_user_step_hook(struct step_hook *hook);

void register_kernel_step_hook(struct step_hook *hook);
void unregister_kernel_step_hook(struct step_hook *hook);

struct break_hook {
 struct list_head node;
 int (*fn)(struct pt_regs *regs, unsigned long esr);
 u16 imm;
 u16 mask; /* These bits are ignored when comparing with imm */
};

void register_user_break_hook(struct break_hook *hook);
void unregister_user_break_hook(struct break_hook *hook);

void register_kernel_break_hook(struct break_hook *hook);
void unregister_kernel_break_hook(struct break_hook *hook);

u8 debug_monitors_arch(void);

enum dbg_active_el {
 DBG_ACTIVE_EL0 = 0,
 DBG_ACTIVE_EL1,
};

void enable_debug_monitors(enum dbg_active_el el);
void disable_debug_monitors(enum dbg_active_el el);

void user_rewind_single_step(struct task_struct *task);
void user_fastforward_single_step(struct task_struct *task);
void user_regs_reset_single_step(struct user_pt_regs *regs,
     struct task_struct *task);

void kernel_enable_single_step(struct pt_regs *regs);
void kernel_disable_single_step(void);
int kernel_active_single_step(void);


int reinstall_suspended_bps(struct pt_regs *regs);







int aarch32_break_handler(struct pt_regs *regs);

void debug_traps_init(void);
# 16 "./arch/arm64/include/asm/kgdb.h" 2



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void arch_kgdb_breakpoint(void)
{
 asm ("brk %0" : : "I" (0x401));
}

extern void kgdb_handle_bus_error(void);
extern int kgdb_fault_expected;



/*
 * gdb remote procotol (well most versions of it) expects the following
 * register layout.
 *
 * General purpose regs:
 *     r0-r30: 64 bit
 *     sp,pc : 64 bit
 *     pstate  : 32 bit
 *     Total: 33 + 1
 * FPU regs:
 *     f0-f31: 128 bit
 *     fpsr & fpcr: 32 bit
 *     Total: 32 + 2
 *
 * To expand a little on the "most versions of it"... when the gdb remote
 * protocol for AArch64 was developed it depended on a statement in the
 * Architecture Reference Manual that claimed "SPSR_ELx is a 32-bit register".
 * and, as a result, allocated only 32-bits for the PSTATE in the remote
 * protocol. In fact this statement is still present in ARM DDI 0487A.i.
 *
 * Unfortunately "is a 32-bit register" has a very special meaning for
 * system registers. It means that "the upper bits, bits[63:32], are
 * RES0.". RES0 is heavily used in the ARM architecture documents as a
 * way to leave space for future architecture changes. So to translate a
 * little for people who don't spend their spare time reading ARM architecture
 * manuals, what "is a 32-bit register" actually means in this context is
 * "is a 64-bit register but one with no meaning allocated to any of the
 * upper 32-bits... *yet*".
 *
 * Perhaps then we should not be surprised that this has led to some
 * confusion. Specifically a patch, influenced by the above translation,
 * that extended PSTATE to 64-bit was accepted into gdb-7.7 but the patch
 * was reverted in gdb-7.8.1 and all later releases, when this was
 * discovered to be an undocumented protocol change.
 *
 * So... it is *not* wrong for us to only allocate 32-bits to PSTATE
 * here even though the kernel itself allocates 64-bits for the same
 * state. That is because this bit of code tells the kernel how the gdb
 * remote protocol (well most versions of it) describes the register state.
 *
 * Note that if you are using one of the versions of gdb that supports
 * the gdb-7.7 version of the protocol you cannot use kgdb directly
 * without providing a custom register description (gdb can load new
 * protocol descriptions at runtime).
 */




/*
 * general purpose registers size in bytes.
 * pstate is only 4 bytes. subtract 4 bytes
 */



/*
 * Size of I/O buffer for gdb packet.
 * considering to hold all register contents, size is set
 */



/*
 * Number of bytes required for gdb_regs buffer.
 * _GP_REGS: 8 bytes, _FP_REGS: 16 bytes and _EXTRA_REGS: 4 bytes each
 * GDB fails to connect for size beyond this with error
 * "'g' packet reply is too long"
 */
# 22 "./include/linux/kgdb.h" 2
# 366 "./include/linux/kgdb.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kgdb_panic(const char *msg) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kgdb_free_init_mem(void) { }
# 12 "./arch/arm64/include/asm/cacheflush.h" 2


/*
 * This flag is used to indicate that the page pointed to by a pte is clean
 * and does not require cleaning before returning it to the user.
 */


/*
 *	MM Cache Management
 *	===================
 *
 *	The arch/arm64/mm/cache.S implements these methods.
 *
 *	Start addresses are inclusive and end addresses are exclusive; start
 *	addresses should be rounded down, end addresses up.
 *
 *	See Documentation/core-api/cachetlb.rst for more information. Please note that
 *	the implementation assumes non-aliasing VIPT D-cache and (aliasing)
 *	VIPT I-cache.
 *
 *	All functions below apply to the interval [start, end)
 *		- start  - virtual start address (inclusive)
 *		- end    - virtual end address (exclusive)
 *
 *	caches_clean_inval_pou(start, end)
 *
 *		Ensure coherency between the I-cache and the D-cache region to
 *		the Point of Unification.
 *
 *	caches_clean_inval_user_pou(start, end)
 *
 *		Ensure coherency between the I-cache and the D-cache region to
 *		the Point of Unification.
 *		Use only if the region might access user memory.
 *
 *	icache_inval_pou(start, end)
 *
 *		Invalidate I-cache region to the Point of Unification.
 *
 *	dcache_clean_inval_poc(start, end)
 *
 *		Clean and invalidate D-cache region to the Point of Coherency.
 *
 *	dcache_inval_poc(start, end)
 *
 *		Invalidate D-cache region to the Point of Coherency.
 *
 *	dcache_clean_poc(start, end)
 *
 *		Clean D-cache region to the Point of Coherency.
 *
 *	dcache_clean_pop(start, end)
 *
 *		Clean D-cache region to the Point of Persistence.
 *
 *	dcache_clean_pou(start, end)
 *
 *		Clean D-cache region to the Point of Unification.
 */
extern void caches_clean_inval_pou(unsigned long start, unsigned long end);
extern void icache_inval_pou(unsigned long start, unsigned long end);
extern void dcache_clean_inval_poc(unsigned long start, unsigned long end);
extern void dcache_inval_poc(unsigned long start, unsigned long end);
extern void dcache_clean_poc(unsigned long start, unsigned long end);
extern void dcache_clean_pop(unsigned long start, unsigned long end);
extern void dcache_clean_pou(unsigned long start, unsigned long end);
extern long caches_clean_inval_user_pou(unsigned long start, unsigned long end);
extern void sync_icache_aliases(unsigned long start, unsigned long end);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_icache_range(unsigned long start, unsigned long end)
{
 caches_clean_inval_pou(start, end);

 /*
	 * IPI all online CPUs so that they undergo a context synchronization
	 * event and are forced to refetch the new instructions.
	 */

 /*
	 * KGDB performs cache maintenance with interrupts disabled, so we
	 * will deadlock trying to IPI the secondary CPUs. In theory, we can
	 * set CACHE_FLUSH_IS_SAFE to 0 to avoid this known issue, but that
	 * just means that KGDB will elide the maintenance altogether! As it
	 * turns out, KGDB uses IPIs to round-up the secondary CPUs during
	 * the patching operation, so we don't need extra IPIs here anyway.
	 * In which case, add a KGDB-specific bodge and return early.
	 */
 if ((0))
  return;

 kick_all_cpus_sync();
}


/*
 * Copy user data from/to a page which is mapped into a different
 * processes address space.  Really, we want to allow our "user
 * space" model to handle this.
 */
extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 unsigned long, void *, const void *, unsigned long);


/*
 * flush_dcache_page is used when the kernel has written to the page
 * cache page at virtual address page->virtual.
 *
 * If this page isn't mapped (ie, page_mapping == NULL), or it might
 * have userspace mappings, then we _must_ always clean + invalidate
 * the dcache entries associated with the kernel mapping.
 *
 * Otherwise we can defer the operation, and clean the cache when we are
 * about to change to user space.  This is the same method as used on SPARC64.
 * See update_mmu_cache for the user space part.
 */

extern void flush_dcache_page(struct page *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) __attribute__((__always_inline__)) void icache_inval_all_pou(void)
{
 if (cpus_have_const_cap(11))
  return;

 asm("ic	ialluis");
 asm volatile("dsb " "ish" : : : "memory");
}

# 1 "./include/asm-generic/cacheflush.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





struct mm_struct;
struct vm_area_struct;
struct page;
struct address_space;

/*
 * The cache doesn't need to be flushed when TLB entries change when
 * the cache is mapped to physical memory, not virtual memory
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_cache_all(void)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_cache_mm(struct mm_struct *mm)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_cache_dup_mm(struct mm_struct *mm)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_cache_range(struct vm_area_struct *vma,
         unsigned long start,
         unsigned long end)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_cache_page(struct vm_area_struct *vma,
        unsigned long vmaddr,
        unsigned long pfn)
{
}
# 59 "./include/asm-generic/cacheflush.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_dcache_mmap_lock(struct address_space *mapping)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_dcache_mmap_unlock(struct address_space *mapping)
{
}
# 81 "./include/asm-generic/cacheflush.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_icache_page(struct vm_area_struct *vma,
         struct page *page)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_icache_user_page(struct vm_area_struct *vma,
        struct page *page,
        unsigned long addr, int len)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_cache_vmap(unsigned long start, unsigned long end)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_cache_vunmap(unsigned long start, unsigned long end)
{
}
# 141 "./arch/arm64/include/asm/cacheflush.h" 2
# 6 "./include/linux/cacheflush.h" 2

struct folio;



void flush_dcache_folio(struct folio *folio);
# 9 "./include/linux/highmem.h" 2
# 1 "./include/linux/kmsan.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * KMSAN API for subsystems.
 *
 * Copyright (C) 2017-2022 Google LLC
 * Author: Alexander Potapenko <glider@google.com>
 *
 */



# 1 "./include/linux/dma-direction.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



enum dma_data_direction {
 DMA_BIDIRECTIONAL = 0,
 DMA_TO_DEVICE = 1,
 DMA_FROM_DEVICE = 2,
 DMA_NONE = 3,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int valid_dma_direction(enum dma_data_direction dir)
{
 return dir == DMA_BIDIRECTIONAL || dir == DMA_TO_DEVICE ||
  dir == DMA_FROM_DEVICE;
}
# 13 "./include/linux/kmsan.h" 2




struct page;
struct kmem_cache;
struct task_struct;
struct scatterlist;
struct urb;
# 230 "./include/linux/kmsan.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_init_shadow(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_init_runtime(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool kmsan_memblock_free_pages(struct page *page,
          unsigned int order)
{
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_task_create(struct task_struct *task)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_task_exit(struct task_struct *task)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int kmsan_alloc_page(struct page *page, unsigned int order,
       gfp_t flags)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_free_page(struct page *page, unsigned int order)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_copy_page_meta(struct page *dst, struct page *src)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_slab_alloc(struct kmem_cache *s, void *object,
        gfp_t flags)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_slab_free(struct kmem_cache *s, void *object)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_kmalloc_large(const void *ptr, size_t size,
           gfp_t flags)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_kfree_large(const void *ptr)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_vmap_pages_range_noflush(unsigned long start,
        unsigned long end,
        pgprot_t prot,
        struct page **pages,
        unsigned int page_shift)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_vunmap_range_noflush(unsigned long start,
           unsigned long end)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_ioremap_page_range(unsigned long start,
         unsigned long end,
         phys_addr_t phys_addr,
         pgprot_t prot,
         unsigned int page_shift)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_iounmap_page_range(unsigned long start,
         unsigned long end)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_handle_dma(struct page *page, size_t offset,
        size_t size, enum dma_data_direction dir)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_handle_dma_sg(struct scatterlist *sg, int nents,
           enum dma_data_direction dir)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_handle_urb(const struct urb *urb, bool is_out)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmsan_unpoison_entry_regs(const struct pt_regs *regs)
{
}
# 10 "./include/linux/highmem.h" 2




# 1 "././include/linux/highmem-internal.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



/*
 * Outside of CONFIG_HIGHMEM to support X86 32bit iomap_atomic() cruft.
 */
# 20 "././include/linux/highmem-internal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmap_local_fork(struct task_struct *tsk) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmap_assert_nomap(void) { }
# 159 "././include/linux/highmem-internal.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *kmap_to_page(void *addr)
{
 return ({ u64 __idx = (((u64)addr) - ((-((((1UL))) << ((48)))))) / ((1UL) << 12); u64 __addr = (-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) + (__idx * sizeof(struct page)); (struct page *)__addr; });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap(struct page *page)
{
 do { do { } while (0); } while (0);
 return lowmem_page_address(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kunmap_high(struct page *page) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmap_flush_unused(void) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kunmap(struct page *page)
{



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_local_page(struct page *page)
{
 return lowmem_page_address(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_local_folio(struct folio *folio, size_t offset)
{
 return lowmem_page_address(&folio->page) + offset;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_local_page_prot(struct page *page, pgprot_t prot)
{
 return kmap_local_page(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_local_pfn(unsigned long pfn)
{
 return kmap_local_page((((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)) + (pfn)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kunmap_local(const void *addr)
{



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_atomic(struct page *page)
{
 if (0)
  migrate_disable();
 else
  do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 pagefault_disable();
 return lowmem_page_address(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_atomic_prot(struct page *page, pgprot_t prot)
{
 return kmap_atomic(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_atomic_pfn(unsigned long pfn)
{
 return kmap_atomic((((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)) + (pfn)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __kunmap_atomic(const void *addr)
{



 pagefault_enable();
 if (0)
  migrate_enable();
 else
  do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int nr_free_highpages(void) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long totalhigh_pages(void) { return 0UL; }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool is_kmap_addr(const void *x)
{
 return false;
}



/**
 * kunmap_atomic - Unmap the virtual address mapped by kmap_atomic() - deprecated!
 * @__addr:       Virtual address to be unmapped
 *
 * Unmaps an address previously mapped by kmap_atomic() and re-enables
 * pagefaults. Depending on PREEMP_RT configuration, re-enables also
 * migration and preemption. Users should not count on these side effects.
 *
 * Mappings should be unmapped in the reverse order that they were mapped.
 * See kmap_local_page() for details on nesting.
 *
 * @__addr can be any address within the mapped page, so there is no need
 * to subtract any offset that has been added. In contrast to kunmap(),
 * this function takes the address returned from kmap_atomic(), not the
 * page passed to it. The compiler will warn you if you pass the page.
 */






/**
 * kunmap_local - Unmap a page mapped via kmap_local_page().
 * @__addr: An address within the page mapped
 *
 * @__addr can be any address within the mapped page.  Commonly it is the
 * address return from kmap_local_page(), but it can also include offsets.
 *
 * Unmapping should be done in the reverse order of the mapping.  See
 * kmap_local_page() for details.
 */
# 15 "./include/linux/highmem.h" 2

/**
 * kmap - Map a page for long term usage
 * @page:	Pointer to the page to be mapped
 *
 * Returns: The virtual address of the mapping
 *
 * Can only be invoked from preemptible task context because on 32bit
 * systems with CONFIG_HIGHMEM enabled this function might sleep.
 *
 * For systems with CONFIG_HIGHMEM=n and for pages in the low memory area
 * this returns the virtual address of the direct kernel mapping.
 *
 * The returned virtual address is globally visible and valid up to the
 * point where it is unmapped via kunmap(). The pointer can be handed to
 * other contexts.
 *
 * For highmem pages on 32bit systems this can be slow as the mapping space
 * is limited and protected by a global lock. In case that there is no
 * mapping slot available the function blocks until a slot is released via
 * kunmap().
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap(struct page *page);

/**
 * kunmap - Unmap the virtual address mapped by kmap()
 * @page:	Pointer to the page which was mapped by kmap()
 *
 * Counterpart to kmap(). A NOOP for CONFIG_HIGHMEM=n and for mappings of
 * pages in the low memory area.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kunmap(struct page *page);

/**
 * kmap_to_page - Get the page for a kmap'ed address
 * @addr:	The address to look up
 *
 * Returns: The page which is mapped to @addr.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *kmap_to_page(void *addr);

/**
 * kmap_flush_unused - Flush all unused kmap mappings in order to
 *		       remove stray mappings
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void kmap_flush_unused(void);

/**
 * kmap_local_page - Map a page for temporary usage
 * @page: Pointer to the page to be mapped
 *
 * Returns: The virtual address of the mapping
 *
 * Can be invoked from any context, including interrupts.
 *
 * Requires careful handling when nesting multiple mappings because the map
 * management is stack based. The unmap has to be in the reverse order of
 * the map operation:
 *
 * addr1 = kmap_local_page(page1);
 * addr2 = kmap_local_page(page2);
 * ...
 * kunmap_local(addr2);
 * kunmap_local(addr1);
 *
 * Unmapping addr1 before addr2 is invalid and causes malfunction.
 *
 * Contrary to kmap() mappings the mapping is only valid in the context of
 * the caller and cannot be handed to other contexts.
 *
 * On CONFIG_HIGHMEM=n kernels and for low memory pages this returns the
 * virtual address of the direct mapping. Only real highmem pages are
 * temporarily mapped.
 *
 * While it is significantly faster than kmap() for the higmem case it
 * comes with restrictions about the pointer validity.
 *
 * On HIGHMEM enabled systems mapping a highmem page has the side effect of
 * disabling migration in order to keep the virtual address stable across
 * preemption. No caller of kmap_local_page() can rely on this side effect.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_local_page(struct page *page);

/**
 * kmap_local_folio - Map a page in this folio for temporary usage
 * @folio: The folio containing the page.
 * @offset: The byte offset within the folio which identifies the page.
 *
 * Requires careful handling when nesting multiple mappings because the map
 * management is stack based. The unmap has to be in the reverse order of
 * the map operation::
 *
 *   addr1 = kmap_local_folio(folio1, offset1);
 *   addr2 = kmap_local_folio(folio2, offset2);
 *   ...
 *   kunmap_local(addr2);
 *   kunmap_local(addr1);
 *
 * Unmapping addr1 before addr2 is invalid and causes malfunction.
 *
 * Contrary to kmap() mappings the mapping is only valid in the context of
 * the caller and cannot be handed to other contexts.
 *
 * On CONFIG_HIGHMEM=n kernels and for low memory pages this returns the
 * virtual address of the direct mapping. Only real highmem pages are
 * temporarily mapped.
 *
 * While it is significantly faster than kmap() for the higmem case it
 * comes with restrictions about the pointer validity. Only use when really
 * necessary.
 *
 * On HIGHMEM enabled systems mapping a highmem page has the side effect of
 * disabling migration in order to keep the virtual address stable across
 * preemption. No caller of kmap_local_folio() can rely on this side effect.
 *
 * Context: Can be invoked from any context.
 * Return: The virtual address of @offset.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_local_folio(struct folio *folio, size_t offset);

/**
 * kmap_atomic - Atomically map a page for temporary usage - Deprecated!
 * @page:	Pointer to the page to be mapped
 *
 * Returns: The virtual address of the mapping
 *
 * In fact a wrapper around kmap_local_page() which also disables pagefaults
 * and, depending on PREEMPT_RT configuration, also CPU migration and
 * preemption. Therefore users should not count on the latter two side effects.
 *
 * Mappings should always be released by kunmap_atomic().
 *
 * Do not use in new code. Use kmap_local_page() instead.
 *
 * It is used in atomic context when code wants to access the contents of a
 * page that might be allocated from high memory (see __GFP_HIGHMEM), for
 * example a page in the pagecache.  The API has two functions, and they
 * can be used in a manner similar to the following::
 *
 *   // Find the page of interest.
 *   struct page *page = find_get_page(mapping, offset);
 *
 *   // Gain access to the contents of that page.
 *   void *vaddr = kmap_atomic(page);
 *
 *   // Do something to the contents of that page.
 *   memset(vaddr, 0, PAGE_SIZE);
 *
 *   // Unmap that page.
 *   kunmap_atomic(vaddr);
 *
 * Note that the kunmap_atomic() call takes the result of the kmap_atomic()
 * call, not the argument.
 *
 * If you need to map two pages because you want to copy from one page to
 * another you need to keep the kmap_atomic calls strictly nested, like:
 *
 * vaddr1 = kmap_atomic(page1);
 * vaddr2 = kmap_atomic(page2);
 *
 * memcpy(vaddr1, vaddr2, PAGE_SIZE);
 *
 * kunmap_atomic(vaddr2);
 * kunmap_atomic(vaddr1);
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *kmap_atomic(struct page *page);

/* Highmem related interfaces for management code */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int nr_free_highpages(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long totalhigh_pages(void);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr)
{
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void flush_kernel_vmap_range(void *vaddr, int size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void invalidate_kernel_vmap_range(void *vaddr, int size)
{
}


/* when CONFIG_HIGHMEM is not set these will be plain clear/copy_page */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_user_highpage(struct page *page, unsigned long vaddr)
{
 void *addr = kmap_local_page(page);
 clear_page(addr);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_386(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((addr), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((addr)), typeof(struct page *))))) __compiletime_assert_386(); } while (0); __kunmap_local(addr); } while (0);
# 208 "./include/linux/highmem.h"
}
# 239 "./include/linux/highmem.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_highpage(struct page *page)
{
 void *kaddr = kmap_local_page(page);
 clear_page(kaddr);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_387(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((kaddr), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((kaddr)), typeof(struct page *))))) __compiletime_assert_387(); } while (0); __kunmap_local(kaddr); } while (0);
# 244 "./include/linux/highmem.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void clear_highpage_kasan_tagged(struct page *page)
{
 u8 tag;

 tag = page_kasan_tag(page);
 page_kasan_tag_reset(page);
 clear_highpage(page);
 page_kasan_tag_set(page, tag);
}
# 264 "./include/linux/highmem.h"
/*
 * If we pass in a base or tail page, we can zero up to PAGE_SIZE.
 * If we pass in a head page, we can zero up to the size of the compound page.
 */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zero_user_segments(struct page *page,
  unsigned start1, unsigned end1,
  unsigned start2, unsigned end2)
{
 void *kaddr = kmap_local_page(page);
 unsigned int i;

 do { if (__builtin_expect(!!(end1 > page_size(page) || end2 > page_size(page)), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/highmem.h\"; .popsection; .long 14472b - .; .short 279; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);

 if (end1 > start1)
  memset(kaddr + start1, 0, end1 - start1);

 if (end2 > start2)
  memset(kaddr + start2, 0, end2 - start2);

 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_388(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((kaddr), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((kaddr)), typeof(struct page *))))) __compiletime_assert_388(); } while (0); __kunmap_local(kaddr); } while (0);
# 288 "./include/linux/highmem.h"
 for (i = 0; i < compound_nr(page); i++)
  flush_dcache_page(page + i);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zero_user_segment(struct page *page,
 unsigned start, unsigned end)
{
 zero_user_segments(page, start, end, 0, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void zero_user(struct page *page,
 unsigned start, unsigned size)
{
 zero_user_segments(page, start, start + size, 0, 0);
}
# 340 "./include/linux/highmem.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int copy_mc_user_highpage(struct page *to, struct page *from,
     unsigned long vaddr, struct vm_area_struct *vma)
{
 copy_user_highpage(to, from, vaddr, vma);
 return 0;
}
# 364 "./include/linux/highmem.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memcpy_page(struct page *dst_page, size_t dst_off,
          struct page *src_page, size_t src_off,
          size_t len)
{
 char *dst = kmap_local_page(dst_page);
 char *src = kmap_local_page(src_page);

 ((void)(sizeof(( long)(dst_off + len > ((1UL) << 12) || src_off + len > ((1UL) << 12)))));
 memcpy(dst + dst_off, src + src_off, len);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_389(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((src), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((src)), typeof(struct page *))))) __compiletime_assert_389(); } while (0); __kunmap_local(src); } while (0);
# 374 "./include/linux/highmem.h"
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_390(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((dst), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((dst)), typeof(struct page *))))) __compiletime_assert_390(); } while (0); __kunmap_local(dst); } while (0);
# 375 "./include/linux/highmem.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memset_page(struct page *page, size_t offset, int val,
          size_t len)
{
 char *addr = kmap_local_page(page);

 ((void)(sizeof(( long)(offset + len > ((1UL) << 12)))));
 memset(addr + offset, val, len);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_391(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((addr), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((addr)), typeof(struct page *))))) __compiletime_assert_391(); } while (0); __kunmap_local(addr); } while (0);
# 385 "./include/linux/highmem.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memcpy_from_page(char *to, struct page *page,
        size_t offset, size_t len)
{
 char *from = kmap_local_page(page);

 ((void)(sizeof(( long)(offset + len > ((1UL) << 12)))));
 memcpy(to, from + offset, len);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_392(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((from), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((from)), typeof(struct page *))))) __compiletime_assert_392(); } while (0); __kunmap_local(from); } while (0);
# 395 "./include/linux/highmem.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memcpy_to_page(struct page *page, size_t offset,
      const char *from, size_t len)
{
 char *to = kmap_local_page(page);

 ((void)(sizeof(( long)(offset + len > ((1UL) << 12)))));
 memcpy(to + offset, from, len);
 flush_dcache_page(page);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_393(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((to), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((to)), typeof(struct page *))))) __compiletime_assert_393(); } while (0); __kunmap_local(to); } while (0);
# 406 "./include/linux/highmem.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memzero_page(struct page *page, size_t offset, size_t len)
{
 char *addr = kmap_local_page(page);

 ((void)(sizeof(( long)(offset + len > ((1UL) << 12)))));
 memset(addr + offset, 0, len);
 flush_dcache_page(page);
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_394(void) __attribute__((__error__("BUILD_BUG_ON failed: " "__same_type((addr), struct page *)"))); if (!(!(__builtin_types_compatible_p(typeof((addr)), typeof(struct page *))))) __compiletime_assert_394(); } while (0); __kunmap_local(addr); } while (0);
# 416 "./include/linux/highmem.h"
}

/**
 * folio_zero_segments() - Zero two byte ranges in a folio.
 * @folio: The folio to write to.
 * @start1: The first byte to zero.
 * @xend1: One more than the last byte in the first range.
 * @start2: The first byte to zero in the second range.
 * @xend2: One more than the last byte in the second range.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_zero_segments(struct folio *folio,
  size_t start1, size_t xend1, size_t start2, size_t xend2)
{
 zero_user_segments(&folio->page, start1, xend1, start2, xend2);
}

/**
 * folio_zero_segment() - Zero a byte range in a folio.
 * @folio: The folio to write to.
 * @start: The first byte to zero.
 * @xend: One more than the last byte to zero.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_zero_segment(struct folio *folio,
  size_t start, size_t xend)
{
 zero_user_segments(&folio->page, start, xend, 0, 0);
}

/**
 * folio_zero_range() - Zero a byte range in a folio.
 * @folio: The folio to write to.
 * @start: The first byte to zero.
 * @length: The number of bytes to zero.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void folio_zero_range(struct folio *folio,
  size_t start, size_t length)
{
 zero_user_segments(&folio->page, start, start + length, 0, 0);
}
# 11 "./include/linux/bvec.h" 2







struct page;

/**
 * struct bio_vec - a contiguous range of physical memory addresses
 * @bv_page:   First page associated with the address range.
 * @bv_len:    Number of bytes in the address range.
 * @bv_offset: Start of the address range relative to the start of @bv_page.
 *
 * The following holds for a bvec if n * PAGE_SIZE < bv_offset + bv_len:
 *
 *   nth_page(@bv_page, n) == @bv_page + n
 *
 * This holds because page_is_mergeable() checks the above property.
 */
struct bio_vec {
 struct page *bv_page;
 unsigned int bv_len;
 unsigned int bv_offset;
};

struct bvec_iter {
 sector_t bi_sector; /* device address in 512 byte
						   sectors */
 unsigned int bi_size; /* residual I/O count */

 unsigned int bi_idx; /* current index into bvl_vec */

 unsigned int bi_bvec_done; /* number of bytes completed in
						   current bvec */
} __attribute__((__packed__));

struct bvec_iter_all {
 struct bio_vec bv;
 int idx;
 unsigned done;
};

/*
 * various member access, note that bio_data should of course not be used
 * on highmem page vectors
 */


/* multi-page (mp_bvec) helpers */
# 82 "./include/linux/bvec.h"
/* For building single-page bvec in flight */
# 101 "./include/linux/bvec.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bvec_iter_advance(const struct bio_vec *bv,
  struct bvec_iter *iter, unsigned bytes)
{
 unsigned int idx = iter->bi_idx;

 if (({ bool __ret_do_once = !!(bytes > iter->bi_size); if (({ static bool __attribute__((__section__(".data.once"))) __already_done; bool __ret_cond = !!(__ret_do_once); bool __ret_once = false; if (__builtin_expect(!!(__ret_cond && !__already_done), 0)) { __already_done = true; __ret_once = true; } __builtin_expect(!!(__ret_once), 0); })) ({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); __warn_printk("Attempted to advance past end of bvec iter\n"); asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/bvec.h\"; .popsection; .long 14472b - .; .short 107; .short (1 << 0)|((1 << 3) /* CUT_HERE already sent */ | ((9) << 8)); .popsection; 14471: brk 0x800");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); __builtin_expect(!!(__ret_do_once), 0); })) {

  iter->bi_size = 0;
  return false;
 }

 iter->bi_size -= bytes;
 bytes += iter->bi_bvec_done;

 while (bytes && bytes >= bv[idx].bv_len) {
  bytes -= bv[idx].bv_len;
  idx++;
 }

 iter->bi_idx = idx;
 iter->bi_bvec_done = bytes;
 return true;
}

/*
 * A simpler version of bvec_iter_advance(), @bytes should not span
 * across multiple bvec entries, i.e. bytes <= bv[i->bi_idx].bv_len
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bvec_iter_advance_single(const struct bio_vec *bv,
    struct bvec_iter *iter, unsigned int bytes)
{
 unsigned int done = iter->bi_bvec_done + bytes;

 if (done == bv[iter->bi_idx].bv_len) {
  done = 0;
  iter->bi_idx++;
 }
 iter->bi_bvec_done = done;
 iter->bi_size -= bytes;
}







/* for iterating one bio from start to end */
# 157 "./include/linux/bvec.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio_vec *bvec_init_iter_all(struct bvec_iter_all *iter_all)
{
 iter_all->done = 0;
 iter_all->idx = 0;

 return &iter_all->bv;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bvec_advance(const struct bio_vec *bvec,
    struct bvec_iter_all *iter_all)
{
 struct bio_vec *bv = &iter_all->bv;

 if (iter_all->done) {
  bv->bv_page++;
  bv->bv_offset = 0;
 } else {
  bv->bv_page = bvec->bv_page + (bvec->bv_offset >> 12);
  bv->bv_offset = bvec->bv_offset & ~(~(((1UL) << 12)-1));
 }
 bv->bv_len = __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)(((1UL) << 12) - bv->bv_offset)) *)1 == (typeof((unsigned int)(bvec->bv_len - iter_all->done)) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(((1UL) << 12) - bv->bv_offset)) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(bvec->bv_len - iter_all->done)) * 0l)) : (int *)8))))), (((unsigned int)(((1UL) << 12) - bv->bv_offset)) < ((unsigned int)(bvec->bv_len - iter_all->done)) ? ((unsigned int)(((1UL) << 12) - bv->bv_offset)) : ((unsigned int)(bvec->bv_len - iter_all->done))), ({ typeof((unsigned int)(((1UL) << 12) - bv->bv_offset)) __UNIQUE_ID___x395 = ((unsigned int)(((1UL) << 12) - bv->bv_offset)); typeof((unsigned int)(bvec->bv_len - iter_all->done)) __UNIQUE_ID___y396 = ((unsigned int)(bvec->bv_len - iter_all->done)); ((__UNIQUE_ID___x395) < (__UNIQUE_ID___y396) ? (__UNIQUE_ID___x395) : (__UNIQUE_ID___y396)); }));

 iter_all->done += bv->bv_len;

 if (iter_all->done == bvec->bv_len) {
  iter_all->idx++;
  iter_all->done = 0;
 }
}

/**
 * bvec_kmap_local - map a bvec into the kernel virtual address space
 * @bvec: bvec to map
 *
 * Must be called on single-page bvecs only.  Call kunmap_local on the returned
 * address to unmap.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *bvec_kmap_local(struct bio_vec *bvec)
{
 return kmap_local_page(bvec->bv_page) + bvec->bv_offset;
}

/**
 * memcpy_from_bvec - copy data from a bvec
 * @bvec: bvec to copy from
 *
 * Must be called on single-page bvecs only.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memcpy_from_bvec(char *to, struct bio_vec *bvec)
{
 memcpy_from_page(to, bvec->bv_page, bvec->bv_offset, bvec->bv_len);
}

/**
 * memcpy_to_bvec - copy data to a bvec
 * @bvec: bvec to copy to
 *
 * Must be called on single-page bvecs only.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memcpy_to_bvec(struct bio_vec *bvec, const char *from)
{
 memcpy_to_page(bvec->bv_page, bvec->bv_offset, from, bvec->bv_len);
}

/**
 * memzero_bvec - zero all data in a bvec
 * @bvec: bvec to zero
 *
 * Must be called on single-page bvecs only.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void memzero_bvec(struct bio_vec *bvec)
{
 memzero_page(bvec->bv_page, bvec->bv_offset, bvec->bv_len);
}

/**
 * bvec_virt - return the virtual address for a bvec
 * @bvec: bvec to return the virtual address for
 *
 * Note: the caller must ensure that @bvec->bv_page is not a highmem page.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *bvec_virt(struct bio_vec *bvec)
{
 ({ int __ret_warn_on = !!(PageHighMem(bvec->bv_page)); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/bvec.h\"; .popsection; .long 14472b - .; .short 240; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 return lowmem_page_address(bvec->bv_page) + bvec->bv_offset;
}
# 11 "./include/linux/blk_types.h" 2
# 1 "./include/linux/device.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 * device.h - generic, centralized driver model
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 *
 * See Documentation/driver-api/driver-model/ for more information.
 */




# 1 "./include/linux/dev_printk.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 * dev_printk.h - printk messages helpers for devices
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 *
 */
# 22 "./include/linux/dev_printk.h"
struct device;




struct dev_printk_info {
 char subsystem[16];
 char device[48];
};



__attribute__((__format__(printf, 3, 0))) __attribute__((__cold__))
int dev_vprintk_emit(int level, const struct device *dev,
       const char *fmt, va_list args);
__attribute__((__format__(printf, 3, 4))) __attribute__((__cold__))
int dev_printk_emit(int level, const struct device *dev, const char *fmt, ...);

__attribute__((__format__(printf, 3, 4))) __attribute__((__cold__))
void _dev_printk(const char *level, const struct device *dev,
   const char *fmt, ...);
__attribute__((__format__(printf, 2, 3))) __attribute__((__cold__))
void _dev_emerg(const struct device *dev, const char *fmt, ...);
__attribute__((__format__(printf, 2, 3))) __attribute__((__cold__))
void _dev_alert(const struct device *dev, const char *fmt, ...);
__attribute__((__format__(printf, 2, 3))) __attribute__((__cold__))
void _dev_crit(const struct device *dev, const char *fmt, ...);
__attribute__((__format__(printf, 2, 3))) __attribute__((__cold__))
void _dev_err(const struct device *dev, const char *fmt, ...);
__attribute__((__format__(printf, 2, 3))) __attribute__((__cold__))
void _dev_warn(const struct device *dev, const char *fmt, ...);
__attribute__((__format__(printf, 2, 3))) __attribute__((__cold__))
void _dev_notice(const struct device *dev, const char *fmt, ...);
__attribute__((__format__(printf, 2, 3))) __attribute__((__cold__))
void _dev_info(const struct device *dev, const char *fmt, ...);
# 100 "./include/linux/dev_printk.h"
/*
 * Need to take variadic arguments even though we don't use them, as dev_fmt()
 * may only just have been expanded and may result in multiple arguments.
 */
# 113 "./include/linux/dev_printk.h"
/*
 * Some callsites directly call dev_printk rather than going through the
 * dev_<level> infrastructure, so we need to emit here as well as inside those
 * level-specific macros. Only one index entry will be produced, either way,
 * since dev_printk's `fmt` isn't known at compile time if going through the
 * dev_<level> macros.
 *
 * dev_fmt() isn't called for dev_printk when used directly, as it's used by
 * the dev_<level> macros internally which already have dev_fmt() processed.
 *
 * We also can't use dev_printk_index_wrap directly, because we have a separate
 * level to process.
 */






/*
 * #defines for all the dev_<level> macros to prefix with whatever
 * possible use of #define dev_fmt(fmt) ...
 */
# 266 "./include/linux/dev_printk.h"
/*
 * dev_WARN*() acts like dev_printk(), but with the key difference of
 * using WARN/WARN_ONCE to include file/line information and a backtrace.
 */
# 16 "./include/linux/device.h" 2
# 1 "./include/linux/energy_model.h" 1
/* SPDX-License-Identifier: GPL-2.0 */



# 1 "./include/linux/device.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 * device.h - generic, centralized driver model
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 *
 * See Documentation/driver-api/driver-model/ for more information.
 */
# 6 "./include/linux/energy_model.h" 2



# 1 "./include/linux/sched/cpufreq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * Interface between cpufreq drivers and the scheduler:
 */




struct cpufreq_policy;

struct update_util_data {
       void (*func)(struct update_util_data *data, u64 time, unsigned int flags);
};

void cpufreq_add_update_util_hook(int cpu, struct update_util_data *data,
                       void (*func)(struct update_util_data *data, u64 time,
        unsigned int flags));
void cpufreq_remove_update_util_hook(int cpu);
bool cpufreq_this_cpu_can_update(struct cpufreq_policy *policy);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long map_util_freq(unsigned long util,
     unsigned long freq, unsigned long cap)
{
 return freq * util / cap;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long map_util_perf(unsigned long util)
{
 return util + (util >> 2);
}
# 10 "./include/linux/energy_model.h" 2
# 1 "./include/linux/sched/topology.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/sched/idle.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





enum cpu_idle_type {
 CPU_IDLE,
 CPU_NOT_IDLE,
 CPU_NEWLY_IDLE,
 CPU_MAX_IDLE_TYPES
};


extern void wake_up_if_idle(int cpu);




/*
 * Idle thread specific functions to determine the need_resched
 * polling state.
 */
# 63 "./include/linux/sched/idle.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __current_set_polling(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __current_clr_polling(void) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __attribute__((__warn_unused_result__)) current_set_polling_and_test(void)
{
 return __builtin_expect(!!(test_ti_thread_flag(((struct thread_info *)get_current()), 1 /* rescheduling necessary */)), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool __attribute__((__warn_unused_result__)) current_clr_polling_and_test(void)
{
 return __builtin_expect(!!(test_ti_thread_flag(((struct thread_info *)get_current()), 1 /* rescheduling necessary */)), 0);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void current_clr_polling(void)
{
 __current_clr_polling();

 /*
	 * Ensure we check TIF_NEED_RESCHED after we clear the polling bit.
	 * Once the bit is cleared, we'll get IPIs with every new
	 * TIF_NEED_RESCHED and the IPI handler, scheduler_ipi(), will also
	 * fold.
	 */
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0); /* paired with resched_curr() */

 do { if (test_ti_thread_flag(((struct thread_info *)get_current()), 1 /* rescheduling necessary */)) set_preempt_need_resched(); } while (0);
}
# 8 "./include/linux/sched/topology.h" 2

/*
 * sched-domains (multiprocessor balancing) declarations:
 */


/* Generate SD flag indexes */

enum {
# 1 "./include/linux/sched/sd_flags.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * sched-domains (multiprocessor balancing) flag declarations.
 */





/*
 * Hierarchical metaflags
 *
 * SHARED_CHILD: These flags are meant to be set from the base domain upwards.
 * If a domain has this flag set, all of its children should have it set. This
 * is usually because the flag describes some shared resource (all CPUs in that
 * domain share the same resource), or because they are tied to a scheduling
 * behaviour that we want to disable at some point in the hierarchy for
 * scalability reasons.
 *
 * In those cases it doesn't make sense to have the flag set for a domain but
 * not have it in (some of) its children: sched domains ALWAYS span their child
 * domains, so operations done with parent domains will cover CPUs in the lower
 * child domains.
 *
 *
 * SHARED_PARENT: These flags are meant to be set from the highest domain
 * downwards. If a domain has this flag set, all of its parents should have it
 * set. This is usually for topology properties that start to appear above a
 * certain level (e.g. domain starts spanning CPUs outside of the base CPU's
 * socket).
 */



/*
 * Behavioural metaflags
 *
 * NEEDS_GROUPS: These flags are only relevant if the domain they are set on has
 * more than one group. This is usually for balancing flags (load balancing
 * involves equalizing a metric between groups), or for flags describing some
 * shared resource (which would be shared between groups).
 */


/*
 * Balance when about to become idle
 *
 * SHARED_CHILD: Set from the base domain up to cpuset.sched_relax_domain_level.
 * NEEDS_GROUPS: Load balancing flag.
 */
__SD_BALANCE_NEWIDLE,

/*
 * Balance on exec
 *
 * SHARED_CHILD: Set from the base domain up to the NUMA reclaim level.
 * NEEDS_GROUPS: Load balancing flag.
 */
__SD_BALANCE_EXEC,

/*
 * Balance on fork, clone
 *
 * SHARED_CHILD: Set from the base domain up to the NUMA reclaim level.
 * NEEDS_GROUPS: Load balancing flag.
 */
__SD_BALANCE_FORK,

/*
 * Balance on wakeup
 *
 * SHARED_CHILD: Set from the base domain up to cpuset.sched_relax_domain_level.
 * NEEDS_GROUPS: Load balancing flag.
 */
__SD_BALANCE_WAKE,

/*
 * Consider waking task on waking CPU.
 *
 * SHARED_CHILD: Set from the base domain up to the NUMA reclaim level.
 */
__SD_WAKE_AFFINE,

/*
 * Domain members have different CPU capacities
 *
 * SHARED_PARENT: Set from the topmost domain down to the first domain where
 *                asymmetry is detected.
 * NEEDS_GROUPS: Per-CPU capacity is asymmetric between groups.
 */
__SD_ASYM_CPUCAPACITY,

/*
 * Domain members have different CPU capacities spanning all unique CPU
 * capacity values.
 *
 * SHARED_PARENT: Set from the topmost domain down to the first domain where
 *		  all available CPU capacities are visible
 * NEEDS_GROUPS: Per-CPU capacity is asymmetric between groups.
 */
__SD_ASYM_CPUCAPACITY_FULL,

/*
 * Domain members share CPU capacity (i.e. SMT)
 *
 * SHARED_CHILD: Set from the base domain up until spanned CPUs no longer share
 *               CPU capacity.
 * NEEDS_GROUPS: Capacity is shared between groups.
 */
__SD_SHARE_CPUCAPACITY,

/*
 * Domain members share CPU package resources (i.e. caches)
 *
 * SHARED_CHILD: Set from the base domain up until spanned CPUs no longer share
 *               the same cache(s).
 * NEEDS_GROUPS: Caches are shared between groups.
 */
__SD_SHARE_PKG_RESOURCES,

/*
 * Only a single load balancing instance
 *
 * SHARED_PARENT: Set for all NUMA levels above NODE. Could be set from a
 *                different level upwards, but it doesn't change that if a
 *                domain has this flag set, then all of its parents need to have
 *                it too (otherwise the serialization doesn't make sense).
 * NEEDS_GROUPS: No point in preserving domain if it has a single group.
 */
__SD_SERIALIZE,

/*
 * Place busy tasks earlier in the domain
 *
 * SHARED_CHILD: Usually set on the SMT level. Technically could be set further
 *               up, but currently assumed to be set from the base domain
 *               upwards (see update_top_cache_domain()).
 * NEEDS_GROUPS: Load balancing flag.
 */
__SD_ASYM_PACKING,

/*
 * Prefer to place tasks in a sibling domain
 *
 * Set up until domains start spanning NUMA nodes. Close to being a SHARED_CHILD
 * flag, but cleared below domains with SD_ASYM_CPUCAPACITY.
 *
 * NEEDS_GROUPS: Load balancing flag.
 */
__SD_PREFER_SIBLING,

/*
 * sched_groups of this level overlap
 *
 * SHARED_PARENT: Set for all NUMA levels above NODE.
 * NEEDS_GROUPS: Overlaps can only exist with more than one group.
 */
__SD_OVERLAP,

/*
 * Cross-node balancing
 *
 * SHARED_PARENT: Set for all NUMA levels above NODE.
 * NEEDS_GROUPS: No point in preserving domain if it has a single group.
 */
__SD_NUMA,
# 18 "./include/linux/sched/topology.h" 2
 __SD_FLAG_CNT,
};

/* Generate SD flag bits */

enum {
# 1 "./include/linux/sched/sd_flags.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * sched-domains (multiprocessor balancing) flag declarations.
 */





/*
 * Hierarchical metaflags
 *
 * SHARED_CHILD: These flags are meant to be set from the base domain upwards.
 * If a domain has this flag set, all of its children should have it set. This
 * is usually because the flag describes some shared resource (all CPUs in that
 * domain share the same resource), or because they are tied to a scheduling
 * behaviour that we want to disable at some point in the hierarchy for
 * scalability reasons.
 *
 * In those cases it doesn't make sense to have the flag set for a domain but
 * not have it in (some of) its children: sched domains ALWAYS span their child
 * domains, so operations done with parent domains will cover CPUs in the lower
 * child domains.
 *
 *
 * SHARED_PARENT: These flags are meant to be set from the highest domain
 * downwards. If a domain has this flag set, all of its parents should have it
 * set. This is usually for topology properties that start to appear above a
 * certain level (e.g. domain starts spanning CPUs outside of the base CPU's
 * socket).
 */



/*
 * Behavioural metaflags
 *
 * NEEDS_GROUPS: These flags are only relevant if the domain they are set on has
 * more than one group. This is usually for balancing flags (load balancing
 * involves equalizing a metric between groups), or for flags describing some
 * shared resource (which would be shared between groups).
 */


/*
 * Balance when about to become idle
 *
 * SHARED_CHILD: Set from the base domain up to cpuset.sched_relax_domain_level.
 * NEEDS_GROUPS: Load balancing flag.
 */
SD_BALANCE_NEWIDLE = 1 << __SD_BALANCE_NEWIDLE,

/*
 * Balance on exec
 *
 * SHARED_CHILD: Set from the base domain up to the NUMA reclaim level.
 * NEEDS_GROUPS: Load balancing flag.
 */
SD_BALANCE_EXEC = 1 << __SD_BALANCE_EXEC,

/*
 * Balance on fork, clone
 *
 * SHARED_CHILD: Set from the base domain up to the NUMA reclaim level.
 * NEEDS_GROUPS: Load balancing flag.
 */
SD_BALANCE_FORK = 1 << __SD_BALANCE_FORK,

/*
 * Balance on wakeup
 *
 * SHARED_CHILD: Set from the base domain up to cpuset.sched_relax_domain_level.
 * NEEDS_GROUPS: Load balancing flag.
 */
SD_BALANCE_WAKE = 1 << __SD_BALANCE_WAKE,

/*
 * Consider waking task on waking CPU.
 *
 * SHARED_CHILD: Set from the base domain up to the NUMA reclaim level.
 */
SD_WAKE_AFFINE = 1 << __SD_WAKE_AFFINE,

/*
 * Domain members have different CPU capacities
 *
 * SHARED_PARENT: Set from the topmost domain down to the first domain where
 *                asymmetry is detected.
 * NEEDS_GROUPS: Per-CPU capacity is asymmetric between groups.
 */
SD_ASYM_CPUCAPACITY = 1 << __SD_ASYM_CPUCAPACITY,

/*
 * Domain members have different CPU capacities spanning all unique CPU
 * capacity values.
 *
 * SHARED_PARENT: Set from the topmost domain down to the first domain where
 *		  all available CPU capacities are visible
 * NEEDS_GROUPS: Per-CPU capacity is asymmetric between groups.
 */
SD_ASYM_CPUCAPACITY_FULL = 1 << __SD_ASYM_CPUCAPACITY_FULL,

/*
 * Domain members share CPU capacity (i.e. SMT)
 *
 * SHARED_CHILD: Set from the base domain up until spanned CPUs no longer share
 *               CPU capacity.
 * NEEDS_GROUPS: Capacity is shared between groups.
 */
SD_SHARE_CPUCAPACITY = 1 << __SD_SHARE_CPUCAPACITY,

/*
 * Domain members share CPU package resources (i.e. caches)
 *
 * SHARED_CHILD: Set from the base domain up until spanned CPUs no longer share
 *               the same cache(s).
 * NEEDS_GROUPS: Caches are shared between groups.
 */
SD_SHARE_PKG_RESOURCES = 1 << __SD_SHARE_PKG_RESOURCES,

/*
 * Only a single load balancing instance
 *
 * SHARED_PARENT: Set for all NUMA levels above NODE. Could be set from a
 *                different level upwards, but it doesn't change that if a
 *                domain has this flag set, then all of its parents need to have
 *                it too (otherwise the serialization doesn't make sense).
 * NEEDS_GROUPS: No point in preserving domain if it has a single group.
 */
SD_SERIALIZE = 1 << __SD_SERIALIZE,

/*
 * Place busy tasks earlier in the domain
 *
 * SHARED_CHILD: Usually set on the SMT level. Technically could be set further
 *               up, but currently assumed to be set from the base domain
 *               upwards (see update_top_cache_domain()).
 * NEEDS_GROUPS: Load balancing flag.
 */
SD_ASYM_PACKING = 1 << __SD_ASYM_PACKING,

/*
 * Prefer to place tasks in a sibling domain
 *
 * Set up until domains start spanning NUMA nodes. Close to being a SHARED_CHILD
 * flag, but cleared below domains with SD_ASYM_CPUCAPACITY.
 *
 * NEEDS_GROUPS: Load balancing flag.
 */
SD_PREFER_SIBLING = 1 << __SD_PREFER_SIBLING,

/*
 * sched_groups of this level overlap
 *
 * SHARED_PARENT: Set for all NUMA levels above NODE.
 * NEEDS_GROUPS: Overlaps can only exist with more than one group.
 */
SD_OVERLAP = 1 << __SD_OVERLAP,

/*
 * Cross-node balancing
 *
 * SHARED_PARENT: Set for all NUMA levels above NODE.
 * NEEDS_GROUPS: No point in preserving domain if it has a single group.
 */
SD_NUMA = 1 << __SD_NUMA,
# 25 "./include/linux/sched/topology.h" 2
};
# 39 "./include/linux/sched/topology.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpu_smt_flags(void)
{
 return SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;
}
# 53 "./include/linux/sched/topology.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpu_core_flags(void)
{
 return SD_SHARE_PKG_RESOURCES;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int cpu_numa_flags(void)
{
 return SD_NUMA;
}


extern int arch_asym_cpu_priority(int cpu);

struct sched_domain_attr {
 int relax_domain_level;
};





extern int sched_domain_level_max;

struct sched_group;

struct sched_domain_shared {
 atomic_t ref;
 atomic_t nr_busy_cpus;
 int has_idle_cores;
 int nr_idle_scan;
};

struct sched_domain {
 /* These fields must be setup */
 struct sched_domain /* nothing */ *parent; /* top domain must be null terminated */
 struct sched_domain /* nothing */ *child; /* bottom domain must be null terminated */
 struct sched_group *groups; /* the balancing groups of the domain */
 unsigned long min_interval; /* Minimum balance interval ms */
 unsigned long max_interval; /* Maximum balance interval ms */
 unsigned int busy_factor; /* less balancing by factor if busy */
 unsigned int imbalance_pct; /* No balance until over watermark */
 unsigned int cache_nice_tries; /* Leave cache hot tasks for # tries */
 unsigned int imb_numa_nr; /* Nr running tasks that allows a NUMA imbalance */

 int nohz_idle; /* NOHZ IDLE status */
 int flags; /* See SD_* */
 int level;

 /* Runtime fields. */
 unsigned long last_balance; /* init to jiffies. units in jiffies */
 unsigned int balance_interval; /* initialise to 1. units in ms. */
 unsigned int nr_balance_failed; /* initialise to 0 */

 /* idle_balance() stats */
 u64 max_newidle_lb_cost;
 unsigned long last_decay_max_lb_cost;

 u64 avg_scan_cost; /* select_idle_sibling */
# 148 "./include/linux/sched/topology.h"
 union {
  void *private; /* used during construction */
  struct callback_head rcu; /* used during destruction */
 };
 struct sched_domain_shared *shared;

 unsigned int span_weight;
 /*
	 * Span of all CPUs in this domain.
	 *
	 * NOTE: this field is variable length. (Allocated dynamically
	 * by attaching extra space to the end of the structure,
	 * depending on how many CPUs the kernel has booted up with)
	 */
 unsigned long span[];
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct cpumask *sched_domain_span(struct sched_domain *sd)
{
 return ((struct cpumask *)(1 ? (sd->span) : (void *)sizeof(__check_is_bitmap(sd->span))));
}

extern void partition_sched_domains_locked(int ndoms_new,
        cpumask_var_t doms_new[],
        struct sched_domain_attr *dattr_new);

extern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
        struct sched_domain_attr *dattr_new);

/* Allocate an array of sched domains, for partition_sched_domains(). */
cpumask_var_t *alloc_sched_domains(unsigned int ndoms);
void free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);

bool cpus_share_cache(int this_cpu, int that_cpu);

typedef const struct cpumask *(*sched_domain_mask_f)(int cpu);
typedef int (*sched_domain_flags_f)(void);



struct sd_data {
 struct sched_domain */* nothing */ *sd;
 struct sched_domain_shared */* nothing */ *sds;
 struct sched_group */* nothing */ *sg;
 struct sched_group_capacity */* nothing */ *sgc;
};

struct sched_domain_topology_level {
 sched_domain_mask_f mask;
 sched_domain_flags_f sd_flags;
 int flags;
 int numa_level;
 struct sd_data data;



};

extern void set_sched_topology(struct sched_domain_topology_level *tl);
# 238 "./include/linux/sched/topology.h"
extern void rebuild_sched_domains_energy(void);
# 278 "./include/linux/sched/topology.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int task_node(const struct task_struct *p)
{
 return cpu_to_node(task_cpu(p));
}
# 11 "./include/linux/energy_model.h" 2


/**
 * struct em_perf_state - Performance state of a performance domain
 * @frequency:	The frequency in KHz, for consistency with CPUFreq
 * @power:	The power consumed at this level (by 1 CPU or by a registered
 *		device). It can be a total power: static and dynamic.
 * @cost:	The cost coefficient associated with this level, used during
 *		energy calculation. Equal to: power * max_frequency / frequency
 * @flags:	see "em_perf_state flags" description below.
 */
struct em_perf_state {
 unsigned long frequency;
 unsigned long power;
 unsigned long cost;
 unsigned long flags;
};

/*
 * em_perf_state flags:
 *
 * EM_PERF_STATE_INEFFICIENT: The performance state is inefficient. There is
 * in this em_perf_domain, another performance state with a higher frequency
 * but a lower or equal power cost. Such inefficient states are ignored when
 * using em_pd_get_efficient_*() functions.
 */


/**
 * struct em_perf_domain - Performance domain
 * @table:		List of performance states, in ascending order
 * @nr_perf_states:	Number of performance states
 * @flags:		See "em_perf_domain flags"
 * @cpus:		Cpumask covering the CPUs of the domain. It's here
 *			for performance reasons to avoid potential cache
 *			misses during energy calculations in the scheduler
 *			and simplifies allocating/freeing that memory region.
 *
 * In case of CPU device, a "performance domain" represents a group of CPUs
 * whose performance is scaled together. All CPUs of a performance domain
 * must have the same micro-architecture. Performance domains often have
 * a 1-to-1 mapping with CPUFreq policies. In case of other devices the @cpus
 * field is unused.
 */
struct em_perf_domain {
 struct em_perf_state *table;
 int nr_perf_states;
 unsigned long flags;
 unsigned long cpus[];
};

/*
 *  em_perf_domain flags:
 *
 *  EM_PERF_DOMAIN_MICROWATTS: The power values are in micro-Watts or some
 *  other scale.
 *
 *  EM_PERF_DOMAIN_SKIP_INEFFICIENCIES: Skip inefficient states when estimating
 *  energy consumption.
 *
 *  EM_PERF_DOMAIN_ARTIFICIAL: The power values are artificial and might be
 *  created by platform missing real power information
 */
# 82 "./include/linux/energy_model.h"
/*
 * The max power value in micro-Watts. The limit of 64 Watts is set as
 * a safety net to not overflow multiplications on 32bit platforms. The
 * 32bit value limit for total Perf Domain power implies a limit of
 * maximum CPUs in such domain to 64.
 */


/*
 * To avoid possible energy estimation overflow on 32bit machines add
 * limits to number of CPUs in the Perf. Domain.
 * We are safe on 64bit machine, thus some big number.
 */






/*
 * To avoid an overflow on 32bit machines while calculating the energy
 * use a different order in the operation. First divide by the 'cpu_scale'
 * which would reduce big value stored in the 'cost' field, then multiply by
 * the 'sum_util'. This would allow to handle existing platforms, which have
 * e.g. power ~1.3 Watt at max freq, so the 'cost' value > 1mln micro-Watts.
 * In such scenario, where there are 4 CPUs in the Perf. Domain the 'sum_util'
 * could be 4096, then multiplication: 'cost' * 'sum_util'  would overflow.
 * This reordering of operations has some limitations, we lose small
 * precision in the estimation (comparing to 64bit platform w/o reordering).
 *
 * We are safe on 64bit machine.
 */
# 122 "./include/linux/energy_model.h"
struct em_data_callback {
 /**
	 * active_power() - Provide power at the next performance state of
	 *		a device
	 * @dev		: Device for which we do this operation (can be a CPU)
	 * @power	: Active power at the performance state
	 *		(modified)
	 * @freq	: Frequency at the performance state in kHz
	 *		(modified)
	 *
	 * active_power() must find the lowest performance state of 'dev' above
	 * 'freq' and update 'power' and 'freq' to the matching active power
	 * and frequency.
	 *
	 * In case of CPUs, the power is the one of a single CPU in the domain,
	 * expressed in micro-Watts or an abstract scale. It is expected to
	 * fit in the [0, EM_MAX_POWER] range.
	 *
	 * Return 0 on success.
	 */
 int (*active_power)(struct device *dev, unsigned long *power,
       unsigned long *freq);

 /**
	 * get_cost() - Provide the cost at the given performance state of
	 *		a device
	 * @dev		: Device for which we do this operation (can be a CPU)
	 * @freq	: Frequency at the performance state in kHz
	 * @cost	: The cost value for the performance state
	 *		(modified)
	 *
	 * In case of CPUs, the cost is the one of a single CPU in the domain.
	 * It is expected to fit in the [0, EM_MAX_POWER] range due to internal
	 * usage in EAS calculation.
	 *
	 * Return 0 on success, or appropriate error value in case of failure.
	 */
 int (*get_cost)(struct device *dev, unsigned long freq,
   unsigned long *cost);
};







struct em_perf_domain *em_cpu_get(int cpu);
struct em_perf_domain *em_pd_get(struct device *dev);
int em_dev_register_perf_domain(struct device *dev, unsigned int nr_states,
    struct em_data_callback *cb, cpumask_t *span,
    bool microwatts);
void em_dev_unregister_perf_domain(struct device *dev);

/**
 * em_pd_get_efficient_state() - Get an efficient performance state from the EM
 * @pd   : Performance domain for which we want an efficient frequency
 * @freq : Frequency to map with the EM
 *
 * It is called from the scheduler code quite frequently and as a consequence
 * doesn't implement any check.
 *
 * Return: An efficient performance state, high enough to meet @freq
 * requirement.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__))
struct em_perf_state *em_pd_get_efficient_state(struct em_perf_domain *pd,
      unsigned long freq)
{
 struct em_perf_state *ps;
 int i;

 for (i = 0; i < pd->nr_perf_states; i++) {
  ps = &pd->table[i];
  if (ps->frequency >= freq) {
   if (pd->flags & ((((1UL))) << (1)) &&
       ps->flags & ((((1UL))) << (0)))
    continue;
   break;
  }
 }

 return ps;
}

/**
 * em_cpu_energy() - Estimates the energy consumed by the CPUs of a
 *		performance domain
 * @pd		: performance domain for which energy has to be estimated
 * @max_util	: highest utilization among CPUs of the domain
 * @sum_util	: sum of the utilization of all CPUs in the domain
 * @allowed_cpu_cap	: maximum allowed CPU capacity for the @pd, which
 *			  might reflect reduced frequency (due to thermal)
 *
 * This function must be used only for CPU devices. There is no validation,
 * i.e. if the EM is a CPU type and has cpumask allocated. It is called from
 * the scheduler code quite frequently and that is why there is not checks.
 *
 * Return: the sum of the energy consumed by the CPUs of the domain assuming
 * a capacity state satisfying the max utilization of the domain.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long em_cpu_energy(struct em_perf_domain *pd,
    unsigned long max_util, unsigned long sum_util,
    unsigned long allowed_cpu_cap)
{
 unsigned long freq, scale_cpu;
 struct em_perf_state *ps;
 int cpu;

 if (!sum_util)
  return 0;

 /*
	 * In order to predict the performance state, map the utilization of
	 * the most utilized CPU of the performance domain to a requested
	 * frequency, like schedutil. Take also into account that the real
	 * frequency might be set lower (due to thermal capping). Thus, clamp
	 * max utilization to the allowed CPU capacity before calculating
	 * effective frequency.
	 */
 cpu = cpumask_first(((struct cpumask *)(1 ? (pd->cpus) : (void *)sizeof(__check_is_bitmap(pd->cpus)))));
 scale_cpu = topology_get_cpu_scale(cpu);
 ps = &pd->table[pd->nr_perf_states - 1];

 max_util = map_util_perf(max_util);
 max_util = __builtin_choose_expr(((!!(sizeof((typeof(max_util) *)1 == (typeof(allowed_cpu_cap) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(max_util) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(allowed_cpu_cap) * 0l)) : (int *)8))))), ((max_util) < (allowed_cpu_cap) ? (max_util) : (allowed_cpu_cap)), ({ typeof(max_util) __UNIQUE_ID___x397 = (max_util); typeof(allowed_cpu_cap) __UNIQUE_ID___y398 = (allowed_cpu_cap); ((__UNIQUE_ID___x397) < (__UNIQUE_ID___y398) ? (__UNIQUE_ID___x397) : (__UNIQUE_ID___y398)); }));
 freq = map_util_freq(max_util, ps->frequency, scale_cpu);

 /*
	 * Find the lowest performance state of the Energy Model above the
	 * requested frequency.
	 */
 ps = em_pd_get_efficient_state(pd, freq);

 /*
	 * The capacity of a CPU in the domain at the performance state (ps)
	 * can be computed as:
	 *
	 *             ps->freq * scale_cpu
	 *   ps->cap = --------------------                          (1)
	 *                 cpu_max_freq
	 *
	 * So, ignoring the costs of idle states (which are not available in
	 * the EM), the energy consumed by this CPU at that performance state
	 * is estimated as:
	 *
	 *             ps->power * cpu_util
	 *   cpu_nrg = --------------------                          (2)
	 *                   ps->cap
	 *
	 * since 'cpu_util / ps->cap' represents its percentage of busy time.
	 *
	 *   NOTE: Although the result of this computation actually is in
	 *         units of power, it can be manipulated as an energy value
	 *         over a scheduling period, since it is assumed to be
	 *         constant during that interval.
	 *
	 * By injecting (1) in (2), 'cpu_nrg' can be re-expressed as a product
	 * of two terms:
	 *
	 *             ps->power * cpu_max_freq   cpu_util
	 *   cpu_nrg = ------------------------ * ---------          (3)
	 *                    ps->freq            scale_cpu
	 *
	 * The first term is static, and is stored in the em_perf_state struct
	 * as 'ps->cost'.
	 *
	 * Since all CPUs of the domain have the same micro-architecture, they
	 * share the same 'ps->cost', and the same CPU capacity. Hence, the
	 * total energy of the domain (which is the simple sum of the energy of
	 * all of its CPUs) can be factorized as:
	 *
	 *            ps->cost * \Sum cpu_util
	 *   pd_nrg = ------------------------                       (4)
	 *                  scale_cpu
	 */
 return (((ps->cost) * (sum_util)) / (scale_cpu));
}

/**
 * em_pd_nr_perf_states() - Get the number of performance states of a perf.
 *				domain
 * @pd		: performance domain for which this must be done
 *
 * Return: the number of performance states in the performance domain table
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int em_pd_nr_perf_states(struct em_perf_domain *pd)
{
 return pd->nr_perf_states;
}
# 17 "./include/linux/device.h" 2


# 1 "./include/linux/klist.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 *	klist.h - Some generic list helpers, extending struct list_head a bit.
 *
 *	Implementations are found in lib/klist.c
 *
 *	Copyright (C) 2005 Patrick Mochel
 */
# 17 "./include/linux/klist.h"
struct klist_node;
struct klist {
 spinlock_t k_lock;
 struct list_head k_list;
 void (*get)(struct klist_node *);
 void (*put)(struct klist_node *);
} __attribute__ ((aligned (sizeof(void *))));
# 34 "./include/linux/klist.h"
extern void klist_init(struct klist *k, void (*get)(struct klist_node *),
         void (*put)(struct klist_node *));

struct klist_node {
 void *n_klist; /* never access directly */
 struct list_head n_node;
 struct kref n_ref;
};

extern void klist_add_tail(struct klist_node *n, struct klist *k);
extern void klist_add_head(struct klist_node *n, struct klist *k);
extern void klist_add_behind(struct klist_node *n, struct klist_node *pos);
extern void klist_add_before(struct klist_node *n, struct klist_node *pos);

extern void klist_del(struct klist_node *n);
extern void klist_remove(struct klist_node *n);

extern int klist_node_attached(struct klist_node *n);


struct klist_iter {
 struct klist *i_klist;
 struct klist_node *i_cur;
};


extern void klist_iter_init(struct klist *k, struct klist_iter *i);
extern void klist_iter_init_node(struct klist *k, struct klist_iter *i,
     struct klist_node *n);
extern void klist_iter_exit(struct klist_iter *i);
extern struct klist_node *klist_prev(struct klist_iter *i);
extern struct klist_node *klist_next(struct klist_iter *i);
# 20 "./include/linux/device.h" 2





# 1 "./include/linux/pm.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 *  pm.h - Power management interface
 *
 *  Copyright (C) 2000 Andrew Henroid
 */
# 20 "./include/linux/pm.h"
/*
 * Callbacks for platform drivers to implement.
 */
extern void (*pm_power_off)(void);

struct device; /* we have a circular dep with device.h */

extern void pm_vt_switch_required(struct device *dev, bool required);
extern void pm_vt_switch_unregister(struct device *dev);
# 41 "./include/linux/pm.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool cxl_mem_active(void)
{
 return false;
}


/*
 * Device power management
 */



extern const char power_group_name[]; /* = "power" */




typedef struct pm_message {
 int event;
} pm_message_t;

/**
 * struct dev_pm_ops - device PM callbacks.
 *
 * @prepare: The principal role of this callback is to prevent new children of
 *	the device from being registered after it has returned (the driver's
 *	subsystem and generally the rest of the kernel is supposed to prevent
 *	new calls to the probe method from being made too once @prepare() has
 *	succeeded).  If @prepare() detects a situation it cannot handle (e.g.
 *	registration of a child already in progress), it may return -EAGAIN, so
 *	that the PM core can execute it once again (e.g. after a new child has
 *	been registered) to recover from the race condition.
 *	This method is executed for all kinds of suspend transitions and is
 *	followed by one of the suspend callbacks: @suspend(), @freeze(), or
 *	@poweroff().  If the transition is a suspend to memory or standby (that
 *	is, not related to hibernation), the return value of @prepare() may be
 *	used to indicate to the PM core to leave the device in runtime suspend
 *	if applicable.  Namely, if @prepare() returns a positive number, the PM
 *	core will understand that as a declaration that the device appears to be
 *	runtime-suspended and it may be left in that state during the entire
 *	transition and during the subsequent resume if all of its descendants
 *	are left in runtime suspend too.  If that happens, @complete() will be
 *	executed directly after @prepare() and it must ensure the proper
 *	functioning of the device after the system resume.
 *	The PM core executes subsystem-level @prepare() for all devices before
 *	starting to invoke suspend callbacks for any of them, so generally
 *	devices may be assumed to be functional or to respond to runtime resume
 *	requests while @prepare() is being executed.  However, device drivers
 *	may NOT assume anything about the availability of user space at that
 *	time and it is NOT valid to request firmware from within @prepare()
 *	(it's too late to do that).  It also is NOT valid to allocate
 *	substantial amounts of memory from @prepare() in the GFP_KERNEL mode.
 *	[To work around these limitations, drivers may register suspend and
 *	hibernation notifiers to be executed before the freezing of tasks.]
 *
 * @complete: Undo the changes made by @prepare().  This method is executed for
 *	all kinds of resume transitions, following one of the resume callbacks:
 *	@resume(), @thaw(), @restore().  Also called if the state transition
 *	fails before the driver's suspend callback: @suspend(), @freeze() or
 *	@poweroff(), can be executed (e.g. if the suspend callback fails for one
 *	of the other devices that the PM core has unsuccessfully attempted to
 *	suspend earlier).
 *	The PM core executes subsystem-level @complete() after it has executed
 *	the appropriate resume callbacks for all devices.  If the corresponding
 *	@prepare() at the beginning of the suspend transition returned a
 *	positive number and the device was left in runtime suspend (without
 *	executing any suspend and resume callbacks for it), @complete() will be
 *	the only callback executed for the device during resume.  In that case,
 *	@complete() must be prepared to do whatever is necessary to ensure the
 *	proper functioning of the device after the system resume.  To this end,
 *	@complete() can check the power.direct_complete flag of the device to
 *	learn whether (unset) or not (set) the previous suspend and resume
 *	callbacks have been executed for it.
 *
 * @suspend: Executed before putting the system into a sleep state in which the
 *	contents of main memory are preserved.  The exact action to perform
 *	depends on the device's subsystem (PM domain, device type, class or bus
 *	type), but generally the device must be quiescent after subsystem-level
 *	@suspend() has returned, so that it doesn't do any I/O or DMA.
 *	Subsystem-level @suspend() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @suspend_late: Continue operations started by @suspend().  For a number of
 *	devices @suspend_late() may point to the same callback routine as the
 *	runtime suspend callback.
 *
 * @resume: Executed after waking the system up from a sleep state in which the
 *	contents of main memory were preserved.  The exact action to perform
 *	depends on the device's subsystem, but generally the driver is expected
 *	to start working again, responding to hardware events and software
 *	requests (the device itself may be left in a low-power state, waiting
 *	for a runtime resume to occur).  The state of the device at the time its
 *	driver's @resume() callback is run depends on the platform and subsystem
 *	the device belongs to.  On most platforms, there are no restrictions on
 *	availability of resources like clocks during @resume().
 *	Subsystem-level @resume() is executed for all devices after invoking
 *	subsystem-level @resume_noirq() for all of them.
 *
 * @resume_early: Prepare to execute @resume().  For a number of devices
 *	@resume_early() may point to the same callback routine as the runtime
 *	resume callback.
 *
 * @freeze: Hibernation-specific, executed before creating a hibernation image.
 *	Analogous to @suspend(), but it should not enable the device to signal
 *	wakeup events or change its power state.  The majority of subsystems
 *	(with the notable exception of the PCI bus type) expect the driver-level
 *	@freeze() to save the device settings in memory to be used by @restore()
 *	during the subsequent resume from hibernation.
 *	Subsystem-level @freeze() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @freeze_late: Continue operations started by @freeze().  Analogous to
 *	@suspend_late(), but it should not enable the device to signal wakeup
 *	events or change its power state.
 *
 * @thaw: Hibernation-specific, executed after creating a hibernation image OR
 *	if the creation of an image has failed.  Also executed after a failing
 *	attempt to restore the contents of main memory from such an image.
 *	Undo the changes made by the preceding @freeze(), so the device can be
 *	operated in the same way as immediately before the call to @freeze().
 *	Subsystem-level @thaw() is executed for all devices after invoking
 *	subsystem-level @thaw_noirq() for all of them.  It also may be executed
 *	directly after @freeze() in case of a transition error.
 *
 * @thaw_early: Prepare to execute @thaw().  Undo the changes made by the
 *	preceding @freeze_late().
 *
 * @poweroff: Hibernation-specific, executed after saving a hibernation image.
 *	Analogous to @suspend(), but it need not save the device's settings in
 *	memory.
 *	Subsystem-level @poweroff() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @poweroff_late: Continue operations started by @poweroff().  Analogous to
 *	@suspend_late(), but it need not save the device's settings in memory.
 *
 * @restore: Hibernation-specific, executed after restoring the contents of main
 *	memory from a hibernation image, analogous to @resume().
 *
 * @restore_early: Prepare to execute @restore(), analogous to @resume_early().
 *
 * @suspend_noirq: Complete the actions started by @suspend().  Carry out any
 *	additional operations required for suspending the device that might be
 *	racing with its driver's interrupt handler, which is guaranteed not to
 *	run while @suspend_noirq() is being executed.
 *	It generally is expected that the device will be in a low-power state
 *	(appropriate for the target system sleep state) after subsystem-level
 *	@suspend_noirq() has returned successfully.  If the device can generate
 *	system wakeup signals and is enabled to wake up the system, it should be
 *	configured to do so at that time.  However, depending on the platform
 *	and device's subsystem, @suspend() or @suspend_late() may be allowed to
 *	put the device into the low-power state and configure it to generate
 *	wakeup signals, in which case it generally is not necessary to define
 *	@suspend_noirq().
 *
 * @resume_noirq: Prepare for the execution of @resume() by carrying out any
 *	operations required for resuming the device that might be racing with
 *	its driver's interrupt handler, which is guaranteed not to run while
 *	@resume_noirq() is being executed.
 *
 * @freeze_noirq: Complete the actions started by @freeze().  Carry out any
 *	additional operations required for freezing the device that might be
 *	racing with its driver's interrupt handler, which is guaranteed not to
 *	run while @freeze_noirq() is being executed.
 *	The power state of the device should not be changed by either @freeze(),
 *	or @freeze_late(), or @freeze_noirq() and it should not be configured to
 *	signal system wakeup by any of these callbacks.
 *
 * @thaw_noirq: Prepare for the execution of @thaw() by carrying out any
 *	operations required for thawing the device that might be racing with its
 *	driver's interrupt handler, which is guaranteed not to run while
 *	@thaw_noirq() is being executed.
 *
 * @poweroff_noirq: Complete the actions started by @poweroff().  Analogous to
 *	@suspend_noirq(), but it need not save the device's settings in memory.
 *
 * @restore_noirq: Prepare for the execution of @restore() by carrying out any
 *	operations required for thawing the device that might be racing with its
 *	driver's interrupt handler, which is guaranteed not to run while
 *	@restore_noirq() is being executed.  Analogous to @resume_noirq().
 *
 * @runtime_suspend: Prepare the device for a condition in which it won't be
 *	able to communicate with the CPU(s) and RAM due to power management.
 *	This need not mean that the device should be put into a low-power state.
 *	For example, if the device is behind a link which is about to be turned
 *	off, the device may remain at full power.  If the device does go to low
 *	power and is capable of generating runtime wakeup events, remote wakeup
 *	(i.e., a hardware mechanism allowing the device to request a change of
 *	its power state via an interrupt) should be enabled for it.
 *
 * @runtime_resume: Put the device into the fully active state in response to a
 *	wakeup event generated by hardware or at the request of software.  If
 *	necessary, put the device into the full-power state and restore its
 *	registers, so that it is fully operational.
 *
 * @runtime_idle: Device appears to be inactive and it might be put into a
 *	low-power state if all of the necessary conditions are satisfied.
 *	Check these conditions, and return 0 if it's appropriate to let the PM
 *	core queue a suspend request for the device.
 *
 * Several device power state transitions are externally visible, affecting
 * the state of pending I/O queues and (for drivers that touch hardware)
 * interrupts, wakeups, DMA, and other hardware state.  There may also be
 * internal transitions to various low-power modes which are transparent
 * to the rest of the driver stack (such as a driver that's ON gating off
 * clocks which are not in active use).
 *
 * The externally visible transitions are handled with the help of callbacks
 * included in this structure in such a way that, typically, two levels of
 * callbacks are involved.  First, the PM core executes callbacks provided by PM
 * domains, device types, classes and bus types.  They are the subsystem-level
 * callbacks expected to execute callbacks provided by device drivers, although
 * they may choose not to do that.  If the driver callbacks are executed, they
 * have to collaborate with the subsystem-level callbacks to achieve the goals
 * appropriate for the given system transition, given transition phase and the
 * subsystem the device belongs to.
 *
 * All of the above callbacks, except for @complete(), return error codes.
 * However, the error codes returned by @resume(), @thaw(), @restore(),
 * @resume_noirq(), @thaw_noirq(), and @restore_noirq(), do not cause the PM
 * core to abort the resume transition during which they are returned.  The
 * error codes returned in those cases are only printed to the system logs for
 * debugging purposes.  Still, it is recommended that drivers only return error
 * codes from their resume methods in case of an unrecoverable failure (i.e.
 * when the device being handled refuses to resume and becomes unusable) to
 * allow the PM core to be modified in the future, so that it can avoid
 * attempting to handle devices that failed to resume and their children.
 *
 * It is allowed to unregister devices while the above callbacks are being
 * executed.  However, a callback routine MUST NOT try to unregister the device
 * it was called for, although it may unregister children of that device (for
 * example, if it detects that a child was unplugged while the system was
 * asleep).
 *
 * There also are callbacks related to runtime power management of devices.
 * Again, as a rule these callbacks are executed by the PM core for subsystems
 * (PM domains, device types, classes and bus types) and the subsystem-level
 * callbacks are expected to invoke the driver callbacks.  Moreover, the exact
 * actions to be performed by a device driver's callbacks generally depend on
 * the platform and subsystem the device belongs to.
 *
 * Refer to Documentation/power/runtime_pm.rst for more information about the
 * role of the @runtime_suspend(), @runtime_resume() and @runtime_idle()
 * callbacks in device runtime power management.
 */
struct dev_pm_ops {
 int (*prepare)(struct device *dev);
 void (*complete)(struct device *dev);
 int (*suspend)(struct device *dev);
 int (*resume)(struct device *dev);
 int (*freeze)(struct device *dev);
 int (*thaw)(struct device *dev);
 int (*poweroff)(struct device *dev);
 int (*restore)(struct device *dev);
 int (*suspend_late)(struct device *dev);
 int (*resume_early)(struct device *dev);
 int (*freeze_late)(struct device *dev);
 int (*thaw_early)(struct device *dev);
 int (*poweroff_late)(struct device *dev);
 int (*restore_early)(struct device *dev);
 int (*suspend_noirq)(struct device *dev);
 int (*resume_noirq)(struct device *dev);
 int (*freeze_noirq)(struct device *dev);
 int (*thaw_noirq)(struct device *dev);
 int (*poweroff_noirq)(struct device *dev);
 int (*restore_noirq)(struct device *dev);
 int (*runtime_suspend)(struct device *dev);
 int (*runtime_resume)(struct device *dev);
 int (*runtime_idle)(struct device *dev);
};
# 392 "./include/linux/pm.h"
/*
 * Use this if you want to use the same suspend and resume callbacks for suspend
 * to RAM and hibernation.
 *
 * If the underlying dev_pm_ops struct symbol has to be exported, use
 * EXPORT_SIMPLE_DEV_PM_OPS() or EXPORT_GPL_SIMPLE_DEV_PM_OPS() instead.
 */
# 419 "./include/linux/pm.h"
/* Deprecated. Use DEFINE_SIMPLE_DEV_PM_OPS() instead. */





/*
 * Use this for defining a set of PM operations to be used in all situations
 * (system suspend, hibernation or runtime PM).
 * NOTE: In general, system suspend callbacks, .suspend() and .resume(), should
 * be different from the corresponding runtime PM callbacks, .runtime_suspend(),
 * and .runtime_resume(), because .runtime_suspend() always works on an already
 * quiescent device, while .suspend() should assume that the device may be doing
 * something when it is called (it should ensure that the device will be
 * quiescent after it has returned).  Therefore it's better to point the "late"
 * suspend and "early" resume callback pointers, .suspend_late() and
 * .resume_early(), to the same routines as .runtime_suspend() and
 * .runtime_resume(), respectively (and analogously for hibernation).
 *
 * Deprecated. You most likely don't want this macro. Use
 * DEFINE_RUNTIME_DEV_PM_OPS() instead.
 */
# 450 "./include/linux/pm.h"
/*
 * PM_EVENT_ messages
 *
 * The following PM_EVENT_ messages are defined for the internal use of the PM
 * core, in order to provide a mechanism allowing the high level suspend and
 * hibernation code to convey the necessary information to the device PM core
 * code:
 *
 * ON		No transition.
 *
 * FREEZE	System is going to hibernate, call ->prepare() and ->freeze()
 *		for all devices.
 *
 * SUSPEND	System is going to suspend, call ->prepare() and ->suspend()
 *		for all devices.
 *
 * HIBERNATE	Hibernation image has been saved, call ->prepare() and
 *		->poweroff() for all devices.
 *
 * QUIESCE	Contents of main memory are going to be restored from a (loaded)
 *		hibernation image, call ->prepare() and ->freeze() for all
 *		devices.
 *
 * RESUME	System is resuming, call ->resume() and ->complete() for all
 *		devices.
 *
 * THAW		Hibernation image has been created, call ->thaw() and
 *		->complete() for all devices.
 *
 * RESTORE	Contents of main memory have been restored from a hibernation
 *		image, call ->restore() and ->complete() for all devices.
 *
 * RECOVER	Creation of a hibernation image or restoration of the main
 *		memory contents from a hibernation image has failed, call
 *		->thaw() and ->complete() for all devices.
 *
 * The following PM_EVENT_ messages are defined for internal use by
 * kernel subsystems.  They are never issued by the PM core.
 *
 * USER_SUSPEND		Manual selective suspend was issued by userspace.
 *
 * USER_RESUME		Manual selective resume was issued by userspace.
 *
 * REMOTE_WAKEUP	Remote-wakeup request was received from the device.
 *
 * AUTO_SUSPEND		Automatic (device idle) runtime suspend was
 *			initiated by the subsystem.
 *
 * AUTO_RESUME		Automatic (device needed) runtime resume was
 *			requested by a driver.
 */
# 546 "./include/linux/pm.h"
/*
 * Device run-time power management status.
 *
 * These status labels are used internally by the PM core to indicate the
 * current status of a device with respect to the PM core operations.  They do
 * not reflect the actual power state of the device or its status as seen by the
 * driver.
 *
 * RPM_ACTIVE		Device is fully operational.  Indicates that the device
 *			bus type's ->runtime_resume() callback has completed
 *			successfully.
 *
 * RPM_SUSPENDED	Device bus type's ->runtime_suspend() callback has
 *			completed successfully.  The device is regarded as
 *			suspended.
 *
 * RPM_RESUMING		Device bus type's ->runtime_resume() callback is being
 *			executed.
 *
 * RPM_SUSPENDING	Device bus type's ->runtime_suspend() callback is being
 *			executed.
 */

enum rpm_status {
 RPM_INVALID = -1,
 RPM_ACTIVE = 0,
 RPM_RESUMING,
 RPM_SUSPENDED,
 RPM_SUSPENDING,
};

/*
 * Device run-time power management request types.
 *
 * RPM_REQ_NONE		Do nothing.
 *
 * RPM_REQ_IDLE		Run the device bus type's ->runtime_idle() callback
 *
 * RPM_REQ_SUSPEND	Run the device bus type's ->runtime_suspend() callback
 *
 * RPM_REQ_AUTOSUSPEND	Same as RPM_REQ_SUSPEND, but not until the device has
 *			been inactive for as long as power.autosuspend_delay
 *
 * RPM_REQ_RESUME	Run the device bus type's ->runtime_resume() callback
 */

enum rpm_request {
 RPM_REQ_NONE = 0,
 RPM_REQ_IDLE,
 RPM_REQ_SUSPEND,
 RPM_REQ_AUTOSUSPEND,
 RPM_REQ_RESUME,
};

struct wakeup_source;
struct wake_irq;
struct pm_domain_data;

struct pm_subsys_data {
 spinlock_t lock;
 unsigned int refcount;

 unsigned int clock_op_might_sleep;
 struct mutex clock_mutex;
 struct list_head clock_list;


 struct pm_domain_data *domain_data;

};

/*
 * Driver flags to control system suspend/resume behavior.
 *
 * These flags can be set by device drivers at the probe time.  They need not be
 * cleared by the drivers as the driver core will take care of that.
 *
 * NO_DIRECT_COMPLETE: Do not apply direct-complete optimization to the device.
 * SMART_PREPARE: Take the driver ->prepare callback return value into account.
 * SMART_SUSPEND: Avoid resuming the device from runtime suspend.
 * MAY_SKIP_RESUME: Allow driver "noirq" and "early" callbacks to be skipped.
 *
 * See Documentation/driver-api/pm/devices.rst for details.
 */





struct dev_pm_info {
 pm_message_t power_state;
 unsigned int can_wakeup:1;
 unsigned int async_suspend:1;
 bool in_dpm_list:1; /* Owned by the PM core */
 bool is_prepared:1; /* Owned by the PM core */
 bool is_suspended:1; /* Ditto */
 bool is_noirq_suspended:1;
 bool is_late_suspended:1;
 bool no_pm:1;
 bool early_init:1; /* Owned by the PM core */
 bool direct_complete:1; /* Owned by the PM core */
 u32 driver_flags;
 spinlock_t lock;

 struct list_head entry;
 struct completion completion;
 struct wakeup_source *wakeup;
 bool wakeup_path:1;
 bool syscore:1;
 bool no_pm_callbacks:1; /* Owned by the PM core */
 unsigned int must_resume:1; /* Owned by the PM core */
 unsigned int may_skip_resume:1; /* Set by subsystems */




 struct hrtimer suspend_timer;
 u64 timer_expires;
 struct work_struct work;
 wait_queue_head_t wait_queue;
 struct wake_irq *wakeirq;
 atomic_t usage_count;
 atomic_t child_count;
 unsigned int disable_depth:3;
 unsigned int idle_notification:1;
 unsigned int request_pending:1;
 unsigned int deferred_resume:1;
 unsigned int needs_force_resume:1;
 unsigned int runtime_auto:1;
 bool ignore_children:1;
 unsigned int no_callbacks:1;
 unsigned int irq_safe:1;
 unsigned int use_autosuspend:1;
 unsigned int timer_autosuspends:1;
 unsigned int memalloc_noio:1;
 unsigned int links_count;
 enum rpm_request request;
 enum rpm_status runtime_status;
 enum rpm_status last_status;
 int runtime_error;
 int autosuspend_delay;
 u64 last_busy;
 u64 active_time;
 u64 suspended_time;
 u64 accounting_timestamp;

 struct pm_subsys_data *subsys_data; /* Owned by the subsystem. */
 void (*set_latency_tolerance)(struct device *, s32);
 struct dev_pm_qos *qos;
};

extern int dev_pm_get_subsys_data(struct device *dev);
extern void dev_pm_put_subsys_data(struct device *dev);

/**
 * struct dev_pm_domain - power management domain representation.
 *
 * @ops: Power management operations associated with this domain.
 * @start: Called when a user needs to start the device via the domain.
 * @detach: Called when removing a device from the domain.
 * @activate: Called before executing probe routines for bus types and drivers.
 * @sync: Called after successful driver probe.
 * @dismiss: Called after unsuccessful driver probe and after driver removal.
 *
 * Power domains provide callbacks that are executed during system suspend,
 * hibernation, system resume and during runtime PM transitions instead of
 * subsystem-level and driver-level callbacks.
 */
struct dev_pm_domain {
 struct dev_pm_ops ops;
 int (*start)(struct device *dev);
 void (*detach)(struct device *dev, bool power_off);
 int (*activate)(struct device *dev);
 void (*sync)(struct device *dev);
 void (*dismiss)(struct device *dev);
};

/*
 * The PM_EVENT_ messages are also used by drivers implementing the legacy
 * suspend framework, based on the ->suspend() and ->resume() callbacks common
 * for suspend and hibernation transitions, according to the rules below.
 */

/* Necessary, because several drivers use PM_EVENT_PRETHAW */


/*
 * One transition is triggered by resume(), after a suspend() call; the
 * message is implicit:
 *
 * ON		Driver starts working again, responding to hardware events
 *		and software requests.  The hardware may have gone through
 *		a power-off reset, or it may have maintained state from the
 *		previous suspend() which the driver will rely on while
 *		resuming.  On most platforms, there are no restrictions on
 *		availability of resources like clocks during resume().
 *
 * Other transitions are triggered by messages sent using suspend().  All
 * these transitions quiesce the driver, so that I/O queues are inactive.
 * That commonly entails turning off IRQs and DMA; there may be rules
 * about how to quiesce that are specific to the bus or the device's type.
 * (For example, network drivers mark the link state.)  Other details may
 * differ according to the message:
 *
 * SUSPEND	Quiesce, enter a low power device state appropriate for
 *		the upcoming system state (such as PCI_D3hot), and enable
 *		wakeup events as appropriate.
 *
 * HIBERNATE	Enter a low power device state appropriate for the hibernation
 *		state (eg. ACPI S4) and enable wakeup events as appropriate.
 *
 * FREEZE	Quiesce operations so that a consistent image can be saved;
 *		but do NOT otherwise enter a low power device state, and do
 *		NOT emit system wakeup events.
 *
 * PRETHAW	Quiesce as if for FREEZE; additionally, prepare for restoring
 *		the system from a snapshot taken after an earlier FREEZE.
 *		Some drivers will need to reset their hardware state instead
 *		of preserving it, to ensure that it's never mistaken for the
 *		state which that earlier snapshot had set up.
 *
 * A minimally power-aware driver treats all messages as SUSPEND, fully
 * reinitializes its device during resume() -- whether or not it was reset
 * during the suspend/resume cycle -- and can't issue wakeup events.
 *
 * More power-aware drivers may also use low power states at runtime as
 * well as during system sleep states like PM_SUSPEND_STANDBY.  They may
 * be able to use wakeup events to exit from runtime low-power states,
 * or from system low-power states such as standby or suspend-to-RAM.
 */


extern void device_pm_lock(void);
extern void dpm_resume_start(pm_message_t state);
extern void dpm_resume_end(pm_message_t state);
extern void dpm_resume_noirq(pm_message_t state);
extern void dpm_resume_early(pm_message_t state);
extern void dpm_resume(pm_message_t state);
extern void dpm_complete(pm_message_t state);

extern void device_pm_unlock(void);
extern int dpm_suspend_end(pm_message_t state);
extern int dpm_suspend_start(pm_message_t state);
extern int dpm_suspend_noirq(pm_message_t state);
extern int dpm_suspend_late(pm_message_t state);
extern int dpm_suspend(pm_message_t state);
extern int dpm_prepare(pm_message_t state);

extern void __suspend_report_result(const char *function, struct device *dev, void *fn, int ret);






extern int device_pm_wait_for_dev(struct device *sub, struct device *dev);
extern void dpm_for_each_dev(void *data, void (*fn)(struct device *, void *));

extern int pm_generic_prepare(struct device *dev);
extern int pm_generic_suspend_late(struct device *dev);
extern int pm_generic_suspend_noirq(struct device *dev);
extern int pm_generic_suspend(struct device *dev);
extern int pm_generic_resume_early(struct device *dev);
extern int pm_generic_resume_noirq(struct device *dev);
extern int pm_generic_resume(struct device *dev);
extern int pm_generic_freeze_noirq(struct device *dev);
extern int pm_generic_freeze_late(struct device *dev);
extern int pm_generic_freeze(struct device *dev);
extern int pm_generic_thaw_noirq(struct device *dev);
extern int pm_generic_thaw_early(struct device *dev);
extern int pm_generic_thaw(struct device *dev);
extern int pm_generic_restore_noirq(struct device *dev);
extern int pm_generic_restore_early(struct device *dev);
extern int pm_generic_restore(struct device *dev);
extern int pm_generic_poweroff_noirq(struct device *dev);
extern int pm_generic_poweroff_late(struct device *dev);
extern int pm_generic_poweroff(struct device *dev);
extern void pm_generic_complete(struct device *dev);

extern bool dev_pm_skip_resume(struct device *dev);
extern bool dev_pm_skip_suspend(struct device *dev);
# 871 "./include/linux/pm.h"
/* How to reorder dpm_list after device_move() */
enum dpm_order {
 DPM_ORDER_NONE,
 DPM_ORDER_DEV_AFTER_PARENT,
 DPM_ORDER_PARENT_BEFORE_DEV,
 DPM_ORDER_DEV_LAST,
};
# 26 "./include/linux/device.h" 2




# 1 "./include/linux/device/bus.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 * bus.h - the bus-specific portions of the driver model
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 * Copyright (c) 2012-2019 Greg Kroah-Hartman <gregkh@linuxfoundation.org>
 * Copyright (c) 2012-2019 Linux Foundation
 *
 * See Documentation/driver-api/driver-model/ for more information.
 */
# 21 "./include/linux/device/bus.h"
struct device_driver;
struct fwnode_handle;

/**
 * struct bus_type - The bus type of the device
 *
 * @name:	The name of the bus.
 * @dev_name:	Used for subsystems to enumerate devices like ("foo%u", dev->id).
 * @dev_root:	Default device to use as the parent.
 * @bus_groups:	Default attributes of the bus.
 * @dev_groups:	Default attributes of the devices on the bus.
 * @drv_groups: Default attributes of the device drivers on the bus.
 * @match:	Called, perhaps multiple times, whenever a new device or driver
 *		is added for this bus. It should return a positive value if the
 *		given device can be handled by the given driver and zero
 *		otherwise. It may also return error code if determining that
 *		the driver supports the device is not possible. In case of
 *		-EPROBE_DEFER it will queue the device for deferred probing.
 * @uevent:	Called when a device is added, removed, or a few other things
 *		that generate uevents to add the environment variables.
 * @probe:	Called when a new device or driver add to this bus, and callback
 *		the specific driver's probe to initial the matched device.
 * @sync_state:	Called to sync device state to software state after all the
 *		state tracking consumers linked to this device (present at
 *		the time of late_initcall) have successfully bound to a
 *		driver. If the device has no consumers, this function will
 *		be called at late_initcall_sync level. If the device has
 *		consumers that are never bound to a driver, this function
 *		will never get called until they do.
 * @remove:	Called when a device removed from this bus.
 * @shutdown:	Called at shut-down time to quiesce the device.
 *
 * @online:	Called to put the device back online (after offlining it).
 * @offline:	Called to put the device offline for hot-removal. May fail.
 *
 * @suspend:	Called when a device on this bus wants to go to sleep mode.
 * @resume:	Called to bring a device on this bus out of sleep mode.
 * @num_vf:	Called to find out how many virtual functions a device on this
 *		bus supports.
 * @dma_configure:	Called to setup DMA configuration on a device on
 *			this bus.
 * @dma_cleanup:	Called to cleanup DMA configuration on a device on
 *			this bus.
 * @pm:		Power management operations of this bus, callback the specific
 *		device driver's pm-ops.
 * @iommu_ops:  IOMMU specific operations for this bus, used to attach IOMMU
 *              driver implementations to a bus and allow the driver to do
 *              bus-specific setup
 * @p:		The private data of the driver core, only the driver core can
 *		touch this.
 * @lock_key:	Lock class key for use by the lock validator
 * @need_parent_lock:	When probing or removing a device on this bus, the
 *			device core should lock the device's parent.
 *
 * A bus is a channel between the processor and one or more devices. For the
 * purposes of the device model, all devices are connected via a bus, even if
 * it is an internal, virtual, "platform" bus. Buses can plug into each other.
 * A USB controller is usually a PCI device, for example. The device model
 * represents the actual connections between buses and the devices they control.
 * A bus is represented by the bus_type structure. It contains the name, the
 * default attributes, the bus' methods, PM operations, and the driver core's
 * private data.
 */
struct bus_type {
 const char *name;
 const char *dev_name;
 struct device *dev_root;
 const struct attribute_group **bus_groups;
 const struct attribute_group **dev_groups;
 const struct attribute_group **drv_groups;

 int (*match)(struct device *dev, struct device_driver *drv);
 int (*uevent)(struct device *dev, struct kobj_uevent_env *env);
 int (*probe)(struct device *dev);
 void (*sync_state)(struct device *dev);
 void (*remove)(struct device *dev);
 void (*shutdown)(struct device *dev);

 int (*online)(struct device *dev);
 int (*offline)(struct device *dev);

 int (*suspend)(struct device *dev, pm_message_t state);
 int (*resume)(struct device *dev);

 int (*num_vf)(struct device *dev);

 int (*dma_configure)(struct device *dev);
 void (*dma_cleanup)(struct device *dev);

 const struct dev_pm_ops *pm;

 const struct iommu_ops *iommu_ops;

 struct subsys_private *p;
 struct lock_class_key lock_key;

 bool need_parent_lock;
};

extern int __attribute__((__warn_unused_result__)) bus_register(struct bus_type *bus);

extern void bus_unregister(struct bus_type *bus);

extern int __attribute__((__warn_unused_result__)) bus_rescan_devices(struct bus_type *bus);

struct bus_attribute {
 struct attribute attr;
 ssize_t (*show)(struct bus_type *bus, char *buf);
 ssize_t (*store)(struct bus_type *bus, const char *buf, size_t count);
};
# 139 "./include/linux/device/bus.h"
extern int __attribute__((__warn_unused_result__)) bus_create_file(struct bus_type *,
     struct bus_attribute *);
extern void bus_remove_file(struct bus_type *, struct bus_attribute *);

/* Generic device matching functions that all busses can use to match with */
int device_match_name(struct device *dev, const void *name);
int device_match_of_node(struct device *dev, const void *np);
int device_match_fwnode(struct device *dev, const void *fwnode);
int device_match_devt(struct device *dev, const void *pdevt);
int device_match_acpi_dev(struct device *dev, const void *adev);
int device_match_acpi_handle(struct device *dev, const void *handle);
int device_match_any(struct device *dev, const void *unused);

/* iterator helpers for buses */
struct subsys_dev_iter {
 struct klist_iter ki;
 const struct device_type *type;
};
void subsys_dev_iter_init(struct subsys_dev_iter *iter,
    struct bus_type *subsys,
    struct device *start,
    const struct device_type *type);
struct device *subsys_dev_iter_next(struct subsys_dev_iter *iter);
void subsys_dev_iter_exit(struct subsys_dev_iter *iter);

int bus_for_each_dev(struct bus_type *bus, struct device *start, void *data,
       int (*fn)(struct device *dev, void *data));
struct device *bus_find_device(struct bus_type *bus, struct device *start,
          const void *data,
          int (*match)(struct device *dev, const void *data));
/**
 * bus_find_device_by_name - device iterator for locating a particular device
 * of a specific name.
 * @bus: bus type
 * @start: Device to begin with
 * @name: name of the device to match
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *bus_find_device_by_name(struct bus_type *bus,
           struct device *start,
           const char *name)
{
 return bus_find_device(bus, start, name, device_match_name);
}

/**
 * bus_find_device_by_of_node : device iterator for locating a particular device
 * matching the of_node.
 * @bus: bus type
 * @np: of_node of the device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
bus_find_device_by_of_node(struct bus_type *bus, const struct device_node *np)
{
 return bus_find_device(bus, ((void *)0), np, device_match_of_node);
}

/**
 * bus_find_device_by_fwnode : device iterator for locating a particular device
 * matching the fwnode.
 * @bus: bus type
 * @fwnode: fwnode of the device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
bus_find_device_by_fwnode(struct bus_type *bus, const struct fwnode_handle *fwnode)
{
 return bus_find_device(bus, ((void *)0), fwnode, device_match_fwnode);
}

/**
 * bus_find_device_by_devt : device iterator for locating a particular device
 * matching the device type.
 * @bus: bus type
 * @devt: device type of the device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *bus_find_device_by_devt(struct bus_type *bus,
           dev_t devt)
{
 return bus_find_device(bus, ((void *)0), &devt, device_match_devt);
}

/**
 * bus_find_next_device - Find the next device after a given device in a
 * given bus.
 * @bus: bus type
 * @cur: device to begin the search with.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
bus_find_next_device(struct bus_type *bus,struct device *cur)
{
 return bus_find_device(bus, cur, ((void *)0), device_match_any);
}


struct acpi_device;

/**
 * bus_find_device_by_acpi_dev : device iterator for locating a particular device
 * matching the ACPI COMPANION device.
 * @bus: bus type
 * @adev: ACPI COMPANION device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
bus_find_device_by_acpi_dev(struct bus_type *bus, const struct acpi_device *adev)
{
 return bus_find_device(bus, ((void *)0), adev, device_match_acpi_dev);
}
# 253 "./include/linux/device/bus.h"
struct device *subsys_find_device_by_id(struct bus_type *bus, unsigned int id,
     struct device *hint);
int bus_for_each_drv(struct bus_type *bus, struct device_driver *start,
       void *data, int (*fn)(struct device_driver *, void *));
void bus_sort_breadthfirst(struct bus_type *bus,
      int (*compare)(const struct device *a,
       const struct device *b));
/*
 * Bus notifiers: Get notified of addition/removal of devices
 * and binding/unbinding of drivers to devices.
 * In the long run, it should be a replacement for the platform
 * notify hooks.
 */
struct notifier_block;

extern int bus_register_notifier(struct bus_type *bus,
     struct notifier_block *nb);
extern int bus_unregister_notifier(struct bus_type *bus,
       struct notifier_block *nb);

/* All 4 notifers below get called with the target struct device *
 * as an argument. Note that those functions are likely to be called
 * with the device lock held in the core, so be careful.
 */
# 289 "./include/linux/device/bus.h"
extern struct kset *bus_get_kset(struct bus_type *bus);
extern struct klist *bus_get_device_klist(struct bus_type *bus);
# 31 "./include/linux/device.h" 2
# 1 "./include/linux/device/class.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 * The class-specific portions of the driver model
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 * Copyright (c) 2012-2019 Greg Kroah-Hartman <gregkh@linuxfoundation.org>
 * Copyright (c) 2012-2019 Linux Foundation
 *
 * See Documentation/driver-api/driver-model/ for more information.
 */
# 22 "./include/linux/device/class.h"
struct device;
struct fwnode_handle;

/**
 * struct class - device classes
 * @name:	Name of the class.
 * @owner:	The module owner.
 * @class_groups: Default attributes of this class.
 * @dev_groups:	Default attributes of the devices that belong to the class.
 * @dev_kobj:	The kobject that represents this class and links it into the hierarchy.
 * @dev_uevent:	Called when a device is added, removed from this class, or a
 *		few other things that generate uevents to add the environment
 *		variables.
 * @devnode:	Callback to provide the devtmpfs.
 * @class_release: Called to release this class.
 * @dev_release: Called to release the device.
 * @shutdown_pre: Called at shut-down time before driver shutdown.
 * @ns_type:	Callbacks so sysfs can detemine namespaces.
 * @namespace:	Namespace of the device belongs to this class.
 * @get_ownership: Allows class to specify uid/gid of the sysfs directories
 *		for the devices belonging to the class. Usually tied to
 *		device's namespace.
 * @pm:		The default device power management operations of this class.
 * @p:		The private data of the driver core, no one other than the
 *		driver core can touch this.
 *
 * A class is a higher-level view of a device that abstracts out low-level
 * implementation details. Drivers may see a SCSI disk or an ATA disk, but,
 * at the class level, they are all simply disks. Classes allow user space
 * to work with devices based on what they do, rather than how they are
 * connected or how they work.
 */
struct class {
 const char *name;
 struct module *owner;

 const struct attribute_group **class_groups;
 const struct attribute_group **dev_groups;
 struct kobject *dev_kobj;

 int (*dev_uevent)(struct device *dev, struct kobj_uevent_env *env);
 char *(*devnode)(struct device *dev, umode_t *mode);

 void (*class_release)(struct class *class);
 void (*dev_release)(struct device *dev);

 int (*shutdown_pre)(struct device *dev);

 const struct kobj_ns_type_operations *ns_type;
 const void *(*namespace)(struct device *dev);

 void (*get_ownership)(struct device *dev, kuid_t *uid, kgid_t *gid);

 const struct dev_pm_ops *pm;

 struct subsys_private *p;
};

struct class_dev_iter {
 struct klist_iter ki;
 const struct device_type *type;
};

extern struct kobject *sysfs_dev_block_kobj;
extern struct kobject *sysfs_dev_char_kobj;
extern int __attribute__((__warn_unused_result__)) __class_register(struct class *class,
      struct lock_class_key *key);
extern void class_unregister(struct class *class);

/* This is a #define to keep the compiler from merging different
 * instances of the __key variable */






struct class_compat;
struct class_compat *class_compat_register(const char *name);
void class_compat_unregister(struct class_compat *cls);
int class_compat_create_link(struct class_compat *cls, struct device *dev,
        struct device *device_link);
void class_compat_remove_link(struct class_compat *cls, struct device *dev,
         struct device *device_link);

extern void class_dev_iter_init(struct class_dev_iter *iter,
    struct class *class,
    struct device *start,
    const struct device_type *type);
extern struct device *class_dev_iter_next(struct class_dev_iter *iter);
extern void class_dev_iter_exit(struct class_dev_iter *iter);

extern int class_for_each_device(struct class *class, struct device *start,
     void *data,
     int (*fn)(struct device *dev, void *data));
extern struct device *class_find_device(struct class *class,
     struct device *start, const void *data,
     int (*match)(struct device *, const void *));

/**
 * class_find_device_by_name - device iterator for locating a particular device
 * of a specific name.
 * @class: class type
 * @name: name of the device to match
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *class_find_device_by_name(struct class *class,
             const char *name)
{
 return class_find_device(class, ((void *)0), name, device_match_name);
}

/**
 * class_find_device_by_of_node : device iterator for locating a particular device
 * matching the of_node.
 * @class: class type
 * @np: of_node of the device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
class_find_device_by_of_node(struct class *class, const struct device_node *np)
{
 return class_find_device(class, ((void *)0), np, device_match_of_node);
}

/**
 * class_find_device_by_fwnode : device iterator for locating a particular device
 * matching the fwnode.
 * @class: class type
 * @fwnode: fwnode of the device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
class_find_device_by_fwnode(struct class *class,
       const struct fwnode_handle *fwnode)
{
 return class_find_device(class, ((void *)0), fwnode, device_match_fwnode);
}

/**
 * class_find_device_by_devt : device iterator for locating a particular device
 * matching the device type.
 * @class: class type
 * @devt: device type of the device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *class_find_device_by_devt(struct class *class,
             dev_t devt)
{
 return class_find_device(class, ((void *)0), &devt, device_match_devt);
}


struct acpi_device;
/**
 * class_find_device_by_acpi_dev : device iterator for locating a particular
 * device matching the ACPI_COMPANION device.
 * @class: class type
 * @adev: ACPI_COMPANION device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
class_find_device_by_acpi_dev(struct class *class, const struct acpi_device *adev)
{
 return class_find_device(class, ((void *)0), adev, device_match_acpi_dev);
}
# 191 "./include/linux/device/class.h"
struct class_attribute {
 struct attribute attr;
 ssize_t (*show)(struct class *class, struct class_attribute *attr,
   char *buf);
 ssize_t (*store)(struct class *class, struct class_attribute *attr,
   const char *buf, size_t count);
};
# 206 "./include/linux/device/class.h"
extern int __attribute__((__warn_unused_result__)) class_create_file_ns(struct class *class,
          const struct class_attribute *attr,
          const void *ns);
extern void class_remove_file_ns(struct class *class,
     const struct class_attribute *attr,
     const void *ns);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) class_create_file(struct class *class,
     const struct class_attribute *attr)
{
 return class_create_file_ns(class, attr, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void class_remove_file(struct class *class,
         const struct class_attribute *attr)
{
 return class_remove_file_ns(class, attr, ((void *)0));
}

/* Simple class attribute that is just a static string */
struct class_attribute_string {
 struct class_attribute attr;
 char *str;
};

/* Currently read-only only */






extern ssize_t show_class_attr_string(struct class *class, struct class_attribute *attr,
                        char *buf);

struct class_interface {
 struct list_head node;
 struct class *class;

 int (*add_dev) (struct device *, struct class_interface *);
 void (*remove_dev) (struct device *, struct class_interface *);
};

extern int __attribute__((__warn_unused_result__)) class_interface_register(struct class_interface *);
extern void class_interface_unregister(struct class_interface *);

extern struct class * __attribute__((__warn_unused_result__)) __class_create(struct module *owner,
        const char *name,
        struct lock_class_key *key);
extern void class_destroy(struct class *cls);

/* This is a #define to keep the compiler from merging different
 * instances of the __key variable */

/**
 * class_create - create a struct class structure
 * @owner: pointer to the module that is to "own" this struct class
 * @name: pointer to a string for the name of this class.
 *
 * This is used to create a struct class pointer that can then be used
 * in calls to device_create().
 *
 * Returns &struct class pointer on success, or ERR_PTR() on error.
 *
 * Note, the pointer created here is to be destroyed when finished by
 * making a call to class_destroy().
 */
# 32 "./include/linux/device.h" 2
# 1 "./include/linux/device/driver.h" 1
// SPDX-License-Identifier: GPL-2.0
/*
 * The driver-specific portions of the driver model
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 * Copyright (c) 2012-2019 Greg Kroah-Hartman <gregkh@linuxfoundation.org>
 * Copyright (c) 2012-2019 Linux Foundation
 *
 * See Documentation/driver-api/driver-model/ for more information.
 */
# 23 "./include/linux/device/driver.h"
/**
 * enum probe_type - device driver probe type to try
 *	Device drivers may opt in for special handling of their
 *	respective probe routines. This tells the core what to
 *	expect and prefer.
 *
 * @PROBE_DEFAULT_STRATEGY: Used by drivers that work equally well
 *	whether probed synchronously or asynchronously.
 * @PROBE_PREFER_ASYNCHRONOUS: Drivers for "slow" devices which
 *	probing order is not essential for booting the system may
 *	opt into executing their probes asynchronously.
 * @PROBE_FORCE_SYNCHRONOUS: Use this to annotate drivers that need
 *	their probe routines to run synchronously with driver and
 *	device registration (with the exception of -EPROBE_DEFER
 *	handling - re-probing always ends up being done asynchronously).
 *
 * Note that the end goal is to switch the kernel to use asynchronous
 * probing by default, so annotating drivers with
 * %PROBE_PREFER_ASYNCHRONOUS is a temporary measure that allows us
 * to speed up boot process while we are validating the rest of the
 * drivers.
 */
enum probe_type {
 PROBE_DEFAULT_STRATEGY,
 PROBE_PREFER_ASYNCHRONOUS,
 PROBE_FORCE_SYNCHRONOUS,
};

/**
 * struct device_driver - The basic device driver structure
 * @name:	Name of the device driver.
 * @bus:	The bus which the device of this driver belongs to.
 * @owner:	The module owner.
 * @mod_name:	Used for built-in modules.
 * @suppress_bind_attrs: Disables bind/unbind via sysfs.
 * @probe_type:	Type of the probe (synchronous or asynchronous) to use.
 * @of_match_table: The open firmware table.
 * @acpi_match_table: The ACPI match table.
 * @probe:	Called to query the existence of a specific device,
 *		whether this driver can work with it, and bind the driver
 *		to a specific device.
 * @sync_state:	Called to sync device state to software state after all the
 *		state tracking consumers linked to this device (present at
 *		the time of late_initcall) have successfully bound to a
 *		driver. If the device has no consumers, this function will
 *		be called at late_initcall_sync level. If the device has
 *		consumers that are never bound to a driver, this function
 *		will never get called until they do.
 * @remove:	Called when the device is removed from the system to
 *		unbind a device from this driver.
 * @shutdown:	Called at shut-down time to quiesce the device.
 * @suspend:	Called to put the device to sleep mode. Usually to a
 *		low power state.
 * @resume:	Called to bring a device from sleep mode.
 * @groups:	Default attributes that get created by the driver core
 *		automatically.
 * @dev_groups:	Additional attributes attached to device instance once
 *		it is bound to the driver.
 * @pm:		Power management operations of the device which matched
 *		this driver.
 * @coredump:	Called when sysfs entry is written to. The device driver
 *		is expected to call the dev_coredump API resulting in a
 *		uevent.
 * @p:		Driver core's private data, no one other than the driver
 *		core can touch this.
 *
 * The device driver-model tracks all of the drivers known to the system.
 * The main reason for this tracking is to enable the driver core to match
 * up drivers with new devices. Once drivers are known objects within the
 * system, however, a number of other things become possible. Device drivers
 * can export information and configuration variables that are independent
 * of any specific device.
 */
struct device_driver {
 const char *name;
 struct bus_type *bus;

 struct module *owner;
 const char *mod_name; /* used for built-in modules */

 bool suppress_bind_attrs; /* disables bind/unbind via sysfs */
 enum probe_type probe_type;

 const struct of_device_id *of_match_table;
 const struct acpi_device_id *acpi_match_table;

 int (*probe) (struct device *dev);
 void (*sync_state)(struct device *dev);
 int (*remove) (struct device *dev);
 void (*shutdown) (struct device *dev);
 int (*suspend) (struct device *dev, pm_message_t state);
 int (*resume) (struct device *dev);
 const struct attribute_group **groups;
 const struct attribute_group **dev_groups;

 const struct dev_pm_ops *pm;
 void (*coredump) (struct device *dev);

 struct driver_private *p;
};


extern int __attribute__((__warn_unused_result__)) driver_register(struct device_driver *drv);
extern void driver_unregister(struct device_driver *drv);

extern struct device_driver *driver_find(const char *name,
      struct bus_type *bus);
extern int driver_probe_done(void);
extern void wait_for_device_probe(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) wait_for_init_devices_probe(void);

/* sysfs interface for exporting driver attributes */

struct driver_attribute {
 struct attribute attr;
 ssize_t (*show)(struct device_driver *driver, char *buf);
 ssize_t (*store)(struct device_driver *driver, const char *buf,
    size_t count);
};
# 150 "./include/linux/device/driver.h"
extern int __attribute__((__warn_unused_result__)) driver_create_file(struct device_driver *driver,
     const struct driver_attribute *attr);
extern void driver_remove_file(struct device_driver *driver,
          const struct driver_attribute *attr);

int driver_set_override(struct device *dev, const char **override,
   const char *s, size_t len);
extern int __attribute__((__warn_unused_result__)) driver_for_each_device(struct device_driver *drv,
            struct device *start,
            void *data,
            int (*fn)(struct device *dev,
        void *));
struct device *driver_find_device(struct device_driver *drv,
      struct device *start, const void *data,
      int (*match)(struct device *dev, const void *data));

/**
 * driver_find_device_by_name - device iterator for locating a particular device
 * of a specific name.
 * @drv: the driver we're iterating
 * @name: name of the device to match
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *driver_find_device_by_name(struct device_driver *drv,
       const char *name)
{
 return driver_find_device(drv, ((void *)0), name, device_match_name);
}

/**
 * driver_find_device_by_of_node- device iterator for locating a particular device
 * by of_node pointer.
 * @drv: the driver we're iterating
 * @np: of_node pointer to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
driver_find_device_by_of_node(struct device_driver *drv,
         const struct device_node *np)
{
 return driver_find_device(drv, ((void *)0), np, device_match_of_node);
}

/**
 * driver_find_device_by_fwnode- device iterator for locating a particular device
 * by fwnode pointer.
 * @drv: the driver we're iterating
 * @fwnode: fwnode pointer to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
driver_find_device_by_fwnode(struct device_driver *drv,
        const struct fwnode_handle *fwnode)
{
 return driver_find_device(drv, ((void *)0), fwnode, device_match_fwnode);
}

/**
 * driver_find_device_by_devt- device iterator for locating a particular device
 * by devt.
 * @drv: the driver we're iterating
 * @devt: devt pointer to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *driver_find_device_by_devt(struct device_driver *drv,
       dev_t devt)
{
 return driver_find_device(drv, ((void *)0), &devt, device_match_devt);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *driver_find_next_device(struct device_driver *drv,
           struct device *start)
{
 return driver_find_device(drv, start, ((void *)0), device_match_any);
}


/**
 * driver_find_device_by_acpi_dev : device iterator for locating a particular
 * device matching the ACPI_COMPANION device.
 * @drv: the driver we're iterating
 * @adev: ACPI_COMPANION device to match.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *
driver_find_device_by_acpi_dev(struct device_driver *drv,
          const struct acpi_device *adev)
{
 return driver_find_device(drv, ((void *)0), adev, device_match_acpi_dev);
}
# 243 "./include/linux/device/driver.h"
extern int driver_deferred_probe_timeout;
void driver_deferred_probe_add(struct device *dev);
int driver_deferred_probe_check_state(struct device *dev);
void driver_init(void);

/**
 * module_driver() - Helper macro for drivers that don't do anything
 * special in module init/exit. This eliminates a lot of boilerplate.
 * Each module may only use this macro once, and calling it replaces
 * module_init() and module_exit().
 *
 * @__driver: driver name
 * @__register: register function for this driver type
 * @__unregister: unregister function for this driver type
 * @...: Additional arguments to be passed to __register and __unregister.
 *
 * Use this macro to construct bus specific macros for registering
 * drivers, and do not use it on its own.
 */
# 274 "./include/linux/device/driver.h"
/**
 * builtin_driver() - Helper macro for drivers that don't do anything
 * special in init and have no exit. This eliminates some boilerplate.
 * Each driver may only use this macro once, and calling it replaces
 * device_initcall (or in some cases, the legacy __initcall).  This is
 * meant to be a direct parallel of module_driver() above but without
 * the __exit stuff that is not used for builtin cases.
 *
 * @__driver: driver name
 * @__register: register function for this driver type
 * @...: Additional arguments to be passed to __register
 *
 * Use this macro to construct bus specific macros for registering
 * drivers, and do not use it on its own.
 */
# 33 "./include/linux/device.h" 2
# 1 "./arch/arm64/include/asm/device.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Copyright (C) 2012 ARM Ltd.
 */



struct dev_archdata {
};

struct pdev_archdata {
};
# 34 "./include/linux/device.h" 2

struct device;
struct device_private;
struct device_driver;
struct driver_private;
struct module;
struct class;
struct subsys_private;
struct device_node;
struct fwnode_handle;
struct iommu_ops;
struct iommu_group;
struct dev_pin_info;
struct dev_iommu;
struct msi_device_data;

/**
 * struct subsys_interface - interfaces to device functions
 * @name:       name of the device function
 * @subsys:     subsystem of the devices to attach to
 * @node:       the list of functions registered at the subsystem
 * @add_dev:    device hookup to device function handler
 * @remove_dev: device hookup to device function handler
 *
 * Simple interfaces attached to a subsystem. Multiple interfaces can
 * attach to a subsystem and its devices. Unlike drivers, they do not
 * exclusively claim or control devices. Interfaces usually represent
 * a specific functionality of a subsystem/class of devices.
 */
struct subsys_interface {
 const char *name;
 struct bus_type *subsys;
 struct list_head node;
 int (*add_dev)(struct device *dev, struct subsys_interface *sif);
 void (*remove_dev)(struct device *dev, struct subsys_interface *sif);
};

int subsys_interface_register(struct subsys_interface *sif);
void subsys_interface_unregister(struct subsys_interface *sif);

int subsys_system_register(struct bus_type *subsys,
      const struct attribute_group **groups);
int subsys_virtual_register(struct bus_type *subsys,
       const struct attribute_group **groups);

/*
 * The type of device, "struct device" is embedded in. A class
 * or bus can contain devices of different types
 * like "partitions" and "disks", "mouse" and "event".
 * This identifies the device type and carries type-specific
 * information, equivalent to the kobj_type of a kobject.
 * If "name" is specified, the uevent will contain it in
 * the DEVTYPE variable.
 */
struct device_type {
 const char *name;
 const struct attribute_group **groups;
 int (*uevent)(struct device *dev, struct kobj_uevent_env *env);
 char *(*devnode)(struct device *dev, umode_t *mode,
    kuid_t *uid, kgid_t *gid);
 void (*release)(struct device *dev);

 const struct dev_pm_ops *pm;
};

/* interface for exporting device attributes */
struct device_attribute {
 struct attribute attr;
 ssize_t (*show)(struct device *dev, struct device_attribute *attr,
   char *buf);
 ssize_t (*store)(struct device *dev, struct device_attribute *attr,
    const char *buf, size_t count);
};

struct dev_ext_attribute {
 struct device_attribute attr;
 void *var;
};

ssize_t device_show_ulong(struct device *dev, struct device_attribute *attr,
     char *buf);
ssize_t device_store_ulong(struct device *dev, struct device_attribute *attr,
      const char *buf, size_t count);
ssize_t device_show_int(struct device *dev, struct device_attribute *attr,
   char *buf);
ssize_t device_store_int(struct device *dev, struct device_attribute *attr,
    const char *buf, size_t count);
ssize_t device_show_bool(struct device *dev, struct device_attribute *attr,
   char *buf);
ssize_t device_store_bool(struct device *dev, struct device_attribute *attr,
    const char *buf, size_t count);
# 154 "./include/linux/device.h"
int device_create_file(struct device *device,
         const struct device_attribute *entry);
void device_remove_file(struct device *dev,
   const struct device_attribute *attr);
bool device_remove_file_self(struct device *dev,
        const struct device_attribute *attr);
int __attribute__((__warn_unused_result__)) device_create_bin_file(struct device *dev,
     const struct bin_attribute *attr);
void device_remove_bin_file(struct device *dev,
       const struct bin_attribute *attr);

/* device resource management */
typedef void (*dr_release_t)(struct device *dev, void *res);
typedef int (*dr_match_t)(struct device *dev, void *res, void *match_data);

void *__devres_alloc_node(dr_release_t release, size_t size, gfp_t gfp,
     int nid, const char *name) __attribute__((__malloc__));





void devres_for_each_res(struct device *dev, dr_release_t release,
    dr_match_t match, void *match_data,
    void (*fn)(struct device *, void *, void *),
    void *data);
void devres_free(void *res);
void devres_add(struct device *dev, void *res);
void *devres_find(struct device *dev, dr_release_t release,
    dr_match_t match, void *match_data);
void *devres_get(struct device *dev, void *new_res,
   dr_match_t match, void *match_data);
void *devres_remove(struct device *dev, dr_release_t release,
      dr_match_t match, void *match_data);
int devres_destroy(struct device *dev, dr_release_t release,
     dr_match_t match, void *match_data);
int devres_release(struct device *dev, dr_release_t release,
     dr_match_t match, void *match_data);

/* devres group */
void * __attribute__((__warn_unused_result__)) devres_open_group(struct device *dev, void *id, gfp_t gfp);
void devres_close_group(struct device *dev, void *id);
void devres_remove_group(struct device *dev, void *id);
int devres_release_group(struct device *dev, void *id);

/* managed devm_k.alloc/kfree for device drivers */
void *devm_kmalloc(struct device *dev, size_t size, gfp_t gfp) __attribute__((__alloc_size__(2))) __attribute__((__malloc__));
void *devm_krealloc(struct device *dev, void *ptr, size_t size,
      gfp_t gfp) __attribute__((__warn_unused_result__)) __attribute__((__alloc_size__(3)));
__attribute__((__format__(printf, 3, 0))) char *devm_kvasprintf(struct device *dev, gfp_t gfp,
         const char *fmt, va_list ap) __attribute__((__malloc__));
__attribute__((__format__(printf, 3, 4))) char *devm_kasprintf(struct device *dev, gfp_t gfp,
        const char *fmt, ...) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *devm_kzalloc(struct device *dev, size_t size, gfp_t gfp)
{
 return devm_kmalloc(dev, size, gfp | (( gfp_t)0x100u));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *devm_kmalloc_array(struct device *dev,
           size_t n, size_t size, gfp_t flags)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(n, size, &bytes))), 0))
  return ((void *)0);

 return devm_kmalloc(dev, bytes, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *devm_kcalloc(struct device *dev,
     size_t n, size_t size, gfp_t flags)
{
 return devm_kmalloc_array(dev, n, size, flags | (( gfp_t)0x100u));
}
void devm_kfree(struct device *dev, const void *p);
char *devm_kstrdup(struct device *dev, const char *s, gfp_t gfp) __attribute__((__malloc__));
const char *devm_kstrdup_const(struct device *dev, const char *s, gfp_t gfp);
void *devm_kmemdup(struct device *dev, const void *src, size_t len, gfp_t gfp)
 __attribute__((__alloc_size__(3)));

unsigned long devm_get_free_pages(struct device *dev,
      gfp_t gfp_mask, unsigned int order);
void devm_free_pages(struct device *dev, unsigned long addr);

void *devm_ioremap_resource(struct device *dev,
        const struct resource *res);
void *devm_ioremap_resource_wc(struct device *dev,
           const struct resource *res);

void *devm_of_iomap(struct device *dev,
       struct device_node *node, int index,
       resource_size_t *size);

/* allows to add/remove a custom action to devres stack */
int devm_add_action(struct device *dev, void (*action)(void *), void *data);
void devm_remove_action(struct device *dev, void (*action)(void *), void *data);
void devm_release_action(struct device *dev, void (*action)(void *), void *data);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int devm_add_action_or_reset(struct device *dev,
        void (*action)(void *), void *data)
{
 int ret;

 ret = devm_add_action(dev, action, data);
 if (ret)
  action(data);

 return ret;
}

/**
 * devm_alloc_percpu - Resource-managed alloc_percpu
 * @dev: Device to allocate per-cpu memory for
 * @type: Type to allocate per-cpu memory for
 *
 * Managed alloc_percpu. Per-cpu memory allocated with this function is
 * automatically freed on driver detach.
 *
 * RETURNS:
 * Pointer to allocated memory on success, NULL on failure.
 */




void /* nothing */ *__devm_alloc_percpu(struct device *dev, size_t size,
       size_t align);
void devm_free_percpu(struct device *dev, void /* nothing */ *pdata);

struct device_dma_parameters {
 /*
	 * a low level driver may set these to teach IOMMU code about
	 * sg limitations.
	 */
 unsigned int max_segment_size;
 unsigned int min_align_mask;
 unsigned long segment_boundary_mask;
};

/**
 * enum device_link_state - Device link states.
 * @DL_STATE_NONE: The presence of the drivers is not being tracked.
 * @DL_STATE_DORMANT: None of the supplier/consumer drivers is present.
 * @DL_STATE_AVAILABLE: The supplier driver is present, but the consumer is not.
 * @DL_STATE_CONSUMER_PROBE: The consumer is probing (supplier driver present).
 * @DL_STATE_ACTIVE: Both the supplier and consumer drivers are present.
 * @DL_STATE_SUPPLIER_UNBIND: The supplier driver is unbinding.
 */
enum device_link_state {
 DL_STATE_NONE = -1,
 DL_STATE_DORMANT = 0,
 DL_STATE_AVAILABLE,
 DL_STATE_CONSUMER_PROBE,
 DL_STATE_ACTIVE,
 DL_STATE_SUPPLIER_UNBIND,
};

/*
 * Device link flags.
 *
 * STATELESS: The core will not remove this link automatically.
 * AUTOREMOVE_CONSUMER: Remove the link automatically on consumer driver unbind.
 * PM_RUNTIME: If set, the runtime PM framework will use this link.
 * RPM_ACTIVE: Run pm_runtime_get_sync() on the supplier during link creation.
 * AUTOREMOVE_SUPPLIER: Remove the link automatically on supplier driver unbind.
 * AUTOPROBE_CONSUMER: Probe consumer driver automatically after supplier binds.
 * MANAGED: The core tracks presence of supplier/consumer drivers (internal).
 * SYNC_STATE_ONLY: Link only affects sync_state() behavior.
 * INFERRED: Inferred from data (eg: firmware) and not from driver actions.
 */
# 332 "./include/linux/device.h"
/**
 * enum dl_dev_state - Device driver presence tracking information.
 * @DL_DEV_NO_DRIVER: There is no driver attached to the device.
 * @DL_DEV_PROBING: A driver is probing.
 * @DL_DEV_DRIVER_BOUND: The driver has been bound to the device.
 * @DL_DEV_UNBINDING: The driver is unbinding from the device.
 */
enum dl_dev_state {
 DL_DEV_NO_DRIVER = 0,
 DL_DEV_PROBING,
 DL_DEV_DRIVER_BOUND,
 DL_DEV_UNBINDING,
};

/**
 * enum device_removable - Whether the device is removable. The criteria for a
 * device to be classified as removable is determined by its subsystem or bus.
 * @DEVICE_REMOVABLE_NOT_SUPPORTED: This attribute is not supported for this
 *				    device (default).
 * @DEVICE_REMOVABLE_UNKNOWN:  Device location is Unknown.
 * @DEVICE_FIXED: Device is not removable by the user.
 * @DEVICE_REMOVABLE: Device is removable by the user.
 */
enum device_removable {
 DEVICE_REMOVABLE_NOT_SUPPORTED = 0, /* must be 0 */
 DEVICE_REMOVABLE_UNKNOWN,
 DEVICE_FIXED,
 DEVICE_REMOVABLE,
};

/**
 * struct dev_links_info - Device data related to device links.
 * @suppliers: List of links to supplier devices.
 * @consumers: List of links to consumer devices.
 * @defer_sync: Hook to global list of devices that have deferred sync_state.
 * @status: Driver status information.
 */
struct dev_links_info {
 struct list_head suppliers;
 struct list_head consumers;
 struct list_head defer_sync;
 enum dl_dev_state status;
};

/**
 * struct dev_msi_info - Device data related to MSI
 * @domain:	The MSI interrupt domain associated to the device
 * @data:	Pointer to MSI device data
 */
struct dev_msi_info {

 struct irq_domain *domain;
 struct msi_device_data *data;

};

/**
 * enum device_physical_location_panel - Describes which panel surface of the
 * system's housing the device connection point resides on.
 * @DEVICE_PANEL_TOP: Device connection point is on the top panel.
 * @DEVICE_PANEL_BOTTOM: Device connection point is on the bottom panel.
 * @DEVICE_PANEL_LEFT: Device connection point is on the left panel.
 * @DEVICE_PANEL_RIGHT: Device connection point is on the right panel.
 * @DEVICE_PANEL_FRONT: Device connection point is on the front panel.
 * @DEVICE_PANEL_BACK: Device connection point is on the back panel.
 * @DEVICE_PANEL_UNKNOWN: The panel with device connection point is unknown.
 */
enum device_physical_location_panel {
 DEVICE_PANEL_TOP,
 DEVICE_PANEL_BOTTOM,
 DEVICE_PANEL_LEFT,
 DEVICE_PANEL_RIGHT,
 DEVICE_PANEL_FRONT,
 DEVICE_PANEL_BACK,
 DEVICE_PANEL_UNKNOWN,
};

/**
 * enum device_physical_location_vertical_position - Describes vertical
 * position of the device connection point on the panel surface.
 * @DEVICE_VERT_POS_UPPER: Device connection point is at upper part of panel.
 * @DEVICE_VERT_POS_CENTER: Device connection point is at center part of panel.
 * @DEVICE_VERT_POS_LOWER: Device connection point is at lower part of panel.
 */
enum device_physical_location_vertical_position {
 DEVICE_VERT_POS_UPPER,
 DEVICE_VERT_POS_CENTER,
 DEVICE_VERT_POS_LOWER,
};

/**
 * enum device_physical_location_horizontal_position - Describes horizontal
 * position of the device connection point on the panel surface.
 * @DEVICE_HORI_POS_LEFT: Device connection point is at left part of panel.
 * @DEVICE_HORI_POS_CENTER: Device connection point is at center part of panel.
 * @DEVICE_HORI_POS_RIGHT: Device connection point is at right part of panel.
 */
enum device_physical_location_horizontal_position {
 DEVICE_HORI_POS_LEFT,
 DEVICE_HORI_POS_CENTER,
 DEVICE_HORI_POS_RIGHT,
};

/**
 * struct device_physical_location - Device data related to physical location
 * of the device connection point.
 * @panel: Panel surface of the system's housing that the device connection
 *         point resides on.
 * @vertical_position: Vertical position of the device connection point within
 *                     the panel.
 * @horizontal_position: Horizontal position of the device connection point
 *                       within the panel.
 * @dock: Set if the device connection point resides in a docking station or
 *        port replicator.
 * @lid: Set if this device connection point resides on the lid of laptop
 *       system.
 */
struct device_physical_location {
 enum device_physical_location_panel panel;
 enum device_physical_location_vertical_position vertical_position;
 enum device_physical_location_horizontal_position horizontal_position;
 bool dock;
 bool lid;
};

/**
 * struct device - The basic device structure
 * @parent:	The device's "parent" device, the device to which it is attached.
 * 		In most cases, a parent device is some sort of bus or host
 * 		controller. If parent is NULL, the device, is a top-level device,
 * 		which is not usually what you want.
 * @p:		Holds the private data of the driver core portions of the device.
 * 		See the comment of the struct device_private for detail.
 * @kobj:	A top-level, abstract class from which other classes are derived.
 * @init_name:	Initial name of the device.
 * @type:	The type of device.
 * 		This identifies the device type and carries type-specific
 * 		information.
 * @mutex:	Mutex to synchronize calls to its driver.
 * @bus:	Type of bus device is on.
 * @driver:	Which driver has allocated this
 * @platform_data: Platform data specific to the device.
 * 		Example: For devices on custom boards, as typical of embedded
 * 		and SOC based hardware, Linux often uses platform_data to point
 * 		to board-specific structures describing devices and how they
 * 		are wired.  That can include what ports are available, chip
 * 		variants, which GPIO pins act in what additional roles, and so
 * 		on.  This shrinks the "Board Support Packages" (BSPs) and
 * 		minimizes board-specific #ifdefs in drivers.
 * @driver_data: Private pointer for driver specific info.
 * @links:	Links to suppliers and consumers of this device.
 * @power:	For device power management.
 *		See Documentation/driver-api/pm/devices.rst for details.
 * @pm_domain:	Provide callbacks that are executed during system suspend,
 * 		hibernation, system resume and during runtime PM transitions
 * 		along with subsystem-level and driver-level callbacks.
 * @em_pd:	device's energy model performance domain
 * @pins:	For device pin management.
 *		See Documentation/driver-api/pin-control.rst for details.
 * @msi:	MSI related data
 * @numa_node:	NUMA node this device is close to.
 * @dma_ops:    DMA mapping operations for this device.
 * @dma_mask:	Dma mask (if dma'ble device).
 * @coherent_dma_mask: Like dma_mask, but for alloc_coherent mapping as not all
 * 		hardware supports 64-bit addresses for consistent allocations
 * 		such descriptors.
 * @bus_dma_limit: Limit of an upstream bridge or bus which imposes a smaller
 *		DMA limit than the device itself supports.
 * @dma_range_map: map for DMA memory ranges relative to that of RAM
 * @dma_parms:	A low level driver may set these to teach IOMMU code about
 * 		segment limitations.
 * @dma_pools:	Dma pools (if dma'ble device).
 * @dma_mem:	Internal for coherent mem override.
 * @cma_area:	Contiguous memory area for dma allocations
 * @dma_io_tlb_mem: Pointer to the swiotlb pool used.  Not for driver use.
 * @archdata:	For arch-specific additions.
 * @of_node:	Associated device tree node.
 * @fwnode:	Associated device node supplied by platform firmware.
 * @devt:	For creating the sysfs "dev".
 * @id:		device instance
 * @devres_lock: Spinlock to protect the resource of the device.
 * @devres_head: The resources list of the device.
 * @knode_class: The node used to add the device to the class list.
 * @class:	The class of the device.
 * @groups:	Optional attribute groups.
 * @release:	Callback to free the device after all references have
 * 		gone away. This should be set by the allocator of the
 * 		device (i.e. the bus driver that discovered the device).
 * @iommu_group: IOMMU group the device belongs to.
 * @iommu:	Per device generic IOMMU runtime data
 * @physical_location: Describes physical location of the device connection
 *		point in the system housing.
 * @removable:  Whether the device can be removed from the system. This
 *              should be set by the subsystem / bus driver that discovered
 *              the device.
 *
 * @offline_disabled: If set, the device is permanently online.
 * @offline:	Set after successful invocation of bus type's .offline().
 * @of_node_reused: Set if the device-tree node is shared with an ancestor
 *              device.
 * @state_synced: The hardware state of this device has been synced to match
 *		  the software state of this device by calling the driver/bus
 *		  sync_state() callback.
 * @can_match:	The device has matched with a driver at least once or it is in
 *		a bus (like AMBA) which can't check for matching drivers until
 *		other devices probe successfully.
 * @dma_coherent: this particular device is dma coherent, even if the
 *		architecture supports non-coherent devices.
 * @dma_ops_bypass: If set to %true then the dma_ops are bypassed for the
 *		streaming DMA operations (->map_* / ->unmap_* / ->sync_*),
 *		and optionall (if the coherent mask is large enough) also
 *		for dma allocations.  This flag is managed by the dma ops
 *		instance from ->dma_supported.
 *
 * At the lowest level, every device in a Linux system is represented by an
 * instance of struct device. The device structure contains the information
 * that the device model core needs to model the system. Most subsystems,
 * however, track additional information about the devices they host. As a
 * result, it is rare for devices to be represented by bare device structures;
 * instead, that structure, like kobject structures, is usually embedded within
 * a higher-level representation of the device.
 */
struct device {
 struct kobject kobj;
 struct device *parent;

 struct device_private *p;

 const char *init_name; /* initial name of the device */
 const struct device_type *type;

 struct bus_type *bus; /* type of bus device is on */
 struct device_driver *driver; /* which driver has allocated this
					   device */
 void *platform_data; /* Platform specific data, device
					   core doesn't touch it */
 void *driver_data; /* Driver data, set and get with
					   dev_set_drvdata/dev_get_drvdata */
 struct mutex mutex; /* mutex to synchronize calls to
					 * its driver.
					 */

 struct dev_links_info links;
 struct dev_pm_info power;
 struct dev_pm_domain *pm_domain;


 struct em_perf_domain *em_pd;



 struct dev_pin_info *pins;

 struct dev_msi_info msi;

 const struct dma_map_ops *dma_ops;

 u64 *dma_mask; /* dma mask (if dma'able device) */
 u64 coherent_dma_mask;/* Like dma_mask, but for
					     alloc_coherent mappings as
					     not all hardware supports
					     64 bit addresses for consistent
					     allocations such descriptors. */
 u64 bus_dma_limit; /* upstream dma constraint */
 const struct bus_dma_region *dma_range_map;

 struct device_dma_parameters *dma_parms;

 struct list_head dma_pools; /* dma pools (if dma'ble) */


 struct dma_coherent_mem *dma_mem; /* internal for coherent mem
					     override */


 struct cma *cma_area; /* contiguous memory area for dma
					   allocations */


 struct io_tlb_mem *dma_io_tlb_mem;

 /* arch specific additions */
 struct dev_archdata archdata;

 struct device_node *of_node; /* associated device tree node */
 struct fwnode_handle *fwnode; /* firmware device node */


 int numa_node; /* NUMA node this device is close to */

 dev_t devt; /* dev_t, creates the sysfs "dev" */
 u32 id; /* device instance */

 spinlock_t devres_lock;
 struct list_head devres_head;

 struct class *class;
 const struct attribute_group **groups; /* optional groups */

 void (*release)(struct device *dev);
 struct iommu_group *iommu_group;
 struct dev_iommu *iommu;

 struct device_physical_location *physical_location;

 enum device_removable removable;

 bool offline_disabled:1;
 bool offline:1;
 bool of_node_reused:1;
 bool state_synced:1;
 bool can_match:1;



 bool dma_coherent:1;




};

/**
 * struct device_link - Device link representation.
 * @supplier: The device on the supplier end of the link.
 * @s_node: Hook to the supplier device's list of links to consumers.
 * @consumer: The device on the consumer end of the link.
 * @c_node: Hook to the consumer device's list of links to suppliers.
 * @link_dev: device used to expose link details in sysfs
 * @status: The state of the link (with respect to the presence of drivers).
 * @flags: Link flags.
 * @rpm_active: Whether or not the consumer device is runtime-PM-active.
 * @kref: Count repeated addition of the same link.
 * @rm_work: Work structure used for removing the link.
 * @supplier_preactivated: Supplier has been made active before consumer probe.
 */
struct device_link {
 struct device *supplier;
 struct list_head s_node;
 struct device *consumer;
 struct list_head c_node;
 struct device link_dev;
 enum device_link_state status;
 u32 flags;
 refcount_t rpm_active;
 struct kref kref;
 struct work_struct rm_work;
 bool supplier_preactivated; /* Owned by consumer probe. */
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device *kobj_to_dev(struct kobject *kobj)
{
 return ({ void *__mptr = (void *)(kobj); _Static_assert(__builtin_types_compatible_p(typeof(*(kobj)), typeof(((struct device *)0)->kobj)) || __builtin_types_compatible_p(typeof(*(kobj)), typeof(void)), "pointer type mismatch in container_of()"); ((struct device *)(__mptr - __builtin_offsetof(struct device, kobj))); });
}

/**
 * device_iommu_mapped - Returns true when the device DMA is translated
 *			 by an IOMMU
 * @dev: Device to perform the check on
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool device_iommu_mapped(struct device *dev)
{
 return (dev->iommu_group != ((void *)0));
}

/* Get the wakeup routines, which depend on struct device */
# 1 "./include/linux/pm_wakeup.h" 1
/* SPDX-License-Identifier: GPL-2.0-or-later */
/*
 *  pm_wakeup.h - Power management wakeup interface
 *
 *  Copyright (C) 2008 Alan Stern
 *  Copyright (C) 2010 Rafael J. Wysocki, Novell Inc.
 */
# 18 "./include/linux/pm_wakeup.h"
struct wake_irq;

/**
 * struct wakeup_source - Representation of wakeup sources
 *
 * @name: Name of the wakeup source
 * @id: Wakeup source id
 * @entry: Wakeup source list entry
 * @lock: Wakeup source lock
 * @wakeirq: Optional device specific wakeirq
 * @timer: Wakeup timer list
 * @timer_expires: Wakeup timer expiration
 * @total_time: Total time this wakeup source has been active.
 * @max_time: Maximum time this wakeup source has been continuously active.
 * @last_time: Monotonic clock when the wakeup source's was touched last time.
 * @prevent_sleep_time: Total time this source has been preventing autosleep.
 * @event_count: Number of signaled wakeup events.
 * @active_count: Number of times the wakeup source was activated.
 * @relax_count: Number of times the wakeup source was deactivated.
 * @expire_count: Number of times the wakeup source's timeout has expired.
 * @wakeup_count: Number of times the wakeup source might abort suspend.
 * @dev: Struct device for sysfs statistics about the wakeup source.
 * @active: Status of the wakeup source.
 * @autosleep_enabled: Autosleep is active, so update @prevent_sleep_time.
 */
struct wakeup_source {
 const char *name;
 int id;
 struct list_head entry;
 spinlock_t lock;
 struct wake_irq *wakeirq;
 struct timer_list timer;
 unsigned long timer_expires;
 ktime_t total_time;
 ktime_t max_time;
 ktime_t last_time;
 ktime_t start_prevent_time;
 ktime_t prevent_sleep_time;
 unsigned long event_count;
 unsigned long active_count;
 unsigned long relax_count;
 unsigned long expire_count;
 unsigned long wakeup_count;
 struct device *dev;
 bool active:1;
 bool autosleep_enabled:1;
};
# 73 "./include/linux/pm_wakeup.h"
/*
 * Changes to device_may_wakeup take effect on the next pm state change.
 */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool device_can_wakeup(struct device *dev)
{
 return dev->power.can_wakeup;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool device_may_wakeup(struct device *dev)
{
 return dev->power.can_wakeup && !!dev->power.wakeup;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool device_wakeup_path(struct device *dev)
{
 return dev->power.wakeup_path;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_set_wakeup_path(struct device *dev)
{
 dev->power.wakeup_path = true;
}

/* drivers/base/power/wakeup.c */
extern struct wakeup_source *wakeup_source_create(const char *name);
extern void wakeup_source_destroy(struct wakeup_source *ws);
extern void wakeup_source_add(struct wakeup_source *ws);
extern void wakeup_source_remove(struct wakeup_source *ws);
extern struct wakeup_source *wakeup_source_register(struct device *dev,
          const char *name);
extern void wakeup_source_unregister(struct wakeup_source *ws);
extern int wakeup_sources_read_lock(void);
extern void wakeup_sources_read_unlock(int idx);
extern struct wakeup_source *wakeup_sources_walk_start(void);
extern struct wakeup_source *wakeup_sources_walk_next(struct wakeup_source *ws);
extern int device_wakeup_enable(struct device *dev);
extern int device_wakeup_disable(struct device *dev);
extern void device_set_wakeup_capable(struct device *dev, bool capable);
extern int device_set_wakeup_enable(struct device *dev, bool enable);
extern void __pm_stay_awake(struct wakeup_source *ws);
extern void pm_stay_awake(struct device *dev);
extern void __pm_relax(struct wakeup_source *ws);
extern void pm_relax(struct device *dev);
extern void pm_wakeup_ws_event(struct wakeup_source *ws, unsigned int msec, bool hard);
extern void pm_wakeup_dev_event(struct device *dev, unsigned int msec, bool hard);
# 197 "./include/linux/pm_wakeup.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __pm_wakeup_event(struct wakeup_source *ws, unsigned int msec)
{
 return pm_wakeup_ws_event(ws, msec, false);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pm_wakeup_event(struct device *dev, unsigned int msec)
{
 return pm_wakeup_dev_event(dev, msec, false);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void pm_wakeup_hard_event(struct device *dev)
{
 return pm_wakeup_dev_event(dev, 0, true);
}

/**
 * device_init_wakeup - Device wakeup initialization.
 * @dev: Device to handle.
 * @enable: Whether or not to enable @dev as a wakeup device.
 *
 * By default, most devices should leave wakeup disabled.  The exceptions are
 * devices that everyone expects to be wakeup sources: keyboards, power buttons,
 * possibly network interfaces, etc.  Also, devices that don't generate their
 * own wakeup requests but merely forward requests from one bus to another
 * (like PCI bridges) should have wakeup enabled by default.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int device_init_wakeup(struct device *dev, bool enable)
{
 if (enable) {
  device_set_wakeup_capable(dev, true);
  return device_wakeup_enable(dev);
 } else {
  device_wakeup_disable(dev);
  device_set_wakeup_capable(dev, false);
  return 0;
 }
}
# 699 "./include/linux/device.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *dev_name(const struct device *dev)
{
 /* Use the init name until the kobject becomes available */
 if (dev->init_name)
  return dev->init_name;

 return kobject_name(&dev->kobj);
}

/**
 * dev_bus_name - Return a device's bus/class name, if at all possible
 * @dev: struct device to get the bus/class name of
 *
 * Will return the name of the bus/class the device is attached to.  If it is
 * not attached to a bus/class, an empty string will be returned.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *dev_bus_name(const struct device *dev)
{
 return dev->bus ? dev->bus->name : (dev->class ? dev->class->name : "");
}

__attribute__((__format__(printf, 2, 3))) int dev_set_name(struct device *dev, const char *name, ...);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int dev_to_node(struct device *dev)
{
 return dev->numa_node;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void set_dev_node(struct device *dev, int node)
{
 dev->numa_node = node;
}
# 742 "./include/linux/device.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct irq_domain *dev_get_msi_domain(const struct device *dev)
{

 return dev->msi.domain;



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dev_set_msi_domain(struct device *dev, struct irq_domain *d)
{

 dev->msi.domain = d;

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *dev_get_drvdata(const struct device *dev)
{
 return dev->driver_data;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dev_set_drvdata(struct device *dev, void *data)
{
 dev->driver_data = data;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pm_subsys_data *dev_to_psd(struct device *dev)
{
 return dev ? dev->power.subsys_data : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int dev_get_uevent_suppress(const struct device *dev)
{
 return dev->kobj.uevent_suppress;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dev_set_uevent_suppress(struct device *dev, int val)
{
 dev->kobj.uevent_suppress = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int device_is_registered(struct device *dev)
{
 return dev->kobj.state_in_sysfs;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_enable_async_suspend(struct device *dev)
{
 if (!dev->power.is_prepared)
  dev->power.async_suspend = true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_disable_async_suspend(struct device *dev)
{
 if (!dev->power.is_prepared)
  dev->power.async_suspend = false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool device_async_suspend_enabled(struct device *dev)
{
 return !!dev->power.async_suspend;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool device_pm_not_required(struct device *dev)
{
 return dev->power.no_pm;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_set_pm_not_required(struct device *dev)
{
 dev->power.no_pm = true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dev_pm_syscore_device(struct device *dev, bool val)
{

 dev->power.syscore = val;

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dev_pm_set_driver_flags(struct device *dev, u32 flags)
{
 dev->power.driver_flags = flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dev_pm_test_driver_flags(struct device *dev, u32 flags)
{
 return !!(dev->power.driver_flags & flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_lock(struct device *dev)
{
 mutex_lock(&dev->mutex);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int device_lock_interruptible(struct device *dev)
{
 return mutex_lock_interruptible(&dev->mutex);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int device_trylock(struct device *dev)
{
 return mutex_trylock(&dev->mutex);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_unlock(struct device *dev)
{
 mutex_unlock(&dev->mutex);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_lock_assert(struct device *dev)
{
 do { (void)(&dev->mutex); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct device_node *dev_of_node(struct device *dev)
{
 if (!1 || !dev)
  return ((void *)0);
 return dev->of_node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dev_has_sync_state(struct device *dev)
{
 if (!dev)
  return false;
 if (dev->driver && dev->driver->sync_state)
  return true;
 if (dev->bus && dev->bus->sync_state)
  return true;
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void dev_set_removable(struct device *dev,
         enum device_removable removable)
{
 dev->removable = removable;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dev_is_removable(struct device *dev)
{
 return dev->removable == DEVICE_REMOVABLE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool dev_removable_is_valid(struct device *dev)
{
 return dev->removable != DEVICE_REMOVABLE_NOT_SUPPORTED;
}

/*
 * High level routines for use by the bus drivers
 */
int __attribute__((__warn_unused_result__)) device_register(struct device *dev);
void device_unregister(struct device *dev);
void device_initialize(struct device *dev);
int __attribute__((__warn_unused_result__)) device_add(struct device *dev);
void device_del(struct device *dev);
int device_for_each_child(struct device *dev, void *data,
     int (*fn)(struct device *dev, void *data));
int device_for_each_child_reverse(struct device *dev, void *data,
      int (*fn)(struct device *dev, void *data));
struct device *device_find_child(struct device *dev, void *data,
     int (*match)(struct device *dev, void *data));
struct device *device_find_child_by_name(struct device *parent,
      const char *name);
struct device *device_find_any_child(struct device *parent);

int device_rename(struct device *dev, const char *new_name);
int device_move(struct device *dev, struct device *new_parent,
  enum dpm_order dpm_order);
int device_change_owner(struct device *dev, kuid_t kuid, kgid_t kgid);
const char *device_get_devnode(struct device *dev, umode_t *mode, kuid_t *uid,
          kgid_t *gid, const char **tmp);
int device_is_dependent(struct device *dev, void *target);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool device_supports_offline(struct device *dev)
{
 return dev->bus && dev->bus->offline && dev->bus->online;
}







/**
 * device_lock_set_class - Specify a temporary lock class while a device
 *			   is attached to a driver
 * @dev: device to modify
 * @key: lock class key data
 *
 * This must be called with the device_lock() already held, for example
 * from driver ->probe(). Take care to only override the default
 * lockdep_no_validate class.
 */
# 951 "./include/linux/device.h"
/**
 * device_lock_reset_class - Return a device to the default lockdep novalidate state
 * @dev: device to modify
 *
 * This must be called with the device_lock() already held, for example
 * from driver ->remove().
 */







void lock_device_hotplug(void);
void unlock_device_hotplug(void);
int lock_device_hotplug_sysfs(void);
int device_offline(struct device *dev);
int device_online(struct device *dev);
void set_primary_fwnode(struct device *dev, struct fwnode_handle *fwnode);
void set_secondary_fwnode(struct device *dev, struct fwnode_handle *fwnode);
void device_set_of_node_from_dev(struct device *dev, const struct device *dev2);
void device_set_node(struct device *dev, struct fwnode_handle *fwnode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int dev_num_vf(struct device *dev)
{
 if (dev->bus && dev->bus->num_vf)
  return dev->bus->num_vf(dev);
 return 0;
}

/*
 * Root device objects for grouping under /sys/devices
 */
struct device *__root_device_register(const char *name, struct module *owner);

/* This is a macro to avoid include problems with THIS_MODULE */



void root_device_unregister(struct device *root);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *dev_get_platdata(const struct device *dev)
{
 return dev->platform_data;
}

/*
 * Manual binding of a device to driver. See drivers/base/bus.c
 * for information on use.
 */
int __attribute__((__warn_unused_result__)) device_driver_attach(struct device_driver *drv,
          struct device *dev);
int __attribute__((__warn_unused_result__)) device_bind_driver(struct device *dev);
void device_release_driver(struct device *dev);
int __attribute__((__warn_unused_result__)) device_attach(struct device *dev);
int __attribute__((__warn_unused_result__)) driver_attach(struct device_driver *drv);
void device_initial_probe(struct device *dev);
int __attribute__((__warn_unused_result__)) device_reprobe(struct device *dev);

bool device_is_bound(struct device *dev);

/*
 * Easy functions for dynamically creating devices on the fly
 */
__attribute__((__format__(printf, 5, 6))) struct device *
device_create(struct class *cls, struct device *parent, dev_t devt,
       void *drvdata, const char *fmt, ...);
__attribute__((__format__(printf, 6, 7))) struct device *
device_create_with_groups(struct class *cls, struct device *parent, dev_t devt,
     void *drvdata, const struct attribute_group **groups,
     const char *fmt, ...);
void device_destroy(struct class *cls, dev_t devt);

int __attribute__((__warn_unused_result__)) device_add_groups(struct device *dev,
       const struct attribute_group **groups);
void device_remove_groups(struct device *dev,
     const struct attribute_group **groups);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) device_add_group(struct device *dev,
     const struct attribute_group *grp)
{
 const struct attribute_group *groups[] = { grp, ((void *)0) };

 return device_add_groups(dev, groups);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void device_remove_group(struct device *dev,
           const struct attribute_group *grp)
{
 const struct attribute_group *groups[] = { grp, ((void *)0) };

 return device_remove_groups(dev, groups);
}

int __attribute__((__warn_unused_result__)) devm_device_add_groups(struct device *dev,
     const struct attribute_group **groups);
void devm_device_remove_groups(struct device *dev,
          const struct attribute_group **groups);
int __attribute__((__warn_unused_result__)) devm_device_add_group(struct device *dev,
           const struct attribute_group *grp);
void devm_device_remove_group(struct device *dev,
         const struct attribute_group *grp);

/*
 * Platform "fixup" functions - allow the platform to have their say
 * about devices and actions that the general device layer doesn't
 * know about.
 */
/* Notify platform of device discovery */
extern int (*platform_notify)(struct device *dev);

extern int (*platform_notify_remove)(struct device *dev);


/*
 * get_device - atomically increment the reference count for the device.
 *
 */
struct device *get_device(struct device *dev);
void put_device(struct device *dev);
bool kill_device(struct device *dev);


int devtmpfs_mount(void);




/* drivers/base/power/shutdown.c */
void device_shutdown(void);

/* debugging and troubleshooting/diagnostic helpers. */
const char *dev_driver_string(const struct device *dev);

/* Device links interface. */
struct device_link *device_link_add(struct device *consumer,
        struct device *supplier, u32 flags);
void device_link_del(struct device_link *link);
void device_link_remove(void *consumer, struct device *supplier);
void device_links_supplier_sync_state_pause(void);
void device_links_supplier_sync_state_resume(void);

extern __attribute__((__format__(printf, 3, 4)))
int dev_err_probe(const struct device *dev, int err, const char *fmt, ...);

/* Create alias, so I can be autoloaded. */
# 12 "./include/linux/blk_types.h" 2


struct bio_set;
struct bio;
struct bio_integrity_payload;
struct page;
struct io_context;
struct cgroup_subsys_state;
typedef void (bio_end_io_t) (struct bio *);
struct bio_crypt_ctx;

/*
 * The basic unit of block I/O is a sector. It is used in a number of contexts
 * in Linux (blk, bio, genhd). The size of one sector is 512 = 2**9
 * bytes. Variables of type sector_t represent an offset or size that is a
 * multiple of 512 bytes. Hence these two constants.
 */
# 40 "./include/linux/blk_types.h"
struct block_device {
 sector_t bd_start_sect;
 sector_t bd_nr_sectors;
 struct disk_stats /* nothing */ *bd_stats;
 unsigned long bd_stamp;
 bool bd_read_only; /* read-only policy */
 dev_t bd_dev;
 atomic_t bd_openers;
 struct inode * bd_inode; /* will die */
 struct super_block * bd_super;
 void * bd_claiming;
 struct device bd_device;
 void * bd_holder;
 int bd_holders;
 bool bd_write_holder;
 struct kobject *bd_holder_dir;
 u8 bd_partno;
 spinlock_t bd_size_lock; /* for bd_inode->i_size updates */
 struct gendisk * bd_disk;
 struct request_queue * bd_queue;

 /* The counter of freeze processes */
 int bd_fsfreeze_count;
 /* Mutex for freeze */
 struct mutex bd_fsfreeze_mutex;
 struct super_block *bd_fsfreeze_sb;

 struct partition_meta_info *bd_meta_info;



} ;
# 82 "./include/linux/blk_types.h"
/*
 * Block error status values.  See block/blk-core:blk_errors for the details.
 * Alpha cannot write a byte atomically, so we need to use 32-bit value.
 */




typedef u8 blk_status_t;
typedef u16 blk_short_t;
# 105 "./include/linux/blk_types.h"
/* hack for device mapper, don't use elsewhere: */


/*
 * BLK_STS_AGAIN should only be returned if RQF_NOWAIT is set
 * and the bio would block (cf bio_wouldblock_error())
 */


/*
 * BLK_STS_DEV_RESOURCE is returned from the driver to the block layer if
 * device related resources are unavailable, but the driver can guarantee
 * that the queue will be rerun in the future once resources become
 * available again. This is typically the case for device specific
 * resources that are consumed for IO. If the driver fails allocating these
 * resources, we know that inflight (or pending) IO will free these
 * resource upon completion.
 *
 * This is different from BLK_STS_RESOURCE in that it explicitly references
 * a device specific resource. For resources of wider scope, allocation
 * failure can happen without having pending IO. This means that we can't
 * rely on request completions freeing these resources, as IO may not be in
 * flight. Examples of that are kernel memory allocations, DMA mappings, or
 * any other system wide resources.
 */


/*
 * BLK_STS_ZONE_RESOURCE is returned from the driver to the block layer if zone
 * related resources are unavailable, but the driver can guarantee the queue
 * will be rerun in the future once the resources become available again.
 *
 * This is different from BLK_STS_DEV_RESOURCE in that it explicitly references
 * a zone specific resource and IO to a different zone on the same device could
 * still be served. Examples of that are zones that are write-locked, but a read
 * to the same zone could be served.
 */


/*
 * BLK_STS_ZONE_OPEN_RESOURCE is returned from the driver in the completion
 * path if the device returns a status indicating that too many zone resources
 * are currently open. The same command should be successful if resubmitted
 * after the number of open zones decreases below the device's limits, which is
 * reported in the request_queue's max_open_zones.
 */


/*
 * BLK_STS_ZONE_ACTIVE_RESOURCE is returned from the driver in the completion
 * path if the device returns a status indicating that too many zone resources
 * are currently active. The same command should be successful if resubmitted
 * after the number of active zones decreases below the device's limits, which
 * is reported in the request_queue's max_active_zones.
 */


/*
 * BLK_STS_OFFLINE is returned from the driver when the target device is offline
 * or is being taken offline. This could help differentiate the case where a
 * device is intentionally being shut down from a real I/O error.
 */


/**
 * blk_path_error - returns true if error may be path related
 * @error: status the request was completed with
 *
 * Description:
 *     This classifies block error status into non-retryable errors and ones
 *     that may be successful if retried on a failover path.
 *
 * Return:
 *     %false - retrying failover path will not help
 *     %true  - may succeed if retried
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_path_error(blk_status_t error)
{
 switch (error) {
 case (( blk_status_t)1):
 case (( blk_status_t)3):
 case (( blk_status_t)5):
 case (( blk_status_t)6):
 case (( blk_status_t)7):
 case (( blk_status_t)8):
  return false;
 }

 /* Anything else could be a path failure, so should be retried */
 return true;
}

/*
 * From most significant bit:
 * 1 bit: reserved for other usage, see below
 * 12 bits: original size of bio
 * 51 bits: issue time of bio
 */
# 212 "./include/linux/blk_types.h"
/* Reserved bit for blk-throtl */


struct bio_issue {
 u64 value;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 __bio_issue_time(u64 time)
{
 return time & ((1ULL << ((64 - 1) - 12)) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 bio_issue_time(struct bio_issue *issue)
{
 return __bio_issue_time(issue->value);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sector_t bio_issue_size(struct bio_issue *issue)
{
 return ((issue->value & (((1ULL << 12) - 1) << ((64 - 1) - 12))) >> ((64 - 1) - 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_issue_init(struct bio_issue *issue,
           sector_t size)
{
 size &= (1ULL << 12) - 1;
 issue->value = ((issue->value & (~((1ULL << (64 - 1)) - 1))) |
   (ktime_get_ns() & ((1ULL << ((64 - 1) - 12)) - 1)) |
   ((u64)size << ((64 - 1) - 12)));
}

typedef __u32 blk_opf_t;

typedef unsigned int blk_qc_t;


/*
 * main unit of I/O for the block layer and lower layers (ie drivers and
 * stacking drivers)
 */
struct bio {
 struct bio *bi_next; /* request queue link */
 struct block_device *bi_bdev;
 blk_opf_t bi_opf; /* bottom bits REQ_OP, top bits
						 * req_flags.
						 */
 unsigned short bi_flags; /* BIO_* below */
 unsigned short bi_ioprio;
 blk_status_t bi_status;
 atomic_t __bi_remaining;

 struct bvec_iter bi_iter;

 blk_qc_t bi_cookie;
 bio_end_io_t *bi_end_io;
 void *bi_private;

 /*
	 * Represents the association of the css and request_queue for the bio.
	 * If a bio goes direct to device, it will not have a blkg as it will
	 * not have a request_queue associated with it.  The reference is put
	 * on release of the bio.
	 */
 struct blkcg_gq *bi_blkg;
 struct bio_issue bi_issue;
# 286 "./include/linux/blk_types.h"
 union {

  struct bio_integrity_payload *bi_integrity; /* data integrity */

 };

 unsigned short bi_vcnt; /* how many bio_vec's */

 /*
	 * Everything starting with bi_max_vecs will be preserved by bio_reset()
	 */

 unsigned short bi_max_vecs; /* max bvl_vecs we can hold */

 atomic_t __bi_cnt; /* pin count */

 struct bio_vec *bi_io_vec; /* the actual vec list */

 struct bio_set *bi_pool;

 /*
	 * We can inline a number of vecs at the end of the bio, to avoid
	 * double allocations for a small number of bio_vecs. This member
	 * MUST obviously be kept at the very end of the bio.
	 */
 struct bio_vec bi_inline_vecs[];
};




/*
 * bio flags
 */
enum {
 BIO_NO_PAGE_REF, /* don't put release vec pages */
 BIO_CLONED, /* doesn't own data */
 BIO_BOUNCED, /* bio is a bounce bio */
 BIO_QUIET, /* Make BIO Quiet */
 BIO_CHAIN, /* chained bio, ->bi_remaining in effect */
 BIO_REFFED, /* bio has elevated ->bi_cnt */
 BIO_BPS_THROTTLED, /* This bio has already been subjected to
				 * throttling rules. Don't do it again. */
 BIO_TRACE_COMPLETION, /* bio_endio() should trace the final completion
				 * of this bio. */
 BIO_CGROUP_ACCT, /* has been accounted to a cgroup */
 BIO_QOS_THROTTLED, /* bio went through rq_qos throttle path */
 BIO_QOS_MERGED, /* but went through rq_qos merge path */
 BIO_REMAPPED,
 BIO_ZONE_WRITE_LOCKED, /* Owns a zoned device zone write lock */
 BIO_FLAG_LAST
};

typedef __u32 blk_mq_req_flags_t;





/**
 * enum req_op - Operations common to the bio and request structures.
 * We use 8 bits for encoding the operation, and the remaining 24 for flags.
 *
 * The least significant bit of the operation number indicates the data
 * transfer direction:
 *
 *   - if the least significant bit is set transfers are TO the device
 *   - if the least significant bit is not set transfers are FROM the device
 *
 * If a operation does not transfer data the least significant bit has no
 * meaning.
 */
enum req_op {
 /* read sectors from the device */
 REQ_OP_READ = ( blk_opf_t)0,
 /* write sectors to the device */
 REQ_OP_WRITE = ( blk_opf_t)1,
 /* flush the volatile write cache */
 REQ_OP_FLUSH = ( blk_opf_t)2,
 /* discard sectors */
 REQ_OP_DISCARD = ( blk_opf_t)3,
 /* securely erase sectors */
 REQ_OP_SECURE_ERASE = ( blk_opf_t)5,
 /* write the zero filled sector many times */
 REQ_OP_WRITE_ZEROES = ( blk_opf_t)9,
 /* Open a zone */
 REQ_OP_ZONE_OPEN = ( blk_opf_t)10,
 /* Close a zone */
 REQ_OP_ZONE_CLOSE = ( blk_opf_t)11,
 /* Transition a zone to full */
 REQ_OP_ZONE_FINISH = ( blk_opf_t)12,
 /* write data at the current zone write pointer */
 REQ_OP_ZONE_APPEND = ( blk_opf_t)13,
 /* reset a zone write pointer */
 REQ_OP_ZONE_RESET = ( blk_opf_t)15,
 /* reset all the zone present on the device */
 REQ_OP_ZONE_RESET_ALL = ( blk_opf_t)17,

 /* Driver private requests */
 REQ_OP_DRV_IN = ( blk_opf_t)34,
 REQ_OP_DRV_OUT = ( blk_opf_t)35,

 REQ_OP_LAST = ( blk_opf_t)36,
};

enum req_flag_bits {
 __REQ_FAILFAST_DEV = /* no driver retries of device errors */
  8,
 __REQ_FAILFAST_TRANSPORT, /* no driver retries of transport errors */
 __REQ_FAILFAST_DRIVER, /* no driver retries of driver errors */
 __REQ_SYNC, /* request is sync (sync write or read) */
 __REQ_META, /* metadata io request */
 __REQ_PRIO, /* boost priority in cfq */
 __REQ_NOMERGE, /* don't touch this for merging */
 __REQ_IDLE, /* anticipate more IO after this one */
 __REQ_INTEGRITY, /* I/O includes block integrity payload */
 __REQ_FUA, /* forced unit access */
 __REQ_PREFLUSH, /* request for cache flush */
 __REQ_RAHEAD, /* read ahead, can fail anytime */
 __REQ_BACKGROUND, /* background IO */
 __REQ_NOWAIT, /* Don't wait if request will block */
 /*
	 * When a shared kthread needs to issue a bio for a cgroup, doing
	 * so synchronously can lead to priority inversions as the kthread
	 * can be trapped waiting for that cgroup.  CGROUP_PUNT flag makes
	 * submit_bio() punt the actual issuing to a dedicated per-blkcg
	 * work item to avoid such priority inversions.
	 */
 __REQ_CGROUP_PUNT,
 __REQ_POLLED, /* caller polls for completion using bio_poll */
 __REQ_ALLOC_CACHE, /* allocate IO from cache if available */
 __REQ_SWAP, /* swap I/O */
 __REQ_DRV, /* for driver use */

 /*
	 * Command specific flags, keep last:
	 */
 /* for REQ_OP_WRITE_ZEROES: */
 __REQ_NOUNMAP, /* do not free blocks when zeroing */

 __REQ_NR_BITS, /* stops here */
};
# 461 "./include/linux/blk_types.h"
enum stat_group {
 STAT_READ,
 STAT_WRITE,
 STAT_DISCARD,
 STAT_FLUSH,

 NR_STAT_GROUPS
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum req_op bio_op(const struct bio *bio)
{
 return bio->bi_opf & ( blk_opf_t)((1 << 8) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool op_is_write(blk_opf_t op)
{
 return !!(op & ( blk_opf_t)1);
}

/*
 * Check if the bio or request is one that needs special treatment in the
 * flush state machine.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool op_is_flush(blk_opf_t op)
{
 return op & (( blk_opf_t)(1ULL << __REQ_FUA) | ( blk_opf_t)(1ULL << __REQ_PREFLUSH));
}

/*
 * Reads are always treated as synchronous, as are requests with the FUA or
 * PREFLUSH flag.  Other operations may be marked as synchronous using the
 * REQ_SYNC flag.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool op_is_sync(blk_opf_t op)
{
 return (op & ( blk_opf_t)((1 << 8) - 1)) == REQ_OP_READ ||
  (op & (( blk_opf_t)(1ULL << __REQ_SYNC) | ( blk_opf_t)(1ULL << __REQ_FUA) | ( blk_opf_t)(1ULL << __REQ_PREFLUSH)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool op_is_discard(blk_opf_t op)
{
 return (op & ( blk_opf_t)((1 << 8) - 1)) == REQ_OP_DISCARD;
}

/*
 * Check if a bio or request operation is a zone management operation, with
 * the exception of REQ_OP_ZONE_RESET_ALL which is treated as a special case
 * due to its different handling in the block layer and device response in
 * case of command failure.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool op_is_zone_mgmt(enum req_op op)
{
 switch (op & ( blk_opf_t)((1 << 8) - 1)) {
 case REQ_OP_ZONE_RESET:
 case REQ_OP_ZONE_OPEN:
 case REQ_OP_ZONE_CLOSE:
 case REQ_OP_ZONE_FINISH:
  return true;
 default:
  return false;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int op_stat_group(enum req_op op)
{
 if (op_is_discard(op))
  return STAT_DISCARD;
 return op_is_write(op);
}

struct blk_rq_stat {
 u64 mean;
 u64 min;
 u64 max;
 u32 nr_samples;
 u64 batch;
};
# 10 "./include/linux/blkdev.h" 2







# 1 "./include/linux/bio.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Copyright (C) 2001 Jens Axboe <axboe@suse.de>
 */



# 1 "./include/linux/mempool.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * memory buffer pool support
 */






struct kmem_cache;

typedef void * (mempool_alloc_t)(gfp_t gfp_mask, void *pool_data);
typedef void (mempool_free_t)(void *element, void *pool_data);

typedef struct mempool_s {
 spinlock_t lock;
 int min_nr; /* nr of elements at *elements */
 int curr_nr; /* Current nr of elements at *elements */
 void **elements;

 void *pool_data;
 mempool_alloc_t *alloc;
 mempool_free_t *free;
 wait_queue_head_t wait;
} mempool_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mempool_initialized(mempool_t *pool)
{
 return pool->elements != ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool mempool_is_saturated(mempool_t *pool)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_399(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(pool->curr_nr) == sizeof(char) || sizeof(pool->curr_nr) == sizeof(short) || sizeof(pool->curr_nr) == sizeof(int) || sizeof(pool->curr_nr) == sizeof(long)) || sizeof(pool->curr_nr) == sizeof(long long))) __compiletime_assert_399(); } while (0); (*(const volatile typeof( _Generic((pool->curr_nr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (pool->curr_nr))) *)&(pool->curr_nr)); }) >= pool->min_nr;
# 36 "./include/linux/mempool.h"
}

void mempool_exit(mempool_t *pool);
int mempool_init_node(mempool_t *pool, int min_nr, mempool_alloc_t *alloc_fn,
        mempool_free_t *free_fn, void *pool_data,
        gfp_t gfp_mask, int node_id);
int mempool_init(mempool_t *pool, int min_nr, mempool_alloc_t *alloc_fn,
   mempool_free_t *free_fn, void *pool_data);

extern mempool_t *mempool_create(int min_nr, mempool_alloc_t *alloc_fn,
   mempool_free_t *free_fn, void *pool_data);
extern mempool_t *mempool_create_node(int min_nr, mempool_alloc_t *alloc_fn,
   mempool_free_t *free_fn, void *pool_data,
   gfp_t gfp_mask, int nid);

extern int mempool_resize(mempool_t *pool, int new_min_nr);
extern void mempool_destroy(mempool_t *pool);
extern void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask) __attribute__((__malloc__));
extern void mempool_free(void *element, mempool_t *pool);

/*
 * A mempool_alloc_t and mempool_free_t that get the memory from
 * a slab cache that is passed in through pool_data.
 * Note: the slab cache may not have a ctor function.
 */
void *mempool_alloc_slab(gfp_t gfp_mask, void *pool_data);
void mempool_free_slab(void *element, void *pool_data);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int
mempool_init_slab_pool(mempool_t *pool, int min_nr, struct kmem_cache *kc)
{
 return mempool_init(pool, min_nr, mempool_alloc_slab,
       mempool_free_slab, (void *) kc);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) mempool_t *
mempool_create_slab_pool(int min_nr, struct kmem_cache *kc)
{
 return mempool_create(min_nr, mempool_alloc_slab, mempool_free_slab,
         (void *) kc);
}

/*
 * a mempool_alloc_t and a mempool_free_t to kmalloc and kfree the
 * amount of memory specified by pool_data
 */
void *mempool_kmalloc(gfp_t gfp_mask, void *pool_data);
void mempool_kfree(void *element, void *pool_data);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mempool_init_kmalloc_pool(mempool_t *pool, int min_nr, size_t size)
{
 return mempool_init(pool, min_nr, mempool_kmalloc,
       mempool_kfree, (void *) size);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) mempool_t *mempool_create_kmalloc_pool(int min_nr, size_t size)
{
 return mempool_create(min_nr, mempool_kmalloc, mempool_kfree,
         (void *) size);
}

/*
 * A mempool_alloc_t and mempool_free_t for a simple page allocator that
 * allocates pages of the order specified by pool_data
 */
void *mempool_alloc_pages(gfp_t gfp_mask, void *pool_data);
void mempool_free_pages(void *element, void *pool_data);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int mempool_init_page_pool(mempool_t *pool, int min_nr, int order)
{
 return mempool_init(pool, min_nr, mempool_alloc_pages,
       mempool_free_pages, (void *)(long)order);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) mempool_t *mempool_create_page_pool(int min_nr, int order)
{
 return mempool_create(min_nr, mempool_alloc_pages, mempool_free_pages,
         (void *)(long)order);
}
# 9 "./include/linux/bio.h" 2
/* struct bio, bio_vec and BIO_* flags are defined in blk_types.h */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bio_max_segs(unsigned int nr_segs)
{
 return __builtin_choose_expr(((!!(sizeof((typeof(nr_segs) *)1 == (typeof(256U) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(nr_segs) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(256U) * 0l)) : (int *)8))))), ((nr_segs) < (256U) ? (nr_segs) : (256U)), ({ typeof(nr_segs) __UNIQUE_ID___x400 = (nr_segs); typeof(256U) __UNIQUE_ID___y401 = (256U); ((__UNIQUE_ID___x400) < (__UNIQUE_ID___y401) ? (__UNIQUE_ID___x400) : (__UNIQUE_ID___y401)); }));
}
# 43 "./include/linux/bio.h"
/*
 * Return the data direction, READ or WRITE.
 */



/*
 * Check whether this bio carries any data or not. A NULL bio is allowed.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bio_has_data(struct bio *bio)
{
 if (bio &&
     bio->bi_iter.bi_size &&
     bio_op(bio) != REQ_OP_DISCARD &&
     bio_op(bio) != REQ_OP_SECURE_ERASE &&
     bio_op(bio) != REQ_OP_WRITE_ZEROES)
  return true;

 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bio_no_advance_iter(const struct bio *bio)
{
 return bio_op(bio) == REQ_OP_DISCARD ||
        bio_op(bio) == REQ_OP_SECURE_ERASE ||
        bio_op(bio) == REQ_OP_WRITE_ZEROES;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *bio_data(struct bio *bio)
{
 if (bio_has_data(bio))
  return lowmem_page_address((((&(((((bio))->bi_io_vec)))[(((((bio)->bi_iter)))).bi_idx])->bv_page) + (((&((((((bio))->bi_io_vec))))[((((((bio)->bi_iter))))).bi_idx])->bv_offset + (((((bio)->bi_iter)))).bi_bvec_done) / ((1UL) << 12)))) + (((&(((((bio))->bi_io_vec)))[(((((bio)->bi_iter)))).bi_idx])->bv_offset + ((((bio)->bi_iter))).bi_bvec_done) % ((1UL) << 12));

 return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bio_next_segment(const struct bio *bio,
        struct bvec_iter_all *iter)
{
 if (iter->idx >= bio->bi_vcnt)
  return false;

 bvec_advance(&bio->bi_io_vec[iter->idx], iter);
 return true;
}

/*
 * drivers should _never_ use the all version - the bio may have been split
 * before it got to the driver and the driver won't own all of it
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_advance_iter(const struct bio *bio,
        struct bvec_iter *iter, unsigned int bytes)
{
 iter->bi_sector += bytes >> 9;

 if (bio_no_advance_iter(bio))
  iter->bi_size -= bytes;
 else
  bvec_iter_advance(bio->bi_io_vec, iter, bytes);
  /* TODO: It is reasonable to complete bio with error here. */
}

/* @bytes should be less or equal to bvec[i->bi_idx].bv_len */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_advance_iter_single(const struct bio *bio,
        struct bvec_iter *iter,
        unsigned int bytes)
{
 iter->bi_sector += bytes >> 9;

 if (bio_no_advance_iter(bio))
  iter->bi_size -= bytes;
 else
  bvec_iter_advance_single(bio->bi_io_vec, iter, bytes);
}

void __bio_advance(struct bio *, unsigned bytes);

/**
 * bio_advance - increment/complete a bio by some number of bytes
 * @bio:	bio to advance
 * @nbytes:	number of bytes to complete
 *
 * This updates bi_sector, bi_size and bi_idx; if the number of bytes to
 * complete doesn't align with a bvec boundary, then bv_len and bv_offset will
 * be updated on the last bvec as well.
 *
 * @bio will then represent the remaining, uncompleted portion of the io.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_advance(struct bio *bio, unsigned int nbytes)
{
 if (nbytes == bio->bi_iter.bi_size) {
  bio->bi_iter.bi_size = 0;
  return;
 }
 __bio_advance(bio, nbytes);
}
# 158 "./include/linux/bio.h"
/* iterate over multi-page bvec */



/*
 * Iterate over all multi-page bvecs. Drivers shouldn't use this version for the
 * same reasons as bio_for_each_segment_all().
 */






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned bio_segments(struct bio *bio)
{
 unsigned segs = 0;
 struct bio_vec bv;
 struct bvec_iter iter;

 /*
	 * We special case discard/write same/write zeroes, because they
	 * interpret bi_size differently:
	 */

 switch (bio_op(bio)) {
 case REQ_OP_DISCARD:
 case REQ_OP_SECURE_ERASE:
 case REQ_OP_WRITE_ZEROES:
  return 0;
 default:
  break;
 }

 for (iter = ((bio)->bi_iter); (iter).bi_size && ((bv = ((struct bio_vec) { .bv_page = (((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_page) + (((&(((((((bio))->bi_io_vec)))))[(((((((iter))))))).bi_idx])->bv_offset + ((((((iter)))))).bi_bvec_done) / ((1UL) << 12))), .bv_len = __builtin_choose_expr(((!!(sizeof((typeof((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((iter))))).bi_size) *)1 == (typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((iter))))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((iter))))).bi_size) < ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) ? ((((((iter))))).bi_size) : ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done)), ({ typeof((((((iter))))).bi_size) __UNIQUE_ID___x402 = ((((((iter))))).bi_size); typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) __UNIQUE_ID___y403 = ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done); ((__UNIQUE_ID___x402) < (__UNIQUE_ID___y403) ? (__UNIQUE_ID___x402) : (__UNIQUE_ID___y403)); })))) *)1 == (typeof((unsigned)(((1UL) << 12) - (((&(((((((bio))->bi_io_vec)))))[(((((((iter))))))).bi_idx])->bv_offset + ((((((iter)))))).bi_bvec_done) % ((1UL) << 12)))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((iter))))).bi_size) *)1 == (typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((iter))))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((iter))))).bi_size) < ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) ? ((((((iter))))).bi_size) : ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done)), ({ typeof((((((iter))))).bi_size) __UNIQUE_ID___x402 = ((((((iter))))).bi_size); typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) __UNIQUE_ID___y403 = ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done); ((__UNIQUE_ID___x402) < (__UNIQUE_ID___y403) ? (__UNIQUE_ID___x402) : (__UNIQUE_ID___y403)); })))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned)(((1UL) << 12) - (((&(((((((bio))->bi_io_vec)))))[(((((((iter))))))).bi_idx])->bv_offset + ((((((iter)))))).bi_bvec_done) % ((1UL) << 12)))) * 0l)) : (int *)8))))), (((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((iter))))).bi_size) *)1 == (typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((iter))))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((iter))))).bi_size) < ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) ? ((((((iter))))).bi_size) : ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done)), ({ typeof((((((iter))))).bi_size) __UNIQUE_ID___x402 = ((((((iter))))).bi_size); typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) __UNIQUE_ID___y403 = ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done); ((__UNIQUE_ID___x402) < (__UNIQUE_ID___y403) ? (__UNIQUE_ID___x402) : (__UNIQUE_ID___y403)); })))) < ((unsigned)(((1UL) << 12) - (((&(((((((bio))->bi_io_vec)))))[(((((((iter))))))).bi_idx])->bv_offset + ((((((iter)))))).bi_bvec_done) % ((1UL) << 12)))) ? ((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((iter))))).bi_size) *)1 == (typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((iter))))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((iter))))).bi_size) < ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) ? ((((((iter))))).bi_size) : ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done)), ({ typeof((((((iter))))).bi_size) __UNIQUE_ID___x402 = ((((((iter))))).bi_size); typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) __UNIQUE_ID___y403 = ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done); ((__UNIQUE_ID___x402) < (__UNIQUE_ID___y403) ? (__UNIQUE_ID___x402) : (__UNIQUE_ID___y403)); })))) : ((unsigned)(((1UL) << 12) - (((&(((((((bio))->bi_io_vec)))))[(((((((iter))))))).bi_idx])->bv_offset + ((((((iter)))))).bi_bvec_done) % ((1UL) << 12))))), ({ typeof((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((iter))))).bi_size) *)1 == (typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((iter))))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((iter))))).bi_size) < ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) ? ((((((iter))))).bi_size) : ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done)), ({ typeof((((((iter))))).bi_size) __UNIQUE_ID___x402 = ((((((iter))))).bi_size); typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) __UNIQUE_ID___y403 = ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done); ((__UNIQUE_ID___x402) < (__UNIQUE_ID___y403) ? (__UNIQUE_ID___x402) : (__UNIQUE_ID___y403)); })))) __UNIQUE_ID___x404 = ((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((iter))))).bi_size) *)1 == (typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((iter))))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((iter))))).bi_size) < ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) ? ((((((iter))))).bi_size) : ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done)), ({ typeof((((((iter))))).bi_size) __UNIQUE_ID___x402 = ((((((iter))))).bi_size); typeof((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done) __UNIQUE_ID___y403 = ((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_len - (((((iter))))).bi_bvec_done); ((__UNIQUE_ID___x402) < (__UNIQUE_ID___y403) ? (__UNIQUE_ID___x402) : (__UNIQUE_ID___y403)); })))); typeof((unsigned)(((1UL) << 12) - (((&(((((((bio))->bi_io_vec)))))[(((((((iter))))))).bi_idx])->bv_offset + ((((((iter)))))).bi_bvec_done) % ((1UL) << 12)))) __UNIQUE_ID___y405 = ((unsigned)(((1UL) << 12) - (((&(((((((bio))->bi_io_vec)))))[(((((((iter))))))).bi_idx])->bv_offset + ((((((iter)))))).bi_bvec_done) % ((1UL) << 12)))); ((__UNIQUE_ID___x404) < (__UNIQUE_ID___y405) ? (__UNIQUE_ID___x404) : (__UNIQUE_ID___y405)); })), .bv_offset = (((&((((((bio))->bi_io_vec))))[((((((iter)))))).bi_idx])->bv_offset + (((((iter))))).bi_bvec_done) % ((1UL) << 12)), })), 1); bio_advance_iter_single((bio), &(iter), (bv).bv_len))
  segs++;

 return segs;
}

/*
 * get a reference to a bio, so it won't disappear. the intended use is
 * something like:
 *
 * bio_get(bio);
 * submit_bio(rw, bio);
 * if (bio->bi_flags ...)
 *	do_something
 * bio_put(bio);
 *
 * without the bio_get(), it could potentially complete I/O before submit_bio
 * returns. and then bio would be freed memory when if (bio->bi_flags ...)
 * runs
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_get(struct bio *bio)
{
 bio->bi_flags |= (1 << BIO_REFFED);
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 atomic_inc(&bio->__bi_cnt);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_cnt_set(struct bio *bio, unsigned int count)
{
 if (count != 1) {
  bio->bi_flags |= (1 << BIO_REFFED);
  do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 }
 atomic_set(&bio->__bi_cnt, count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bio_flagged(struct bio *bio, unsigned int bit)
{
 return (bio->bi_flags & (1U << bit)) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_set_flag(struct bio *bio, unsigned int bit)
{
 bio->bi_flags |= (1U << bit);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_clear_flag(struct bio *bio, unsigned int bit)
{
 bio->bi_flags &= ~(1U << bit);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio_vec *bio_first_bvec_all(struct bio *bio)
{
 ({ int __ret_warn_on = !!(bio_flagged(bio, BIO_CLONED)); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/bio.h\"; .popsection; .long 14472b - .; .short 245; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 return bio->bi_io_vec;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *bio_first_page_all(struct bio *bio)
{
 return bio_first_bvec_all(bio)->bv_page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio_vec *bio_last_bvec_all(struct bio *bio)
{
 ({ int __ret_warn_on = !!(bio_flagged(bio, BIO_CLONED)); if (__builtin_expect(!!(__ret_warn_on), 0)) asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/bio.h\"; .popsection; .long 14472b - .; .short 256; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 14471: brk 0x800");; __builtin_expect(!!(__ret_warn_on), 0); });
 return &bio->bi_io_vec[bio->bi_vcnt - 1];
}

/**
 * struct folio_iter - State for iterating all folios in a bio.
 * @folio: The current folio we're iterating.  NULL after the last folio.
 * @offset: The byte offset within the current folio.
 * @length: The number of bytes in this iteration (will not cross folio
 *	boundary).
 */
struct folio_iter {
 struct folio *folio;
 size_t offset;
 size_t length;
 /* private: for use by the iterator */
 struct folio *_next;
 size_t _seg_count;
 int _i;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_first_folio(struct folio_iter *fi, struct bio *bio,
       int i)
{
 struct bio_vec *bvec = bio_first_bvec_all(bio) + i;

 fi->folio = (_Generic((bvec->bv_page), const struct page *: (const struct folio *)_compound_head(bvec->bv_page), struct page *: (struct folio *)_compound_head(bvec->bv_page)));
 fi->offset = bvec->bv_offset +
   ((1UL) << 12) * (bvec->bv_page - &fi->folio->page);
 fi->_seg_count = bvec->bv_len;
 fi->length = __builtin_choose_expr(((!!(sizeof((typeof(folio_size(fi->folio) - fi->offset) *)1 == (typeof(fi->_seg_count) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(folio_size(fi->folio) - fi->offset) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(fi->_seg_count) * 0l)) : (int *)8))))), ((folio_size(fi->folio) - fi->offset) < (fi->_seg_count) ? (folio_size(fi->folio) - fi->offset) : (fi->_seg_count)), ({ typeof(folio_size(fi->folio) - fi->offset) __UNIQUE_ID___x406 = (folio_size(fi->folio) - fi->offset); typeof(fi->_seg_count) __UNIQUE_ID___y407 = (fi->_seg_count); ((__UNIQUE_ID___x406) < (__UNIQUE_ID___y407) ? (__UNIQUE_ID___x406) : (__UNIQUE_ID___y407)); }));
 fi->_next = folio_next(fi->folio);
 fi->_i = i;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_next_folio(struct folio_iter *fi, struct bio *bio)
{
 fi->_seg_count -= fi->length;
 if (fi->_seg_count) {
  fi->folio = fi->_next;
  fi->offset = 0;
  fi->length = __builtin_choose_expr(((!!(sizeof((typeof(folio_size(fi->folio)) *)1 == (typeof(fi->_seg_count) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(folio_size(fi->folio)) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(fi->_seg_count) * 0l)) : (int *)8))))), ((folio_size(fi->folio)) < (fi->_seg_count) ? (folio_size(fi->folio)) : (fi->_seg_count)), ({ typeof(folio_size(fi->folio)) __UNIQUE_ID___x408 = (folio_size(fi->folio)); typeof(fi->_seg_count) __UNIQUE_ID___y409 = (fi->_seg_count); ((__UNIQUE_ID___x408) < (__UNIQUE_ID___y409) ? (__UNIQUE_ID___x408) : (__UNIQUE_ID___y409)); }));
  fi->_next = folio_next(fi->folio);
 } else if (fi->_i + 1 < bio->bi_vcnt) {
  bio_first_folio(fi, bio, fi->_i + 1);
 } else {
  fi->folio = ((void *)0);
 }
}

/**
 * bio_for_each_folio_all - Iterate over each folio in a bio.
 * @fi: struct folio_iter which is updated for each folio.
 * @bio: struct bio to iterate over.
 */



enum bip_flags {
 BIP_BLOCK_INTEGRITY = 1 << 0, /* block layer owns integrity data */
 BIP_MAPPED_INTEGRITY = 1 << 1, /* ref tag has been remapped */
 BIP_CTRL_NOCHECK = 1 << 2, /* disable HBA integrity checking */
 BIP_DISK_NOCHECK = 1 << 3, /* disable disk integrity checking */
 BIP_IP_CHECKSUM = 1 << 4, /* IP checksum */
};

/*
 * bio integrity payload
 */
struct bio_integrity_payload {
 struct bio *bip_bio; /* parent bio */

 struct bvec_iter bip_iter;

 unsigned short bip_vcnt; /* # of integrity bio_vecs */
 unsigned short bip_max_vcnt; /* integrity bio_vec slots */
 unsigned short bip_flags; /* control flags */

 struct bvec_iter bio_iter; /* for rewinding parent bio */

 struct work_struct bip_work; /* I/O completion */

 struct bio_vec *bip_vec;
 struct bio_vec bip_inline_vecs[];/* embedded bvec array */
};



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio_integrity_payload *bio_integrity(struct bio *bio)
{
 if (bio->bi_opf & ( blk_opf_t)(1ULL << __REQ_INTEGRITY))
  return bio->bi_integrity;

 return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bio_integrity_flagged(struct bio *bio, enum bip_flags flag)
{
 struct bio_integrity_payload *bip = bio_integrity(bio);

 if (bip)
  return bip->bip_flags & flag;

 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sector_t bip_get_seed(struct bio_integrity_payload *bip)
{
 return bip->bip_iter.bi_sector;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bip_set_seed(struct bio_integrity_payload *bip,
    sector_t seed)
{
 bip->bip_iter.bi_sector = seed;
}



void bio_trim(struct bio *bio, sector_t offset, sector_t size);
extern struct bio *bio_split(struct bio *bio, int sectors,
        gfp_t gfp, struct bio_set *bs);

/**
 * bio_next_split - get next @sectors from a bio, splitting if necessary
 * @bio:	bio to split
 * @sectors:	number of sectors to split from the front of @bio
 * @gfp:	gfp mask
 * @bs:		bio set to allocate from
 *
 * Return: a bio representing the next @sectors of @bio - if the bio is smaller
 * than @sectors, returns the original bio unchanged.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio *bio_next_split(struct bio *bio, int sectors,
      gfp_t gfp, struct bio_set *bs)
{
 if (sectors >= (((bio)->bi_iter).bi_size >> 9))
  return bio;

 return bio_split(bio, sectors, gfp, bs);
}

enum {
 BIOSET_NEED_BVECS = ((((1UL))) << (0)),
 BIOSET_NEED_RESCUER = ((((1UL))) << (1)),
 BIOSET_PERCPU_CACHE = ((((1UL))) << (2)),
};
extern int bioset_init(struct bio_set *, unsigned int, unsigned int, int flags);
extern void bioset_exit(struct bio_set *);
extern int biovec_init_pool(mempool_t *pool, int pool_entries);

struct bio *bio_alloc_bioset(struct block_device *bdev, unsigned short nr_vecs,
        blk_opf_t opf, gfp_t gfp_mask,
        struct bio_set *bs);
struct bio *bio_kmalloc(unsigned short nr_vecs, gfp_t gfp_mask);
extern void bio_put(struct bio *);

struct bio *bio_alloc_clone(struct block_device *bdev, struct bio *bio_src,
  gfp_t gfp, struct bio_set *bs);
int bio_init_clone(struct block_device *bdev, struct bio *bio,
  struct bio *bio_src, gfp_t gfp);

extern struct bio_set fs_bio_set;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio *bio_alloc(struct block_device *bdev,
  unsigned short nr_vecs, blk_opf_t opf, gfp_t gfp_mask)
{
 return bio_alloc_bioset(bdev, nr_vecs, opf, gfp_mask, &fs_bio_set);
}

void submit_bio(struct bio *bio);

extern void bio_endio(struct bio *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_io_error(struct bio *bio)
{
 bio->bi_status = (( blk_status_t)10);
 bio_endio(bio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_wouldblock_error(struct bio *bio)
{
 bio_set_flag(bio, BIO_QUIET);
 bio->bi_status = (( blk_status_t)12);
 bio_endio(bio);
}

/*
 * Calculate number of bvec segments that should be allocated to fit data
 * pointed by @iter. If @iter is backed by bvec it's going to be reused
 * instead of allocating a new one.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int bio_iov_vecs_to_alloc(struct iov_iter *iter, int max_segs)
{
 if (iov_iter_is_bvec(iter))
  return 0;
 return iov_iter_npages(iter, max_segs);
}

struct request_queue;

extern int submit_bio_wait(struct bio *bio);
void bio_init(struct bio *bio, struct block_device *bdev, struct bio_vec *table,
       unsigned short max_vecs, blk_opf_t opf);
extern void bio_uninit(struct bio *);
void bio_reset(struct bio *bio, struct block_device *bdev, blk_opf_t opf);
void bio_chain(struct bio *, struct bio *);

int bio_add_page(struct bio *, struct page *, unsigned len, unsigned off);
bool bio_add_folio(struct bio *, struct folio *, size_t len, size_t off);
extern int bio_add_pc_page(struct request_queue *, struct bio *, struct page *,
      unsigned int, unsigned int);
int bio_add_zone_append_page(struct bio *bio, struct page *page,
        unsigned int len, unsigned int offset);
void __bio_add_page(struct bio *bio, struct page *page,
  unsigned int len, unsigned int off);
int bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter);
void bio_iov_bvec_set(struct bio *bio, struct iov_iter *iter);
void __bio_release_pages(struct bio *bio, bool mark_dirty);
extern void bio_set_pages_dirty(struct bio *bio);
extern void bio_check_pages_dirty(struct bio *bio);

extern void bio_copy_data(struct bio *dst, struct bio *src);
extern void bio_free_pages(struct bio *bio);
void guard_bio_eod(struct bio *bio);
void zero_fill_bio(struct bio *bio);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_release_pages(struct bio *bio, bool mark_dirty)
{
 if (!bio_flagged(bio, BIO_NO_PAGE_REF))
  __bio_release_pages(bio, mark_dirty);
}





void bio_associate_blkg(struct bio *bio);
void bio_associate_blkg_from_css(struct bio *bio,
     struct cgroup_subsys_state *css);
void bio_clone_blkg_association(struct bio *dst, struct bio *src);
# 506 "./include/linux/bio.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_set_dev(struct bio *bio, struct block_device *bdev)
{
 bio_clear_flag(bio, BIO_REMAPPED);
 if (bio->bi_bdev != bdev)
  bio_clear_flag(bio, BIO_BPS_THROTTLED);
 bio->bi_bdev = bdev;
 bio_associate_blkg(bio);
}

/*
 * BIO list management for use by remapping drivers (e.g. DM or MD) and loop.
 *
 * A bio_list anchors a singly-linked list of bios chained through the bi_next
 * member of the bio.  The bio_list also caches the last list member to allow
 * fast access to the tail.
 */
struct bio_list {
 struct bio *head;
 struct bio *tail;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int bio_list_empty(const struct bio_list *bl)
{
 return bl->head == ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_list_init(struct bio_list *bl)
{
 bl->head = bl->tail = ((void *)0);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned bio_list_size(const struct bio_list *bl)
{
 unsigned sz = 0;
 struct bio *bio;

 for (bio = (bl)->head; bio; bio = bio->bi_next)
  sz++;

 return sz;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_list_add(struct bio_list *bl, struct bio *bio)
{
 bio->bi_next = ((void *)0);

 if (bl->tail)
  bl->tail->bi_next = bio;
 else
  bl->head = bio;

 bl->tail = bio;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_list_add_head(struct bio_list *bl, struct bio *bio)
{
 bio->bi_next = bl->head;

 bl->head = bio;

 if (!bl->tail)
  bl->tail = bio;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_list_merge(struct bio_list *bl, struct bio_list *bl2)
{
 if (!bl2->head)
  return;

 if (bl->tail)
  bl->tail->bi_next = bl2->head;
 else
  bl->head = bl2->head;

 bl->tail = bl2->tail;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_list_merge_head(struct bio_list *bl,
           struct bio_list *bl2)
{
 if (!bl2->head)
  return;

 if (bl->head)
  bl2->tail->bi_next = bl->head;
 else
  bl->tail = bl2->tail;

 bl->head = bl2->head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio *bio_list_peek(struct bio_list *bl)
{
 return bl->head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio *bio_list_pop(struct bio_list *bl)
{
 struct bio *bio = bl->head;

 if (bio) {
  bl->head = bl->head->bi_next;
  if (!bl->head)
   bl->tail = ((void *)0);

  bio->bi_next = ((void *)0);
 }

 return bio;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio *bio_list_get(struct bio_list *bl)
{
 struct bio *bio = bl->head;

 bl->head = bl->tail = ((void *)0);

 return bio;
}

/*
 * Increment chain count for the bio. Make sure the CHAIN flag update
 * is visible before the raised count.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_inc_remaining(struct bio *bio)
{
 bio_set_flag(bio, BIO_CHAIN);
 do { do { } while (0); asm volatile("dmb " "ish" : : : "memory"); } while (0);
 atomic_inc(&bio->__bi_remaining);
}

/*
 * bio_set is used to allow other portions of the IO system to
 * allocate their own private memory pools for bio and iovec structures.
 * These memory pools in turn all allocate from the bio_slab
 * and the bvec_slabs[].
 */


struct bio_set {
 struct kmem_cache *bio_slab;
 unsigned int front_pad;

 /*
	 * per-cpu bio alloc cache
	 */
 struct bio_alloc_cache /* nothing */ *cache;

 mempool_t bio_pool;
 mempool_t bvec_pool;

 mempool_t bio_integrity_pool;
 mempool_t bvec_integrity_pool;


 unsigned int back_pad;
 /*
	 * Deadlock avoidance for stacking block drivers: see comments in
	 * bio_alloc_bioset() for details
	 */
 spinlock_t rescue_lock;
 struct bio_list rescue_list;
 struct work_struct rescue_work;
 struct workqueue_struct *rescue_workqueue;

 /*
	 * Hot un-plug notifier for the per-cpu cache, if used
	 */
 struct hlist_node cpuhp_dead;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bioset_initialized(struct bio_set *bs)
{
 return bs->bio_slab != ((void *)0);
}
# 696 "./include/linux/bio.h"
extern struct bio_integrity_payload *bio_integrity_alloc(struct bio *, gfp_t, unsigned int);
extern int bio_integrity_add_page(struct bio *, struct page *, unsigned int, unsigned int);
extern bool bio_integrity_prep(struct bio *);
extern void bio_integrity_advance(struct bio *, unsigned int);
extern void bio_integrity_trim(struct bio *);
extern int bio_integrity_clone(struct bio *, struct bio *, gfp_t);
extern int bioset_integrity_create(struct bio_set *, int);
extern void bioset_integrity_free(struct bio_set *);
extern void bio_integrity_init(void);
# 769 "./include/linux/bio.h"
/*
 * Mark a bio as polled. Note that for async polled IO, the caller must
 * expect -EWOULDBLOCK if we cannot allocate a request (or other resources).
 * We cannot block waiting for requests on polled IO, as those completions
 * must be found by the caller. This is different than IRQ driven IO, where
 * it's safe to wait for IO to complete.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_set_polled(struct bio *bio, struct kiocb *kiocb)
{
 bio->bi_opf |= ( blk_opf_t)(1ULL << __REQ_POLLED);
 if (!is_sync_kiocb(kiocb))
  bio->bi_opf |= ( blk_opf_t)(1ULL << __REQ_NOWAIT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_clear_polled(struct bio *bio)
{
 /* can't support alloc cache if we turn off polling */
 bio->bi_opf &= ~(( blk_opf_t)(1ULL << __REQ_POLLED) | ( blk_opf_t)(1ULL << __REQ_ALLOC_CACHE));
}

struct bio *blk_next_bio(struct bio *bio, struct block_device *bdev,
  unsigned int nr_pages, blk_opf_t opf, gfp_t gfp);
# 18 "./include/linux/blkdev.h" 2




# 1 "./include/uapi/linux/blkzoned.h" 1
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * Zoned block devices handling.
 *
 * Copyright (C) 2015 Seagate Technology PLC
 *
 * Written by: Shaun Tancheff <shaun.tancheff@seagate.com>
 *
 * Modified by: Damien Le Moal <damien.lemoal@hgst.com>
 * Copyright (C) 2016 Western Digital
 *
 * This file is licensed under  the terms of the GNU General Public
 * License version 2. This program is licensed "as is" without any
 * warranty of any kind, whether express or implied.
 */






/**
 * enum blk_zone_type - Types of zones allowed in a zoned device.
 *
 * @BLK_ZONE_TYPE_CONVENTIONAL: The zone has no write pointer and can be writen
 *                              randomly. Zone reset has no effect on the zone.
 * @BLK_ZONE_TYPE_SEQWRITE_REQ: The zone must be written sequentially
 * @BLK_ZONE_TYPE_SEQWRITE_PREF: The zone can be written non-sequentially
 *
 * Any other value not defined is reserved and must be considered as invalid.
 */
enum blk_zone_type {
 BLK_ZONE_TYPE_CONVENTIONAL = 0x1,
 BLK_ZONE_TYPE_SEQWRITE_REQ = 0x2,
 BLK_ZONE_TYPE_SEQWRITE_PREF = 0x3,
};

/**
 * enum blk_zone_cond - Condition [state] of a zone in a zoned device.
 *
 * @BLK_ZONE_COND_NOT_WP: The zone has no write pointer, it is conventional.
 * @BLK_ZONE_COND_EMPTY: The zone is empty.
 * @BLK_ZONE_COND_IMP_OPEN: The zone is open, but not explicitly opened.
 * @BLK_ZONE_COND_EXP_OPEN: The zones was explicitly opened by an
 *                          OPEN ZONE command.
 * @BLK_ZONE_COND_CLOSED: The zone was [explicitly] closed after writing.
 * @BLK_ZONE_COND_FULL: The zone is marked as full, possibly by a zone
 *                      FINISH ZONE command.
 * @BLK_ZONE_COND_READONLY: The zone is read-only.
 * @BLK_ZONE_COND_OFFLINE: The zone is offline (sectors cannot be read/written).
 *
 * The Zone Condition state machine in the ZBC/ZAC standards maps the above
 * deinitions as:
 *   - ZC1: Empty         | BLK_ZONE_EMPTY
 *   - ZC2: Implicit Open | BLK_ZONE_COND_IMP_OPEN
 *   - ZC3: Explicit Open | BLK_ZONE_COND_EXP_OPEN
 *   - ZC4: Closed        | BLK_ZONE_CLOSED
 *   - ZC5: Full          | BLK_ZONE_FULL
 *   - ZC6: Read Only     | BLK_ZONE_READONLY
 *   - ZC7: Offline       | BLK_ZONE_OFFLINE
 *
 * Conditions 0x5 to 0xC are reserved by the current ZBC/ZAC spec and should
 * be considered invalid.
 */
enum blk_zone_cond {
 BLK_ZONE_COND_NOT_WP = 0x0,
 BLK_ZONE_COND_EMPTY = 0x1,
 BLK_ZONE_COND_IMP_OPEN = 0x2,
 BLK_ZONE_COND_EXP_OPEN = 0x3,
 BLK_ZONE_COND_CLOSED = 0x4,
 BLK_ZONE_COND_READONLY = 0xD,
 BLK_ZONE_COND_FULL = 0xE,
 BLK_ZONE_COND_OFFLINE = 0xF,
};

/**
 * enum blk_zone_report_flags - Feature flags of reported zone descriptors.
 *
 * @BLK_ZONE_REP_CAPACITY: Zone descriptor has capacity field.
 */
enum blk_zone_report_flags {
 BLK_ZONE_REP_CAPACITY = (1 << 0),
};

/**
 * struct blk_zone - Zone descriptor for BLKREPORTZONE ioctl.
 *
 * @start: Zone start in 512 B sector units
 * @len: Zone length in 512 B sector units
 * @wp: Zone write pointer location in 512 B sector units
 * @type: see enum blk_zone_type for possible values
 * @cond: see enum blk_zone_cond for possible values
 * @non_seq: Flag indicating that the zone is using non-sequential resources
 *           (for host-aware zoned block devices only).
 * @reset: Flag indicating that a zone reset is recommended.
 * @resv: Padding for 8B alignment.
 * @capacity: Zone usable capacity in 512 B sector units
 * @reserved: Padding to 64 B to match the ZBC, ZAC and ZNS defined zone
 *            descriptor size.
 *
 * start, len, capacity and wp use the regular 512 B sector unit, regardless
 * of the device logical block size. The overall structure size is 64 B to
 * match the ZBC, ZAC and ZNS defined zone descriptor and allow support for
 * future additional zone information.
 */
struct blk_zone {
 __u64 start; /* Zone start sector */
 __u64 len; /* Zone length in number of sectors */
 __u64 wp; /* Zone write pointer position */
 __u8 type; /* Zone type */
 __u8 cond; /* Zone condition */
 __u8 non_seq; /* Non-sequential write resources active */
 __u8 reset; /* Reset write pointer recommended */
 __u8 resv[4];
 __u64 capacity; /* Zone capacity in number of sectors */
 __u8 reserved[24];
};

/**
 * struct blk_zone_report - BLKREPORTZONE ioctl request/reply
 *
 * @sector: starting sector of report
 * @nr_zones: IN maximum / OUT actual
 * @flags: one or more flags as defined by enum blk_zone_report_flags.
 * @zones: Space to hold @nr_zones @zones entries on reply.
 *
 * The array of at most @nr_zones must follow this structure in memory.
 */
struct blk_zone_report {
 __u64 sector;
 __u32 nr_zones;
 __u32 flags;
 struct blk_zone zones[];
};

/**
 * struct blk_zone_range - BLKRESETZONE/BLKOPENZONE/
 *                         BLKCLOSEZONE/BLKFINISHZONE ioctl
 *                         requests
 * @sector: Starting sector of the first zone to operate on.
 * @nr_sectors: Total number of sectors of all zones to operate on.
 */
struct blk_zone_range {
 __u64 sector;
 __u64 nr_sectors;
};

/**
 * Zoned block device ioctl's:
 *
 * @BLKREPORTZONE: Get zone information. Takes a zone report as argument.
 *                 The zone report will start from the zone containing the
 *                 sector specified in the report request structure.
 * @BLKRESETZONE: Reset the write pointer of the zones in the specified
 *                sector range. The sector range must be zone aligned.
 * @BLKGETZONESZ: Get the device zone size in number of 512 B sectors.
 * @BLKGETNRZONES: Get the total number of zones of the device.
 * @BLKOPENZONE: Open the zones in the specified sector range.
 *               The 512 B sector range must be zone aligned.
 * @BLKCLOSEZONE: Close the zones in the specified sector range.
 *                The 512 B sector range must be zone aligned.
 * @BLKFINISHZONE: Mark the zones as full in the specified sector range.
 *                 The 512 B sector range must be zone aligned.
 */
# 23 "./include/linux/blkdev.h" 2

# 1 "./include/linux/sbitmap.h" 1
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 * Fast and scalable bitmaps.
 *
 * Copyright (C) 2016 Facebook
 * Copyright (C) 2013-2014 Jens Axboe
 */
# 24 "./include/linux/sbitmap.h"
struct seq_file;

/**
 * struct sbitmap_word - Word in a &struct sbitmap.
 */
struct sbitmap_word {
 /**
	 * @word: word holding free bits
	 */
 unsigned long word;

 /**
	 * @cleared: word holding cleared bits
	 */
 unsigned long cleared __attribute__((__aligned__((1 << (6)))));
} __attribute__((__aligned__((1 << (6)))));

/**
 * struct sbitmap - Scalable bitmap.
 *
 * A &struct sbitmap is spread over multiple cachelines to avoid ping-pong. This
 * trades off higher memory usage for better scalability.
 */
struct sbitmap {
 /**
	 * @depth: Number of bits used in the whole bitmap.
	 */
 unsigned int depth;

 /**
	 * @shift: log2(number of bits used per word)
	 */
 unsigned int shift;

 /**
	 * @map_nr: Number of words (cachelines) being used for the bitmap.
	 */
 unsigned int map_nr;

 /**
	 * @round_robin: Allocate bits in strict round-robin order.
	 */
 bool round_robin;

 /**
	 * @map: Allocated bitmap.
	 */
 struct sbitmap_word *map;

 /*
	 * @alloc_hint: Cache of last successfully allocated or freed bit.
	 *
	 * This is per-cpu, which allows multiple users to stick to different
	 * cachelines until the map is exhausted.
	 */
 unsigned int /* nothing */ *alloc_hint;
};




/**
 * struct sbq_wait_state - Wait queue in a &struct sbitmap_queue.
 */
struct sbq_wait_state {
 /**
	 * @wait: Wait queue.
	 */
 wait_queue_head_t wait;
} __attribute__((__aligned__((1 << (6)))));

/**
 * struct sbitmap_queue - Scalable bitmap with the added ability to wait on free
 * bits.
 *
 * A &struct sbitmap_queue uses multiple wait queues and rolling wakeups to
 * avoid contention on the wait queue spinlock. This ensures that we don't hit a
 * scalability wall when we run out of free bits and have to start putting tasks
 * to sleep.
 */
struct sbitmap_queue {
 /**
	 * @sb: Scalable bitmap.
	 */
 struct sbitmap sb;

 /**
	 * @wake_batch: Number of bits which must be freed before we wake up any
	 * waiters.
	 */
 unsigned int wake_batch;

 /**
	 * @wake_index: Next wait queue in @ws to wake up.
	 */
 atomic_t wake_index;

 /**
	 * @ws: Wait queues.
	 */
 struct sbq_wait_state *ws;

 /*
	 * @ws_active: count of currently active ws waitqueues
	 */
 atomic_t ws_active;

 /**
	 * @min_shallow_depth: The minimum shallow depth which may be passed to
	 * sbitmap_queue_get_shallow()
	 */
 unsigned int min_shallow_depth;

 /**
	 * @completion_cnt: Number of bits cleared passed to the
	 * wakeup function.
	 */
 atomic_t completion_cnt;

 /**
	 * @wakeup_cnt: Number of thread wake ups issued.
	 */
 atomic_t wakeup_cnt;
};

/**
 * sbitmap_init_node() - Initialize a &struct sbitmap on a specific memory node.
 * @sb: Bitmap to initialize.
 * @depth: Number of bits to allocate.
 * @shift: Use 2^@shift bits per word in the bitmap; if a negative number if
 *         given, a good default is chosen.
 * @flags: Allocation flags.
 * @node: Memory node to allocate on.
 * @round_robin: If true, be stricter about allocation order; always allocate
 *               starting from the last allocated bit. This is less efficient
 *               than the default behavior (false).
 * @alloc_hint: If true, apply percpu hint for where to start searching for
 *              a free bit.
 *
 * Return: Zero on success or negative errno on failure.
 */
int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
        gfp_t flags, int node, bool round_robin, bool alloc_hint);

/* sbitmap internal helper */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __map_depth(const struct sbitmap *sb, int index)
{
 if (index == sb->map_nr - 1)
  return sb->depth - (index << sb->shift);
 return 1U << sb->shift;
}

/**
 * sbitmap_free() - Free memory used by a &struct sbitmap.
 * @sb: Bitmap to free.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbitmap_free(struct sbitmap *sb)
{
 free_percpu(sb->alloc_hint);
 kvfree(sb->map);
 sb->map = ((void *)0);
}

/**
 * sbitmap_resize() - Resize a &struct sbitmap.
 * @sb: Bitmap to resize.
 * @depth: New number of bits to resize to.
 *
 * Doesn't reallocate anything. It's up to the caller to ensure that the new
 * depth doesn't exceed the depth that the sb was initialized with.
 */
void sbitmap_resize(struct sbitmap *sb, unsigned int depth);

/**
 * sbitmap_get() - Try to allocate a free bit from a &struct sbitmap.
 * @sb: Bitmap to allocate from.
 *
 * This operation provides acquire barrier semantics if it succeeds.
 *
 * Return: Non-negative allocated bit number if successful, -1 otherwise.
 */
int sbitmap_get(struct sbitmap *sb);

/**
 * sbitmap_get_shallow() - Try to allocate a free bit from a &struct sbitmap,
 * limiting the depth used from each word.
 * @sb: Bitmap to allocate from.
 * @shallow_depth: The maximum number of bits to allocate from a single word.
 *
 * This rather specific operation allows for having multiple users with
 * different allocation limits. E.g., there can be a high-priority class that
 * uses sbitmap_get() and a low-priority class that uses sbitmap_get_shallow()
 * with a @shallow_depth of (1 << (@sb->shift - 1)). Then, the low-priority
 * class can only allocate half of the total bits in the bitmap, preventing it
 * from starving out the high-priority class.
 *
 * Return: Non-negative allocated bit number if successful, -1 otherwise.
 */
int sbitmap_get_shallow(struct sbitmap *sb, unsigned long shallow_depth);

/**
 * sbitmap_any_bit_set() - Check for a set bit in a &struct sbitmap.
 * @sb: Bitmap to check.
 *
 * Return: true if any bit in the bitmap is set, false otherwise.
 */
bool sbitmap_any_bit_set(const struct sbitmap *sb);




typedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);

/**
 * __sbitmap_for_each_set() - Iterate over each set bit in a &struct sbitmap.
 * @start: Where to start the iteration.
 * @sb: Bitmap to iterate over.
 * @fn: Callback. Should return true to continue or false to break early.
 * @data: Pointer to pass to callback.
 *
 * This is inline even though it's non-trivial so that the function calls to the
 * callback will hopefully get optimized away.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __sbitmap_for_each_set(struct sbitmap *sb,
       unsigned int start,
       sb_for_each_fn fn, void *data)
{
 unsigned int index;
 unsigned int nr;
 unsigned int scanned = 0;

 if (start >= sb->depth)
  start = 0;
 index = ((start) >> (sb)->shift);
 nr = ((start) & ((1U << (sb)->shift) - 1U));

 while (scanned < sb->depth) {
  unsigned long word;
  unsigned int depth = __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)(__map_depth(sb, index) - nr)) *)1 == (typeof((unsigned int)(sb->depth - scanned)) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(__map_depth(sb, index) - nr)) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(sb->depth - scanned)) * 0l)) : (int *)8))))), (((unsigned int)(__map_depth(sb, index) - nr)) < ((unsigned int)(sb->depth - scanned)) ? ((unsigned int)(__map_depth(sb, index) - nr)) : ((unsigned int)(sb->depth - scanned))), ({ typeof((unsigned int)(__map_depth(sb, index) - nr)) __UNIQUE_ID___x410 = ((unsigned int)(__map_depth(sb, index) - nr)); typeof((unsigned int)(sb->depth - scanned)) __UNIQUE_ID___y411 = ((unsigned int)(sb->depth - scanned)); ((__UNIQUE_ID___x410) < (__UNIQUE_ID___y411) ? (__UNIQUE_ID___x410) : (__UNIQUE_ID___y411)); }));



  scanned += depth;
  word = sb->map[index].word & ~sb->map[index].cleared;
  if (!word)
   goto next;

  /*
		 * On the first iteration of the outer loop, we need to add the
		 * bit offset back to the size of the word for find_next_bit().
		 * On all other iterations, nr is zero, so this is a noop.
		 */
  depth += nr;
  while (1) {
   nr = find_next_bit(&word, depth, nr);
   if (nr >= depth)
    break;
   if (!fn(sb, (index << sb->shift) + nr, data))
    return;

   nr++;
  }
next:
  nr = 0;
  if (++index >= sb->map_nr)
   index = 0;
 }
}

/**
 * sbitmap_for_each_set() - Iterate over each set bit in a &struct sbitmap.
 * @sb: Bitmap to iterate over.
 * @fn: Callback. Should return true to continue or false to break early.
 * @data: Pointer to pass to callback.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbitmap_for_each_set(struct sbitmap *sb, sb_for_each_fn fn,
     void *data)
{
 __sbitmap_for_each_set(sb, 0, fn, data);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long *__sbitmap_word(struct sbitmap *sb,
         unsigned int bitnr)
{
 return &sb->map[((bitnr) >> (sb)->shift)].word;
}

/* Helpers equivalent to the operations in asm/bitops.h and linux/bitmap.h */

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbitmap_set_bit(struct sbitmap *sb, unsigned int bitnr)
{
 set_bit(((bitnr) & ((1U << (sb)->shift) - 1U)), __sbitmap_word(sb, bitnr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbitmap_clear_bit(struct sbitmap *sb, unsigned int bitnr)
{
 clear_bit(((bitnr) & ((1U << (sb)->shift) - 1U)), __sbitmap_word(sb, bitnr));
}

/*
 * This one is special, since it doesn't actually clear the bit, rather it
 * sets the corresponding bit in the ->cleared mask instead. Paired with
 * the caller doing sbitmap_deferred_clear() if a given index is full, which
 * will clear the previously freed entries in the corresponding ->word.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbitmap_deferred_clear_bit(struct sbitmap *sb, unsigned int bitnr)
{
 unsigned long *addr = &sb->map[((bitnr) >> (sb)->shift)].cleared;

 set_bit(((bitnr) & ((1U << (sb)->shift) - 1U)), addr);
}

/*
 * Pair of sbitmap_get, and this one applies both cleared bit and
 * allocation hint.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbitmap_put(struct sbitmap *sb, unsigned int bitnr)
{
 sbitmap_deferred_clear_bit(sb, bitnr);

 if (__builtin_expect(!!(sb->alloc_hint && !sb->round_robin && bitnr < sb->depth), 1))
  *({ do { const void /* nothing */ *__vpp_verify = (typeof((sb->alloc_hint) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(sb->alloc_hint)) *)(sb->alloc_hint)); (typeof((typeof(*(sb->alloc_hint)) *)(sb->alloc_hint))) (__ptr + ((__kern_my_cpu_offset()))); }); }) = bitnr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sbitmap_test_bit(struct sbitmap *sb, unsigned int bitnr)
{
 return ((__builtin_constant_p(((bitnr) & ((1U << (sb)->shift) - 1U))) && __builtin_constant_p((uintptr_t)(__sbitmap_word(sb, bitnr)) != (uintptr_t)((void *)0)) && (uintptr_t)(__sbitmap_word(sb, bitnr)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(__sbitmap_word(sb, bitnr)))) ? const_test_bit(((bitnr) & ((1U << (sb)->shift) - 1U)), __sbitmap_word(sb, bitnr)) : generic_test_bit(((bitnr) & ((1U << (sb)->shift) - 1U)), __sbitmap_word(sb, bitnr)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sbitmap_calculate_shift(unsigned int depth)
{
 int shift = ( __builtin_constant_p(64) ? ((64) < 2 ? 0 : 63 - __builtin_clzll(64)) : (sizeof(64) <= 4) ? __ilog2_u32(64) : __ilog2_u64(64) );

 /*
	 * If the bitmap is small, shrink the number of bits per word so
	 * we spread over a few cachelines, at least. If less than 4
	 * bits, just forget about it, it's not going to work optimally
	 * anyway.
	 */
 if (depth >= 4) {
  while ((4U << shift) > depth)
   shift--;
 }

 return shift;
}

/**
 * sbitmap_show() - Dump &struct sbitmap information to a &struct seq_file.
 * @sb: Bitmap to show.
 * @m: struct seq_file to write to.
 *
 * This is intended for debugging. The format may change at any time.
 */
void sbitmap_show(struct sbitmap *sb, struct seq_file *m);


/**
 * sbitmap_weight() - Return how many set and not cleared bits in a &struct
 * sbitmap.
 * @sb: Bitmap to check.
 *
 * Return: How many set and not cleared bits set
 */
unsigned int sbitmap_weight(const struct sbitmap *sb);

/**
 * sbitmap_bitmap_show() - Write a hex dump of a &struct sbitmap to a &struct
 * seq_file.
 * @sb: Bitmap to show.
 * @m: struct seq_file to write to.
 *
 * This is intended for debugging. The output isn't guaranteed to be internally
 * consistent.
 */
void sbitmap_bitmap_show(struct sbitmap *sb, struct seq_file *m);

/**
 * sbitmap_queue_init_node() - Initialize a &struct sbitmap_queue on a specific
 * memory node.
 * @sbq: Bitmap queue to initialize.
 * @depth: See sbitmap_init_node().
 * @shift: See sbitmap_init_node().
 * @round_robin: See sbitmap_get().
 * @flags: Allocation flags.
 * @node: Memory node to allocate on.
 *
 * Return: Zero on success or negative errno on failure.
 */
int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
       int shift, bool round_robin, gfp_t flags, int node);

/**
 * sbitmap_queue_free() - Free memory used by a &struct sbitmap_queue.
 *
 * @sbq: Bitmap queue to free.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbitmap_queue_free(struct sbitmap_queue *sbq)
{
 kfree(sbq->ws);
 sbitmap_free(&sbq->sb);
}

/**
 * sbitmap_queue_recalculate_wake_batch() - Recalculate wake batch
 * @sbq: Bitmap queue to recalculate wake batch.
 * @users: Number of shares.
 *
 * Like sbitmap_queue_update_wake_batch(), this will calculate wake batch
 * by depth. This interface is for HCTX shared tags or queue shared tags.
 */
void sbitmap_queue_recalculate_wake_batch(struct sbitmap_queue *sbq,
         unsigned int users);

/**
 * sbitmap_queue_resize() - Resize a &struct sbitmap_queue.
 * @sbq: Bitmap queue to resize.
 * @depth: New number of bits to resize to.
 *
 * Like sbitmap_resize(), this doesn't reallocate anything. It has to do
 * some extra work on the &struct sbitmap_queue, so it's not safe to just
 * resize the underlying &struct sbitmap.
 */
void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth);

/**
 * __sbitmap_queue_get() - Try to allocate a free bit from a &struct
 * sbitmap_queue with preemption already disabled.
 * @sbq: Bitmap queue to allocate from.
 *
 * Return: Non-negative allocated bit number if successful, -1 otherwise.
 */
int __sbitmap_queue_get(struct sbitmap_queue *sbq);

/**
 * __sbitmap_queue_get_batch() - Try to allocate a batch of free bits
 * @sbq: Bitmap queue to allocate from.
 * @nr_tags: number of tags requested
 * @offset: offset to add to returned bits
 *
 * Return: Mask of allocated tags, 0 if none are found. Each tag allocated is
 * a bit in the mask returned, and the caller must add @offset to the value to
 * get the absolute tag value.
 */
unsigned long __sbitmap_queue_get_batch(struct sbitmap_queue *sbq, int nr_tags,
     unsigned int *offset);

/**
 * sbitmap_queue_get_shallow() - Try to allocate a free bit from a &struct
 * sbitmap_queue, limiting the depth used from each word, with preemption
 * already disabled.
 * @sbq: Bitmap queue to allocate from.
 * @shallow_depth: The maximum number of bits to allocate from a single word.
 * See sbitmap_get_shallow().
 *
 * If you call this, make sure to call sbitmap_queue_min_shallow_depth() after
 * initializing @sbq.
 *
 * Return: Non-negative allocated bit number if successful, -1 otherwise.
 */
int sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
         unsigned int shallow_depth);

/**
 * sbitmap_queue_get() - Try to allocate a free bit from a &struct
 * sbitmap_queue.
 * @sbq: Bitmap queue to allocate from.
 * @cpu: Output parameter; will contain the CPU we ran on (e.g., to be passed to
 *       sbitmap_queue_clear()).
 *
 * Return: Non-negative allocated bit number if successful, -1 otherwise.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sbitmap_queue_get(struct sbitmap_queue *sbq,
        unsigned int *cpu)
{
 int nr;

 *cpu = ({ do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); (*({ do { const void /* nothing */ *__vpp_verify = (typeof((&cpu_number) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&cpu_number)) *)(&cpu_number)); (typeof((typeof(*(&cpu_number)) *)(&cpu_number))) (__ptr + ((__kern_my_cpu_offset()))); }); })); });
 nr = __sbitmap_queue_get(sbq);
 do { __asm__ __volatile__("": : :"memory"); if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0)) preempt_schedule(); } while (0);
 return nr;
}

/**
 * sbitmap_queue_min_shallow_depth() - Inform a &struct sbitmap_queue of the
 * minimum shallow depth that will be used.
 * @sbq: Bitmap queue in question.
 * @min_shallow_depth: The minimum shallow depth that will be passed to
 * sbitmap_queue_get_shallow() or __sbitmap_queue_get_shallow().
 *
 * sbitmap_queue_clear() batches wakeups as an optimization. The batch size
 * depends on the depth of the bitmap. Since the shallow allocation functions
 * effectively operate with a different depth, the shallow depth must be taken
 * into account when calculating the batch size. This function must be called
 * with the minimum shallow depth that will be used. Failure to do so can result
 * in missed wakeups.
 */
void sbitmap_queue_min_shallow_depth(struct sbitmap_queue *sbq,
         unsigned int min_shallow_depth);

/**
 * sbitmap_queue_clear() - Free an allocated bit and wake up waiters on a
 * &struct sbitmap_queue.
 * @sbq: Bitmap to free from.
 * @nr: Bit number to free.
 * @cpu: CPU the bit was allocated on.
 */
void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
    unsigned int cpu);

/**
 * sbitmap_queue_clear_batch() - Free a batch of allocated bits
 * &struct sbitmap_queue.
 * @sbq: Bitmap to free from.
 * @offset: offset for each tag in array
 * @tags: array of tags
 * @nr_tags: number of tags in array
 */
void sbitmap_queue_clear_batch(struct sbitmap_queue *sbq, int offset,
    int *tags, int nr_tags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sbq_index_inc(int index)
{
 return (index + 1) & (8 - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sbq_index_atomic_inc(atomic_t *index)
{
 int old = atomic_read(index);
 int new = sbq_index_inc(old);
 atomic_cmpxchg(index, old, new);
}

/**
 * sbq_wait_ptr() - Get the next wait queue to use for a &struct
 * sbitmap_queue.
 * @sbq: Bitmap queue to wait on.
 * @wait_index: A counter per "user" of @sbq.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct sbq_wait_state *sbq_wait_ptr(struct sbitmap_queue *sbq,
        atomic_t *wait_index)
{
 struct sbq_wait_state *ws;

 ws = &sbq->ws[atomic_read(wait_index)];
 sbq_index_atomic_inc(wait_index);
 return ws;
}

/**
 * sbitmap_queue_wake_all() - Wake up everything waiting on a &struct
 * sbitmap_queue.
 * @sbq: Bitmap queue to wake up.
 */
void sbitmap_queue_wake_all(struct sbitmap_queue *sbq);

/**
 * sbitmap_queue_wake_up() - Wake up some of waiters in one waitqueue
 * on a &struct sbitmap_queue.
 * @sbq: Bitmap queue to wake up.
 * @nr: Number of bits cleared.
 */
void sbitmap_queue_wake_up(struct sbitmap_queue *sbq, int nr);

/**
 * sbitmap_queue_show() - Dump &struct sbitmap_queue information to a &struct
 * seq_file.
 * @sbq: Bitmap queue to show.
 * @m: struct seq_file to write to.
 *
 * This is intended for debugging. The format may change at any time.
 */
void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m);

struct sbq_wait {
 struct sbitmap_queue *sbq; /* if set, sbq_wait is accounted */
 struct wait_queue_entry wait;
};
# 613 "./include/linux/sbitmap.h"
/*
 * Wrapper around prepare_to_wait_exclusive(), which maintains some extra
 * internal state.
 */
void sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,
    struct sbq_wait_state *ws,
    struct sbq_wait *sbq_wait, int state);

/*
 * Must be paired with sbitmap_prepare_to_wait().
 */
void sbitmap_finish_wait(struct sbitmap_queue *sbq, struct sbq_wait_state *ws,
    struct sbq_wait *sbq_wait);

/*
 * Wrapper around add_wait_queue(), which maintains some extra internal state
 */
void sbitmap_add_wait_queue(struct sbitmap_queue *sbq,
       struct sbq_wait_state *ws,
       struct sbq_wait *sbq_wait);

/*
 * Must be paired with sbitmap_add_wait_queue()
 */
void sbitmap_del_wait_queue(struct sbq_wait *sbq_wait);
# 25 "./include/linux/blkdev.h" 2



struct module;
struct request_queue;
struct elevator_queue;
struct blk_trace;
struct request;
struct sg_io_hdr;
struct blkcg_gq;
struct blk_flush_queue;
struct kiocb;
struct pr_ops;
struct rq_qos;
struct blk_queue_stats;
struct blk_stat_callback;
struct blk_crypto_profile;

extern const struct device_type disk_type;
extern struct device_type part_type;
extern struct class block_class;

/* Must be consistent with blk_mq_poll_stats_bkt() */


/* Doing classic polling */


/*
 * Maximum number of blkcg policies allowed to be registered concurrently.
 * Defined here to simplify include dependency.
 */






/*
 * Enough for the string representation of any kind of UUID plus NULL.
 * EFI UUID is 36 characters. MSDOS UUID is 11 characters.
 */


struct partition_meta_info {
 char uuid[(36 + 1)];
 u8 volname[64];
};

/**
 * DOC: genhd capability flags
 *
 * ``GENHD_FL_REMOVABLE``: indicates that the block device gives access to
 * removable media.  When set, the device remains present even when media is not
 * inserted.  Shall not be set for devices which are removed entirely when the
 * media is removed.
 *
 * ``GENHD_FL_HIDDEN``: the block device is hidden; it doesn't produce events,
 * doesn't appear in sysfs, and can't be opened from userspace or using
 * blkdev_get*. Used for the underlying components of multipath devices.
 *
 * ``GENHD_FL_NO_PART``: partition support is disabled.  The kernel will not
 * scan for partitions from add_disk, and users can't add partitions manually.
 *
 */
enum {
 GENHD_FL_REMOVABLE = 1 << 0,
 GENHD_FL_HIDDEN = 1 << 1,
 GENHD_FL_NO_PART = 1 << 2,
};

enum {
 DISK_EVENT_MEDIA_CHANGE = 1 << 0, /* media changed */
 DISK_EVENT_EJECT_REQUEST = 1 << 1, /* eject requested */
};

enum {
 /* Poll even if events_poll_msecs is unset */
 DISK_EVENT_FLAG_POLL = 1 << 0,
 /* Forward events to udev */
 DISK_EVENT_FLAG_UEVENT = 1 << 1,
 /* Block event polling when open for exclusive write */
 DISK_EVENT_FLAG_BLOCK_ON_EXCL_WRITE = 1 << 2,
};

struct disk_events;
struct badblocks;

struct blk_integrity {
 const struct blk_integrity_profile *profile;
 unsigned char flags;
 unsigned char tuple_size;
 unsigned char interval_exp;
 unsigned char tag_size;
};

struct gendisk {
 /*
	 * major/first_minor/minors should not be set by any new driver, the
	 * block core will take care of allocating them automatically.
	 */
 int major;
 int first_minor;
 int minors;

 char disk_name[32]; /* name of major driver */

 unsigned short events; /* supported events */
 unsigned short event_flags; /* flags related to event processing */

 struct xarray part_tbl;
 struct block_device *part0;

 const struct block_device_operations *fops;
 struct request_queue *queue;
 void *private_data;

 struct bio_set bio_split;

 int flags;
 unsigned long state;
# 154 "./include/linux/blkdev.h"
 struct mutex open_mutex; /* open/close mutex */
 unsigned open_partitions; /* number of open partitions */

 struct backing_dev_info *bdi;
 struct kobject queue_kobj; /* the queue/ directory */
 struct kobject *slave_dir;

 struct list_head slave_bdevs;

 struct timer_rand_state *random;
 atomic_t sync_io; /* RAID */
 struct disk_events *ev;

 struct kobject integrity_kobj;
# 195 "./include/linux/blkdev.h"
 int node_id;
 struct badblocks *bb;
 struct lockdep_map lockdep_map;
 u64 diskseq;

 /*
	 * Independent sector access ranges. This is always NULL for
	 * devices that do not have multiple independent access ranges.
	 */
 struct blk_independent_access_ranges *ia_ranges;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool disk_live(struct gendisk *disk)
{
 return !inode_unhashed(disk->part0->bd_inode);
}

/**
 * disk_openers - returns how many openers are there for a disk
 * @disk: disk to check
 *
 * This returns the number of openers for a disk.  Note that this value is only
 * stable if disk->open_mutex is held.
 *
 * Note: Due to a quirk in the block layer open code, each open partition is
 * only counted once even if there are multiple openers.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int disk_openers(struct gendisk *disk)
{
 return atomic_read(&disk->part0->bd_openers);
}

/*
 * The gendisk is refcounted by the part0 block_device, and the bd_device
 * therein is also used for device model presentation in sysfs.
 */
# 242 "./include/linux/blkdev.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) dev_t disk_devt(struct gendisk *disk)
{
 return (((disk->major) << 20) | (disk->first_minor));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blk_validate_block_size(unsigned long bsize)
{
 if (bsize < 512 || bsize > ((1UL) << 12) || !is_power_of_2(bsize))
  return -22 /* Invalid argument */;

 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_op_is_passthrough(blk_opf_t op)
{
 op &= ( blk_opf_t)((1 << 8) - 1);
 return op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;
}

/*
 * Zoned block device models (zoned limit).
 *
 * Note: This needs to be ordered from the least to the most severe
 * restrictions for the inheritance in blk_stack_limits() to work.
 */
enum blk_zoned_model {
 BLK_ZONED_NONE = 0, /* Regular block device */
 BLK_ZONED_HA, /* Host-aware zoned block device */
 BLK_ZONED_HM, /* Host-managed zoned block device */
};

/*
 * BLK_BOUNCE_NONE:	never bounce (default)
 * BLK_BOUNCE_HIGH:	bounce all highmem pages
 */
enum blk_bounce {
 BLK_BOUNCE_NONE,
 BLK_BOUNCE_HIGH,
};

struct queue_limits {
 enum blk_bounce bounce;
 unsigned long seg_boundary_mask;
 unsigned long virt_boundary_mask;

 unsigned int max_hw_sectors;
 unsigned int max_dev_sectors;
 unsigned int chunk_sectors;
 unsigned int max_sectors;
 unsigned int max_segment_size;
 unsigned int physical_block_size;
 unsigned int logical_block_size;
 unsigned int alignment_offset;
 unsigned int io_min;
 unsigned int io_opt;
 unsigned int max_discard_sectors;
 unsigned int max_hw_discard_sectors;
 unsigned int max_secure_erase_sectors;
 unsigned int max_write_zeroes_sectors;
 unsigned int max_zone_append_sectors;
 unsigned int discard_granularity;
 unsigned int discard_alignment;
 unsigned int zone_write_granularity;

 unsigned short max_segments;
 unsigned short max_integrity_segments;
 unsigned short max_discard_segments;

 unsigned char misaligned;
 unsigned char discard_misaligned;
 unsigned char raid_partial_stripes_expensive;
 enum blk_zoned_model zoned;

 /*
	 * Drivers that set dma_alignment to less than 511 must be prepared to
	 * handle individual bvec's that are not a multiple of a SECTOR_SIZE
	 * due to possible offsets.
	 */
 unsigned int dma_alignment;
};

typedef int (*report_zones_cb)(struct blk_zone *zone, unsigned int idx,
          void *data);

void disk_set_zoned(struct gendisk *disk, enum blk_zoned_model model);
# 347 "./include/linux/blkdev.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_nr_zones(struct block_device *bdev)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blkdev_report_zones_ioctl(struct block_device *bdev,
         fmode_t mode, unsigned int cmd,
         unsigned long arg)
{
 return -25 /* Not a typewriter */;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blkdev_zone_mgmt_ioctl(struct block_device *bdev,
      fmode_t mode, unsigned int cmd,
      unsigned long arg)
{
 return -25 /* Not a typewriter */;
}



/*
 * Independent access ranges: struct blk_independent_access_range describes
 * a range of contiguous sectors that can be accessed using device command
 * execution resources that are independent from the resources used for
 * other access ranges. This is typically found with single-LUN multi-actuator
 * HDDs where each access range is served by a different set of heads.
 * The set of independent ranges supported by the device is defined using
 * struct blk_independent_access_ranges. The independent ranges must not overlap
 * and must include all sectors within the disk capacity (no sector holes
 * allowed).
 * For a device with multiple ranges, requests targeting sectors in different
 * ranges can be executed in parallel. A request can straddle an access range
 * boundary.
 */
struct blk_independent_access_range {
 struct kobject kobj;
 sector_t sector;
 sector_t nr_sectors;
};

struct blk_independent_access_ranges {
 struct kobject kobj;
 bool sysfs_registered;
 unsigned int nr_ia_ranges;
 struct blk_independent_access_range ia_range[];
};

struct request_queue {
 struct request *last_merge;
 struct elevator_queue *elevator;

 struct percpu_ref q_usage_counter;

 struct blk_queue_stats *stats;
 struct rq_qos *rq_qos;

 const struct blk_mq_ops *mq_ops;

 /* sw queues */
 struct blk_mq_ctx /* nothing */ *queue_ctx;

 unsigned int queue_depth;

 /* hw dispatch queues */
 struct xarray hctx_table;
 unsigned int nr_hw_queues;

 /*
	 * The queue owner gets to use this for whatever they like.
	 * ll_rw_blk doesn't touch it.
	 */
 void *queuedata;

 /*
	 * various queue flags, see QUEUE_* below
	 */
 unsigned long queue_flags;
 /*
	 * Number of contexts that have called blk_set_pm_only(). If this
	 * counter is above zero then only RQF_PM requests are processed.
	 */
 atomic_t pm_only;

 /*
	 * ida allocated id for this queue.  Used to index queues from
	 * ioctx.
	 */
 int id;

 spinlock_t queue_lock;

 struct gendisk *disk;

 refcount_t refs;

 /*
	 * mq queue kobject
	 */
 struct kobject *mq_kobj;


 struct blk_integrity integrity;



 struct device *dev;
 enum rpm_status rpm_status;


 /*
	 * queue settings
	 */
 unsigned long nr_requests; /* Max # of requests */

 unsigned int dma_pad_mask;






 unsigned int rq_timeout;
 int poll_nsec;

 struct blk_stat_callback *poll_cb;
 struct blk_rq_stat *poll_stat;

 struct timer_list timeout;
 struct work_struct timeout_work;

 atomic_t nr_active_requests_shared_tags;

 struct blk_mq_tags *sched_shared_tags;

 struct list_head icq_list;

 unsigned long blkcg_pols[(((6) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];
 struct blkcg_gq *root_blkg;
 struct list_head blkg_list;


 struct queue_limits limits;

 unsigned int required_elevator_features;

 int node;



 /*
	 * for flush operations
	 */
 struct blk_flush_queue *fq;

 struct list_head requeue_list;
 spinlock_t requeue_lock;
 struct delayed_work requeue_work;

 struct mutex sysfs_lock;
 struct mutex sysfs_dir_lock;

 /*
	 * for reusing dead hctx instance in case of updating
	 * nr_hw_queues
	 */
 struct list_head unused_hctx_list;
 spinlock_t unused_hctx_lock;

 int mq_freeze_depth;





 struct callback_head callback_head;
 wait_queue_head_t mq_freeze_wq;
 /*
	 * Protect concurrent access to q_usage_counter by
	 * percpu_ref_kill() and percpu_ref_reinit().
	 */
 struct mutex mq_freeze_lock;

 int quiesce_depth;

 struct blk_mq_tag_set *tag_set;
 struct list_head tag_set_list;

 struct dentry *debugfs_dir;
 struct dentry *sched_debugfs_dir;
 struct dentry *rqos_debugfs_dir;
 /*
	 * Serializes all debugfs metadata operations using the above dentries.
	 */
 struct mutex debugfs_mutex;

 bool mq_sysfs_init_done;
};

/* Keep blk_queue_flag_name[] in sync with the definitions below */
# 579 "./include/linux/blkdev.h"
void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
# 616 "./include/linux/blkdev.h"
extern void blk_set_pm_only(struct request_queue *q);
extern void blk_clear_pm_only(struct request_queue *q);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool queue_is_mq(struct request_queue *q)
{
 return q->mq_ops;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum rpm_status queue_rpm_status(struct request_queue *q)
{
 return q->rpm_status;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum blk_zoned_model
blk_queue_zoned_model(struct request_queue *q)
{
 if (0)
  return q->limits.zoned;
 return BLK_ZONED_NONE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_queue_is_zoned(struct request_queue *q)
{
 switch (blk_queue_zoned_model(q)) {
 case BLK_ZONED_HA:
 case BLK_ZONED_HM:
  return true;
 default:
  return false;
 }
}
# 706 "./include/linux/blkdev.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int disk_nr_zones(struct gendisk *disk)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool disk_zone_is_seq(struct gendisk *disk, sector_t sector)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int disk_zone_no(struct gendisk *disk, sector_t sector)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_max_open_zones(struct block_device *bdev)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_max_active_zones(struct block_device *bdev)
{
 return 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_queue_depth(struct request_queue *q)
{
 if (q->queue_depth)
  return q->queue_depth;

 return q->nr_requests;
}

/*
 * default timeout for SG_IO if none specified
 */



/* This should not be used directly - use rq_for_each_segment */



int __attribute__((__warn_unused_result__)) device_add_disk(struct device *parent, struct gendisk *disk,
     const struct attribute_group **groups);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) add_disk(struct gendisk *disk)
{
 return device_add_disk(((void *)0), disk, ((void *)0));
}
void del_gendisk(struct gendisk *gp);
void invalidate_disk(struct gendisk *disk);
void set_disk_ro(struct gendisk *disk, bool read_only);
void disk_uevent(struct gendisk *disk, enum kobject_action action);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int get_disk_ro(struct gendisk *disk)
{
 return disk->part0->bd_read_only ||
  ((__builtin_constant_p(1) && __builtin_constant_p((uintptr_t)(&disk->state) != (uintptr_t)((void *)0)) && (uintptr_t)(&disk->state) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&disk->state))) ? const_test_bit(1, &disk->state) : generic_test_bit(1, &disk->state));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int bdev_read_only(struct block_device *bdev)
{
 return bdev->bd_read_only || get_disk_ro(bdev->bd_disk);
}

bool set_capacity_and_notify(struct gendisk *disk, sector_t size);
bool disk_force_media_change(struct gendisk *disk, unsigned int events);

void add_disk_randomness(struct gendisk *disk) ;
void rand_initialize_disk(struct gendisk *disk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sector_t get_start_sect(struct block_device *bdev)
{
 return bdev->bd_start_sect;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sector_t bdev_nr_sectors(struct block_device *bdev)
{
 return bdev->bd_nr_sectors;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) loff_t bdev_nr_bytes(struct block_device *bdev)
{
 return (loff_t)bdev_nr_sectors(bdev) << 9;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sector_t get_capacity(struct gendisk *disk)
{
 return bdev_nr_sectors(disk->part0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u64 sb_bdev_nr_blocks(struct super_block *sb)
{
 return bdev_nr_sectors(sb->s_bdev) >>
  (sb->s_blocksize_bits - 9);
}

int bdev_disk_changed(struct gendisk *disk, bool invalidate);

void put_disk(struct gendisk *disk);
struct gendisk *__blk_alloc_disk(int node, struct lock_class_key *lkclass);

/**
 * blk_alloc_disk - allocate a gendisk structure
 * @node_id: numa node to allocate on
 *
 * Allocate and pre-initialize a gendisk structure for use with BIO based
 * drivers.
 *
 * Context: can sleep
 */







int __register_blkdev(unsigned int major, const char *name,
  void (*probe)(dev_t devt));


void unregister_blkdev(unsigned int major, const char *name);

bool bdev_check_media_change(struct block_device *bdev);
int __invalidate_device(struct block_device *bdev, bool kill_dirty);
void set_capacity(struct gendisk *disk, sector_t size);


int bd_link_disk_holder(struct block_device *bdev, struct gendisk *disk);
void bd_unlink_disk_holder(struct block_device *bdev, struct gendisk *disk);
# 847 "./include/linux/blkdev.h"
dev_t part_devt(struct gendisk *disk, u8 partno);
void inc_diskseq(struct gendisk *disk);
dev_t blk_lookup_devt(const char *name, int partno);
void blk_request_module(dev_t devt);

extern int blk_register_queue(struct gendisk *disk);
extern void blk_unregister_queue(struct gendisk *disk);
void submit_bio_noacct(struct bio *bio);
struct bio *bio_split_to_limits(struct bio *bio);

extern int blk_lld_busy(struct request_queue *q);
extern int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags);
extern void blk_queue_exit(struct request_queue *q);
extern void blk_sync_queue(struct request_queue *q);

/* Helper to convert REQ_OP_XXX to its string format XXX */
extern const char *blk_op_str(enum req_op op);

int blk_status_to_errno(blk_status_t status);
blk_status_t errno_to_blk_status(int errno);

/* only poll the hardware once, don't continue until a completion was found */

/* do not sleep to wait for the expected completion time */

int bio_poll(struct bio *bio, struct io_comp_batch *iob, unsigned int flags);
int iocb_bio_iopoll(struct kiocb *kiocb, struct io_comp_batch *iob,
   unsigned int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct request_queue *bdev_get_queue(struct block_device *bdev)
{
 return bdev->bd_queue; /* this is never NULL */
}

/* Helper to convert BLK_ZONE_ZONE_XXX to its string format XXX */
const char *blk_zone_cond_str(enum blk_zone_cond zone_cond);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bio_zone_no(struct bio *bio)
{
 return disk_zone_no(bio->bi_bdev->bd_disk, bio->bi_iter.bi_sector);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bio_zone_is_seq(struct bio *bio)
{
 return disk_zone_is_seq(bio->bi_bdev->bd_disk, bio->bi_iter.bi_sector);
}

/*
 * Return how much of the chunk is left to be used for I/O at a given offset.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_chunk_sectors_left(sector_t offset,
  unsigned int chunk_sectors)
{
 if (__builtin_expect(!!(!is_power_of_2(chunk_sectors)), 0))
  return chunk_sectors - ({ uint32_t __base = (chunk_sectors); uint32_t __rem; __rem = ((uint64_t)(offset)) % __base; (offset) = ((uint64_t)(offset)) / __base; __rem; });
 return chunk_sectors - (offset & (chunk_sectors - 1));
}

/*
 * Access functions for manipulating queue properties
 */
void blk_queue_bounce_limit(struct request_queue *q, enum blk_bounce limit);
extern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);
extern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);
extern void blk_queue_max_segments(struct request_queue *, unsigned short);
extern void blk_queue_max_discard_segments(struct request_queue *,
  unsigned short);
void blk_queue_max_secure_erase_sectors(struct request_queue *q,
  unsigned int max_sectors);
extern void blk_queue_max_segment_size(struct request_queue *, unsigned int);
extern void blk_queue_max_discard_sectors(struct request_queue *q,
  unsigned int max_discard_sectors);
extern void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
  unsigned int max_write_same_sectors);
extern void blk_queue_logical_block_size(struct request_queue *, unsigned int);
extern void blk_queue_max_zone_append_sectors(struct request_queue *q,
  unsigned int max_zone_append_sectors);
extern void blk_queue_physical_block_size(struct request_queue *, unsigned int);
void blk_queue_zone_write_granularity(struct request_queue *q,
          unsigned int size);
extern void blk_queue_alignment_offset(struct request_queue *q,
           unsigned int alignment);
void disk_update_readahead(struct gendisk *disk);
extern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);
extern void blk_queue_io_min(struct request_queue *q, unsigned int min);
extern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);
extern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);
extern void blk_set_queue_depth(struct request_queue *q, unsigned int depth);
extern void blk_set_stacking_limits(struct queue_limits *lim);
extern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
       sector_t offset);
extern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,
         sector_t offset);
extern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);
extern void blk_queue_segment_boundary(struct request_queue *, unsigned long);
extern void blk_queue_virt_boundary(struct request_queue *, unsigned long);
extern void blk_queue_dma_alignment(struct request_queue *, int);
extern void blk_queue_update_dma_alignment(struct request_queue *, int);
extern void blk_queue_rq_timeout(struct request_queue *, unsigned int);
extern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);

struct blk_independent_access_ranges *
disk_alloc_independent_access_ranges(struct gendisk *disk, int nr_ia_ranges);
void disk_set_independent_access_ranges(struct gendisk *disk,
    struct blk_independent_access_ranges *iars);

/*
 * Elevator features for blk_queue_required_elevator_features:
 */
/* Supports zoned block devices sequential write constraint */


extern void blk_queue_required_elevator_features(struct request_queue *q,
       unsigned int features);
extern bool blk_queue_can_use_dma_map_merging(struct request_queue *q,
           struct device *dev);

bool __attribute__((__warn_unused_result__)) blk_get_queue(struct request_queue *);
extern void blk_put_queue(struct request_queue *);

void blk_mark_disk_dead(struct gendisk *disk);


/*
 * blk_plug permits building a queue of related requests by holding the I/O
 * fragments for a short period. This allows merging of sequential requests
 * into single larger request. As the requests are moved from a per-task list to
 * the device's request_queue in a batch, this results in improved scalability
 * as the lock contention for request_queue lock is reduced.
 *
 * It is ok not to disable preemption when adding the request to the plug list
 * or when attempting a merge. For details, please see schedule() where
 * blk_flush_plug() is called.
 */
struct blk_plug {
 struct request *mq_list; /* blk-mq requests */

 /* if ios_left is > 1, we can batch tag/rq allocations */
 struct request *cached_rq;
 unsigned short nr_ios;

 unsigned short rq_count;

 bool multiple_queues;
 bool has_elevator;
 bool nowait;

 struct list_head cb_list; /* md requires an unplug callback */
};

struct blk_plug_cb;
typedef void (*blk_plug_cb_fn)(struct blk_plug_cb *, bool);
struct blk_plug_cb {
 struct list_head list;
 blk_plug_cb_fn callback;
 void *data;
};
extern struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug,
          void *data, int size);
extern void blk_start_plug(struct blk_plug *);
extern void blk_start_plug_nr_ios(struct blk_plug *, unsigned short);
extern void blk_finish_plug(struct blk_plug *);

void __blk_flush_plug(struct blk_plug *plug, bool from_schedule);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_flush_plug(struct blk_plug *plug, bool async)
{
 if (plug)
  __blk_flush_plug(plug, async);
}

int blkdev_issue_flush(struct block_device *bdev);
long nr_blockdev_pages(void);
# 1051 "./include/linux/blkdev.h"
extern void blk_io_schedule(void);

int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
  sector_t nr_sects, gfp_t gfp_mask);
int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
  sector_t nr_sects, gfp_t gfp_mask, struct bio **biop);
int blkdev_issue_secure_erase(struct block_device *bdev, sector_t sector,
  sector_t nr_sects, gfp_t gfp);




extern int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
  sector_t nr_sects, gfp_t gfp_mask, struct bio **biop,
  unsigned flags);
extern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,
  sector_t nr_sects, gfp_t gfp_mask, unsigned flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sb_issue_discard(struct super_block *sb, sector_t block,
  sector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)
{
 return blkdev_issue_discard(sb->s_bdev,
        block << (sb->s_blocksize_bits -
           9),
        nr_blocks << (sb->s_blocksize_bits -
        9),
        gfp_mask);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sb_issue_zeroout(struct super_block *sb, sector_t block,
  sector_t nr_blocks, gfp_t gfp_mask)
{
 return blkdev_issue_zeroout(sb->s_bdev,
        block << (sb->s_blocksize_bits -
           9),
        nr_blocks << (sb->s_blocksize_bits -
        9),
        gfp_mask, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_is_partition(struct block_device *bdev)
{
 return bdev->bd_partno;
}

enum blk_default_limits {
 BLK_MAX_SEGMENTS = 128,
 BLK_SAFE_MAX_SECTORS = 255,
 BLK_DEF_MAX_SECTORS = 2560,
 BLK_MAX_SEGMENT_SIZE = 65536,
 BLK_SEG_BOUNDARY_MASK = 0xFFFFFFFFUL,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long queue_segment_boundary(const struct request_queue *q)
{
 return q->limits.seg_boundary_mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long queue_virt_boundary(const struct request_queue *q)
{
 return q->limits.virt_boundary_mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_max_sectors(const struct request_queue *q)
{
 return q->limits.max_sectors;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_max_bytes(struct request_queue *q)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)(queue_max_sectors(q))) *)1 == (typeof((unsigned int)(((int)(~0U >> 1)) >> 9)) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(queue_max_sectors(q))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(((int)(~0U >> 1)) >> 9)) * 0l)) : (int *)8))))), (((unsigned int)(queue_max_sectors(q))) < ((unsigned int)(((int)(~0U >> 1)) >> 9)) ? ((unsigned int)(queue_max_sectors(q))) : ((unsigned int)(((int)(~0U >> 1)) >> 9))), ({ typeof((unsigned int)(queue_max_sectors(q))) __UNIQUE_ID___x412 = ((unsigned int)(queue_max_sectors(q))); typeof((unsigned int)(((int)(~0U >> 1)) >> 9)) __UNIQUE_ID___y413 = ((unsigned int)(((int)(~0U >> 1)) >> 9)); ((__UNIQUE_ID___x412) < (__UNIQUE_ID___y413) ? (__UNIQUE_ID___x412) : (__UNIQUE_ID___y413)); })) << 9;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_max_hw_sectors(const struct request_queue *q)
{
 return q->limits.max_hw_sectors;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned short queue_max_segments(const struct request_queue *q)
{
 return q->limits.max_segments;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned short queue_max_discard_segments(const struct request_queue *q)
{
 return q->limits.max_discard_segments;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_max_segment_size(const struct request_queue *q)
{
 return q->limits.max_segment_size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_max_zone_append_sectors(const struct request_queue *q)
{

 const struct queue_limits *l = &q->limits;

 return __builtin_choose_expr(((!!(sizeof((typeof(l->max_zone_append_sectors) *)1 == (typeof(l->max_sectors) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(l->max_zone_append_sectors) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(l->max_sectors) * 0l)) : (int *)8))))), ((l->max_zone_append_sectors) < (l->max_sectors) ? (l->max_zone_append_sectors) : (l->max_sectors)), ({ typeof(l->max_zone_append_sectors) __UNIQUE_ID___x414 = (l->max_zone_append_sectors); typeof(l->max_sectors) __UNIQUE_ID___y415 = (l->max_sectors); ((__UNIQUE_ID___x414) < (__UNIQUE_ID___y415) ? (__UNIQUE_ID___x414) : (__UNIQUE_ID___y415)); }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int
bdev_max_zone_append_sectors(struct block_device *bdev)
{
 return queue_max_zone_append_sectors(bdev_get_queue(bdev));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_max_segments(struct block_device *bdev)
{
 return queue_max_segments(bdev_get_queue(bdev));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned queue_logical_block_size(const struct request_queue *q)
{
 int retval = 512;

 if (q && q->limits.logical_block_size)
  retval = q->limits.logical_block_size;

 return retval;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_logical_block_size(struct block_device *bdev)
{
 return queue_logical_block_size(bdev_get_queue(bdev));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_physical_block_size(const struct request_queue *q)
{
 return q->limits.physical_block_size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_physical_block_size(struct block_device *bdev)
{
 return queue_physical_block_size(bdev_get_queue(bdev));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_io_min(const struct request_queue *q)
{
 return q->limits.io_min;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int bdev_io_min(struct block_device *bdev)
{
 return queue_io_min(bdev_get_queue(bdev));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int queue_io_opt(const struct request_queue *q)
{
 return q->limits.io_opt;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int bdev_io_opt(struct block_device *bdev)
{
 return queue_io_opt(bdev_get_queue(bdev));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int
queue_zone_write_granularity(const struct request_queue *q)
{
 return q->limits.zone_write_granularity;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int
bdev_zone_write_granularity(struct block_device *bdev)
{
 return queue_zone_write_granularity(bdev_get_queue(bdev));
}

int bdev_alignment_offset(struct block_device *bdev);
unsigned int bdev_discard_alignment(struct block_device *bdev);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_max_discard_sectors(struct block_device *bdev)
{
 return bdev_get_queue(bdev)->limits.max_discard_sectors;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_discard_granularity(struct block_device *bdev)
{
 return bdev_get_queue(bdev)->limits.discard_granularity;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int
bdev_max_secure_erase_sectors(struct block_device *bdev)
{
 return bdev_get_queue(bdev)->limits.max_secure_erase_sectors;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_write_zeroes_sectors(struct block_device *bdev)
{
 struct request_queue *q = bdev_get_queue(bdev);

 if (q)
  return q->limits.max_write_zeroes_sectors;

 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_nonrot(struct block_device *bdev)
{
 return ((__builtin_constant_p(6 /* non-rotational device (SSD) */) && __builtin_constant_p((uintptr_t)(&(bdev_get_queue(bdev))->queue_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&(bdev_get_queue(bdev))->queue_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&(bdev_get_queue(bdev))->queue_flags))) ? const_test_bit(6 /* non-rotational device (SSD) */, &(bdev_get_queue(bdev))->queue_flags) : generic_test_bit(6 /* non-rotational device (SSD) */, &(bdev_get_queue(bdev))->queue_flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_stable_writes(struct block_device *bdev)
{
 return ((__builtin_constant_p(15 /* don't modify blks until WB is done */) && __builtin_constant_p((uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&bdev_get_queue(bdev)->queue_flags))) ? const_test_bit(15 /* don't modify blks until WB is done */, &bdev_get_queue(bdev)->queue_flags) : generic_test_bit(15 /* don't modify blks until WB is done */, &bdev_get_queue(bdev)->queue_flags));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_write_cache(struct block_device *bdev)
{
 return ((__builtin_constant_p(17 /* Write back caching */) && __builtin_constant_p((uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&bdev_get_queue(bdev)->queue_flags))) ? const_test_bit(17 /* Write back caching */, &bdev_get_queue(bdev)->queue_flags) : generic_test_bit(17 /* Write back caching */, &bdev_get_queue(bdev)->queue_flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_fua(struct block_device *bdev)
{
 return ((__builtin_constant_p(18 /* device supports FUA writes */) && __builtin_constant_p((uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&bdev_get_queue(bdev)->queue_flags))) ? const_test_bit(18 /* device supports FUA writes */, &bdev_get_queue(bdev)->queue_flags) : generic_test_bit(18 /* device supports FUA writes */, &bdev_get_queue(bdev)->queue_flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_nowait(struct block_device *bdev)
{
 return ((__builtin_constant_p(29 /* device supports NOWAIT */) && __builtin_constant_p((uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&bdev_get_queue(bdev)->queue_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&bdev_get_queue(bdev)->queue_flags))) ? const_test_bit(29 /* device supports NOWAIT */, &bdev_get_queue(bdev)->queue_flags) : generic_test_bit(29 /* device supports NOWAIT */, &bdev_get_queue(bdev)->queue_flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum blk_zoned_model bdev_zoned_model(struct block_device *bdev)
{
 struct request_queue *q = bdev_get_queue(bdev);

 if (q)
  return blk_queue_zoned_model(q);

 return BLK_ZONED_NONE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_is_zoned(struct block_device *bdev)
{
 struct request_queue *q = bdev_get_queue(bdev);

 if (q)
  return blk_queue_is_zoned(q);

 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_op_is_zoned_write(struct block_device *bdev,
       blk_opf_t op)
{
 if (!bdev_is_zoned(bdev))
  return false;

 return op == REQ_OP_WRITE || op == REQ_OP_WRITE_ZEROES;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sector_t bdev_zone_sectors(struct block_device *bdev)
{
 struct request_queue *q = bdev_get_queue(bdev);

 if (!blk_queue_is_zoned(q))
  return 0;
 return q->limits.chunk_sectors;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int queue_dma_alignment(const struct request_queue *q)
{
 return q ? q->limits.dma_alignment : 511;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int bdev_dma_alignment(struct block_device *bdev)
{
 return queue_dma_alignment(bdev_get_queue(bdev));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool bdev_iter_is_aligned(struct block_device *bdev,
     struct iov_iter *iter)
{
 return iov_iter_is_aligned(iter, bdev_dma_alignment(bdev),
       bdev_logical_block_size(bdev) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blk_rq_aligned(struct request_queue *q, unsigned long addr,
     unsigned int len)
{
 unsigned int alignment = queue_dma_alignment(q) | q->dma_pad_mask;
 return !(addr & alignment) && !(len & alignment);
}

/* assumes size > 256 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blksize_bits(unsigned int size)
{
 return ( __builtin_constant_p(size >> 9) ? ( ((size >> 9) == 0 || (size >> 9) == 1) ? 0 : ( __builtin_constant_p((size >> 9) - 1) ? (((size >> 9) - 1) < 2 ? 0 : 63 - __builtin_clzll((size >> 9) - 1)) : (sizeof((size >> 9) - 1) <= 4) ? __ilog2_u32((size >> 9) - 1) : __ilog2_u64((size >> 9) - 1) ) + 1) : __order_base_2(size >> 9) ) + 9;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int block_size(struct block_device *bdev)
{
 return 1 << bdev->bd_inode->i_blkbits;
}

int kblockd_schedule_work(struct work_struct *work);
int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);
# 1362 "./include/linux/blkdev.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_crypto_register(struct blk_crypto_profile *profile,
           struct request_queue *q)
{
 return true;
}



enum blk_unique_id {
 /* these match the Designator Types specified in SPC */
 BLK_UID_T10 = 1,
 BLK_UID_EUI64 = 2,
 BLK_UID_NAA = 3,
};



struct block_device_operations {
 void (*submit_bio)(struct bio *bio);
 int (*poll_bio)(struct bio *bio, struct io_comp_batch *iob,
   unsigned int flags);
 int (*open) (struct block_device *, fmode_t);
 void (*release) (struct gendisk *, fmode_t);
 int (*rw_page)(struct block_device *, sector_t, struct page *, enum req_op);
 int (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 int (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);
 unsigned int (*check_events) (struct gendisk *disk,
          unsigned int clearing);
 void (*unlock_native_capacity) (struct gendisk *);
 int (*getgeo)(struct block_device *, struct hd_geometry *);
 int (*set_read_only)(struct block_device *bdev, bool ro);
 void (*free_disk)(struct gendisk *disk);
 /* this callback is with swap_lock and sometimes page table lock held */
 void (*swap_slot_free_notify) (struct block_device *, unsigned long);
 int (*report_zones)(struct gendisk *, sector_t sector,
   unsigned int nr_zones, report_zones_cb cb, void *data);
 /* returns the length of the identifier or a negative errno: */
 int (*get_unique_id)(struct gendisk *disk, u8 id[16],
   enum blk_unique_id id_type);
 struct module *owner;
 const struct pr_ops *pr_ops;

 /*
	 * Special callback for probing GPT entry at a given sector.
	 * Needed by Android devices, used by GPT scanner and MMC blk
	 * driver.
	 */
 int (*alternative_gpt_sector)(struct gendisk *disk, sector_t *sector);
};


extern int blkdev_compat_ptr_ioctl(struct block_device *, fmode_t,
          unsigned int, unsigned long);




extern int bdev_read_page(struct block_device *, sector_t, struct page *);
extern int bdev_write_page(struct block_device *, sector_t, struct page *,
      struct writeback_control *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_wake_io_task(struct task_struct *waiter)
{
 /*
	 * If we're polling, the task itself is doing the completions. For
	 * that case, we don't need to signal a wakeup, it's enough to just
	 * mark us as RUNNING.
	 */
 if (waiter == get_current())
  do { do { } while (0); do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_416(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(get_current()->__state) == sizeof(char) || sizeof(get_current()->__state) == sizeof(short) || sizeof(get_current()->__state) == sizeof(int) || sizeof(get_current()->__state) == sizeof(long)) || sizeof(get_current()->__state) == sizeof(long long))) __compiletime_assert_416(); } while (0); do { *(volatile typeof(get_current()->__state) *)&(get_current()->__state) = ((0x00000000)); } while (0); } while (0); } while (0);
# 1432 "./include/linux/blkdev.h"
 else
  wake_up_process(waiter);
}

unsigned long bdev_start_io_acct(struct block_device *bdev,
     unsigned int sectors, enum req_op op,
     unsigned long start_time);
void bdev_end_io_acct(struct block_device *bdev, enum req_op op,
  unsigned long start_time);

unsigned long bio_start_io_acct(struct bio *bio);
void bio_end_io_acct_remapped(struct bio *bio, unsigned long start_time,
  struct block_device *orig_bdev);

/**
 * bio_end_io_acct - end I/O accounting for bio based drivers
 * @bio:	bio to end account for
 * @start_time:	start time returned by bio_start_io_acct()
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void bio_end_io_acct(struct bio *bio, unsigned long start_time)
{
 return bio_end_io_acct_remapped(bio, start_time, bio->bi_bdev);
}

int bdev_read_only(struct block_device *bdev);
int set_blocksize(struct block_device *bdev, int size);

int lookup_bdev(const char *pathname, dev_t *dev);

void blkdev_show(struct seq_file *seqf, off_t offset);
# 1471 "./include/linux/blkdev.h"
struct block_device *blkdev_get_by_path(const char *path, fmode_t mode,
  void *holder);
struct block_device *blkdev_get_by_dev(dev_t dev, fmode_t mode, void *holder);
int bd_prepare_to_claim(struct block_device *bdev, void *holder);
void bd_abort_claiming(struct block_device *bdev, void *holder);
void blkdev_put(struct block_device *bdev, fmode_t mode);

/* just for blk-cgroup, don't use elsewhere */
struct block_device *blkdev_get_no_open(dev_t dev);
void blkdev_put_no_open(struct block_device *bdev);

struct block_device *bdev_alloc(struct gendisk *disk, u8 partno);
void bdev_add(struct block_device *bdev, dev_t dev);
struct block_device *I_BDEV(struct inode *inode);
int truncate_bdev_range(struct block_device *bdev, fmode_t mode, loff_t lstart,
  loff_t lend);


void invalidate_bdev(struct block_device *bdev);
int sync_blockdev(struct block_device *bdev);
int sync_blockdev_range(struct block_device *bdev, loff_t lstart, loff_t lend);
int sync_blockdev_nowait(struct block_device *bdev);
void sync_bdevs(bool wait);
void bdev_statx_dioalign(struct inode *inode, struct kstat *stat);
void printk_all_partitions(void);
# 1519 "./include/linux/blkdev.h"
int fsync_bdev(struct block_device *bdev);

int freeze_bdev(struct block_device *bdev);
int thaw_bdev(struct block_device *bdev);

struct io_comp_batch {
 struct request *req_list;
 bool need_ts;
 void (*complete)(struct io_comp_batch *);
};
# 4 "drivers/scsi/scsi_devinfo.c" 2




# 1 "./include/linux/proc_fs.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * The proc filesystem constants/structures
 */







struct proc_dir_entry;
struct seq_file;
struct seq_operations;

enum {
 /*
	 * All /proc entries using this ->proc_ops instance are never removed.
	 *
	 * If in doubt, ignore this flag.
	 */



 PROC_ENTRY_PERMANENT = 1U << 0,

};

struct proc_ops {
 unsigned int proc_flags;
 int (*proc_open)(struct inode *, struct file *);
 ssize_t (*proc_read)(struct file *, char /* nothing */ *, size_t, loff_t *);
 ssize_t (*proc_read_iter)(struct kiocb *, struct iov_iter *);
 ssize_t (*proc_write)(struct file *, const char /* nothing */ *, size_t, loff_t *);
 /* mandatory unless nonseekable_open() or equivalent is used */
 loff_t (*proc_lseek)(struct file *, loff_t, int);
 int (*proc_release)(struct inode *, struct file *);
 __poll_t (*proc_poll)(struct file *, struct poll_table_struct *);
 long (*proc_ioctl)(struct file *, unsigned int, unsigned long);

 long (*proc_compat_ioctl)(struct file *, unsigned int, unsigned long);

 int (*proc_mmap)(struct file *, struct vm_area_struct *);
 unsigned long (*proc_get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
} ;

/* definitions for hide_pid field */
enum proc_hidepid {
 HIDEPID_OFF = 0,
 HIDEPID_NO_ACCESS = 1,
 HIDEPID_INVISIBLE = 2,
 HIDEPID_NOT_PTRACEABLE = 4, /* Limit pids to only ptraceable pids */
};

/* definitions for proc mount option pidonly */
enum proc_pidonly {
 PROC_PIDONLY_OFF = 0,
 PROC_PIDONLY_ON = 1,
};

struct proc_fs_info {
 struct pid_namespace *pid_ns;
 struct dentry *proc_self; /* For /proc/self */
 struct dentry *proc_thread_self; /* For /proc/thread-self */
 kgid_t pid_gid;
 enum proc_hidepid hide_pid;
 enum proc_pidonly pidonly;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct proc_fs_info *proc_sb_info(struct super_block *sb)
{
 return sb->s_fs_info;
}



typedef int (*proc_write_t)(struct file *, char *, size_t);

extern void proc_root_init(void);
extern void proc_flush_pid(struct pid *);

extern struct proc_dir_entry *proc_symlink(const char *,
  struct proc_dir_entry *, const char *);
struct proc_dir_entry *_proc_mkdir(const char *, umode_t, struct proc_dir_entry *, void *, bool);
extern struct proc_dir_entry *proc_mkdir(const char *, struct proc_dir_entry *);
extern struct proc_dir_entry *proc_mkdir_data(const char *, umode_t,
           struct proc_dir_entry *, void *);
extern struct proc_dir_entry *proc_mkdir_mode(const char *, umode_t,
           struct proc_dir_entry *);
struct proc_dir_entry *proc_create_mount_point(const char *name);

struct proc_dir_entry *proc_create_seq_private(const char *name, umode_t mode,
  struct proc_dir_entry *parent, const struct seq_operations *ops,
  unsigned int state_size, void *data);




struct proc_dir_entry *proc_create_single_data(const char *name, umode_t mode,
  struct proc_dir_entry *parent,
  int (*show)(struct seq_file *, void *), void *data);



extern struct proc_dir_entry *proc_create_data(const char *, umode_t,
            struct proc_dir_entry *,
            const struct proc_ops *,
            void *);

struct proc_dir_entry *proc_create(const char *name, umode_t mode, struct proc_dir_entry *parent, const struct proc_ops *proc_ops);
extern void proc_set_size(struct proc_dir_entry *, loff_t);
extern void proc_set_user(struct proc_dir_entry *, kuid_t, kgid_t);

/*
 * Obtain the private data passed by user through proc_create_data() or
 * related.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *pde_data(const struct inode *inode)
{
 return inode->i_private;
}

extern void *proc_get_parent_data(const struct inode *);
extern void proc_remove(struct proc_dir_entry *);
extern void remove_proc_entry(const char *, struct proc_dir_entry *);
extern int remove_proc_subtree(const char *, struct proc_dir_entry *);

struct proc_dir_entry *proc_create_net_data(const char *name, umode_t mode,
  struct proc_dir_entry *parent, const struct seq_operations *ops,
  unsigned int state_size, void *data);


struct proc_dir_entry *proc_create_net_single(const char *name, umode_t mode,
  struct proc_dir_entry *parent,
  int (*show)(struct seq_file *, void *), void *data);
struct proc_dir_entry *proc_create_net_data_write(const char *name, umode_t mode,
        struct proc_dir_entry *parent,
        const struct seq_operations *ops,
        proc_write_t write,
        unsigned int state_size, void *data);
struct proc_dir_entry *proc_create_net_single_write(const char *name, umode_t mode,
          struct proc_dir_entry *parent,
          int (*show)(struct seq_file *, void *),
          proc_write_t write,
          void *data);
extern struct pid *tgid_pidfd_to_pid(const struct file *file);

struct bpf_iter_aux_info;
extern int bpf_iter_init_seq_net(void *priv_data, struct bpf_iter_aux_info *aux);
extern void bpf_iter_fini_seq_net(void *priv_data);
# 223 "./include/linux/proc_fs.h"
struct net;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct proc_dir_entry *proc_net_mkdir(
 struct net *net, const char *name, struct proc_dir_entry *parent)
{
 return _proc_mkdir(name, 0, parent, net, true);
}

struct ns_common;
int open_related_ns(struct ns_common *ns,
     struct ns_common *(*get_ns)(struct ns_common *ns));

/* get the associated pid namespace for a file in procfs */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct pid_namespace *proc_pid_ns(struct super_block *sb)
{
 return proc_sb_info(sb)->pid_ns;
}

bool proc_ns_file(const struct file *file);
# 9 "drivers/scsi/scsi_devinfo.c" 2
# 1 "./include/linux/seq_file.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





# 1 "./include/linux/string_helpers.h" 1
/* SPDX-License-Identifier: GPL-2.0 */




# 1 "./include/linux/ctype.h" 1
/* SPDX-License-Identifier: GPL-2.0 */





/*
 * NOTE! This ctype does not handle EOF like the standard C
 * library is required to.
 */
# 21 "./include/linux/ctype.h"
extern const unsigned char _ctype[];
# 32 "./include/linux/ctype.h"
/* Note: isspace() must return false for %NUL-terminator */
# 43 "./include/linux/ctype.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int isdigit(int c)
{
 return '0' <= c && c <= '9';
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned char __tolower(unsigned char c)
{
 if ((((_ctype[(int)(unsigned char)(c)])&(0x01 /* upper */)) != 0))
  c -= 'A'-'a';
 return c;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned char __toupper(unsigned char c)
{
 if ((((_ctype[(int)(unsigned char)(c)])&(0x02 /* lower */)) != 0))
  c -= 'a'-'A';
 return c;
}




/*
 * Fast implementation of tolower() for internal usage. Do not use in your
 * code.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) char _tolower(const char c)
{
 return c | 0x20;
}

/* Fast check for octal digit */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int isodigit(const char c)
{
 return c >= '0' && c <= '7';
}
# 7 "./include/linux/string_helpers.h" 2



struct device;
struct file;
struct task_struct;

/* Descriptions of the types of units to
 * print in */
enum string_size_units {
 STRING_UNITS_10, /* use powers of 10^3 (standard SI) */
 STRING_UNITS_2, /* use binary powers of 2^10 */
};

void string_get_size(u64 size, u64 blk_size, enum string_size_units units,
       char *buf, int len);

int parse_int_array_user(const char /* nothing */ *from, size_t count, int **array);
# 35 "./include/linux/string_helpers.h"
int string_unescape(char *src, char *dst, size_t size, unsigned int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int string_unescape_inplace(char *buf, unsigned int flags)
{
 return string_unescape(buf, buf, 0, flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int string_unescape_any(char *src, char *dst, size_t size)
{
 return string_unescape(src, dst, size, (((((1UL))) << (0)) | ((((1UL))) << (1)) | ((((1UL))) << (2)) | ((((1UL))) << (3))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int string_unescape_any_inplace(char *buf)
{
 return string_unescape_any(buf, buf, 0);
}
# 67 "./include/linux/string_helpers.h"
int string_escape_mem(const char *src, size_t isz, char *dst, size_t osz,
  unsigned int flags, const char *only);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int string_escape_mem_any_np(const char *src, size_t isz,
  char *dst, size_t osz, const char *only)
{
 return string_escape_mem(src, isz, dst, osz, ((((((1UL))) << (0)) | ((((1UL))) << (3)) | ((((1UL))) << (1)) | ((((1UL))) << (2))) | ((((1UL))) << (4))), only);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int string_escape_str(const char *src, char *dst, size_t sz,
  unsigned int flags, const char *only)
{
 return string_escape_mem(src, strlen(src), dst, sz, flags, only);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int string_escape_str_any_np(const char *src, char *dst,
  size_t sz, const char *only)
{
 return string_escape_str(src, dst, sz, ((((((1UL))) << (0)) | ((((1UL))) << (3)) | ((((1UL))) << (1)) | ((((1UL))) << (2))) | ((((1UL))) << (4))), only);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void string_upper(char *dst, const char *src)
{
 do {
  *dst++ = __toupper(*src);
 } while (*src++);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void string_lower(char *dst, const char *src)
{
 do {
  *dst++ = __tolower(*src);
 } while (*src++);
}

char *kstrdup_quotable(const char *src, gfp_t gfp);
char *kstrdup_quotable_cmdline(struct task_struct *task, gfp_t gfp);
char *kstrdup_quotable_file(struct file *file, gfp_t gfp);

char **kasprintf_strarray(gfp_t gfp, const char *prefix, size_t n);
void kfree_strarray(char **array, size_t n);

char **devm_kasprintf_strarray(struct device *dev, const char *prefix, size_t n);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *str_yes_no(bool v)
{
 return v ? "yes" : "no";
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *str_on_off(bool v)
{
 return v ? "on" : "off";
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *str_enable_disable(bool v)
{
 return v ? "enable" : "disable";
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *str_enabled_disabled(bool v)
{
 return v ? "enabled" : "disabled";
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) const char *str_read_write(bool v)
{
 return v ? "read" : "write";
}
# 8 "./include/linux/seq_file.h" 2







struct seq_operations;

struct seq_file {
 char *buf;
 size_t size;
 size_t from;
 size_t count;
 size_t pad_until;
 loff_t index;
 loff_t read_pos;
 struct mutex lock;
 const struct seq_operations *op;
 int poll_event;
 const struct file *file;
 void *private;
};

struct seq_operations {
 void * (*start) (struct seq_file *m, loff_t *pos);
 void (*stop) (struct seq_file *m, void *v);
 void * (*next) (struct seq_file *m, void *v, loff_t *pos);
 int (*show) (struct seq_file *m, void *v);
};



/**
 * seq_has_overflowed - check if the buffer has overflowed
 * @m: the seq_file handle
 *
 * seq_files have a buffer which may overflow. When this happens a larger
 * buffer is reallocated and all the data will be printed again.
 * The overflow state is true when m->count == m->size.
 *
 * Returns true if the buffer received more than it can hold.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool seq_has_overflowed(struct seq_file *m)
{
 return m->count == m->size;
}

/**
 * seq_get_buf - get buffer to write arbitrary data to
 * @m: the seq_file handle
 * @bufp: the beginning of the buffer is stored here
 *
 * Return the number of bytes available in the buffer, or zero if
 * there's no space.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) size_t seq_get_buf(struct seq_file *m, char **bufp)
{
 do { if (__builtin_expect(!!(m->count > m->size), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/seq_file.h\"; .popsection; .long 14472b - .; .short 66; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 if (m->count < m->size)
  *bufp = m->buf + m->count;
 else
  *bufp = ((void *)0);

 return m->size - m->count;
}

/**
 * seq_commit - commit data to the buffer
 * @m: the seq_file handle
 * @num: the number of bytes to commit
 *
 * Commit @num bytes of data written to a buffer previously acquired
 * by seq_buf_get.  To signal an error condition, or that the data
 * didn't fit in the available space, pass a negative @num value.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void seq_commit(struct seq_file *m, int num)
{
 if (num < 0) {
  m->count = m->size;
 } else {
  do { if (__builtin_expect(!!(m->count + num > m->size), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/seq_file.h\"; .popsection; .long 14472b - .; .short 89; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
  m->count += num;
 }
}

/**
 * seq_setwidth - set padding width
 * @m: the seq_file handle
 * @size: the max number of bytes to pad.
 *
 * Call seq_setwidth() for setting max width, then call seq_printf() etc. and
 * finally call seq_pad() to pad the remaining bytes.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void seq_setwidth(struct seq_file *m, size_t size)
{
 m->pad_until = m->count + size;
}
void seq_pad(struct seq_file *m, char c);

char *mangle_path(char *s, const char *p, const char *esc);
int seq_open(struct file *, const struct seq_operations *);
ssize_t seq_read(struct file *, char /* nothing */ *, size_t, loff_t *);
ssize_t seq_read_iter(struct kiocb *iocb, struct iov_iter *iter);
loff_t seq_lseek(struct file *, loff_t, int);
int seq_release(struct inode *, struct file *);
int seq_write(struct seq_file *seq, const void *data, size_t len);

__attribute__((__format__(printf, 2, 0)))
void seq_vprintf(struct seq_file *m, const char *fmt, va_list args);
__attribute__((__format__(printf, 2, 3)))
void seq_printf(struct seq_file *m, const char *fmt, ...);
void seq_putc(struct seq_file *m, char c);
void seq_puts(struct seq_file *m, const char *s);
void seq_put_decimal_ull_width(struct seq_file *m, const char *delimiter,
          unsigned long long num, unsigned int width);
void seq_put_decimal_ull(struct seq_file *m, const char *delimiter,
    unsigned long long num);
void seq_put_decimal_ll(struct seq_file *m, const char *delimiter, long long num);
void seq_put_hex_ll(struct seq_file *m, const char *delimiter,
      unsigned long long v, unsigned int width);

void seq_escape_mem(struct seq_file *m, const char *src, size_t len,
      unsigned int flags, const char *esc);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void seq_escape_str(struct seq_file *m, const char *src,
      unsigned int flags, const char *esc)
{
 seq_escape_mem(m, src, strlen(src), flags, esc);
}

/**
 * seq_escape - print string into buffer, escaping some characters
 * @m: target buffer
 * @s: NULL-terminated string
 * @esc: set of characters that need escaping
 *
 * Puts string into buffer, replacing each occurrence of character from
 * @esc with usual octal escape.
 *
 * Use seq_has_overflowed() to check for errors.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void seq_escape(struct seq_file *m, const char *s, const char *esc)
{
 seq_escape_str(m, s, ((((1UL))) << (3)), esc);
}

void seq_hex_dump(struct seq_file *m, const char *prefix_str, int prefix_type,
    int rowsize, int groupsize, const void *buf, size_t len,
    bool ascii);

int seq_path(struct seq_file *, const struct path *, const char *);
int seq_file_path(struct seq_file *, struct file *, const char *);
int seq_dentry(struct seq_file *, struct dentry *, const char *);
int seq_path_root(struct seq_file *m, const struct path *path,
    const struct path *root, const char *esc);

void *single_start(struct seq_file *, loff_t *);
int single_open(struct file *, int (*)(struct seq_file *, void *), void *);
int single_open_size(struct file *, int (*)(struct seq_file *, void *), void *, size_t);
int single_release(struct inode *, struct file *);
void *__seq_open_private(struct file *, const struct seq_operations *, int);
int seq_open_private(struct file *, const struct seq_operations *, int);
int seq_release_private(struct inode *, struct file *);


void seq_bprintf(struct seq_file *m, const char *f, const u32 *binary);
# 223 "./include/linux/seq_file.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct user_namespace *seq_user_ns(struct seq_file *seq)
{

 return seq->file->f_cred->user_ns;




}

/**
 * seq_show_options - display mount options with appropriate escapes.
 * @m: the seq_file handle
 * @name: the mount option name
 * @value: the mount option name's value, can be NULL
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void seq_show_option(struct seq_file *m, const char *name,
       const char *value)
{
 seq_putc(m, ',');
 seq_escape(m, name, ",= \t\n\\");
 if (value) {
  seq_putc(m, '=');
  seq_escape(m, value, ", \t\n\\");
 }
}

/**
 * seq_show_option_n - display mount options with appropriate escapes
 *		       where @value must be a specific length.
 * @m: the seq_file handle
 * @name: the mount option name
 * @value: the mount option name's value, cannot be NULL
 * @length: the length of @value to display
 *
 * This is a macro since this uses "length" to define the size of the
 * stack buffer.
 */
# 269 "./include/linux/seq_file.h"
/*
 * Helpers for iteration over list_head-s in seq_files
 */

extern struct list_head *seq_list_start(struct list_head *head,
  loff_t pos);
extern struct list_head *seq_list_start_head(struct list_head *head,
  loff_t pos);
extern struct list_head *seq_list_next(void *v, struct list_head *head,
  loff_t *ppos);

extern struct list_head *seq_list_start_rcu(struct list_head *head, loff_t pos);
extern struct list_head *seq_list_start_head_rcu(struct list_head *head, loff_t pos);
extern struct list_head *seq_list_next_rcu(void *v, struct list_head *head, loff_t *ppos);

/*
 * Helpers for iteration over hlist_head-s in seq_files
 */

extern struct hlist_node *seq_hlist_start(struct hlist_head *head,
       loff_t pos);
extern struct hlist_node *seq_hlist_start_head(struct hlist_head *head,
            loff_t pos);
extern struct hlist_node *seq_hlist_next(void *v, struct hlist_head *head,
      loff_t *ppos);

extern struct hlist_node *seq_hlist_start_rcu(struct hlist_head *head,
           loff_t pos);
extern struct hlist_node *seq_hlist_start_head_rcu(struct hlist_head *head,
         loff_t pos);
extern struct hlist_node *seq_hlist_next_rcu(void *v,
         struct hlist_head *head,
         loff_t *ppos);

/* Helpers for iterating over per-cpu hlist_head-s in seq_files */
extern struct hlist_node *seq_hlist_start_percpu(struct hlist_head /* nothing */ *head, int *cpu, loff_t pos);

extern struct hlist_node *seq_hlist_next_percpu(void *v, struct hlist_head /* nothing */ *head, int *cpu, loff_t *pos);

void seq_file_init(void);
# 10 "drivers/scsi/scsi_devinfo.c" 2


# 1 "./include/scsi/scsi_device.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






# 1 "./include/linux/blk-mq.h" 1
/* SPDX-License-Identifier: GPL-2.0 */






# 1 "./include/linux/scatterlist.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
# 11 "./include/linux/scatterlist.h"
struct scatterlist {
 unsigned long page_link;
 unsigned int offset;
 unsigned int length;
 dma_addr_t dma_address;

 unsigned int dma_length;




};

/*
 * These macros should be used after a dma_map_sg call has been done
 * to get bus addresses of each of the SG entries and their lengths.
 * You should only work with the number of sg entries dma_map_sg
 * returns, or alternatively stop on the first sg_dma_len(sg) which
 * is 0.
 */
# 39 "./include/linux/scatterlist.h"
struct sg_table {
 struct scatterlist *sgl; /* the list */
 unsigned int nents; /* number of mapped entries */
 unsigned int orig_nents; /* original size of list */
};

struct sg_append_table {
 struct sg_table sgt; /* The scatter list table */
 struct scatterlist *prv; /* last populated sge in the table */
 unsigned int total_nents; /* Total entries in the table */
};

/*
 * Notes on SG table design.
 *
 * We use the unsigned long page_link field in the scatterlist struct to place
 * the page pointer AND encode information about the sg table as well. The two
 * lower bits are reserved for this information.
 *
 * If bit 0 is set, then the page_link contains a pointer to the next sg
 * table list. Otherwise the next entry is at sg + 1.
 *
 * If bit 1 is set, then this sg entry is the last element in a list.
 *
 * See sg_next().
 *
 */




/*
 * We overload the LSB of the page pointer to indicate whether it's
 * a valid sg entry, or whether it points to the start of a new scatterlist.
 * Those low bits are there for everyone! (thanks mason :-)
 */


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int __sg_flags(struct scatterlist *sg)
{
 return sg->page_link & (0x01UL | 0x02UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct scatterlist *sg_chain_ptr(struct scatterlist *sg)
{
 return (struct scatterlist *)(sg->page_link & ~(0x01UL | 0x02UL));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sg_is_chain(struct scatterlist *sg)
{
 return __sg_flags(sg) & 0x01UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sg_is_last(struct scatterlist *sg)
{
 return __sg_flags(sg) & 0x02UL;
}

/**
 * sg_assign_page - Assign a given page to an SG entry
 * @sg:		    SG entry
 * @page:	    The page
 *
 * Description:
 *   Assign page to sg entry. Also see sg_set_page(), the most commonly used
 *   variant.
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_assign_page(struct scatterlist *sg, struct page *page)
{
 unsigned long page_link = sg->page_link & (0x01UL | 0x02UL);

 /*
	 * In order for the low bit stealing approach to work, pages
	 * must be aligned at a 32-bit boundary as a minimum.
	 */
 do { if (__builtin_expect(!!((unsigned long)page & (0x01UL | 0x02UL)), 0)) do { asm volatile (".pushsection __bug_table,\"aw\"; .align 2; 14470: .long 14471f - .; .pushsection .rodata.str,\"aMS\",@progbits,1; 14472: .string \"include/linux/scatterlist.h\"; .popsection; .long 14472b - .; .short 115; .short 0; .popsection; 14471: brk 0x800");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);



 sg->page_link = page_link | (unsigned long) page;
}

/**
 * sg_set_page - Set sg entry to point at given page
 * @sg:		 SG entry
 * @page:	 The page
 * @len:	 Length of data
 * @offset:	 Offset into page
 *
 * Description:
 *   Use this function to set an sg entry pointing at a page, never assign
 *   the page directly. We encode sg table information in the lower bits
 *   of the page pointer. See sg_page() for looking up the page belonging
 *   to an sg entry.
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_set_page(struct scatterlist *sg, struct page *page,
          unsigned int len, unsigned int offset)
{
 sg_assign_page(sg, page);
 sg->offset = offset;
 sg->length = len;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *sg_page(struct scatterlist *sg)
{



 return (struct page *)((sg)->page_link & ~(0x01UL | 0x02UL));
}

/**
 * sg_set_buf - Set sg entry to point at given data
 * @sg:		 SG entry
 * @buf:	 Data
 * @buflen:	 Data length
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_set_buf(struct scatterlist *sg, const void *buf,
         unsigned int buflen)
{



 sg_set_page(sg, ({ u64 __idx = (((u64)buf) - ((-((((1UL))) << ((48)))))) / ((1UL) << 12); u64 __addr = (-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) + (__idx * sizeof(struct page)); (struct page *)__addr; }), buflen, ((unsigned long)(buf) & ~(~(((1UL) << 12)-1))));
}

/*
 * Loop over each sg element, following the pointer to a new list if necessary
 */



/*
 * Loop over each sg element in the given sg_table object.
 */



/*
 * Loop over each sg element in the given *DMA mapped* sg_table object.
 * Please use sg_dma_address(sg) and sg_dma_len(sg) to extract DMA addresses
 * of the each element.
 */



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void __sg_chain(struct scatterlist *chain_sg,
         struct scatterlist *sgl)
{
 /*
	 * offset and length are unused for chain entry. Clear them.
	 */
 chain_sg->offset = 0;
 chain_sg->length = 0;

 /*
	 * Set lowest bit to indicate a link pointer, and make sure to clear
	 * the termination bit if it happens to be set.
	 */
 chain_sg->page_link = ((unsigned long) sgl | 0x01UL) & ~0x02UL;
}

/**
 * sg_chain - Chain two sglists together
 * @prv:	First scatterlist
 * @prv_nents:	Number of entries in prv
 * @sgl:	Second scatterlist
 *
 * Description:
 *   Links @prv@ and @sgl@ together, to form a longer scatterlist.
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_chain(struct scatterlist *prv, unsigned int prv_nents,
       struct scatterlist *sgl)
{
 __sg_chain(&prv[prv_nents - 1], sgl);
}

/**
 * sg_mark_end - Mark the end of the scatterlist
 * @sg:		 SG entryScatterlist
 *
 * Description:
 *   Marks the passed in sg entry as the termination point for the sg
 *   table. A call to sg_next() on this entry will return NULL.
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_mark_end(struct scatterlist *sg)
{
 /*
	 * Set termination bit, clear potential chain bit
	 */
 sg->page_link |= 0x02UL;
 sg->page_link &= ~0x01UL;
}

/**
 * sg_unmark_end - Undo setting the end of the scatterlist
 * @sg:		 SG entryScatterlist
 *
 * Description:
 *   Removes the termination marker from the given entry of the scatterlist.
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_unmark_end(struct scatterlist *sg)
{
 sg->page_link &= ~0x02UL;
}

/*
 * CONFGI_PCI_P2PDMA depends on CONFIG_64BIT which means there is 4 bytes
 * in struct scatterlist (assuming also CONFIG_NEED_SG_DMA_LENGTH is set).
 * Use this padding for DMA flags bits to indicate when a specific
 * dma address is a bus address.
 */
# 304 "./include/linux/scatterlist.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool sg_is_dma_bus_address(struct scatterlist *sg)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_dma_mark_bus_address(struct scatterlist *sg)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_dma_unmark_bus_address(struct scatterlist *sg)
{
}



/**
 * sg_phys - Return physical address of an sg entry
 * @sg:	     SG entry
 *
 * Description:
 *   This calls page_to_phys() on the page in this sg entry, and adds the
 *   sg offset. The caller must know that it is legal to call page_to_phys()
 *   on the sg page.
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) dma_addr_t sg_phys(struct scatterlist *sg)
{
 return (((phys_addr_t)((unsigned long)((sg_page(sg)) - ((struct page *)(-((((1UL))) << ((48) - (12 - (( __builtin_constant_p(sizeof(struct page)) ? ( ((sizeof(struct page)) == 0 || (sizeof(struct page)) == 1) ? 0 : ( __builtin_constant_p((sizeof(struct page)) - 1) ? (((sizeof(struct page)) - 1) < 2 ? 0 : 63 - __builtin_clzll((sizeof(struct page)) - 1)) : (sizeof((sizeof(struct page)) - 1) <= 4) ? __ilog2_u32((sizeof(struct page)) - 1) : __ilog2_u64((sizeof(struct page)) - 1) ) + 1) : __order_base_2(sizeof(struct page)) )))))) - (memstart_addr >> 12)))) << 12)) + sg->offset;
}

/**
 * sg_virt - Return virtual address of an sg entry
 * @sg:      SG entry
 *
 * Description:
 *   This calls page_address() on the page in this sg entry, and adds the
 *   sg offset. The caller must know that the sg page has a valid virtual
 *   mapping.
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *sg_virt(struct scatterlist *sg)
{
 return lowmem_page_address(sg_page(sg)) + sg->offset;
}

/**
 * sg_init_marker - Initialize markers in sg table
 * @sgl:	   The SG table
 * @nents:	   Number of entries in table
 *
 **/
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void sg_init_marker(struct scatterlist *sgl,
      unsigned int nents)
{
 sg_mark_end(&sgl[nents - 1]);
}

int sg_nents(struct scatterlist *sg);
int sg_nents_for_len(struct scatterlist *sg, u64 len);
struct scatterlist *sg_next(struct scatterlist *);
struct scatterlist *sg_last(struct scatterlist *s, unsigned int);
void sg_init_table(struct scatterlist *, unsigned int);
void sg_init_one(struct scatterlist *, const void *, unsigned int);
int sg_split(struct scatterlist *in, const int in_mapped_nents,
      const off_t skip, const int nb_splits,
      const size_t *split_sizes,
      struct scatterlist **out, int *out_mapped_nents,
      gfp_t gfp_mask);

typedef struct scatterlist *(sg_alloc_fn)(unsigned int, gfp_t);
typedef void (sg_free_fn)(struct scatterlist *, unsigned int);

void __sg_free_table(struct sg_table *, unsigned int, unsigned int,
       sg_free_fn *, unsigned int);
void sg_free_table(struct sg_table *);
void sg_free_append_table(struct sg_append_table *sgt);
int __sg_alloc_table(struct sg_table *, unsigned int, unsigned int,
       struct scatterlist *, unsigned int, gfp_t, sg_alloc_fn *);
int sg_alloc_table(struct sg_table *, unsigned int, gfp_t);
int sg_alloc_append_table_from_pages(struct sg_append_table *sgt,
         struct page **pages, unsigned int n_pages,
         unsigned int offset, unsigned long size,
         unsigned int max_segment,
         unsigned int left_pages, gfp_t gfp_mask);
int sg_alloc_table_from_pages_segment(struct sg_table *sgt, struct page **pages,
          unsigned int n_pages, unsigned int offset,
          unsigned long size,
          unsigned int max_segment, gfp_t gfp_mask);

/**
 * sg_alloc_table_from_pages - Allocate and initialize an sg table from
 *			       an array of pages
 * @sgt:	 The sg table header to use
 * @pages:	 Pointer to an array of page pointers
 * @n_pages:	 Number of pages in the pages array
 * @offset:      Offset from start of the first page to the start of a buffer
 * @size:        Number of valid bytes in the buffer (after offset)
 * @gfp_mask:	 GFP allocation mask
 *
 *  Description:
 *    Allocate and initialize an sg table from a list of pages. Contiguous
 *    ranges of the pages are squashed into a single scatterlist node. A user
 *    may provide an offset at a start and a size of valid data in a buffer
 *    specified by the page array. The returned sg table is released by
 *    sg_free_table.
 *
 * Returns:
 *   0 on success, negative error on failure
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int sg_alloc_table_from_pages(struct sg_table *sgt,
         struct page **pages,
         unsigned int n_pages,
         unsigned int offset,
         unsigned long size, gfp_t gfp_mask)
{
 return sg_alloc_table_from_pages_segment(sgt, pages, n_pages, offset,
       size, (~0U), gfp_mask);
}


struct scatterlist *sgl_alloc_order(unsigned long long length,
        unsigned int order, bool chainable,
        gfp_t gfp, unsigned int *nent_p);
struct scatterlist *sgl_alloc(unsigned long long length, gfp_t gfp,
         unsigned int *nent_p);
void sgl_free_n_order(struct scatterlist *sgl, int nents, int order);
void sgl_free_order(struct scatterlist *sgl, int order);
void sgl_free(struct scatterlist *sgl);


size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,
        size_t buflen, off_t skip, bool to_buffer);

size_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,
      const void *buf, size_t buflen);
size_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents,
    void *buf, size_t buflen);

size_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,
       const void *buf, size_t buflen, off_t skip);
size_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,
     void *buf, size_t buflen, off_t skip);
size_t sg_zero_buffer(struct scatterlist *sgl, unsigned int nents,
         size_t buflen, off_t skip);

/*
 * Maximum number of entries that will be allocated in one piece, if
 * a list larger than this is required then chaining will be utilized.
 */


/*
 * The maximum number of SG segments that we will put inside a
 * scatterlist (unless chaining is used). Should ideally fit inside a
 * single page, to avoid a higher order allocation.  We could define this
 * to SG_MAX_SINGLE_ALLOC to pack correctly at the highest order.  The
 * minimum value is 32
 */


/*
 * Like SG_CHUNK_SIZE, but for archs that have sg chaining. This limit
 * is totally arbitrary, a setting of 2048 will get you at least 8mb ios.
 */







void sg_free_table_chained(struct sg_table *table,
      unsigned nents_first_chunk);
int sg_alloc_table_chained(struct sg_table *table, int nents,
      struct scatterlist *first_chunk,
      unsigned nents_first_chunk);


/*
 * sg page iterator
 *
 * Iterates over sg entries page-by-page.  On each successful iteration, you
 * can call sg_page_iter_page(@piter) to get the current page.
 * @piter->sg will point to the sg holding this page and @piter->sg_pgoffset to
 * the page's page offset within the sg. The iteration will stop either when a
 * maximum number of sg entries was reached or a terminating sg
 * (sg_last(sg) == true) was reached.
 */
struct sg_page_iter {
 struct scatterlist *sg; /* sg holding the page */
 unsigned int sg_pgoffset; /* page offset within the sg */

 /* these are internal states, keep away */
 unsigned int __nents; /* remaining sg entries */
 int __pg_advance; /* nr pages to advance at the
						 * next step */
};

/*
 * sg page iterator for DMA addresses
 *
 * This is the same as sg_page_iter however you can call
 * sg_page_iter_dma_address(@dma_iter) to get the page's DMA
 * address. sg_page_iter_page() cannot be called on this iterator.
 */
struct sg_dma_page_iter {
 struct sg_page_iter base;
};

bool __sg_page_iter_next(struct sg_page_iter *piter);
bool __sg_page_iter_dma_next(struct sg_dma_page_iter *dma_iter);
void __sg_page_iter_start(struct sg_page_iter *piter,
     struct scatterlist *sglist, unsigned int nents,
     unsigned long pgoffset);
/**
 * sg_page_iter_page - get the current page held by the page iterator
 * @piter:	page iterator holding the page
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct page *sg_page_iter_page(struct sg_page_iter *piter)
{
 return ((sg_page(piter->sg)) + (piter->sg_pgoffset));
}

/**
 * sg_page_iter_dma_address - get the dma address of the current page held by
 * the page iterator.
 * @dma_iter:	page iterator holding the page
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) dma_addr_t
sg_page_iter_dma_address(struct sg_dma_page_iter *dma_iter)
{
 return ((dma_iter->base.sg)->dma_address) +
        (dma_iter->base.sg_pgoffset << 12);
}

/**
 * for_each_sg_page - iterate over the pages of the given sg list
 * @sglist:	sglist to iterate over
 * @piter:	page iterator to hold current page, sg, sg_pgoffset
 * @nents:	maximum number of sg entries to iterate over
 * @pgoffset:	starting page offset (in pages)
 *
 * Callers may use sg_page_iter_page() to get each page pointer.
 * In each loop it operates on PAGE_SIZE unit.
 */




/**
 * for_each_sg_dma_page - iterate over the pages of the given sg list
 * @sglist:	sglist to iterate over
 * @dma_iter:	DMA page iterator to hold current page
 * @dma_nents:	maximum number of sg entries to iterate over, this is the value
 *              returned from dma_map_sg
 * @pgoffset:	starting page offset (in pages)
 *
 * Callers may use sg_page_iter_dma_address() to get each page's DMA address.
 * In each loop it operates on PAGE_SIZE unit.
 */





/**
 * for_each_sgtable_page - iterate over all pages in the sg_table object
 * @sgt:	sg_table object to iterate over
 * @piter:	page iterator to hold current page
 * @pgoffset:	starting page offset (in pages)
 *
 * Iterates over the all memory pages in the buffer described by
 * a scatterlist stored in the given sg_table object.
 * See also for_each_sg_page(). In each loop it operates on PAGE_SIZE unit.
 */



/**
 * for_each_sgtable_dma_page - iterate over the DMA mapped sg_table object
 * @sgt:	sg_table object to iterate over
 * @dma_iter:	DMA page iterator to hold current page
 * @pgoffset:	starting page offset (in pages)
 *
 * Iterates over the all DMA mapped pages in the buffer described by
 * a scatterlist stored in the given sg_table object.
 * See also for_each_sg_dma_page(). In each loop it operates on PAGE_SIZE
 * unit.
 */




/*
 * Mapping sg iterator
 *
 * Iterates over sg entries mapping page-by-page.  On each successful
 * iteration, @miter->page points to the mapped page and
 * @miter->length bytes of data can be accessed at @miter->addr.  As
 * long as an iteration is enclosed between start and stop, the user
 * is free to choose control structure and when to stop.
 *
 * @miter->consumed is set to @miter->length on each iteration.  It
 * can be adjusted if the user can't consume all the bytes in one go.
 * Also, a stopped iteration can be resumed by calling next on it.
 * This is useful when iteration needs to release all resources and
 * continue later (e.g. at the next interrupt).
 */





struct sg_mapping_iter {
 /* the following three fields can be accessed directly */
 struct page *page; /* currently mapped page */
 void *addr; /* pointer to the mapped area */
 size_t length; /* length of the mapped area */
 size_t consumed; /* number of consumed bytes */
 struct sg_page_iter piter; /* page iterator */

 /* these are internal states, keep away */
 unsigned int __offset; /* offset within page */
 unsigned int __remaining; /* remaining bytes on page */
 unsigned int __flags;
};

void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
      unsigned int nents, unsigned int flags);
bool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset);
bool sg_miter_next(struct sg_mapping_iter *miter);
void sg_miter_stop(struct sg_mapping_iter *miter);
# 9 "./include/linux/blk-mq.h" 2
# 1 "./include/linux/prefetch.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 *  Generic cache management functions. Everything is arch-specific,
 *  but this header exists to make sure the defines/functions can be
 *  used in a generic way.
 *
 *  2000-11-13  Arjan van de Ven   <arjan@fenrus.demon.nl>
 *
 */
# 18 "./include/linux/prefetch.h"
struct page;
/*
	prefetch(x) attempts to pre-emptively get the memory pointed to
	by address "x" into the CPU L1 cache.
	prefetch(x) should not cause any kind of exception, prefetch(0) is
	specifically ok.

	prefetch() should be defined by the architecture, if not, the
	#define below provides a no-op define.

	There are 3 prefetch() macros:

	prefetch(x)  	- prefetches the cacheline at "x" for read
	prefetchw(x)	- prefetches the cacheline at "x" for write
	spin_lock_prefetch(x) - prefetches the spinlock *x for taking

	there is also PREFETCH_STRIDE which is the architecure-preferred
	"lookahead" size for prefetching streamed operations.

*/
# 55 "./include/linux/prefetch.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void prefetch_range(void *addr, size_t len)
{

 char *cp;
 char *end = addr + len;

 for (cp = addr; cp < end; cp += (4*(1 << (6))))
  prefetch(cp);

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void prefetch_page_address(struct page *page)
{



}
# 10 "./include/linux/blk-mq.h" 2


struct blk_mq_tags;
struct blk_flush_queue;




enum rq_end_io_ret {
 RQ_END_IO_NONE,
 RQ_END_IO_FREE,
};

typedef enum rq_end_io_ret (rq_end_io_fn)(struct request *, blk_status_t);

/*
 * request flags */
typedef __u32 req_flags_t;

/* drive already may have started this one */

/* may not be passed by ioscheduler */

/* request for flush sequence */

/* merge of different types, fail separately */

/* track inflight for MQ */

/* don't call prep for this one */

/* vaguely specified driver internal error.  Ignored by the block layer */

/* don't warn about errors */

/* elevator private data attached */

/* account into disk and partition IO statistics */

/* runtime pm request */

/* on IO scheduler merge hash */

/* track IO completion time */

/* Look at ->special_vec for the actual data payload instead of the
   bio chain. */

/* The per-zone write lock is held for this request */

/* already slept for hybrid poll */

/* ->timeout has been called, don't expire again */

/* queue has elevator attached */



/* flags that prevent us from merging requests: */



enum mq_rq_state {
 MQ_RQ_IDLE = 0,
 MQ_RQ_IN_FLIGHT = 1,
 MQ_RQ_COMPLETE = 2,
};

/*
 * Try to put the fields that are referenced together in the same cacheline.
 *
 * If you modify this structure, make sure to update blk_rq_init() and
 * especially blk_mq_rq_ctx_init() to take care of the added fields.
 */
struct request {
 struct request_queue *q;
 struct blk_mq_ctx *mq_ctx;
 struct blk_mq_hw_ctx *mq_hctx;

 blk_opf_t cmd_flags; /* op and common flags */
 req_flags_t rq_flags;

 int tag;
 int internal_tag;

 unsigned int timeout;

 /* the following two fields are internal, NEVER access directly */
 unsigned int __data_len; /* total data len */
 sector_t __sector; /* sector cursor */

 struct bio *bio;
 struct bio *biotail;

 union {
  struct list_head queuelist;
  struct request *rq_next;
 };

 struct block_device *part;




 /* Time that this request was allocated for this IO. */
 u64 start_time_ns;
 /* Time that I/O was submitted to the device. */
 u64 io_start_time_ns;




 /*
	 * rq sectors used for blk stats. It has the same value
	 * with blk_rq_sectors(rq), except that it never be zeroed
	 * by completion.
	 */
 unsigned short stats_sectors;

 /*
	 * Number of scatter-gather DMA addr+len pairs after
	 * physical address coalescing is performed.
	 */
 unsigned short nr_phys_segments;


 unsigned short nr_integrity_segments;







 unsigned short ioprio;

 enum mq_rq_state state;
 atomic_t ref;

 unsigned long deadline;

 /*
	 * The hash is used inside the scheduler, and killed once the
	 * request reaches the dispatch list. The ipi_list is only used
	 * to queue the request for softirq completion, which is long
	 * after the request has been unhashed (and even removed from
	 * the dispatch list).
	 */
 union {
  struct hlist_node hash; /* merge hash */
  struct llist_node ipi_list;
 };

 /*
	 * The rb_node is only used inside the io scheduler, requests
	 * are pruned when moved to the dispatch queue. So let the
	 * completion_data share space with the rb_node.
	 */
 union {
  struct rb_node rb_node; /* sort/lookup */
  struct bio_vec special_vec;
  void *completion_data;
 };


 /*
	 * Three pointers are available for the IO schedulers, if they need
	 * more they have to dynamically allocate it.  Flush requests are
	 * never put on the IO scheduler. So let the flush fields share
	 * space with the elevator data.
	 */
 union {
  struct {
   struct io_cq *icq;
   void *priv[2];
  } elv;

  struct {
   unsigned int seq;
   struct list_head list;
   rq_end_io_fn *saved_end_io;
  } flush;
 };

 union {
  struct __call_single_data csd;
  u64 fifo_time;
 };

 /*
	 * completion callback.
	 */
 rq_end_io_fn *end_io;
 void *end_io_data;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum req_op req_op(const struct request *req)
{
 return req->cmd_flags & ( blk_opf_t)((1 << 8) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_rq_is_passthrough(struct request *rq)
{
 return blk_op_is_passthrough(req_op(rq));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned short req_get_ioprio(struct request *req)
{
 return req->ioprio;
}
# 259 "./include/linux/blk-mq.h"
/**
 * rq_list_move() - move a struct request from one list to another
 * @src: The source list @rq is currently in
 * @dst: The destination list that @rq will be appended to
 * @rq: The request to move
 * @prev: The request preceding @rq in @src (NULL if @rq is the head)
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void rq_list_move(struct request **src, struct request **dst,
    struct request *rq, struct request *prev)
{
 if (prev)
  prev->rq_next = rq->rq_next;
 else
  *src = rq->rq_next;
 do { (rq)->rq_next = *(dst); *(dst) = rq; } while (0);
}

/**
 * enum blk_eh_timer_return - How the timeout handler should proceed
 * @BLK_EH_DONE: The block driver completed the command or will complete it at
 *	a later time.
 * @BLK_EH_RESET_TIMER: Reset the request timer and continue waiting for the
 *	request to complete.
 */
enum blk_eh_timer_return {
 BLK_EH_DONE,
 BLK_EH_RESET_TIMER,
};




/**
 * struct blk_mq_hw_ctx - State for a hardware queue facing the hardware
 * block device
 */
struct blk_mq_hw_ctx {
 struct {
  /** @lock: Protects the dispatch list. */
  spinlock_t lock;
  /**
		 * @dispatch: Used for requests that are ready to be
		 * dispatched to the hardware but for some reason (e.g. lack of
		 * resources) could not be sent to the hardware. As soon as the
		 * driver can send new requests, requests at this list will
		 * be sent first for a fairer dispatch.
		 */
  struct list_head dispatch;
   /**
		  * @state: BLK_MQ_S_* flags. Defines the state of the hw
		  * queue (active, scheduled to restart, stopped).
		  */
  unsigned long state;
 } __attribute__((__aligned__((1 << (6)))));

 /**
	 * @run_work: Used for scheduling a hardware queue run at a later time.
	 */
 struct delayed_work run_work;
 /** @cpumask: Map of available CPUs where this hctx can run. */
 cpumask_var_t cpumask;
 /**
	 * @next_cpu: Used by blk_mq_hctx_next_cpu() for round-robin CPU
	 * selection from @cpumask.
	 */
 int next_cpu;
 /**
	 * @next_cpu_batch: Counter of how many works left in the batch before
	 * changing to the next CPU.
	 */
 int next_cpu_batch;

 /** @flags: BLK_MQ_F_* flags. Defines the behaviour of the queue. */
 unsigned long flags;

 /**
	 * @sched_data: Pointer owned by the IO scheduler attached to a request
	 * queue. It's up to the IO scheduler how to use this pointer.
	 */
 void *sched_data;
 /**
	 * @queue: Pointer to the request queue that owns this hardware context.
	 */
 struct request_queue *queue;
 /** @fq: Queue of requests that need to perform a flush operation. */
 struct blk_flush_queue *fq;

 /**
	 * @driver_data: Pointer to data owned by the block driver that created
	 * this hctx
	 */
 void *driver_data;

 /**
	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
	 * pending request in that software queue.
	 */
 struct sbitmap ctx_map;

 /**
	 * @dispatch_from: Software queue to be used when no scheduler was
	 * selected.
	 */
 struct blk_mq_ctx *dispatch_from;
 /**
	 * @dispatch_busy: Number used by blk_mq_update_dispatch_busy() to
	 * decide if the hw_queue is busy using Exponential Weighted Moving
	 * Average algorithm.
	 */
 unsigned int dispatch_busy;

 /** @type: HCTX_TYPE_* flags. Type of hardware queue. */
 unsigned short type;
 /** @nr_ctx: Number of software queues. */
 unsigned short nr_ctx;
 /** @ctxs: Array of software queues. */
 struct blk_mq_ctx **ctxs;

 /** @dispatch_wait_lock: Lock for dispatch_wait queue. */
 spinlock_t dispatch_wait_lock;
 /**
	 * @dispatch_wait: Waitqueue to put requests when there is no tag
	 * available at the moment, to wait for another try in the future.
	 */
 wait_queue_entry_t dispatch_wait;

 /**
	 * @wait_index: Index of next available dispatch_wait queue to insert
	 * requests.
	 */
 atomic_t wait_index;

 /**
	 * @tags: Tags owned by the block driver. A tag at this set is only
	 * assigned when a request is dispatched from a hardware queue.
	 */
 struct blk_mq_tags *tags;
 /**
	 * @sched_tags: Tags owned by I/O scheduler. If there is an I/O
	 * scheduler associated with a request queue, a tag is assigned when
	 * that request is allocated. Else, this member is not used.
	 */
 struct blk_mq_tags *sched_tags;

 /** @queued: Number of queued requests. */
 unsigned long queued;
 /** @run: Number of dispatched requests. */
 unsigned long run;

 /** @numa_node: NUMA node the storage adapter has been connected to. */
 unsigned int numa_node;
 /** @queue_num: Index of this hardware queue. */
 unsigned int queue_num;

 /**
	 * @nr_active: Number of active requests. Only used when a tag set is
	 * shared across request queues.
	 */
 atomic_t nr_active;

 /** @cpuhp_online: List to store request if CPU is going to die */
 struct hlist_node cpuhp_online;
 /** @cpuhp_dead: List to store request if some CPU die. */
 struct hlist_node cpuhp_dead;
 /** @kobj: Kernel object for sysfs. */
 struct kobject kobj;


 /**
	 * @debugfs_dir: debugfs directory for this hardware queue. Named
	 * as cpu<cpu_number>.
	 */
 struct dentry *debugfs_dir;
 /** @sched_debugfs_dir:	debugfs directory for the scheduler. */
 struct dentry *sched_debugfs_dir;


 /**
	 * @hctx_list: if this hctx is not in use, this is an entry in
	 * q->unused_hctx_list.
	 */
 struct list_head hctx_list;
};

/**
 * struct blk_mq_queue_map - Map software queues to hardware queues
 * @mq_map:       CPU ID to hardware queue index map. This is an array
 *	with nr_cpu_ids elements. Each element has a value in the range
 *	[@queue_offset, @queue_offset + @nr_queues).
 * @nr_queues:    Number of hardware queues to map CPU IDs onto.
 * @queue_offset: First hardware queue to map onto. Used by the PCIe NVMe
 *	driver to map each hardware queue type (enum hctx_type) onto a distinct
 *	set of hardware queues.
 */
struct blk_mq_queue_map {
 unsigned int *mq_map;
 unsigned int nr_queues;
 unsigned int queue_offset;
};

/**
 * enum hctx_type - Type of hardware queue
 * @HCTX_TYPE_DEFAULT:	All I/O not otherwise accounted for.
 * @HCTX_TYPE_READ:	Just for READ I/O.
 * @HCTX_TYPE_POLL:	Polled I/O of any kind.
 * @HCTX_MAX_TYPES:	Number of types of hctx.
 */
enum hctx_type {
 HCTX_TYPE_DEFAULT,
 HCTX_TYPE_READ,
 HCTX_TYPE_POLL,

 HCTX_MAX_TYPES,
};

/**
 * struct blk_mq_tag_set - tag set that can be shared between request queues
 * @map:	   One or more ctx -> hctx mappings. One map exists for each
 *		   hardware queue type (enum hctx_type) that the driver wishes
 *		   to support. There are no restrictions on maps being of the
 *		   same size, and it's perfectly legal to share maps between
 *		   types.
 * @nr_maps:	   Number of elements in the @map array. A number in the range
 *		   [1, HCTX_MAX_TYPES].
 * @ops:	   Pointers to functions that implement block driver behavior.
 * @nr_hw_queues:  Number of hardware queues supported by the block driver that
 *		   owns this data structure.
 * @queue_depth:   Number of tags per hardware queue, reserved tags included.
 * @reserved_tags: Number of tags to set aside for BLK_MQ_REQ_RESERVED tag
 *		   allocations.
 * @cmd_size:	   Number of additional bytes to allocate per request. The block
 *		   driver owns these additional bytes.
 * @numa_node:	   NUMA node the storage adapter has been connected to.
 * @timeout:	   Request processing timeout in jiffies.
 * @flags:	   Zero or more BLK_MQ_F_* flags.
 * @driver_data:   Pointer to data owned by the block driver that created this
 *		   tag set.
 * @tags:	   Tag sets. One tag set per hardware queue. Has @nr_hw_queues
 *		   elements.
 * @shared_tags:
 *		   Shared set of tags. Has @nr_hw_queues elements. If set,
 *		   shared by all @tags.
 * @tag_list_lock: Serializes tag_list accesses.
 * @tag_list:	   List of the request queues that use this tag set. See also
 *		   request_queue.tag_set_list.
 * @srcu:	   Use as lock when type of the request queue is blocking
 *		   (BLK_MQ_F_BLOCKING).
 */
struct blk_mq_tag_set {
 struct blk_mq_queue_map map[HCTX_MAX_TYPES];
 unsigned int nr_maps;
 const struct blk_mq_ops *ops;
 unsigned int nr_hw_queues;
 unsigned int queue_depth;
 unsigned int reserved_tags;
 unsigned int cmd_size;
 int numa_node;
 unsigned int timeout;
 unsigned int flags;
 void *driver_data;

 struct blk_mq_tags **tags;

 struct blk_mq_tags *shared_tags;

 struct mutex tag_list_lock;
 struct list_head tag_list;
 struct srcu_struct *srcu;
};

/**
 * struct blk_mq_queue_data - Data about a request inserted in a queue
 *
 * @rq:   Request pointer.
 * @last: If it is the last request in the queue.
 */
struct blk_mq_queue_data {
 struct request *rq;
 bool last;
};

typedef bool (busy_tag_iter_fn)(struct request *, void *);

/**
 * struct blk_mq_ops - Callback functions that implements block driver
 * behaviour.
 */
struct blk_mq_ops {
 /**
	 * @queue_rq: Queue a new request from block IO.
	 */
 blk_status_t (*queue_rq)(struct blk_mq_hw_ctx *,
     const struct blk_mq_queue_data *);

 /**
	 * @commit_rqs: If a driver uses bd->last to judge when to submit
	 * requests to hardware, it must define this function. In case of errors
	 * that make us stop issuing further requests, this hook serves the
	 * purpose of kicking the hardware (which the last request otherwise
	 * would have done).
	 */
 void (*commit_rqs)(struct blk_mq_hw_ctx *);

 /**
	 * @queue_rqs: Queue a list of new requests. Driver is guaranteed
	 * that each request belongs to the same queue. If the driver doesn't
	 * empty the @rqlist completely, then the rest will be queued
	 * individually by the block layer upon return.
	 */
 void (*queue_rqs)(struct request **rqlist);

 /**
	 * @get_budget: Reserve budget before queue request, once .queue_rq is
	 * run, it is driver's responsibility to release the
	 * reserved budget. Also we have to handle failure case
	 * of .get_budget for avoiding I/O deadlock.
	 */
 int (*get_budget)(struct request_queue *);

 /**
	 * @put_budget: Release the reserved budget.
	 */
 void (*put_budget)(struct request_queue *, int);

 /**
	 * @set_rq_budget_token: store rq's budget token
	 */
 void (*set_rq_budget_token)(struct request *, int);
 /**
	 * @get_rq_budget_token: retrieve rq's budget token
	 */
 int (*get_rq_budget_token)(struct request *);

 /**
	 * @timeout: Called on request timeout.
	 */
 enum blk_eh_timer_return (*timeout)(struct request *);

 /**
	 * @poll: Called to poll for completion of a specific tag.
	 */
 int (*poll)(struct blk_mq_hw_ctx *, struct io_comp_batch *);

 /**
	 * @complete: Mark the request as complete.
	 */
 void (*complete)(struct request *);

 /**
	 * @init_hctx: Called when the block layer side of a hardware queue has
	 * been set up, allowing the driver to allocate/init matching
	 * structures.
	 */
 int (*init_hctx)(struct blk_mq_hw_ctx *, void *, unsigned int);
 /**
	 * @exit_hctx: Ditto for exit/teardown.
	 */
 void (*exit_hctx)(struct blk_mq_hw_ctx *, unsigned int);

 /**
	 * @init_request: Called for every command allocated by the block layer
	 * to allow the driver to set up driver specific data.
	 *
	 * Tag greater than or equal to queue_depth is for setting up
	 * flush request.
	 */
 int (*init_request)(struct blk_mq_tag_set *set, struct request *,
       unsigned int, unsigned int);
 /**
	 * @exit_request: Ditto for exit/teardown.
	 */
 void (*exit_request)(struct blk_mq_tag_set *set, struct request *,
        unsigned int);

 /**
	 * @cleanup_rq: Called before freeing one request which isn't completed
	 * yet, and usually for freeing the driver private data.
	 */
 void (*cleanup_rq)(struct request *);

 /**
	 * @busy: If set, returns whether or not this queue currently is busy.
	 */
 bool (*busy)(struct request_queue *);

 /**
	 * @map_queues: This allows drivers specify their own queue mapping by
	 * overriding the setup-time function that builds the mq_map.
	 */
 void (*map_queues)(struct blk_mq_tag_set *set);


 /**
	 * @show_rq: Used by the debugfs implementation to show driver-specific
	 * information about a request.
	 */
 void (*show_rq)(struct seq_file *m, struct request *rq);

};

enum {
 BLK_MQ_F_SHOULD_MERGE = 1 << 0,
 BLK_MQ_F_TAG_QUEUE_SHARED = 1 << 1,
 /*
	 * Set when this device requires underlying blk-mq device for
	 * completing IO:
	 */
 BLK_MQ_F_STACKING = 1 << 2,
 BLK_MQ_F_TAG_HCTX_SHARED = 1 << 3,
 BLK_MQ_F_BLOCKING = 1 << 5,
 /* Do not allow an I/O scheduler to be configured. */
 BLK_MQ_F_NO_SCHED = 1 << 6,
 /*
	 * Select 'none' during queue registration in case of a single hwq
	 * or shared hwqs instead of 'mq-deadline'.
	 */
 BLK_MQ_F_NO_SCHED_BY_DEFAULT = 1 << 7,
 BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 BLK_MQ_F_ALLOC_POLICY_BITS = 1,

 BLK_MQ_S_STOPPED = 0,
 BLK_MQ_S_TAG_ACTIVE = 1,
 BLK_MQ_S_SCHED_RESTART = 2,

 /* hw queue is inactive after all its CPUs become offline */
 BLK_MQ_S_INACTIVE = 3,

 BLK_MQ_MAX_DEPTH = 10240,

 BLK_MQ_CPU_WORK_BATCH = 8,
};
# 699 "./include/linux/blk-mq.h"
struct gendisk *__blk_mq_alloc_disk(struct blk_mq_tag_set *set, void *queuedata,
  struct lock_class_key *lkclass);






struct gendisk *blk_mq_alloc_disk_for_queue(struct request_queue *q,
  struct lock_class_key *lkclass);
struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);
int blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
  struct request_queue *q);
void blk_mq_destroy_queue(struct request_queue *);

int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set);
int blk_mq_alloc_sq_tag_set(struct blk_mq_tag_set *set,
  const struct blk_mq_ops *ops, unsigned int queue_depth,
  unsigned int set_flags);
void blk_mq_free_tag_set(struct blk_mq_tag_set *set);

void blk_mq_free_request(struct request *rq);

bool blk_mq_queue_inflight(struct request_queue *q);

enum {
 /* return when out of requests */
 BLK_MQ_REQ_NOWAIT = ( blk_mq_req_flags_t)(1 << 0),
 /* allocate from reserved pool */
 BLK_MQ_REQ_RESERVED = ( blk_mq_req_flags_t)(1 << 1),
 /* set RQF_PM */
 BLK_MQ_REQ_PM = ( blk_mq_req_flags_t)(1 << 2),
};

struct request *blk_mq_alloc_request(struct request_queue *q, blk_opf_t opf,
  blk_mq_req_flags_t flags);
struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
  blk_opf_t opf, blk_mq_req_flags_t flags,
  unsigned int hctx_idx);

/*
 * Tag address space map.
 */
struct blk_mq_tags {
 unsigned int nr_tags;
 unsigned int nr_reserved_tags;

 atomic_t active_queues;

 struct sbitmap_queue bitmap_tags;
 struct sbitmap_queue breserved_tags;

 struct request **rqs;
 struct request **static_rqs;
 struct list_head page_list;

 /*
	 * used to clear request reference in rqs[] before freeing one
	 * request pool
	 */
 spinlock_t lock;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags,
            unsigned int tag)
{
 if (tag < tags->nr_tags) {
  prefetch(tags->rqs[tag]);
  return tags->rqs[tag];
 }

 return ((void *)0);
}

enum {
 BLK_MQ_UNIQUE_TAG_BITS = 16,
 BLK_MQ_UNIQUE_TAG_MASK = (1 << BLK_MQ_UNIQUE_TAG_BITS) - 1,
};

u32 blk_mq_unique_tag(struct request *rq);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 blk_mq_unique_tag_to_hwq(u32 unique_tag)
{
 return unique_tag >> BLK_MQ_UNIQUE_TAG_BITS;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) u16 blk_mq_unique_tag_to_tag(u32 unique_tag)
{
 return unique_tag & BLK_MQ_UNIQUE_TAG_MASK;
}

/**
 * blk_mq_rq_state() - read the current MQ_RQ_* state of a request
 * @rq: target request.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) enum mq_rq_state blk_mq_rq_state(struct request *rq)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_417(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(rq->state) == sizeof(char) || sizeof(rq->state) == sizeof(short) || sizeof(rq->state) == sizeof(int) || sizeof(rq->state) == sizeof(long)) || sizeof(rq->state) == sizeof(long long))) __compiletime_assert_417(); } while (0); (*(const volatile typeof( _Generic((rq->state), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (rq->state))) *)&(rq->state)); });
# 797 "./include/linux/blk-mq.h"
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blk_mq_request_started(struct request *rq)
{
 return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blk_mq_request_completed(struct request *rq)
{
 return blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;
}

/*
 *
 * Set the state to complete when completing a request from inside ->queue_rq.
 * This is used by drivers that want to ensure special complete actions that
 * need access to the request are called on failure, e.g. by nvme for
 * multipathing.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_mq_set_request_complete(struct request *rq)
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_418(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(rq->state) == sizeof(char) || sizeof(rq->state) == sizeof(short) || sizeof(rq->state) == sizeof(int) || sizeof(rq->state) == sizeof(long)) || sizeof(rq->state) == sizeof(long long))) __compiletime_assert_418(); } while (0); do { *(volatile typeof(rq->state) *)&(rq->state) = (MQ_RQ_COMPLETE); } while (0); } while (0);
# 819 "./include/linux/blk-mq.h"
}

/*
 * Complete the request directly instead of deferring it to softirq or
 * completing it another CPU. Useful in preemptible instead of an interrupt.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_mq_complete_request_direct(struct request *rq,
     void (*complete)(struct request *rq))
{
 do { do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_419(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(rq->state) == sizeof(char) || sizeof(rq->state) == sizeof(short) || sizeof(rq->state) == sizeof(int) || sizeof(rq->state) == sizeof(long)) || sizeof(rq->state) == sizeof(long long))) __compiletime_assert_419(); } while (0); do { *(volatile typeof(rq->state) *)&(rq->state) = (MQ_RQ_COMPLETE); } while (0); } while (0);
# 829 "./include/linux/blk-mq.h"
 complete(rq);
}

void blk_mq_start_request(struct request *rq);
void blk_mq_end_request(struct request *rq, blk_status_t error);
void __blk_mq_end_request(struct request *rq, blk_status_t error);
void blk_mq_end_request_batch(struct io_comp_batch *ib);

/*
 * Only need start/end time stamping if we have iostat or
 * blk stats enabled, or using an IO scheduler.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_mq_need_time_stamp(struct request *rq)
{
 return (rq->rq_flags & ((( req_flags_t)(1 << 13)) | (( req_flags_t)(1 << 17)) | (( req_flags_t)(1 << 22))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_mq_is_reserved_rq(struct request *rq)
{
 return rq->rq_flags & (( req_flags_t)(1 << 23));
}

/*
 * Batched completions only work when there is no I/O error and no special
 * ->end_io handler.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_mq_add_to_batch(struct request *req,
           struct io_comp_batch *iob, int ioerror,
           void (*complete)(struct io_comp_batch *))
{
 if (!iob || (req->rq_flags & (( req_flags_t)(1 << 22))) || ioerror ||
   (req->end_io && !blk_rq_is_passthrough(req)))
  return false;

 if (!iob->complete)
  iob->complete = complete;
 else if (iob->complete != complete)
  return false;
 iob->need_ts |= blk_mq_need_time_stamp(req);
 do { (req)->rq_next = *(&iob->req_list); *(&iob->req_list) = req; } while (0);
 return true;
}

void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);
void blk_mq_kick_requeue_list(struct request_queue *q);
void blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);
void blk_mq_complete_request(struct request *rq);
bool blk_mq_complete_request_remote(struct request *rq);
void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);
void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);
void blk_mq_stop_hw_queues(struct request_queue *q);
void blk_mq_start_hw_queues(struct request_queue *q);
void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);
void blk_mq_quiesce_queue(struct request_queue *q);
void blk_mq_wait_quiesce_done(struct blk_mq_tag_set *set);
void blk_mq_quiesce_tagset(struct blk_mq_tag_set *set);
void blk_mq_unquiesce_tagset(struct blk_mq_tag_set *set);
void blk_mq_unquiesce_queue(struct request_queue *q);
void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);
void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);
void blk_mq_run_hw_queues(struct request_queue *q, bool async);
void blk_mq_delay_run_hw_queues(struct request_queue *q, unsigned long msecs);
void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
  busy_tag_iter_fn *fn, void *priv);
void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset);
void blk_mq_freeze_queue(struct request_queue *q);
void blk_mq_unfreeze_queue(struct request_queue *q);
void blk_freeze_queue_start(struct request_queue *q);
void blk_mq_freeze_queue_wait(struct request_queue *q);
int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
         unsigned long timeout);

void blk_mq_map_queues(struct blk_mq_queue_map *qmap);
void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);

void blk_mq_quiesce_queue_nowait(struct request_queue *q);

unsigned int blk_mq_rq_cpu(struct request *rq);

bool __blk_should_fake_timeout(struct request_queue *q);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_should_fake_timeout(struct request_queue *q)
{
 if (0 &&
     ((__builtin_constant_p(5 /* fake timeout */) && __builtin_constant_p((uintptr_t)(&q->queue_flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&q->queue_flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&q->queue_flags))) ? const_test_bit(5 /* fake timeout */, &q->queue_flags) : generic_test_bit(5 /* fake timeout */, &q->queue_flags)))
  return __blk_should_fake_timeout(q);
 return false;
}

/**
 * blk_mq_rq_from_pdu - cast a PDU to a request
 * @pdu: the PDU (Protocol Data Unit) to be casted
 *
 * Return: request
 *
 * Driver command data is immediately after the request. So subtract request
 * size to get back to the original request.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct request *blk_mq_rq_from_pdu(void *pdu)
{
 return pdu - sizeof(struct request);
}

/**
 * blk_mq_rq_to_pdu - cast a request to a PDU
 * @rq: the request to be casted
 *
 * Return: pointer to the PDU
 *
 * Driver command data is immediately after the request. So add request to get
 * the PDU.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void *blk_mq_rq_to_pdu(struct request *rq)
{
 return rq + 1;
}
# 953 "./include/linux/blk-mq.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_mq_cleanup_rq(struct request *rq)
{
 if (rq->q->mq_ops->cleanup_rq)
  rq->q->mq_ops->cleanup_rq(rq);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_rq_bio_prep(struct request *rq, struct bio *bio,
  unsigned int nr_segs)
{
 rq->nr_phys_segments = nr_segs;
 rq->__data_len = bio->bi_iter.bi_size;
 rq->bio = rq->biotail = bio;
 rq->ioprio = (bio)->bi_ioprio;
}

void blk_mq_hctx_set_fq_lock_class(struct blk_mq_hw_ctx *hctx,
  struct lock_class_key *key);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool rq_is_sync(struct request *rq)
{
 return op_is_sync(rq->cmd_flags);
}

void blk_rq_init(struct request_queue *q, struct request *rq);
int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
  struct bio_set *bs, gfp_t gfp_mask,
  int (*bio_ctr)(struct bio *, struct bio *, void *), void *data);
void blk_rq_unprep_clone(struct request *rq);
blk_status_t blk_insert_cloned_request(struct request *rq);

struct rq_map_data {
 struct page **pages;
 unsigned long offset;
 unsigned short page_order;
 unsigned short nr_entries;
 bool null_mapped;
 bool from_user;
};

int blk_rq_map_user(struct request_queue *, struct request *,
  struct rq_map_data *, void /* nothing */ *, unsigned long, gfp_t);
int blk_rq_map_user_io(struct request *, struct rq_map_data *,
  void /* nothing */ *, unsigned long, gfp_t, bool, int, bool, int);
int blk_rq_map_user_iov(struct request_queue *, struct request *,
  struct rq_map_data *, const struct iov_iter *, gfp_t);
int blk_rq_unmap_user(struct bio *);
int blk_rq_map_kern(struct request_queue *, struct request *, void *,
  unsigned int, gfp_t);
int blk_rq_append_bio(struct request *rq, struct bio *bio);
void blk_execute_rq_nowait(struct request *rq, bool at_head);
blk_status_t blk_execute_rq(struct request *rq, bool at_head);
bool blk_rq_is_poll(struct request *rq);

struct req_iterator {
 struct bvec_iter iter;
 struct bio *bio;
};
# 1027 "./include/linux/blk-mq.h"
/*
 * blk_rq_pos()			: the current sector
 * blk_rq_bytes()		: bytes left in the entire request
 * blk_rq_cur_bytes()		: bytes left in the current segment
 * blk_rq_sectors()		: sectors left in the entire request
 * blk_rq_cur_sectors()		: sectors left in the current segment
 * blk_rq_stats_sectors()	: sectors of the entire request used for stats
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) sector_t blk_rq_pos(const struct request *rq)
{
 return rq->__sector;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_rq_bytes(const struct request *rq)
{
 return rq->__data_len;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blk_rq_cur_bytes(const struct request *rq)
{
 if (!rq->bio)
  return 0;
 if (!bio_has_data(rq->bio)) /* dataless requests such as discard */
  return rq->bio->bi_iter.bi_size;
 return ((struct bio_vec) { .bv_page = (((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_page) + (((&(((((((rq->bio))->bi_io_vec)))))[(((((((rq->bio)->bi_iter)))))).bi_idx])->bv_offset + ((((((rq->bio)->bi_iter))))).bi_bvec_done) / ((1UL) << 12))), .bv_len = __builtin_choose_expr(((!!(sizeof((typeof((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((rq->bio)->bi_iter)))).bi_size) *)1 == (typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((rq->bio)->bi_iter)))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((rq->bio)->bi_iter)))).bi_size) < ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) ? ((((((rq->bio)->bi_iter)))).bi_size) : ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done)), ({ typeof((((((rq->bio)->bi_iter)))).bi_size) __UNIQUE_ID___x420 = ((((((rq->bio)->bi_iter)))).bi_size); typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) __UNIQUE_ID___y421 = ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done); ((__UNIQUE_ID___x420) < (__UNIQUE_ID___y421) ? (__UNIQUE_ID___x420) : (__UNIQUE_ID___y421)); })))) *)1 == (typeof((unsigned)(((1UL) << 12) - (((&(((((((rq->bio))->bi_io_vec)))))[(((((((rq->bio)->bi_iter)))))).bi_idx])->bv_offset + ((((((rq->bio)->bi_iter))))).bi_bvec_done) % ((1UL) << 12)))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((rq->bio)->bi_iter)))).bi_size) *)1 == (typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((rq->bio)->bi_iter)))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((rq->bio)->bi_iter)))).bi_size) < ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) ? ((((((rq->bio)->bi_iter)))).bi_size) : ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done)), ({ typeof((((((rq->bio)->bi_iter)))).bi_size) __UNIQUE_ID___x420 = ((((((rq->bio)->bi_iter)))).bi_size); typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) __UNIQUE_ID___y421 = ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done); ((__UNIQUE_ID___x420) < (__UNIQUE_ID___y421) ? (__UNIQUE_ID___x420) : (__UNIQUE_ID___y421)); })))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned)(((1UL) << 12) - (((&(((((((rq->bio))->bi_io_vec)))))[(((((((rq->bio)->bi_iter)))))).bi_idx])->bv_offset + ((((((rq->bio)->bi_iter))))).bi_bvec_done) % ((1UL) << 12)))) * 0l)) : (int *)8))))), (((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((rq->bio)->bi_iter)))).bi_size) *)1 == (typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((rq->bio)->bi_iter)))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((rq->bio)->bi_iter)))).bi_size) < ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) ? ((((((rq->bio)->bi_iter)))).bi_size) : ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done)), ({ typeof((((((rq->bio)->bi_iter)))).bi_size) __UNIQUE_ID___x420 = ((((((rq->bio)->bi_iter)))).bi_size); typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) __UNIQUE_ID___y421 = ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done); ((__UNIQUE_ID___x420) < (__UNIQUE_ID___y421) ? (__UNIQUE_ID___x420) : (__UNIQUE_ID___y421)); })))) < ((unsigned)(((1UL) << 12) - (((&(((((((rq->bio))->bi_io_vec)))))[(((((((rq->bio)->bi_iter)))))).bi_idx])->bv_offset + ((((((rq->bio)->bi_iter))))).bi_bvec_done) % ((1UL) << 12)))) ? ((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((rq->bio)->bi_iter)))).bi_size) *)1 == (typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((rq->bio)->bi_iter)))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((rq->bio)->bi_iter)))).bi_size) < ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) ? ((((((rq->bio)->bi_iter)))).bi_size) : ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done)), ({ typeof((((((rq->bio)->bi_iter)))).bi_size) __UNIQUE_ID___x420 = ((((((rq->bio)->bi_iter)))).bi_size); typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) __UNIQUE_ID___y421 = ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done); ((__UNIQUE_ID___x420) < (__UNIQUE_ID___y421) ? (__UNIQUE_ID___x420) : (__UNIQUE_ID___y421)); })))) : ((unsigned)(((1UL) << 12) - (((&(((((((rq->bio))->bi_io_vec)))))[(((((((rq->bio)->bi_iter)))))).bi_idx])->bv_offset + ((((((rq->bio)->bi_iter))))).bi_bvec_done) % ((1UL) << 12))))), ({ typeof((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((rq->bio)->bi_iter)))).bi_size) *)1 == (typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((rq->bio)->bi_iter)))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((rq->bio)->bi_iter)))).bi_size) < ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) ? ((((((rq->bio)->bi_iter)))).bi_size) : ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done)), ({ typeof((((((rq->bio)->bi_iter)))).bi_size) __UNIQUE_ID___x420 = ((((((rq->bio)->bi_iter)))).bi_size); typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) __UNIQUE_ID___y421 = ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done); ((__UNIQUE_ID___x420) < (__UNIQUE_ID___y421) ? (__UNIQUE_ID___x420) : (__UNIQUE_ID___y421)); })))) __UNIQUE_ID___x422 = ((unsigned)(__builtin_choose_expr(((!!(sizeof((typeof((((((rq->bio)->bi_iter)))).bi_size) *)1 == (typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((((((rq->bio)->bi_iter)))).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) * 0l)) : (int *)8))))), (((((((rq->bio)->bi_iter)))).bi_size) < ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) ? ((((((rq->bio)->bi_iter)))).bi_size) : ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done)), ({ typeof((((((rq->bio)->bi_iter)))).bi_size) __UNIQUE_ID___x420 = ((((((rq->bio)->bi_iter)))).bi_size); typeof((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done) __UNIQUE_ID___y421 = ((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_len - (((((rq->bio)->bi_iter)))).bi_bvec_done); ((__UNIQUE_ID___x420) < (__UNIQUE_ID___y421) ? (__UNIQUE_ID___x420) : (__UNIQUE_ID___y421)); })))); typeof((unsigned)(((1UL) << 12) - (((&(((((((rq->bio))->bi_io_vec)))))[(((((((rq->bio)->bi_iter)))))).bi_idx])->bv_offset + ((((((rq->bio)->bi_iter))))).bi_bvec_done) % ((1UL) << 12)))) __UNIQUE_ID___y423 = ((unsigned)(((1UL) << 12) - (((&(((((((rq->bio))->bi_io_vec)))))[(((((((rq->bio)->bi_iter)))))).bi_idx])->bv_offset + ((((((rq->bio)->bi_iter))))).bi_bvec_done) % ((1UL) << 12)))); ((__UNIQUE_ID___x422) < (__UNIQUE_ID___y423) ? (__UNIQUE_ID___x422) : (__UNIQUE_ID___y423)); })), .bv_offset = (((&((((((rq->bio))->bi_io_vec))))[((((((rq->bio)->bi_iter))))).bi_idx])->bv_offset + (((((rq->bio)->bi_iter)))).bi_bvec_done) % ((1UL) << 12)), }).bv_len;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_rq_sectors(const struct request *rq)
{
 return blk_rq_bytes(rq) >> 9;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_rq_cur_sectors(const struct request *rq)
{
 return blk_rq_cur_bytes(rq) >> 9;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_rq_stats_sectors(const struct request *rq)
{
 return rq->stats_sectors;
}

/*
 * Some commands like WRITE SAME have a payload or data transfer size which
 * is different from the size of the request.  Any driver that supports such
 * commands using the RQF_SPECIAL_PAYLOAD flag needs to use this helper to
 * calculate the data transfer size.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_rq_payload_bytes(struct request *rq)
{
 if (rq->rq_flags & (( req_flags_t)(1 << 18)))
  return rq->special_vec.bv_len;
 return blk_rq_bytes(rq);
}

/*
 * Return the first full biovec in the request.  The caller needs to check that
 * there are any bvecs before calling this helper.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct bio_vec req_bvec(struct request *rq)
{
 if (rq->rq_flags & (( req_flags_t)(1 << 18)))
  return rq->special_vec;
 return ((struct bio_vec) { .bv_page = ((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_page), .bv_len = __builtin_choose_expr(((!!(sizeof((typeof(((rq->bio->bi_iter)).bi_size) *)1 == (typeof((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_len - ((rq->bio->bi_iter)).bi_bvec_done) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(((rq->bio->bi_iter)).bi_size) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_len - ((rq->bio->bi_iter)).bi_bvec_done) * 0l)) : (int *)8))))), ((((rq->bio->bi_iter)).bi_size) < ((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_len - ((rq->bio->bi_iter)).bi_bvec_done) ? (((rq->bio->bi_iter)).bi_size) : ((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_len - ((rq->bio->bi_iter)).bi_bvec_done)), ({ typeof(((rq->bio->bi_iter)).bi_size) __UNIQUE_ID___x424 = (((rq->bio->bi_iter)).bi_size); typeof((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_len - ((rq->bio->bi_iter)).bi_bvec_done) __UNIQUE_ID___y425 = ((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_len - ((rq->bio->bi_iter)).bi_bvec_done); ((__UNIQUE_ID___x424) < (__UNIQUE_ID___y425) ? (__UNIQUE_ID___x424) : (__UNIQUE_ID___y425)); })), .bv_offset = ((&(((rq->bio->bi_io_vec)))[(((rq->bio->bi_iter))).bi_idx])->bv_offset + ((rq->bio->bi_iter)).bi_bvec_done), });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int blk_rq_count_bios(struct request *rq)
{
 unsigned int nr_bios = 0;
 struct bio *bio;

 if ((rq->bio)) for (bio = (rq)->bio; bio; bio = bio->bi_next)
  nr_bios++;

 return nr_bios;
}

void blk_steal_bios(struct bio_list *list, struct request *rq);

/*
 * Request completion related functions.
 *
 * blk_update_request() completes given number of bytes and updates
 * the request without completing it.
 */
bool blk_update_request(struct request *rq, blk_status_t error,
          unsigned int nr_bytes);
void blk_abort_request(struct request *);

/*
 * Number of physical segments as sent to the device.
 *
 * Normally this is the number of discontiguous data segments sent by the
 * submitter.  But for data-less command like discard we might have no
 * actual data segments submitted, but the driver might have to add it's
 * own special payload.  In that case we still return 1 here so that this
 * special payload will be mapped.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned short blk_rq_nr_phys_segments(struct request *rq)
{
 if (rq->rq_flags & (( req_flags_t)(1 << 18)))
  return 1;
 return rq->nr_phys_segments;
}

/*
 * Number of discard segments (or ranges) the driver needs to fill in.
 * Each discard bio merged into a request is counted as one segment.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned short blk_rq_nr_discard_segments(struct request *rq)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned short)(rq->nr_phys_segments)) *)1 == (typeof((unsigned short)(1)) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned short)(rq->nr_phys_segments)) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned short)(1)) * 0l)) : (int *)8))))), (((unsigned short)(rq->nr_phys_segments)) > ((unsigned short)(1)) ? ((unsigned short)(rq->nr_phys_segments)) : ((unsigned short)(1))), ({ typeof((unsigned short)(rq->nr_phys_segments)) __UNIQUE_ID___x426 = ((unsigned short)(rq->nr_phys_segments)); typeof((unsigned short)(1)) __UNIQUE_ID___y427 = ((unsigned short)(1)); ((__UNIQUE_ID___x426) > (__UNIQUE_ID___y427) ? (__UNIQUE_ID___x426) : (__UNIQUE_ID___y427)); }));
}

int __blk_rq_map_sg(struct request_queue *q, struct request *rq,
  struct scatterlist *sglist, struct scatterlist **last_sg);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int blk_rq_map_sg(struct request_queue *q, struct request *rq,
  struct scatterlist *sglist)
{
 struct scatterlist *last_sg = ((void *)0);

 return __blk_rq_map_sg(q, rq, sglist, &last_sg);
}
void blk_dump_rq_flags(struct request *, char *);
# 1193 "./include/linux/blk-mq.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_req_needs_zone_write_lock(struct request *rq)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_req_zone_write_lock(struct request *rq)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void blk_req_zone_write_unlock(struct request *rq)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_req_zone_is_write_locked(struct request *rq)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool blk_req_can_dispatch_to_zone(struct request *rq)
{
 return true;
}
# 9 "./include/scsi/scsi_device.h" 2
# 1 "./include/scsi/scsi.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * This header file contains public constants and structures used by
 * the SCSI initiator code.
 */






# 1 "./include/scsi/scsi_common.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * Functions used by both the SCSI initiator code and the SCSI target code.
 */





# 1 "./include/scsi/scsi_proto.h" 1
/* SPDX-License-Identifier: GPL-2.0 */
/*
 * This header file contains public constants and structures used by
 * both the SCSI initiator and the SCSI target code.
 *
 * For documentation on the OPCODES, MESSAGES, and SENSE values,
 * please consult the SCSI standard.
 */






/*
 *      SCSI opcodes
 */
# 125 "./include/scsi/scsi_proto.h"
/* values for service action in */



/* values for maintenance in */
# 138 "./include/scsi/scsi_proto.h"
/* value for MI_REPORT_TARGET_PGS ext header */

/* values for maintenance out */






/* values for ZBC_IN */

/* values for ZBC_OUT */




/* values for variable length command */
# 166 "./include/scsi/scsi_proto.h"
/* Values for T10/04-262r7 */



/* Vendor specific CDBs start here */


/*
 *	SCSI command lengths
 */



/* defined in T10 SCSI Primary Commands-2 (SPC2) */
struct scsi_varlen_cdb_hdr {
 __u8 opcode; /* opcode always == VARIABLE_LENGTH_CMD */
 __u8 control;
 __u8 misc[5];
 __u8 additional_cdb_length; /* total cdb length - 8 */
 __be16 service_action;
 /* service specific data follows */
};

/*
 *  SCSI Architecture Model (SAM) Status codes. Taken from SAM-3 draft
 *  T10/1561-D Revision 4 Draft dated 7th November 2002.
 */
enum sam_status {
 SAM_STAT_GOOD = 0x00,
 SAM_STAT_CHECK_CONDITION = 0x02,
 SAM_STAT_CONDITION_MET = 0x04,
 SAM_STAT_BUSY = 0x08,
 SAM_STAT_INTERMEDIATE = 0x10,
 SAM_STAT_INTERMEDIATE_CONDITION_MET = 0x14,
 SAM_STAT_RESERVATION_CONFLICT = 0x18,
 SAM_STAT_COMMAND_TERMINATED = 0x22, /* obsolete in SAM-3 */
 SAM_STAT_TASK_SET_FULL = 0x28,
 SAM_STAT_ACA_ACTIVE = 0x30,
 SAM_STAT_TASK_ABORTED = 0x40,
};



/*
 *  SENSE KEYS
 */
# 228 "./include/scsi/scsi_proto.h"
/*
 *  DEVICE TYPES
 *  Please keep them in 0x%02x format for $MODALIAS to work
 */
# 252 "./include/scsi/scsi_proto.h"
/* SCSI protocols; these are taken from SPC-3 section 7.5 */
enum scsi_protocol {
 SCSI_PROTOCOL_FCP = 0, /* Fibre Channel */
 SCSI_PROTOCOL_SPI = 1, /* parallel SCSI */
 SCSI_PROTOCOL_SSA = 2, /* Serial Storage Architecture - Obsolete */
 SCSI_PROTOCOL_SBP = 3, /* firewire */
 SCSI_PROTOCOL_SRP = 4, /* Infiniband RDMA */
 SCSI_PROTOCOL_ISCSI = 5,
 SCSI_PROTOCOL_SAS = 6,
 SCSI_PROTOCOL_ADT = 7, /* Media Changers */
 SCSI_PROTOCOL_ATA = 8,
 SCSI_PROTOCOL_UNSPEC = 0xf, /* No specific protocol */
};

/*
 * ScsiLun: 8 byte LUN.
 */
struct scsi_lun {
 __u8 scsi_lun[8];
};

/* SPC asymmetric access states */
# 282 "./include/scsi/scsi_proto.h"
/* Values for REPORT TARGET GROUP STATES */



/* Reporting options for REPORT ZONES */
enum zbc_zone_reporting_options {
 ZBC_ZONE_REPORTING_OPTION_ALL = 0x00,
 ZBC_ZONE_REPORTING_OPTION_EMPTY = 0x01,
 ZBC_ZONE_REPORTING_OPTION_IMPLICIT_OPEN = 0x02,
 ZBC_ZONE_REPORTING_OPTION_EXPLICIT_OPEN = 0x03,
 ZBC_ZONE_REPORTING_OPTION_CLOSED = 0x04,
 ZBC_ZONE_REPORTING_OPTION_FULL = 0x05,
 ZBC_ZONE_REPORTING_OPTION_READONLY = 0x06,
 ZBC_ZONE_REPORTING_OPTION_OFFLINE = 0x07,
 /* 0x08 to 0x0f are reserved */
 ZBC_ZONE_REPORTING_OPTION_NEED_RESET_WP = 0x10,
 ZBC_ZONE_REPORTING_OPTION_NON_SEQWRITE = 0x11,
 /* 0x12 to 0x3e are reserved */
 ZBC_ZONE_REPORTING_OPTION_NON_WP = 0x3f,
};



/* Zone types of REPORT ZONES zone descriptors */
enum zbc_zone_type {
 ZBC_ZONE_TYPE_CONV = 0x1,
 ZBC_ZONE_TYPE_SEQWRITE_REQ = 0x2,
 ZBC_ZONE_TYPE_SEQWRITE_PREF = 0x3,
 ZBC_ZONE_TYPE_SEQ_OR_BEFORE_REQ = 0x4,
 ZBC_ZONE_TYPE_GAP = 0x5,
 /* 0x6 to 0xf are reserved */
};

/* Zone conditions of REPORT ZONES zone descriptors */
enum zbc_zone_cond {
 ZBC_ZONE_COND_NO_WP = 0x0,
 ZBC_ZONE_COND_EMPTY = 0x1,
 ZBC_ZONE_COND_IMP_OPEN = 0x2,
 ZBC_ZONE_COND_EXP_OPEN = 0x3,
 ZBC_ZONE_COND_CLOSED = 0x4,
 /* 0x5 to 0xc are reserved */
 ZBC_ZONE_COND_READONLY = 0xd,
 ZBC_ZONE_COND_FULL = 0xe,
 ZBC_ZONE_COND_OFFLINE = 0xf,
};

enum zbc_zone_alignment_method {
 ZBC_CONSTANT_ZONE_LENGTH = 0x1,
 ZBC_CONSTANT_ZONE_START_OFFSET = 0x8,
};

/* Version descriptor values for INQUIRY */
enum scsi_version_descriptor {
 SCSI_VERSION_DESCRIPTOR_FCP4 = 0x0a40,
 SCSI_VERSION_DESCRIPTOR_ISCSI = 0x0960,
 SCSI_VERSION_DESCRIPTOR_SAM5 = 0x00a0,
 SCSI_VERSION_DESCRIPTOR_SAS3 = 0x0c60,
 SCSI_VERSION_DESCRIPTOR_SBC3 = 0x04c0,
 SCSI_VERSION_DESCRIPTOR_SBP3 = 0x0980,
 SCSI_VERSION_DESCRIPTOR_SPC4 = 0x0460,
 SCSI_VERSION_DESCRIPTOR_SRP = 0x0940
};

enum scsi_support_opcode {
 SCSI_SUPPORT_NO_INFO = 0,
 SCSI_SUPPORT_NOT_SUPPORTED = 1,
 SCSI_SUPPORT_FULL = 3,
 SCSI_SUPPORT_VENDOR = 5,
};
# 11 "./include/scsi/scsi_common.h" 2

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned
scsi_varlen_cdb_length(const void *hdr)
{
 return ((struct scsi_varlen_cdb_hdr *)hdr)->additional_cdb_length + 8;
}

extern const unsigned char scsi_command_size_tbl[8];


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned
scsi_command_size(const unsigned char *cmnd)
{
 return (cmnd[0] == 0x7f) ?
  scsi_varlen_cdb_length(cmnd) : scsi_command_size_tbl[((cmnd[0]) >> 5) & 7];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned char
scsi_command_control(const unsigned char *cmnd)
{
 return (cmnd[0] == 0x7f) ?
  cmnd[1] : cmnd[scsi_command_size_tbl[((cmnd[0]) >> 5) & 7] - 1];
}

/* Returns a human-readable name for the device */
extern const char *scsi_device_type(unsigned type);

extern void int_to_scsilun(u64, struct scsi_lun *);
extern u64 scsilun_to_int(struct scsi_lun *);

/*
 * This is a slightly modified SCSI sense "descriptor" format header.
 * The addition is to allow the 0x70 and 0x71 response codes. The idea
 * is to place the salient data from either "fixed" or "descriptor" sense
 * format into one structure to ease application processing.
 *
 * The original sense buffer should be kept around for those cases
 * in which more information is required (e.g. the LBA of a MEDIUM ERROR).
 */
struct scsi_sense_hdr { /* See SPC-3 section 4.5 */
 u8 response_code; /* permit: 0x0, 0x70, 0x71, 0x72, 0x73 */
 u8 sense_key;
 u8 asc;
 u8 ascq;
 u8 byte4;
 u8 byte5;
 u8 byte6;
 u8 additional_length; /* always 0 for fixed sense format */
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool scsi_sense_valid(const struct scsi_sense_hdr *sshdr)
{
 if (!sshdr)
  return false;

 return (sshdr->response_code & 0x70) == 0x70;
}

extern bool scsi_normalize_sense(const u8 *sense_buffer, int sb_len,
     struct scsi_sense_hdr *sshdr);

extern void scsi_build_sense_buffer(int desc, u8 *buf, u8 key, u8 asc, u8 ascq);
int scsi_set_sense_information(u8 *buf, int buf_len, u64 info);
int scsi_set_sense_field_pointer(u8 *buf, int buf_len, u16 fp, u8 bp, bool cd);
extern const u8 * scsi_sense_desc_find(const u8 * sense_buffer, int sb_len,
           int desc_type);
# 13 "./include/scsi/scsi.h" 2

# 1 "./include/scsi/scsi_status.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







/* Message codes. */
enum scsi_msg_byte {
 COMMAND_COMPLETE = 0x00,
 EXTENDED_MESSAGE = 0x01,
 SAVE_POINTERS = 0x02,
 RESTORE_POINTERS = 0x03,
 DISCONNECT = 0x04,
 INITIATOR_ERROR = 0x05,
 ABORT_TASK_SET = 0x06,
 MESSAGE_REJECT = 0x07,
 NOP = 0x08,
 MSG_PARITY_ERROR = 0x09,
 LINKED_CMD_COMPLETE = 0x0a,
 LINKED_FLG_CMD_COMPLETE = 0x0b,
 TARGET_RESET = 0x0c,
 ABORT_TASK = 0x0d,
 CLEAR_TASK_SET = 0x0e,
 INITIATE_RECOVERY = 0x0f, /* SCSI-II only */
 RELEASE_RECOVERY = 0x10, /* SCSI-II only */
 TERMINATE_IO_PROC = 0x11, /* SCSI-II only */
 CLEAR_ACA = 0x16,
 LOGICAL_UNIT_RESET = 0x17,
 SIMPLE_QUEUE_TAG = 0x20,
 HEAD_OF_QUEUE_TAG = 0x21,
 ORDERED_QUEUE_TAG = 0x22,
 IGNORE_WIDE_RESIDUE = 0x23,
 ACA = 0x24,
 QAS_REQUEST = 0x55,

 /* Old SCSI2 names, don't use in new code */
 BUS_DEVICE_RESET = TARGET_RESET,
 ABORT = ABORT_TASK_SET,
};

/* Host byte codes. */
enum scsi_host_status {
 DID_OK = 0x00, /* NO error                                */
 DID_NO_CONNECT = 0x01, /* Couldn't connect before timeout period  */
 DID_BUS_BUSY = 0x02, /* BUS stayed busy through time out period */
 DID_TIME_OUT = 0x03, /* TIMED OUT for other reason              */
 DID_BAD_TARGET = 0x04, /* BAD target.                             */
 DID_ABORT = 0x05, /* Told to abort for some other reason     */
 DID_PARITY = 0x06, /* Parity error                            */
 DID_ERROR = 0x07, /* Internal error                          */
 DID_RESET = 0x08, /* Reset by somebody.                      */
 DID_BAD_INTR = 0x09, /* Got an interrupt we weren't expecting.  */
 DID_PASSTHROUGH = 0x0a, /* Force command past mid-layer            */
 DID_SOFT_ERROR = 0x0b, /* The low level driver just wish a retry  */
 DID_IMM_RETRY = 0x0c, /* Retry without decrementing retry count  */
 DID_REQUEUE = 0x0d, /* Requeue command (no immediate retry) also
				 * without decrementing the retry count	   */
 DID_TRANSPORT_DISRUPTED = 0x0e, /* Transport error disrupted execution
					 * and the driver blocked the port to
					 * recover the link. Transport class will
					 * retry or fail IO */
 DID_TRANSPORT_FAILFAST = 0x0f, /* Transport class fastfailed the io */
 /*
	 * We used to have DID_TARGET_FAILURE, DID_NEXUS_FAILURE,
	 * DID_ALLOC_FAILURE and DID_MEDIUM_ERROR at 0x10 - 0x13. For compat
	 * with userspace apps that parse the host byte for SG IO, we leave
	 * that block of codes unused and start at 0x14 below.
	 */
 DID_TRANSPORT_MARGINAL = 0x14, /* Transport marginal errors */
};
# 15 "./include/scsi/scsi.h" 2

struct scsi_cmnd;

enum scsi_timeouts {
 SCSI_DEFAULT_EH_TIMEOUT = 10 * 250 /* Internal kernel timer frequency */,
};

/*
 * DIX-capable adapters effectively support infinite chaining for the
 * protection information scatterlist
 */


/*
 * Special value for scanning to specify scanning or rescanning of all
 * possible channels, (target) ids, or luns on a given shost.
 */


/*
 * standard mode-select header prepended to all mode-select commands
 */

struct ccs_modesel_head {
 __u8 _r1; /* reserved */
 __u8 medium; /* device-specific medium type */
 __u8 _r2; /* reserved */
 __u8 block_desc_length; /* block descriptor length */
 __u8 density; /* device-specific density code */
 __u8 number_blocks_hi; /* number of blocks in this block desc */
 __u8 number_blocks_med;
 __u8 number_blocks_lo;
 __u8 _r3;
 __u8 block_length_hi; /* block length for blocks in this desc */
 __u8 block_length_med;
 __u8 block_length_lo;
};

/*
 * The Well Known LUNS (SAM-3) in our int representation of a LUN
 */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_is_wlun(u64 lun)
{
 return (lun & 0xff00) == 0xc100;
}

/**
 * scsi_status_is_check_condition - check the status return.
 *
 * @status: the status passed up from the driver (including host and
 *          driver components)
 *
 * This returns true if the status code is SAM_STAT_CHECK_CONDITION.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_status_is_check_condition(int status)
{
 if (status < 0)
  return false;
 status &= 0xfe;
 return status == SAM_STAT_CHECK_CONDITION;
}

/*
 *  Extended message codes.
 */







/*
 * Internal return values.
 */
enum scsi_disposition {
 NEEDS_RETRY = 0x2001,
 SUCCESS = 0x2002,
 FAILED = 0x2003,
 QUEUED = 0x2004,
 SOFT_ERROR = 0x2005,
 ADD_TO_MLQUEUE = 0x2006,
 TIMEOUT_ERROR = 0x2007,
 SCSI_RETURN_NOT_HANDLED = 0x2008,
 FAST_IO_FAIL = 0x2009,
};

/*
 * Midlevel queue return values.
 */





/*
 *  Use these to separate status msg and our bytes
 *
 *  These are set by:
 *
 *      status byte = set from target device
 *      msg_byte    (unused)
 *      host_byte   = set by low-level driver to indicate status.
 */






/*
 * default timeouts
*/
# 145 "./include/scsi/scsi.h"
/*
 *  struct scsi_device::scsi_level values. For SCSI devices other than those
 *  prior to SCSI-2 (i.e. over 12 years old) this value is (resp[2] + 1)
 *  where "resp" is a byte array of the response to an INQUIRY. The scsi_level
 *  variable is visible to the user via sysfs.
 */
# 160 "./include/scsi/scsi.h"
/*
 * INQ PERIPHERAL QUALIFIERS
 */





/*
 * Here are some scsi specific ioctl commands which are sometimes useful.
 *
 * Note that include/linux/cdrom.h also defines IOCTL 0x5300 - 0x5395
 */

/* Used to obtain PUN and LUN info.  Conflicts with CDROMAUDIOBUFSIZ */


/* 0x5383 and 0x5384 were used for SCSI_IOCTL_TAGGED_{ENABLE,DISABLE} */

/* Used to obtain the host number of a device. */


/* Used to obtain the bus number for a device */


/* Used to obtain the PCI location of a device */


/** scsi_status_is_good - check the status return.
 *
 * @status: the status passed up from the driver (including host and
 *          driver components)
 *
 * This returns true for known good conditions that may be treated as
 * command completed normally
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) bool scsi_status_is_good(int status)
{
 if (status < 0)
  return false;

 if ((((status) >> 16) & 0xff) == DID_NO_CONNECT)
  return false;

 /*
	 * FIXME: bit0 is listed as reserved in SCSI-2, but is
	 * significant in SCSI-3.  For now, we follow the SCSI-2
	 * behaviour and ignore reserved bits.
	 */
 status &= 0xfe;
 return ((status == SAM_STAT_GOOD) ||
  (status == SAM_STAT_CONDITION_MET) ||
  /* Next two "intermediate" statuses are obsolete in SAM-4 */
  (status == SAM_STAT_INTERMEDIATE) ||
  (status == SAM_STAT_INTERMEDIATE_CONDITION_MET) ||
  /* FIXME: this is obsolete in SAM-3 */
  (status == SAM_STAT_COMMAND_TERMINATED));
}
# 10 "./include/scsi/scsi_device.h" 2



struct bsg_device;
struct device;
struct request_queue;
struct scsi_cmnd;
struct scsi_lun;
struct scsi_sense_hdr;

typedef __u64 blist_flags_t;



struct scsi_mode_data {
 __u32 length;
 __u16 block_descriptor_length;
 __u8 medium_type;
 __u8 device_specific;
 __u8 header_length;
 __u8 longlba:1;
};

/*
 * sdev state: If you alter this, you also need to alter scsi_sysfs.c
 * (for the ascii descriptions) and the state model enforcer:
 * scsi_lib:scsi_device_set_state().
 */
enum scsi_device_state {
 SDEV_CREATED = 1, /* device created but not added to sysfs
				 * Only internal commands allowed (for inq) */
 SDEV_RUNNING, /* device properly configured
				 * All commands allowed */
 SDEV_CANCEL, /* beginning to delete device
				 * Only error handler commands allowed */
 SDEV_DEL, /* device deleted
				 * no commands allowed */
 SDEV_QUIESCE, /* Device quiescent.  No block commands
				 * will be accepted, only specials (which
				 * originate in the mid-layer) */
 SDEV_OFFLINE, /* Device offlined (by error handling or
				 * user request */
 SDEV_TRANSPORT_OFFLINE, /* Offlined by transport class error handler */
 SDEV_BLOCK, /* Device blocked by scsi lld.  No
				 * scsi commands from user or midlayer
				 * should be issued to the scsi
				 * lld. */
 SDEV_CREATED_BLOCK, /* same as above but for created devices */
};

enum scsi_scan_mode {
 SCSI_SCAN_INITIAL = 0,
 SCSI_SCAN_RESCAN,
 SCSI_SCAN_MANUAL,
};

enum scsi_device_event {
 SDEV_EVT_MEDIA_CHANGE = 1, /* media has changed */
 SDEV_EVT_INQUIRY_CHANGE_REPORTED, /* 3F 03  UA reported */
 SDEV_EVT_CAPACITY_CHANGE_REPORTED, /* 2A 09  UA reported */
 SDEV_EVT_SOFT_THRESHOLD_REACHED_REPORTED, /* 38 07  UA reported */
 SDEV_EVT_MODE_PARAMETER_CHANGE_REPORTED, /* 2A 01  UA reported */
 SDEV_EVT_LUN_CHANGE_REPORTED, /* 3F 0E  UA reported */
 SDEV_EVT_ALUA_STATE_CHANGE_REPORTED, /* 2A 06  UA reported */
 SDEV_EVT_POWER_ON_RESET_OCCURRED, /* 29 00  UA reported */

 SDEV_EVT_FIRST = SDEV_EVT_MEDIA_CHANGE,
 SDEV_EVT_LAST = SDEV_EVT_POWER_ON_RESET_OCCURRED,

 SDEV_EVT_MAXBITS = SDEV_EVT_LAST + 1
};

struct scsi_event {
 enum scsi_device_event evt_type;
 struct list_head node;

 /* put union of data structures, for non-simple event types,
	 * here
	 */
};

/**
 * struct scsi_vpd - SCSI Vital Product Data
 * @rcu: For kfree_rcu().
 * @len: Length in bytes of @data.
 * @data: VPD data as defined in various T10 SCSI standard documents.
 */
struct scsi_vpd {
 struct callback_head rcu;
 int len;
 unsigned char data[];
};

enum scsi_vpd_parameters {
 SCSI_VPD_HEADER_SIZE = 4,
};

struct scsi_device {
 struct Scsi_Host *host;
 struct request_queue *request_queue;

 /* the next two are protected by the host->host_lock */
 struct list_head siblings; /* list of all devices on this host */
 struct list_head same_target_siblings; /* just the devices sharing same target id */

 struct sbitmap budget_map;
 atomic_t device_blocked; /* Device returned QUEUE_FULL. */

 atomic_t restarts;
 spinlock_t list_lock;
 struct list_head starved_entry;
 unsigned short queue_depth; /* How deep of a queue we want */
 unsigned short max_queue_depth; /* max queue depth */
 unsigned short last_queue_full_depth; /* These two are used by */
 unsigned short last_queue_full_count; /* scsi_track_queue_full() */
 unsigned long last_queue_full_time; /* last queue full time */
 unsigned long queue_ramp_up_period; /* ramp up period in jiffies */


 unsigned long last_queue_ramp_up; /* last queue ramp up time */

 unsigned int id, channel;
 u64 lun;
 unsigned int manufacturer; /* Manufacturer of device, for using
					 * vendor-specific cmd's */
 unsigned sector_size; /* size in bytes */

 void *hostdata; /* available to low-level driver */
 unsigned char type;
 char scsi_level;
 char inq_periph_qual; /* PQ from INQUIRY data */
 struct mutex inquiry_mutex;
 unsigned char inquiry_len; /* valid bytes in 'inquiry' */
 unsigned char * inquiry; /* INQUIRY response data */
 const char * vendor; /* [back_compat] point into 'inquiry' ... */
 const char * model; /* ... after scan; point to static string */
 const char * rev; /* ... "nullnullnullnull" before scan */

 struct scsi_vpd /* nothing */ *vpd_pg0;
 struct scsi_vpd /* nothing */ *vpd_pg83;
 struct scsi_vpd /* nothing */ *vpd_pg80;
 struct scsi_vpd /* nothing */ *vpd_pg89;
 struct scsi_vpd /* nothing */ *vpd_pgb0;
 struct scsi_vpd /* nothing */ *vpd_pgb1;
 struct scsi_vpd /* nothing */ *vpd_pgb2;

 struct scsi_target *sdev_target;

 blist_flags_t sdev_bflags; /* black/white flags as also found in
				 * scsi_devinfo.[hc]. For now used only to
				 * pass settings from slave_alloc to scsi
				 * core. */
 unsigned int eh_timeout; /* Error handling timeout */
 unsigned removable:1;
 unsigned changed:1; /* Data invalid due to media change */
 unsigned busy:1; /* Used to prevent races */
 unsigned lockable:1; /* Able to prevent media removal */
 unsigned locked:1; /* Media removal disabled */
 unsigned borken:1; /* Tell the Seagate driver to be
				 * painfully slow on this device */
 unsigned disconnect:1; /* can disconnect */
 unsigned soft_reset:1; /* Uses soft reset option */
 unsigned sdtr:1; /* Device supports SDTR messages */
 unsigned wdtr:1; /* Device supports WDTR messages */
 unsigned ppr:1; /* Device supports PPR messages */
 unsigned tagged_supported:1; /* Supports SCSI-II tagged queuing */
 unsigned simple_tags:1; /* simple queue tag messages are enabled */
 unsigned was_reset:1; /* There was a bus reset on the bus for
				 * this device */
 unsigned expecting_cc_ua:1; /* Expecting a CHECK_CONDITION/UNIT_ATTN
				     * because we did a bus reset. */
 unsigned use_10_for_rw:1; /* first try 10-byte read / write */
 unsigned use_10_for_ms:1; /* first try 10-byte mode sense/select */
 unsigned set_dbd_for_ms:1; /* Set "DBD" field in mode sense */
 unsigned no_report_opcodes:1; /* no REPORT SUPPORTED OPERATION CODES */
 unsigned no_write_same:1; /* no WRITE SAME command */
 unsigned use_16_for_rw:1; /* Use read/write(16) over read/write(10) */
 unsigned use_16_for_sync:1; /* Use sync (16) over sync (10) */
 unsigned skip_ms_page_8:1; /* do not use MODE SENSE page 0x08 */
 unsigned skip_ms_page_3f:1; /* do not use MODE SENSE page 0x3f */
 unsigned skip_vpd_pages:1; /* do not read VPD pages */
 unsigned try_vpd_pages:1; /* attempt to read VPD pages */
 unsigned use_192_bytes_for_3f:1; /* ask for 192 bytes from page 0x3f */
 unsigned no_start_on_add:1; /* do not issue start on add */
 unsigned allow_restart:1; /* issue START_UNIT in error handler */
 unsigned manage_start_stop:1; /* Let HLD (sd) manage start/stop */
 unsigned start_stop_pwr_cond:1; /* Set power cond. in START_STOP_UNIT */
 unsigned no_uld_attach:1; /* disable connecting to upper level drivers */
 unsigned select_no_atn:1;
 unsigned fix_capacity:1; /* READ_CAPACITY is too high by 1 */
 unsigned guess_capacity:1; /* READ_CAPACITY might be too high by 1 */
 unsigned retry_hwerror:1; /* Retry HARDWARE_ERROR */
 unsigned last_sector_bug:1; /* do not use multisector accesses on
					   SD_LAST_BUGGY_SECTORS */
 unsigned no_read_disc_info:1; /* Avoid READ_DISC_INFO cmds */
 unsigned no_read_capacity_16:1; /* Avoid READ_CAPACITY_16 cmds */
 unsigned try_rc_10_first:1; /* Try READ_CAPACACITY_10 first */
 unsigned security_supported:1; /* Supports Security Protocols */
 unsigned is_visible:1; /* is the device visible in sysfs */
 unsigned wce_default_on:1; /* Cache is ON by default */
 unsigned no_dif:1; /* T10 PI (DIF) should be disabled */
 unsigned broken_fua:1; /* Don't set FUA bit */
 unsigned lun_in_cdb:1; /* Store LUN bits in CDB[1] */
 unsigned unmap_limit_for_ws:1; /* Use the UNMAP limit for WRITE SAME */
 unsigned rpm_autosuspend:1; /* Enable runtime autosuspend at device
					 * creation time */
 unsigned ignore_media_change:1; /* Ignore MEDIA CHANGE on resume */
 unsigned silence_suspend:1; /* Do not print runtime PM related messages */

 unsigned int queue_stopped; /* request queue is quiesced */
 bool offline_already; /* Device offline message logged */

 atomic_t disk_events_disable_depth; /* disable depth for disk events */

 unsigned long supported_events[(((SDEV_EVT_MAXBITS) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))]; /* supported events */
 unsigned long pending_events[(((SDEV_EVT_MAXBITS) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))]; /* pending events */
 struct list_head event_list; /* asserted events */
 struct work_struct event_work;

 unsigned int max_device_blocked; /* what device_blocked counts down from  */


 atomic_t iorequest_cnt;
 atomic_t iodone_cnt;
 atomic_t ioerr_cnt;
 atomic_t iotmo_cnt;

 struct device sdev_gendev,
    sdev_dev;

 struct work_struct requeue_work;

 struct scsi_device_handler *handler;
 void *handler_data;

 size_t dma_drain_len;
 void *dma_drain_buf;

 unsigned int sg_timeout;
 unsigned int sg_reserved_size;

 struct bsg_device *bsg_dev;
 unsigned char access_state;
 struct mutex state_mutex;
 enum scsi_device_state sdev_state;
 struct task_struct *quiesced_by;
 unsigned long sdev_data[];
} __attribute__((aligned(sizeof(unsigned long))));
# 269 "./include/scsi/scsi_device.h"
/*
 * like scmd_printk, but the device name is passed in
 * as a string pointer
 */
__attribute__((__format__(printf, 4, 5))) void
sdev_prefix_printk(const char *, const struct scsi_device *, const char *,
  const char *, ...);




__attribute__((__format__(printf, 3, 4))) void
scmd_printk(const char *, const struct scsi_cmnd *, const char *, ...);
# 294 "./include/scsi/scsi_device.h"
enum scsi_target_state {
 STARGET_CREATED = 1,
 STARGET_RUNNING,
 STARGET_REMOVE,
 STARGET_CREATED_REMOVE,
 STARGET_DEL,
};

/*
 * scsi_target: representation of a scsi target, for now, this is only
 * used for single_lun devices. If no one has active IO to the target,
 * starget_sdev_user is NULL, else it points to the active sdev.
 */
struct scsi_target {
 struct scsi_device *starget_sdev_user;
 struct list_head siblings;
 struct list_head devices;
 struct device dev;
 struct kref reap_ref; /* last put renders target invisible */
 unsigned int channel;
 unsigned int id; /* target id ... replace
				     * scsi_device.id eventually */
 unsigned int create:1; /* signal that it needs to be added */
 unsigned int single_lun:1; /* Indicates we should only
						 * allow I/O to one of the luns
						 * for the device at a time. */
 unsigned int pdt_1f_for_no_lun:1; /* PDT = 0x1f
						 * means no lun present. */
 unsigned int no_report_luns:1; /* Don't use
						 * REPORT LUNS for scanning. */
 unsigned int expecting_lun_change:1; /* A device has reported
						 * a 3F/0E UA, other devices on
						 * the same target will also. */
 /* commands actually active on LLD. */
 atomic_t target_busy;
 atomic_t target_blocked;

 /*
	 * LLDs should set this in the slave_alloc host template callout.
	 * If set to zero then there is not limit.
	 */
 unsigned int can_queue;
 unsigned int max_target_blocked;


 char scsi_level;
 enum scsi_target_state state;
 void *hostdata; /* available to low-level driver */
 unsigned long starget_data[]; /* for the transport */
 /* starget_data must be the last element!!!! */
} __attribute__((aligned(sizeof(unsigned long))));


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) struct scsi_target *scsi_target(struct scsi_device *sdev)
{
 return ({ void *__mptr = (void *)(sdev->sdev_gendev.parent); _Static_assert(__builtin_types_compatible_p(typeof(*(sdev->sdev_gendev.parent)), typeof(((struct scsi_target *)0)->dev)) || __builtin_types_compatible_p(typeof(*(sdev->sdev_gendev.parent)), typeof(void)), "pointer type mismatch in container_of()"); ((struct scsi_target *)(__mptr - __builtin_offsetof(struct scsi_target, dev))); });
}






extern struct scsi_device *__scsi_add_device(struct Scsi_Host *,
  uint, uint, u64, void *hostdata);
extern int scsi_add_device(struct Scsi_Host *host, uint channel,
      uint target, u64 lun);
extern int scsi_register_device_handler(struct scsi_device_handler *scsi_dh);
extern void scsi_remove_device(struct scsi_device *);
extern int scsi_unregister_device_handler(struct scsi_device_handler *scsi_dh);
void scsi_attach_vpd(struct scsi_device *sdev);

extern struct scsi_device *scsi_device_from_queue(struct request_queue *q);
extern int __attribute__((__warn_unused_result__)) scsi_device_get(struct scsi_device *);
extern void scsi_device_put(struct scsi_device *);
extern struct scsi_device *scsi_device_lookup(struct Scsi_Host *,
           uint, uint, u64);
extern struct scsi_device *__scsi_device_lookup(struct Scsi_Host *,
      uint, uint, u64);
extern struct scsi_device *scsi_device_lookup_by_target(struct scsi_target *,
       u64);
extern struct scsi_device *__scsi_device_lookup_by_target(struct scsi_target *,
         u64);
extern void starget_for_each_device(struct scsi_target *, void *,
       void (*fn)(struct scsi_device *, void *));
extern void __starget_for_each_device(struct scsi_target *, void *,
          void (*fn)(struct scsi_device *,
       void *));

/* only exposed to implement shost_for_each_device */
extern struct scsi_device *__scsi_iterate_devices(struct Scsi_Host *,
        struct scsi_device *);

/**
 * shost_for_each_device - iterate over all devices of a host
 * @sdev: the &struct scsi_device to use as a cursor
 * @shost: the &struct scsi_host to iterate over
 *
 * Iterator that returns each device attached to @shost.  This loop
 * takes a reference on each device and releases it at the end.  If
 * you break out of the loop, you must call scsi_device_put(sdev).
 */





/**
 * __shost_for_each_device - iterate over all devices of a host (UNLOCKED)
 * @sdev: the &struct scsi_device to use as a cursor
 * @shost: the &struct scsi_host to iterate over
 *
 * Iterator that returns each device attached to @shost.  It does _not_
 * take a reference on the scsi_device, so the whole loop must be
 * protected by shost->host_lock.
 *
 * Note: The only reason to use this is because you need to access the
 * device list in interrupt context.  Otherwise you really want to use
 * shost_for_each_device instead.
 */



extern int scsi_change_queue_depth(struct scsi_device *, int);
extern int scsi_track_queue_full(struct scsi_device *, int);

extern int scsi_set_medium_removal(struct scsi_device *, char);

extern int scsi_mode_sense(struct scsi_device *sdev, int dbd, int modepage,
      unsigned char *buffer, int len, int timeout,
      int retries, struct scsi_mode_data *data,
      struct scsi_sense_hdr *);
extern int scsi_mode_select(struct scsi_device *sdev, int pf, int sp,
       unsigned char *buffer, int len, int timeout,
       int retries, struct scsi_mode_data *data,
       struct scsi_sense_hdr *);
extern int scsi_test_unit_ready(struct scsi_device *sdev, int timeout,
    int retries, struct scsi_sense_hdr *sshdr);
extern int scsi_get_vpd_page(struct scsi_device *, u8 page, unsigned char *buf,
        int buf_len);
extern int scsi_report_opcode(struct scsi_device *sdev, unsigned char *buffer,
         unsigned int len, unsigned char opcode);
extern int scsi_device_set_state(struct scsi_device *sdev,
     enum scsi_device_state state);
extern struct scsi_event *sdev_evt_alloc(enum scsi_device_event evt_type,
       gfp_t gfpflags);
extern void sdev_evt_send(struct scsi_device *sdev, struct scsi_event *evt);
extern void sdev_evt_send_simple(struct scsi_device *sdev,
     enum scsi_device_event evt_type, gfp_t gfpflags);
extern int scsi_device_quiesce(struct scsi_device *sdev);
extern void scsi_device_resume(struct scsi_device *sdev);
extern void scsi_target_quiesce(struct scsi_target *);
extern void scsi_target_resume(struct scsi_target *);
extern void scsi_scan_target(struct device *parent, unsigned int channel,
        unsigned int id, u64 lun,
        enum scsi_scan_mode rescan);
extern void scsi_target_reap(struct scsi_target *);
extern void scsi_target_block(struct device *);
extern void scsi_target_unblock(struct device *, enum scsi_device_state);
extern void scsi_remove_target(struct device *);
extern const char *scsi_device_state_name(enum scsi_device_state);
extern int scsi_is_sdev_device(const struct device *);
extern int scsi_is_target_device(const struct device *);
extern void scsi_sanitize_inquiry_string(unsigned char *s, int len);
extern int __scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,
   int data_direction, void *buffer, unsigned bufflen,
   unsigned char *sense, struct scsi_sense_hdr *sshdr,
   int timeout, int retries, blk_opf_t flags,
   req_flags_t rq_flags, int *resid);
/* Make sure any sense buffer is the correct size. */
# 473 "./include/scsi/scsi_device.h"
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_execute_req(struct scsi_device *sdev,
 const unsigned char *cmd, int data_direction, void *buffer,
 unsigned bufflen, struct scsi_sense_hdr *sshdr, int timeout,
 int retries, int *resid)
{
 return ({ do { /*							\
		 * __noreturn is needed to give the compiler enough	\
		 * information to avoid certain possibly-uninitialized	\
		 * warnings (regardless of the build failing).		\
		 */ __attribute__((__noreturn__)) extern void __compiletime_assert_428(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(((void *)0)) != NULL && sizeof(((void *)0)) != SCSI_SENSE_BUFFERSIZE"))); if (!(!((((void *)0)) != ((void *)0) && sizeof(((void *)0)) != 96))) __compiletime_assert_428(); } while (0); __scsi_execute(sdev, cmd, data_direction, buffer, bufflen, ((void *)0), sshdr, timeout, retries, 0, 0, resid); });
# 480 "./include/scsi/scsi_device.h"
}
extern void sdev_disable_disk_events(struct scsi_device *sdev);
extern void sdev_enable_disk_events(struct scsi_device *sdev);
extern int scsi_vpd_lun_id(struct scsi_device *, char *, size_t);
extern int scsi_vpd_tpg_id(struct scsi_device *, int *);


extern int scsi_autopm_get_device(struct scsi_device *);
extern void scsi_autopm_put_device(struct scsi_device *);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int __attribute__((__warn_unused_result__)) scsi_device_reprobe(struct scsi_device *sdev)
{
 return device_reprobe(&sdev->sdev_gendev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int sdev_channel(struct scsi_device *sdev)
{
 return sdev->channel;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned int sdev_id(struct scsi_device *sdev)
{
 return sdev->id;
}




/*
 * checks for positions of the SCSI state machine
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_online(struct scsi_device *sdev)
{
 return (sdev->sdev_state != SDEV_OFFLINE &&
  sdev->sdev_state != SDEV_TRANSPORT_OFFLINE &&
  sdev->sdev_state != SDEV_DEL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_blocked(struct scsi_device *sdev)
{
 return sdev->sdev_state == SDEV_BLOCK ||
  sdev->sdev_state == SDEV_CREATED_BLOCK;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_created(struct scsi_device *sdev)
{
 return sdev->sdev_state == SDEV_CREATED ||
  sdev->sdev_state == SDEV_CREATED_BLOCK;
}

int scsi_internal_device_block_nowait(struct scsi_device *sdev);
int scsi_internal_device_unblock_nowait(struct scsi_device *sdev,
     enum scsi_device_state new_state);

/* accessor functions for the SCSI parameters */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_sync(struct scsi_device *sdev)
{
 return sdev->sdtr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_wide(struct scsi_device *sdev)
{
 return sdev->wdtr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_dt(struct scsi_device *sdev)
{
 return sdev->ppr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_dt_only(struct scsi_device *sdev)
{
 if (sdev->inquiry_len < 57)
  return 0;
 return (sdev->inquiry[56] & 0x0c) == 0x04;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_ius(struct scsi_device *sdev)
{
 if (sdev->inquiry_len < 57)
  return 0;
 return sdev->inquiry[56] & 0x01;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_qas(struct scsi_device *sdev)
{
 if (sdev->inquiry_len < 57)
  return 0;
 return sdev->inquiry[56] & 0x02;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_enclosure(struct scsi_device *sdev)
{
 return sdev->inquiry ? (sdev->inquiry[6] & (1<<6)) : 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_protection(struct scsi_device *sdev)
{
 if (sdev->no_dif)
  return 0;

 return sdev->scsi_level > 3 && sdev->inquiry[5] & (1<<0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_tpgs(struct scsi_device *sdev)
{
 return sdev->inquiry ? (sdev->inquiry[5] >> 4) & 0x3 : 0;
}

/**
 * scsi_device_supports_vpd - test if a device supports VPD pages
 * @sdev: the &struct scsi_device to test
 *
 * If the 'try_vpd_pages' flag is set it takes precedence.
 * Otherwise we will assume VPD pages are supported if the
 * SCSI level is at least SPC-3 and 'skip_vpd_pages' is not set.
 */
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_supports_vpd(struct scsi_device *sdev)
{
 /* Attempt VPD inquiry if the device blacklist explicitly calls
	 * for it.
	 */
 if (sdev->try_vpd_pages)
  return 1;
 /*
	 * Although VPD inquiries can go to SCSI-2 type devices,
	 * some USB ones crash on receiving them, and the pages
	 * we currently ask for are mandatory for SPC-2 and beyond
	 */
 if (sdev->scsi_level >= 5 && !sdev->skip_vpd_pages)
  return 1;
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) int scsi_device_busy(struct scsi_device *sdev)
{
 return sbitmap_weight(&sdev->budget_map);
}
# 13 "drivers/scsi/scsi_devinfo.c" 2
# 1 "./include/scsi/scsi_devinfo.h" 1
/* SPDX-License-Identifier: GPL-2.0 */


/*
 * Flags for SCSI devices that need special treatment
 */

/* Only scan LUN 0 */

/* Known to have LUNs, force scanning.
 * DEPRECATED: Use max_luns=N */

/* Flag for broken handshaking */

/* unlock by special command */

/* Do not use LUNs in parallel */

/* Buggy Tagged Command Queuing */

/* Non consecutive LUN numbering */

/* Avoid LUNS >= 5 */

/* Treat as (removable) CD-ROM */

/* LUNs past 7 on a SCSI-2 device */

/* override additional length field */

/* ignore MEDIA CHANGE unit attention after resuming from runtime suspend */

/* do not do automatic start on add */





/* try REPORT_LUNS even for SCSI-2 devs (if HBA supports more than 8 LUNs) */

/* don't try REPORT_LUNS scan (SCSI-3 devs) */

/* don't use PREVENT-ALLOW commands */

/* device is actually for RAID config */

/* select without ATN */

/* retry HARDWARE_ERROR */

/* maximum 512 sector cdb length */


/* Disable T10 PI (DIF) */

/* Ignore SBC-3 VPD pages */


/* Attempt to read VPD pages */

/* don't try to issue RSOC */

/* maximum 1024 sector cdb length */

/* Use UNMAP limit for WRITE SAME */

/* Always retry ABORTED_COMMAND with Internal Target Failure */

/* Always retry ABORTED_COMMAND with ASC 0xc1 */
# 14 "drivers/scsi/scsi_devinfo.c" 2

# 1 "drivers/scsi/scsi_priv.h" 1
/* SPDX-License-Identifier: GPL-2.0 */







struct bsg_device;
struct request_queue;
struct request;
struct scsi_cmnd;
struct scsi_device;
struct scsi_target;
struct scsi_host_template;
struct Scsi_Host;
struct scsi_nl_hdr;



/*
 * Error codes used by scsi-ml internally. These must not be used by drivers.
 */
enum scsi_ml_status {
 SCSIML_STAT_OK = 0x00,
 SCSIML_STAT_RESV_CONFLICT = 0x01, /* Reservation conflict */
 SCSIML_STAT_NOSPC = 0x02, /* Space allocation on the dev failed */
 SCSIML_STAT_MED_ERROR = 0x03, /* Medium error */
 SCSIML_STAT_TGT_FAILURE = 0x04, /* Permanent target failure */
};

/*
 * Scsi Error Handler Flags
 */





/* hosts.c */
extern int scsi_init_hosts(void);
extern void scsi_exit_hosts(void);

/* scsi.c */
int scsi_init_sense_cache(struct Scsi_Host *shost);
void scsi_init_command(struct scsi_device *dev, struct scsi_cmnd *cmd);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void scsi_log_send(struct scsi_cmnd *cmd)
 { };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void scsi_log_completion(struct scsi_cmnd *cmd, int disposition)
 { };


/* scsi_devinfo.c */

/* list of keys for the lists */
enum scsi_devinfo_key {
 SCSI_DEVINFO_GLOBAL = 0,
 SCSI_DEVINFO_SPI,
};

extern blist_flags_t scsi_get_device_flags(struct scsi_device *sdev,
        const unsigned char *vendor,
        const unsigned char *model);
extern blist_flags_t scsi_get_device_flags_keyed(struct scsi_device *sdev,
       const unsigned char *vendor,
       const unsigned char *model,
       enum scsi_devinfo_key key);
extern int scsi_dev_info_list_add_keyed(int compatible, char *vendor,
     char *model, char *strflags,
     blist_flags_t flags,
     enum scsi_devinfo_key key);
extern int scsi_dev_info_list_del_keyed(char *vendor, char *model,
     enum scsi_devinfo_key key);
extern int scsi_dev_info_add_list(enum scsi_devinfo_key key, const char *name);
extern int scsi_dev_info_remove_list(enum scsi_devinfo_key key);

extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) scsi_init_devinfo(void);
extern void scsi_exit_devinfo(void);

/* scsi_error.c */
extern void scmd_eh_abort_handler(struct work_struct *work);
extern enum blk_eh_timer_return scsi_timeout(struct request *req);
extern int scsi_error_handler(void *host);
extern enum scsi_disposition scsi_decide_disposition(struct scsi_cmnd *cmd);
extern void scsi_eh_wakeup(struct Scsi_Host *shost);
extern void scsi_eh_scmd_add(struct scsi_cmnd *);
void scsi_eh_ready_devs(struct Scsi_Host *shost,
   struct list_head *work_q,
   struct list_head *done_q);
int scsi_eh_get_sense(struct list_head *work_q,
        struct list_head *done_q);
bool scsi_noretry_cmd(struct scsi_cmnd *scmd);
void scsi_eh_done(struct scsi_cmnd *scmd);

/* scsi_lib.c */
extern int scsi_maybe_unblock_host(struct scsi_device *sdev);
extern void scsi_device_unbusy(struct scsi_device *sdev, struct scsi_cmnd *cmd);
extern void scsi_queue_insert(struct scsi_cmnd *cmd, int reason);
extern void scsi_io_completion(struct scsi_cmnd *, unsigned int);
extern void scsi_run_host_queues(struct Scsi_Host *shost);
extern void scsi_requeue_run_queue(struct work_struct *work);
extern void scsi_start_queue(struct scsi_device *sdev);
extern int scsi_mq_setup_tags(struct Scsi_Host *shost);
extern void scsi_mq_free_tags(struct kref *kref);
extern void scsi_exit_queue(void);
extern void scsi_evt_thread(struct work_struct *work);

/* scsi_proc.c */
# 129 "drivers/scsi/scsi_priv.h"
/* scsi_scan.c */
void scsi_enable_async_suspend(struct device *dev);
extern int scsi_complete_async_scans(void);
extern int scsi_scan_host_selected(struct Scsi_Host *, unsigned int,
       unsigned int, u64, enum scsi_scan_mode);
extern void scsi_forget_host(struct Scsi_Host *);
extern void scsi_rescan_device(struct device *);

/* scsi_sysctl.c */

extern int scsi_init_sysctl(void);
extern void scsi_exit_sysctl(void);





/* scsi_sysfs.c */
extern int scsi_sysfs_add_sdev(struct scsi_device *);
extern int scsi_sysfs_add_host(struct Scsi_Host *);
extern int scsi_sysfs_register(void);
extern void scsi_sysfs_unregister(void);
extern void scsi_sysfs_device_initialize(struct scsi_device *);
extern int scsi_sysfs_target_initialize(struct scsi_device *);
extern struct scsi_transport_template blank_transport_template;
extern void __scsi_remove_device(struct scsi_device *);

extern struct bus_type scsi_bus_type;
extern const struct attribute_group *scsi_shost_groups[];

/* scsi_netlink.c */





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void scsi_netlink_init(void) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void scsi_netlink_exit(void) {}


/* scsi_pm.c */

extern const struct dev_pm_ops scsi_bus_pm_ops;

extern void scsi_autopm_get_target(struct scsi_target *);
extern void scsi_autopm_put_target(struct scsi_target *);
extern int scsi_autopm_get_host(struct Scsi_Host *);
extern void scsi_autopm_put_host(struct Scsi_Host *);







/* scsi_dh.c */




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void scsi_dh_add_device(struct scsi_device *sdev) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) void scsi_dh_release_device(struct scsi_device *sdev) { }


struct bsg_device *scsi_bsg_register_queue(struct scsi_device *sdev);

extern int scsi_device_max_queue_depth(struct scsi_device *sdev);

/*
 * internal scsi timeout functions: for use by mid-layer and transport
 * classes.
 */
# 16 "drivers/scsi/scsi_devinfo.c" 2


/*
 * scsi_dev_info_list: structure to hold black/white listed devices.
 */
struct scsi_dev_info_list {
 struct list_head dev_info_list;
 char vendor[8];
 char model[16];
 blist_flags_t flags;
 unsigned compatible; /* for use with scsi_static_device_list entries */
};

struct scsi_dev_info_list_table {
 struct list_head node; /* our node for being on the master list */
 struct list_head scsi_dev_info_list; /* head of dev info list */
 const char *name; /* name of list for /proc (NULL for global) */
 int key; /* unique numeric identifier */
};


static blist_flags_t scsi_default_dev_flags;
static struct list_head scsi_dev_info_list = { &(scsi_dev_info_list), &(scsi_dev_info_list) };
static char scsi_dev_flags[256];

/*
 * scsi_static_device_list: deprecated list of devices that require
 * settings that differ from the default, includes black-listed (broken)
 * devices. The entries here are added to the tail of scsi_dev_info_list
 * via scsi_dev_info_list_init.
 *
 * Do not add to this list, use the command line or proc interface to add
 * to the scsi_dev_info_list. This table will eventually go away.
 */
static struct {
 char *vendor;
 char *model;
 char *revision; /* revision known to be bad, unused */
 blist_flags_t flags;
} scsi_static_device_list[] __attribute__((__section__(".init.data"))) = {
 /*
	 * The following devices are known not to tolerate a lun != 0 scan
	 * for one reason or another. Some will respond to all luns,
	 * others will lock up.
	 */
 {"Aashima", "IMAGERY 2400SP", "1.03", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"CHINON", "CD-ROM CDS-431", "H42", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"CHINON", "CD-ROM CDS-535", "Q14", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"DENON", "DRD-25X", "V", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"HITACHI", "DK312C", "CM81", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"HITACHI", "DK314C", "CR21", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"IBM", "2104-DU3", ((void *)0), (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"IBM", "2104-TU3", ((void *)0), (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"IMS", "CDD521/10", "2.06", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"MAXTOR", "XT-3280", "PR02", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"MAXTOR", "XT-4380S", "B3C", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"MAXTOR", "MXT-1240S", "I1.2", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"MAXTOR", "XT-4170S", "B5A", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"MAXTOR", "XT-8760S", "B7B", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"MEDIAVIS", "RENO CD-ROMX2A", "2.03", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"MICROTEK", "ScanMakerIII", "2.30", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"NEC", "CD-ROM DRIVE:841", "1.0", (( blist_flags_t)(1ULL << 0))},/* locks up */
 {"PHILIPS", "PCA80SC", "V4-2", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"RODIME", "RO3000S", "2.33", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"SUN", "SENA", ((void *)0), (( blist_flags_t)(1ULL << 0))}, /* responds to all luns */
 /*
	 * The following causes a failed REQUEST SENSE on lun 1 for
	 * aha152x controller, which causes SCSI code to reset bus.
	 */
 {"SANYO", "CRD-250S", "1.20", (( blist_flags_t)(1ULL << 0))},
 /*
	 * The following causes a failed REQUEST SENSE on lun 1 for
	 * aha152x controller, which causes SCSI code to reset bus.
	 */
 {"SEAGATE", "ST157N", "\004|j", (( blist_flags_t)(1ULL << 0))},
 {"SEAGATE", "ST296", "921", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"SEAGATE", "ST1581", "6538", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"SONY", "CD-ROM CDU-541", "4.3d", (( blist_flags_t)(1ULL << 0))},
 {"SONY", "CD-ROM CDU-55S", "1.0i", (( blist_flags_t)(1ULL << 0))},
 {"SONY", "CD-ROM CDU-561", "1.7x", (( blist_flags_t)(1ULL << 0))},
 {"SONY", "CD-ROM CDU-8012", ((void *)0), (( blist_flags_t)(1ULL << 0))},
 {"SONY", "SDT-5000", "3.17", (( blist_flags_t)(1ULL << 21))},
 {"TANDBERG", "TDC 3600", "U07", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"TEAC", "CD-R55S", "1.0H", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 /*
	 * The following causes a failed REQUEST SENSE on lun 1 for
	 * seagate controller, which causes SCSI code to reset bus.
	 */
 {"TEAC", "CD-ROM", "1.06", (( blist_flags_t)(1ULL << 0))},
 {"TEAC", "MT-2ST/45S2-27", "RV M", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 /*
	 * The following causes a failed REQUEST SENSE on lun 1 for
	 * seagate controller, which causes SCSI code to reset bus.
	 */
 {"HP", "C1750A", "3226", (( blist_flags_t)(1ULL << 0))}, /* scanjet iic */
 {"HP", "C1790A", ((void *)0), (( blist_flags_t)(1ULL << 0))}, /* scanjet iip */
 {"HP", "C2500A", ((void *)0), (( blist_flags_t)(1ULL << 0))}, /* scanjet iicx */
 {"MEDIAVIS", "CDR-H93MV", "1.31", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"MICROTEK", "ScanMaker II", "5.61", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"MITSUMI", "CD-R CR-2201CS", "6119", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"NEC", "D3856", "0009", (( blist_flags_t)(1ULL << 0))},
 {"QUANTUM", "LPS525S", "3110", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"QUANTUM", "PD1225S", "3110", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"QUANTUM", "FIREBALL ST4.3S", "0F0C", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"RELISYS", "Scorpio", ((void *)0), (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */
 {"SANKYO", "CP525", "6.64", (( blist_flags_t)(1ULL << 0))}, /* causes failed REQ SENSE, extra reset */
 {"TEXEL", "CD-ROM", "1.06", (( blist_flags_t)(1ULL << 0)) | (( blist_flags_t)(1ULL << 2))},
 {"transtec", "T5008", "0001", (( blist_flags_t)(1ULL << 18)) },
 {"YAMAHA", "CDR100", "1.00", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"YAMAHA", "CDR102", "1.00", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"YAMAHA", "CRW8424S", "1.0", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"YAMAHA", "CRW6416S", "1.0c", (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"", "Scanner", "1.80", (( blist_flags_t)(1ULL << 0))}, /* responds to all lun */

 /*
	 * Other types of devices that have special flags.
	 * Note that all USB devices should have the BLIST_INQUIRY_36 flag.
	 */
 {"3PARdata", "VV", ((void *)0), (( blist_flags_t)(1ULL << 17))},
 {"ADAPTEC", "AACRAID", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"ADAPTEC", "Adaptec 5400S", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"AIX", "VDASD", ((void *)0), (( blist_flags_t)(1ULL << 28))},
 {"AFT PRO", "-IX CF", "0.0>", (( blist_flags_t)(1ULL << 1))},
 {"BELKIN", "USB 2 HS-CF", "1.95", (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 10))},
 {"BROWNIE", "1200U3P", ((void *)0), (( blist_flags_t)(1ULL << 18))},
 {"BROWNIE", "1600U3P", ((void *)0), (( blist_flags_t)(1ULL << 18))},
 {"CANON", "IPUBJD", ((void *)0), (( blist_flags_t)(1ULL << 6))},
 {"CBOX3", "USB Storage-SMC", "300A", (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 10))},
 {"CMD", "CRA-7280", ((void *)0), (( blist_flags_t)(1ULL << 6))}, /* CMD RAID Controller */
 {"CNSI", "G7324", ((void *)0), (( blist_flags_t)(1ULL << 6))}, /* Chaparral G7324 RAID */
 {"CNSi", "G8324", ((void *)0), (( blist_flags_t)(1ULL << 6))}, /* Chaparral G8324 RAID */
 {"COMPAQ", "ARRAY CONTROLLER", ((void *)0), (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 9)) |
  (( blist_flags_t)(1ULL << 23)) | (( blist_flags_t)(1ULL << 17))}, /* Compaq RA4x00 */
 {"COMPAQ", "LOGICAL VOLUME", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 23))}, /* Compaq RA4x00 */
 {"COMPAQ", "CR3500", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"COMPAQ", "MSA1000", ((void *)0), (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 12))},
 {"COMPAQ", "MSA1000 VOLUME", ((void *)0), (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 12))},
 {"COMPAQ", "HSV110", ((void *)0), (( blist_flags_t)(1ULL << 17)) | (( blist_flags_t)(1ULL << 12))},
 {"DDN", "SAN DataDirector", "*", (( blist_flags_t)(1ULL << 6))},
 {"DEC", "HSG80", ((void *)0), (( blist_flags_t)(1ULL << 17)) | (( blist_flags_t)(1ULL << 12))},
 {"DELL", "PV660F", ((void *)0), (( blist_flags_t)(1ULL << 6))},
 {"DELL", "PV660F   PSEUDO", ((void *)0), (( blist_flags_t)(1ULL << 6))},
 {"DELL", "PSEUDO DEVICE .", ((void *)0), (( blist_flags_t)(1ULL << 6))}, /* Dell PV 530F */
 {"DELL", "PV530F", ((void *)0), (( blist_flags_t)(1ULL << 6))},
 {"DELL", "PERCRAID", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"DGC", "RAID", ((void *)0), (( blist_flags_t)(1ULL << 6))}, /* EMC CLARiiON, storage on LUN 0 */
 {"DGC", "DISK", ((void *)0), (( blist_flags_t)(1ULL << 6))}, /* EMC CLARiiON, no storage on LUN 0 */
 {"EMC", "Invista", "*", (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 9))},
 {"EMC", "SYMMETRIX", ((void *)0), (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 9)) |
  (( blist_flags_t)(1ULL << 17)) | (( blist_flags_t)(1ULL << 32))},
 {"EMULEX", "MD21/S2     ESDI", ((void *)0), (( blist_flags_t)(1ULL << 4))},
 {"easyRAID", "16P", ((void *)0), (( blist_flags_t)(1ULL << 18))},
 {"easyRAID", "X6P", ((void *)0), (( blist_flags_t)(1ULL << 18))},
 {"easyRAID", "F8", ((void *)0), (( blist_flags_t)(1ULL << 18))},
 {"FSC", "CentricStor", "*", (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 9))},
 {"FUJITSU", "ETERNUS_DXM", "*", (( blist_flags_t)(1ULL << 33))},
 {"Generic", "USB SD Reader", "1.00", (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 10))},
 {"Generic", "USB Storage-SMC", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 10))}, /* FW: 0180 and 0207 */
 {"Generic", "Ultra HS-SD/MMC", "2.09", (( blist_flags_t)(1ULL << 11)) | (( blist_flags_t)(1ULL << 10))},
 {"HITACHI", "DF400", "*", (( blist_flags_t)(1ULL << 17))},
 {"HITACHI", "DF500", "*", (( blist_flags_t)(1ULL << 17))},
 {"HITACHI", "DISK-SUBSYSTEM", "*", (( blist_flags_t)(1ULL << 17))},
 {"HITACHI", "HUS1530", "*", (( blist_flags_t)(1ULL << 25))},
 {"HITACHI", "OPEN-", "*", (( blist_flags_t)(1ULL << 17)) | (( blist_flags_t)(1ULL << 28))},
 {"HP", "A6189A", ((void *)0), (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 9))}, /* HP VA7400 */
 {"HP", "OPEN-", "*", (( blist_flags_t)(1ULL << 17)) | (( blist_flags_t)(1ULL << 28))}, /* HP XP Arrays */
 {"HP", "NetRAID-4M", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"HP", "HSV100", ((void *)0), (( blist_flags_t)(1ULL << 17)) | (( blist_flags_t)(1ULL << 12))},
 {"HP", "C1557A", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"HP", "C3323-300", "4269", (( blist_flags_t)(1ULL << 5))},
 {"HP", "C5713A", ((void *)0), (( blist_flags_t)(1ULL << 18))},
 {"HP", "DISK-SUBSYSTEM", "*", (( blist_flags_t)(1ULL << 17))},
 {"HPE", "OPEN-", "*", (( blist_flags_t)(1ULL << 17)) | (( blist_flags_t)(1ULL << 28))},
 {"IBM", "AuSaV1S2", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"IBM", "ProFibre 4000R", "*", (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 9))},
 {"IBM", "2105", ((void *)0), (( blist_flags_t)(1ULL << 22))},
 {"iomega", "jaz 1GB", "J.86", (( blist_flags_t)(1ULL << 5)) | (( blist_flags_t)(1ULL << 0))},
 {"IOMEGA", "ZIP", ((void *)0), (( blist_flags_t)(1ULL << 5)) | (( blist_flags_t)(1ULL << 0))},
 {"IOMEGA", "Io20S         *F", ((void *)0), (( blist_flags_t)(1ULL << 3))},
 {"INSITE", "Floptical   F*8I", ((void *)0), (( blist_flags_t)(1ULL << 3))},
 {"INSITE", "I325VM", ((void *)0), (( blist_flags_t)(1ULL << 3))},
 {"Intel", "Multi-Flex", ((void *)0), (( blist_flags_t)(1ULL << 29))},
 {"iRiver", "iFP Mass Driver", ((void *)0), (( blist_flags_t)(1ULL << 19)) | (( blist_flags_t)(1ULL << 10))},
 {"LASOUND", "CDX7405", "3.10", (( blist_flags_t)(1ULL << 7)) | (( blist_flags_t)(1ULL << 4))},
 {"Marvell", "Console", ((void *)0), (( blist_flags_t)(1ULL << 26))},
 {"Marvell", "91xx Config", "1.01", (( blist_flags_t)(1ULL << 26))},
 {"MATSHITA", "PD-1", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"MATSHITA", "DMC-LC5", ((void *)0), (( blist_flags_t)(1ULL << 19)) | (( blist_flags_t)(1ULL << 10))},
 {"MATSHITA", "DMC-LC40", ((void *)0), (( blist_flags_t)(1ULL << 19)) | (( blist_flags_t)(1ULL << 10))},
 {"Medion", "Flash XL  MMC/SD", "2.6D", (( blist_flags_t)(1ULL << 1))},
 {"MegaRAID", "LD", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"MICROP", "4110", ((void *)0), (( blist_flags_t)(1ULL << 5))},
 {"MSFT", "Virtual HD", ((void *)0), (( blist_flags_t)(1ULL << 30)) | (( blist_flags_t)(1ULL << 29))},
 {"MYLEX", "DACARMRB", "*", (( blist_flags_t)(1ULL << 17))},
 {"nCipher", "Fastness Crypto", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"NAKAMICH", "MJ-4.8S", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"NAKAMICH", "MJ-5.16S", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"NEC", "PD-1 ODX654P", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"NEC", "iStorage", ((void *)0), (( blist_flags_t)(1ULL << 17))},
 {"NRC", "MBR-7", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"NRC", "MBR-7.4", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"PIONEER", "CD-ROM DRM-600", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"PIONEER", "CD-ROM DRM-602X", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"PIONEER", "CD-ROM DRM-604X", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"PIONEER", "CD-ROM DRM-624X", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 4))},
 {"Promise", "VTrak E610f", ((void *)0), (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 29))},
 {"Promise", "", ((void *)0), (( blist_flags_t)(1ULL << 6))},
 {"QEMU", "QEMU CD-ROM", ((void *)0), (( blist_flags_t)(1ULL << 26))},
 {"QNAP", "iSCSI Storage", ((void *)0), (( blist_flags_t)(1ULL << 30))},
 {"SYNOLOGY", "iSCSI Storage", ((void *)0), (( blist_flags_t)(1ULL << 30))},
 {"QUANTUM", "XP34301", "1071", (( blist_flags_t)(1ULL << 5))},
 {"REGAL", "CDC-4X", ((void *)0), (( blist_flags_t)(1ULL << 7)) | (( blist_flags_t)(1ULL << 4))},
 {"SanDisk", "ImageMate CF-SD1", ((void *)0), (( blist_flags_t)(1ULL << 1))},
 {"SEAGATE", "ST34555N", "0930", (( blist_flags_t)(1ULL << 5))}, /* Chokes on tagged INQUIRY */
 {"SEAGATE", "ST3390N", "9546", (( blist_flags_t)(1ULL << 5))},
 {"SEAGATE", "ST900MM0006", ((void *)0), (( blist_flags_t)(1ULL << 26))},
 {"SGI", "RAID3", "*", (( blist_flags_t)(1ULL << 6))},
 {"SGI", "RAID5", "*", (( blist_flags_t)(1ULL << 6))},
 {"SGI", "TP9100", "*", (( blist_flags_t)(1ULL << 17))},
 {"SGI", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"IBM", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"SUN", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"DELL", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"STK", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"NETAPP", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"LSI", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"ENGENIO", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"LENOVO", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"FUJITSU", "Universal Xport", "*", (( blist_flags_t)(1ULL << 20))},
 {"SanDisk", "Cruzer Blade", ((void *)0), (( blist_flags_t)(1ULL << 28)) |
  (( blist_flags_t)(1ULL << 10))},
 {"SMSC", "USB 2 HS-CF", ((void *)0), (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 10))},
 {"SONY", "CD-ROM CDU-8001", ((void *)0), (( blist_flags_t)(1ULL << 2))},
 {"SONY", "TSL", ((void *)0), (( blist_flags_t)(1ULL << 1))}, /* DDS3 & DDS4 autoloaders */
 {"ST650211", "CF", ((void *)0), (( blist_flags_t)(1ULL << 22))},
 {"SUN", "T300", "*", (( blist_flags_t)(1ULL << 6))},
 {"SUN", "T4", "*", (( blist_flags_t)(1ULL << 6))},
 {"Tornado-", "F4", "*", (( blist_flags_t)(1ULL << 18))},
 {"TOSHIBA", "CDROM", ((void *)0), (( blist_flags_t)(1ULL << 8))},
 {"TOSHIBA", "CD-ROM", ((void *)0), (( blist_flags_t)(1ULL << 8))},
 {"Traxdata", "CDR4120", ((void *)0), (( blist_flags_t)(1ULL << 0))}, /* locks up */
 {"USB2.0", "SMARTMEDIA/XD", ((void *)0), (( blist_flags_t)(1ULL << 1)) | (( blist_flags_t)(1ULL << 10))},
 {"WangDAT", "Model 2600", "01.7", (( blist_flags_t)(1ULL << 21))},
 {"WangDAT", "Model 3200", "02.2", (( blist_flags_t)(1ULL << 21))},
 {"WangDAT", "Model 1300", "02.4", (( blist_flags_t)(1ULL << 21))},
 {"WDC WD25", "00JB-00FUA0", ((void *)0), (( blist_flags_t)(1ULL << 18))},
 {"XYRATEX", "RS", "*", (( blist_flags_t)(1ULL << 6)) | (( blist_flags_t)(1ULL << 9))},
 {"Zzyzx", "RocketStor 500S", ((void *)0), (( blist_flags_t)(1ULL << 6))},
 {"Zzyzx", "RocketStor 2000", ((void *)0), (( blist_flags_t)(1ULL << 6))},
 { ((void *)0), ((void *)0), ((void *)0), 0 },
};

static struct scsi_dev_info_list_table *scsi_devinfo_lookup_by_key(int key)
{
 struct scsi_dev_info_list_table *devinfo_table;
 int found = 0;

 for (devinfo_table = ({ void *__mptr = (void *)((&scsi_dev_info_list)->next); _Static_assert(__builtin_types_compatible_p(typeof(*((&scsi_dev_info_list)->next)), typeof(((typeof(*devinfo_table) *)0)->node)) || __builtin_types_compatible_p(typeof(*((&scsi_dev_info_list)->next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*devinfo_table) *)(__mptr - __builtin_offsetof(typeof(*devinfo_table), node))); }); !(&devinfo_table->node == (&scsi_dev_info_list)); devinfo_table = ({ void *__mptr = (void *)((devinfo_table)->node.next); _Static_assert(__builtin_types_compatible_p(typeof(*((devinfo_table)->node.next)), typeof(((typeof(*(devinfo_table)) *)0)->node)) || __builtin_types_compatible_p(typeof(*((devinfo_table)->node.next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*(devinfo_table)) *)(__mptr - __builtin_offsetof(typeof(*(devinfo_table)), node))); }))
  if (devinfo_table->key == key) {
   found = 1;
   break;
  }
 if (!found)
  return ERR_PTR(-22 /* Invalid argument */);

 return devinfo_table;
}

/*
 * scsi_strcpy_devinfo: called from scsi_dev_info_list_add to copy into
 * devinfo vendor and model strings.
 */
static void scsi_strcpy_devinfo(char *name, char *to, size_t to_length,
    char *from, int compatible)
{
 size_t from_length;

 from_length = strlen(from);
 /* This zero-pads the destination */
 strncpy(to, from, to_length);
 if (from_length < to_length && !compatible) {
  /*
		 * space pad the string if it is short.
		 */
  memset(&to[from_length], ' ', to_length - from_length);
 }
 if (from_length > to_length)
   ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "4" /* warning conditions */ "%s: %s string '%s' is too long\n", __func__, name, from); });

}

/**
 * scsi_dev_info_list_add - add one dev_info list entry.
 * @compatible: if true, null terminate short strings.  Otherwise space pad.
 * @vendor:	vendor string
 * @model:	model (product) string
 * @strflags:	integer string
 * @flags:	if strflags NULL, use this flag value
 *
 * Description:
 *	Create and add one dev_info entry for @vendor, @model, @strflags or
 *	@flag. If @compatible, add to the tail of the list, do not space
 *	pad, and set devinfo->compatible. The scsi_static_device_list entries
 *	are added with @compatible 1 and @clfags NULL.
 *
 * Returns: 0 OK, -error on failure.
 **/
static int scsi_dev_info_list_add(int compatible, char *vendor, char *model,
       char *strflags, blist_flags_t flags)
{
 return scsi_dev_info_list_add_keyed(compatible, vendor, model,
         strflags, flags,
         SCSI_DEVINFO_GLOBAL);
}

/**
 * scsi_dev_info_list_add_keyed - add one dev_info list entry.
 * @compatible: if true, null terminate short strings.  Otherwise space pad.
 * @vendor:	vendor string
 * @model:	model (product) string
 * @strflags:	integer string
 * @flags:	if strflags NULL, use this flag value
 * @key:	specify list to use
 *
 * Description:
 *	Create and add one dev_info entry for @vendor, @model,
 *	@strflags or @flag in list specified by @key. If @compatible,
 *	add to the tail of the list, do not space pad, and set
 *	devinfo->compatible. The scsi_static_device_list entries are
 *	added with @compatible 1 and @clfags NULL.
 *
 * Returns: 0 OK, -error on failure.
 **/
int scsi_dev_info_list_add_keyed(int compatible, char *vendor, char *model,
     char *strflags, blist_flags_t flags,
     enum scsi_devinfo_key key)
{
 struct scsi_dev_info_list *devinfo;
 struct scsi_dev_info_list_table *devinfo_table =
  scsi_devinfo_lookup_by_key(key);

 if (IS_ERR(devinfo_table))
  return PTR_ERR(devinfo_table);

 devinfo = kmalloc(sizeof(*devinfo), ((( gfp_t)(0x400u|0x800u)) | (( gfp_t)0x40u) | (( gfp_t)0x80u)));
 if (!devinfo) {
  ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "3" /* error conditions */ "%s: no memory\n", __func__); });
  return -12 /* Out of memory */;
 }

 scsi_strcpy_devinfo("vendor", devinfo->vendor, sizeof(devinfo->vendor),
       vendor, compatible);
 scsi_strcpy_devinfo("model", devinfo->model, sizeof(devinfo->model),
       model, compatible);

 if (strflags) {
  unsigned long long val;
  int ret = kstrtoull(strflags, 0, &val);

  if (ret != 0) {
   kfree(devinfo);
   return ret;
  }
  flags = ( blist_flags_t)val;
 }
 if (flags & ((( blist_flags_t)(1ULL << 13)) | (( blist_flags_t)(1ULL << 14)) | (( blist_flags_t)(1ULL << 15)) | (( blist_flags_t)(1ULL << 16)) | (( blist_flags_t)(1ULL << 24)) | (( blist_flags_t)(1ULL << 27)) | (~((( blist_flags_t)(1ULL << 33)) | ( blist_flags_t) (( __u64)(( blist_flags_t)(1ULL << 33)) - 1ULL))))) {
  ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "3" /* error conditions */ "scsi_devinfo (%s:%s): unsupported flags 0x%llx", vendor, model, flags & ((( blist_flags_t)(1ULL << 13)) | (( blist_flags_t)(1ULL << 14)) | (( blist_flags_t)(1ULL << 15)) | (( blist_flags_t)(1ULL << 16)) | (( blist_flags_t)(1ULL << 24)) | (( blist_flags_t)(1ULL << 27)) | (~((( blist_flags_t)(1ULL << 33)) | ( blist_flags_t) (( __u64)(( blist_flags_t)(1ULL << 33)) - 1ULL))))); });

  kfree(devinfo);
  return -22 /* Invalid argument */;
 }
 devinfo->flags = flags;
 devinfo->compatible = compatible;

 if (compatible)
  list_add_tail(&devinfo->dev_info_list,
         &devinfo_table->scsi_dev_info_list);
 else
  list_add(&devinfo->dev_info_list,
    &devinfo_table->scsi_dev_info_list);

 return 0;
}
extern typeof(scsi_dev_info_list_add_keyed) scsi_dev_info_list_add_keyed; extern const char __kstrtab_scsi_dev_info_list_add_keyed[]; extern const char __kstrtabns_scsi_dev_info_list_add_keyed[]; asm("	.section \"__ksymtab_strings\",\"aMS\",%progbits,1	\n" "__kstrtab_" "scsi_dev_info_list_add_keyed" ":					\n" "	.asciz 	\"" "scsi_dev_info_list_add_keyed" "\"					\n" "__kstrtabns_" "scsi_dev_info_list_add_keyed" ":					\n" "	.asciz 	\"" "" "\"					\n" "	.previous						\n"); static void * __attribute__((__used__)) __attribute__((__section__(".discard.addressable"))) __UNIQUE_ID___addressable_scsi_dev_info_list_add_keyed429 = (void *)&scsi_dev_info_list_add_keyed; asm("	.section \"___ksymtab" "" "+" "scsi_dev_info_list_add_keyed" "\", \"a\"	\n" "	.balign	4					\n" "__ksymtab_" "scsi_dev_info_list_add_keyed" ":				\n" "	.long	" "scsi_dev_info_list_add_keyed" "- .				\n" "	.long	__kstrtab_" "scsi_dev_info_list_add_keyed" "- .			\n" "	.long	__kstrtabns_" "scsi_dev_info_list_add_keyed" "- .			\n" "	.previous					\n");

/**
 * scsi_dev_info_list_find - find a matching dev_info list entry.
 * @vendor:	full vendor string
 * @model:	full model (product) string
 * @key:	specify list to use
 *
 * Description:
 *	Finds the first dev_info entry matching @vendor, @model
 *	in list specified by @key.
 *
 * Returns: pointer to matching entry, or ERR_PTR on failure.
 **/
static struct scsi_dev_info_list *scsi_dev_info_list_find(const char *vendor,
  const char *model, enum scsi_devinfo_key key)
{
 struct scsi_dev_info_list *devinfo;
 struct scsi_dev_info_list_table *devinfo_table =
  scsi_devinfo_lookup_by_key(key);
 size_t vmax, mmax, mlen;
 const char *vskip, *mskip;

 if (IS_ERR(devinfo_table))
  return (struct scsi_dev_info_list *) devinfo_table;

 /* Prepare for "compatible" matches */

 /*
	 * XXX why skip leading spaces? If an odd INQUIRY
	 * value, that should have been part of the
	 * scsi_static_device_list[] entry, such as "  FOO"
	 * rather than "FOO". Since this code is already
	 * here, and we don't know what device it is
	 * trying to work with, leave it as-is.
	 */
 vmax = sizeof(devinfo->vendor);
 vskip = vendor;
 while (vmax > 0 && *vskip == ' ') {
  vmax--;
  vskip++;
 }
 /* Also skip trailing spaces */
 while (vmax > 0 && vskip[vmax - 1] == ' ')
  --vmax;

 mmax = sizeof(devinfo->model);
 mskip = model;
 while (mmax > 0 && *mskip == ' ') {
  mmax--;
  mskip++;
 }
 while (mmax > 0 && mskip[mmax - 1] == ' ')
  --mmax;

 for (devinfo = ({ void *__mptr = (void *)((&devinfo_table->scsi_dev_info_list)->next); _Static_assert(__builtin_types_compatible_p(typeof(*((&devinfo_table->scsi_dev_info_list)->next)), typeof(((typeof(*devinfo) *)0)->dev_info_list)) || __builtin_types_compatible_p(typeof(*((&devinfo_table->scsi_dev_info_list)->next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*devinfo) *)(__mptr - __builtin_offsetof(typeof(*devinfo), dev_info_list))); }); !(&devinfo->dev_info_list == (&devinfo_table->scsi_dev_info_list)); devinfo = ({ void *__mptr = (void *)((devinfo)->dev_info_list.next); _Static_assert(__builtin_types_compatible_p(typeof(*((devinfo)->dev_info_list.next)), typeof(((typeof(*(devinfo)) *)0)->dev_info_list)) || __builtin_types_compatible_p(typeof(*((devinfo)->dev_info_list.next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*(devinfo)) *)(__mptr - __builtin_offsetof(typeof(*(devinfo)), dev_info_list))); })) {

  if (devinfo->compatible) {
   /*
			 * vendor strings must be an exact match
			 */
   if (vmax != strnlen(devinfo->vendor,
         sizeof(devinfo->vendor)) ||
       memcmp(devinfo->vendor, vskip, vmax))
    continue;

   /*
			 * @model specifies the full string, and
			 * must be larger or equal to devinfo->model
			 */
   mlen = strnlen(devinfo->model, sizeof(devinfo->model));
   if (mmax < mlen || memcmp(devinfo->model, mskip, mlen))
    continue;
   return devinfo;
  } else {
   if (!memcmp(devinfo->vendor, vendor,
        sizeof(devinfo->vendor)) &&
       !memcmp(devinfo->model, model,
        sizeof(devinfo->model)))
    return devinfo;
  }
 }

 return ERR_PTR(-2 /* No such file or directory */);
}

/**
 * scsi_dev_info_list_del_keyed - remove one dev_info list entry.
 * @vendor:	vendor string
 * @model:	model (product) string
 * @key:	specify list to use
 *
 * Description:
 *	Remove and destroy one dev_info entry for @vendor, @model
 *	in list specified by @key.
 *
 * Returns: 0 OK, -error on failure.
 **/
int scsi_dev_info_list_del_keyed(char *vendor, char *model,
     enum scsi_devinfo_key key)
{
 struct scsi_dev_info_list *found;

 found = scsi_dev_info_list_find(vendor, model, key);
 if (IS_ERR(found))
  return PTR_ERR(found);

 list_del(&found->dev_info_list);
 kfree(found);
 return 0;
}
extern typeof(scsi_dev_info_list_del_keyed) scsi_dev_info_list_del_keyed; extern const char __kstrtab_scsi_dev_info_list_del_keyed[]; extern const char __kstrtabns_scsi_dev_info_list_del_keyed[]; asm("	.section \"__ksymtab_strings\",\"aMS\",%progbits,1	\n" "__kstrtab_" "scsi_dev_info_list_del_keyed" ":					\n" "	.asciz 	\"" "scsi_dev_info_list_del_keyed" "\"					\n" "__kstrtabns_" "scsi_dev_info_list_del_keyed" ":					\n" "	.asciz 	\"" "" "\"					\n" "	.previous						\n"); static void * __attribute__((__used__)) __attribute__((__section__(".discard.addressable"))) __UNIQUE_ID___addressable_scsi_dev_info_list_del_keyed430 = (void *)&scsi_dev_info_list_del_keyed; asm("	.section \"___ksymtab" "" "+" "scsi_dev_info_list_del_keyed" "\", \"a\"	\n" "	.balign	4					\n" "__ksymtab_" "scsi_dev_info_list_del_keyed" ":				\n" "	.long	" "scsi_dev_info_list_del_keyed" "- .				\n" "	.long	__kstrtab_" "scsi_dev_info_list_del_keyed" "- .			\n" "	.long	__kstrtabns_" "scsi_dev_info_list_del_keyed" "- .			\n" "	.previous					\n");

/**
 * scsi_dev_info_list_add_str - parse dev_list and add to the scsi_dev_info_list.
 * @dev_list:	string of device flags to add
 *
 * Description:
 *	Parse dev_list, and add entries to the scsi_dev_info_list.
 *	dev_list is of the form "vendor:product:flag,vendor:product:flag".
 *	dev_list is modified via strsep. Can be called for command line
 *	addition, for proc or mabye a sysfs interface.
 *
 * Returns: 0 if OK, -error on failure.
 **/
static int scsi_dev_info_list_add_str(char *dev_list)
{
 char *vendor, *model, *strflags, *next;
 char *next_check;
 int res = 0;

 next = dev_list;
 if (next && next[0] == '"') {
  /*
		 * Ignore both the leading and trailing quote.
		 */
  next++;
  next_check = ",\"";
 } else {
  next_check = ",";
 }

 /*
	 * For the leading and trailing '"' case, the for loop comes
	 * through the last time with vendor[0] == '\0'.
	 */
 for (vendor = strsep(&next, ":"); vendor && (vendor[0] != '\0')
      && (res == 0); vendor = strsep(&next, ":")) {
  strflags = ((void *)0);
  model = strsep(&next, ":");
  if (model)
   strflags = strsep(&next, next_check);
  if (!model || !strflags) {
   ({ do {} while (0); _printk("\001" /* ASCII Start Of Header */ "3" /* error conditions */ "%s: bad dev info string '%s' '%s'" " '%s'\n", __func__, vendor, model, strflags); });


   res = -22 /* Invalid argument */;
  } else
   res = scsi_dev_info_list_add(0 /* compatible */, vendor,
           model, strflags, 0);
 }
 return res;
}

/**
 * scsi_get_device_flags - get device specific flags from the dynamic
 *	device list.
 * @sdev:       &scsi_device to get flags for
 * @vendor:	vendor name
 * @model:	model name
 *
 * Description:
 *     Search the global scsi_dev_info_list (specified by list zero)
 *     for an entry matching @vendor and @model, if found, return the
 *     matching flags value, else return the host or global default
 *     settings.  Called during scan time.
 **/
blist_flags_t scsi_get_device_flags(struct scsi_device *sdev,
        const unsigned char *vendor,
        const unsigned char *model)
{
 return scsi_get_device_flags_keyed(sdev, vendor, model,
        SCSI_DEVINFO_GLOBAL);
}


/**
 * scsi_get_device_flags_keyed - get device specific flags from the dynamic device list
 * @sdev:       &scsi_device to get flags for
 * @vendor:	vendor name
 * @model:	model name
 * @key:	list to look up
 *
 * Description:
 *     Search the scsi_dev_info_list specified by @key for an entry
 *     matching @vendor and @model, if found, return the matching
 *     flags value, else return the host or global default settings.
 *     Called during scan time.
 **/
blist_flags_t scsi_get_device_flags_keyed(struct scsi_device *sdev,
    const unsigned char *vendor,
    const unsigned char *model,
    enum scsi_devinfo_key key)
{
 struct scsi_dev_info_list *devinfo;

 devinfo = scsi_dev_info_list_find(vendor, model, key);
 if (!IS_ERR(devinfo))
  return devinfo->flags;

 /* key or device not found: return nothing */
 if (key != SCSI_DEVINFO_GLOBAL)
  return 0;

 /* except for the global list, where we have an exception */
 if (sdev->sdev_bflags)
  return sdev->sdev_bflags;

 return scsi_default_dev_flags;
}
extern typeof(scsi_get_device_flags_keyed) scsi_get_device_flags_keyed; extern const char __kstrtab_scsi_get_device_flags_keyed[]; extern const char __kstrtabns_scsi_get_device_flags_keyed[]; asm("	.section \"__ksymtab_strings\",\"aMS\",%progbits,1	\n" "__kstrtab_" "scsi_get_device_flags_keyed" ":					\n" "	.asciz 	\"" "scsi_get_device_flags_keyed" "\"					\n" "__kstrtabns_" "scsi_get_device_flags_keyed" ":					\n" "	.asciz 	\"" "" "\"					\n" "	.previous						\n"); static void * __attribute__((__used__)) __attribute__((__section__(".discard.addressable"))) __UNIQUE_ID___addressable_scsi_get_device_flags_keyed431 = (void *)&scsi_get_device_flags_keyed; asm("	.section \"___ksymtab" "" "+" "scsi_get_device_flags_keyed" "\", \"a\"	\n" "	.balign	4					\n" "__ksymtab_" "scsi_get_device_flags_keyed" ":				\n" "	.long	" "scsi_get_device_flags_keyed" "- .				\n" "	.long	__kstrtab_" "scsi_get_device_flags_keyed" "- .			\n" "	.long	__kstrtabns_" "scsi_get_device_flags_keyed" "- .			\n" "	.previous					\n");
# 752 "drivers/scsi/scsi_devinfo.c"
static const struct kparam_string __param_string_dev_flags = { sizeof(scsi_dev_flags), scsi_dev_flags }; /* Default value instead of permissions? */ static const char __param_str_dev_flags[] = "scsi_mod" "." "dev_flags"; static struct kernel_param const __param_dev_flags __attribute__((__used__)) __attribute__((__section__("__param"))) __attribute__((__aligned__(__alignof__(struct kernel_param)))) = { __param_str_dev_flags, ((struct module *)0), &param_ops_string, (((int)(sizeof(struct { int:(-!!((0) < 0)); }))) + ((int)(sizeof(struct { int:(-!!((0) > 0777)); }))) + /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */ ((int)(sizeof(struct { int:(-!!((((0) >> 6) & 4) < (((0) >> 3) & 4))); }))) + ((int)(sizeof(struct { int:(-!!((((0) >> 3) & 4) < ((0) & 4))); }))) + /* USER_WRITABLE >= GROUP_WRITABLE */ ((int)(sizeof(struct { int:(-!!((((0) >> 6) & 2) < (((0) >> 3) & 2))); }))) + /* OTHER_WRITABLE?  Generally considered a bad idea. */ ((int)(sizeof(struct { int:(-!!((0) & 2)); }))) + (0)), -1, 0, { .str = &__param_string_dev_flags } }; static const char __UNIQUE_ID_dev_flagstype432[] __attribute__((__used__)) __attribute__((__section__(".modinfo"))) __attribute__((__aligned__(1))) = "scsi_mod" "." "parmtype" "=" "dev_flags" ":" "string";
static const char __UNIQUE_ID_dev_flags433[] __attribute__((__used__)) __attribute__((__section__(".modinfo"))) __attribute__((__aligned__(1))) = "scsi_mod" "." "parm" "=" "dev_flags" ":" "Given scsi_dev_flags=vendor:model:flags[,v:m:f] add black/white" " list entries for vendor and model with an integer value of flags" " to the scsi device info list";




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((__no_instrument_function__)) unsigned long long __attribute__((__unused__)) *__check_default_dev_flags(void) { return(&(scsi_default_dev_flags)); }; /* Default value instead of permissions? */ static const char __param_str_default_dev_flags[] = "scsi_mod" "." "default_dev_flags"; static struct kernel_param const __param_default_dev_flags __attribute__((__used__)) __attribute__((__section__("__param"))) __attribute__((__aligned__(__alignof__(struct kernel_param)))) = { __param_str_default_dev_flags, ((struct module *)0), &param_ops_ullong, (((int)(sizeof(struct { int:(-!!((0644) < 0)); }))) + ((int)(sizeof(struct { int:(-!!((0644) > 0777)); }))) + /* USER_READABLE >= GROUP_READABLE >= OTHER_READABLE */ ((int)(sizeof(struct { int:(-!!((((0644) >> 6) & 4) < (((0644) >> 3) & 4))); }))) + ((int)(sizeof(struct { int:(-!!((((0644) >> 3) & 4) < ((0644) & 4))); }))) + /* USER_WRITABLE >= GROUP_WRITABLE */ ((int)(sizeof(struct { int:(-!!((((0644) >> 6) & 2) < (((0644) >> 3) & 2))); }))) + /* OTHER_WRITABLE?  Generally considered a bad idea. */ ((int)(sizeof(struct { int:(-!!((0644) & 2)); }))) + (0644)), -1, 0, { &scsi_default_dev_flags } }; static const char __UNIQUE_ID_default_dev_flagstype434[] __attribute__((__used__)) __attribute__((__section__(".modinfo"))) __attribute__((__aligned__(1))) = "scsi_mod" "." "parmtype" "=" "default_dev_flags" ":" "ullong";
static const char __UNIQUE_ID_default_dev_flags435[] __attribute__((__used__)) __attribute__((__section__(".modinfo"))) __attribute__((__aligned__(1))) = "scsi_mod" "." "parm" "=" "default_dev_flags" ":" "scsi default device flag uint64_t value";


/**
 * scsi_exit_devinfo - remove /proc/scsi/device_info & the scsi_dev_info_list
 **/
void scsi_exit_devinfo(void)
{




 scsi_dev_info_remove_list(SCSI_DEVINFO_GLOBAL);
}

/**
 * scsi_dev_info_add_list - add a new devinfo list
 * @key:	key of the list to add
 * @name:	Name of the list to add (for /proc/scsi/device_info)
 *
 * Adds the requested list, returns zero on success, -EEXIST if the
 * key is already registered to a list, or other error on failure.
 */
int scsi_dev_info_add_list(enum scsi_devinfo_key key, const char *name)
{
 struct scsi_dev_info_list_table *devinfo_table =
  scsi_devinfo_lookup_by_key(key);

 if (!IS_ERR(devinfo_table))
  /* list already exists */
  return -17 /* File exists */;

 devinfo_table = kmalloc(sizeof(*devinfo_table), ((( gfp_t)(0x400u|0x800u)) | (( gfp_t)0x40u) | (( gfp_t)0x80u)));

 if (!devinfo_table)
  return -12 /* Out of memory */;

 INIT_LIST_HEAD(&devinfo_table->node);
 INIT_LIST_HEAD(&devinfo_table->scsi_dev_info_list);
 devinfo_table->name = name;
 devinfo_table->key = key;
 list_add_tail(&devinfo_table->node, &scsi_dev_info_list);

 return 0;
}
extern typeof(scsi_dev_info_add_list) scsi_dev_info_add_list; extern const char __kstrtab_scsi_dev_info_add_list[]; extern const char __kstrtabns_scsi_dev_info_add_list[]; asm("	.section \"__ksymtab_strings\",\"aMS\",%progbits,1	\n" "__kstrtab_" "scsi_dev_info_add_list" ":					\n" "	.asciz 	\"" "scsi_dev_info_add_list" "\"					\n" "__kstrtabns_" "scsi_dev_info_add_list" ":					\n" "	.asciz 	\"" "" "\"					\n" "	.previous						\n"); static void * __attribute__((__used__)) __attribute__((__section__(".discard.addressable"))) __UNIQUE_ID___addressable_scsi_dev_info_add_list436 = (void *)&scsi_dev_info_add_list; asm("	.section \"___ksymtab" "" "+" "scsi_dev_info_add_list" "\", \"a\"	\n" "	.balign	4					\n" "__ksymtab_" "scsi_dev_info_add_list" ":				\n" "	.long	" "scsi_dev_info_add_list" "- .				\n" "	.long	__kstrtab_" "scsi_dev_info_add_list" "- .			\n" "	.long	__kstrtabns_" "scsi_dev_info_add_list" "- .			\n" "	.previous					\n");

/**
 * scsi_dev_info_remove_list - destroy an added devinfo list
 * @key: key of the list to destroy
 *
 * Iterates over the entire list first, freeing all the values, then
 * frees the list itself.  Returns 0 on success or -EINVAL if the key
 * can't be found.
 */
int scsi_dev_info_remove_list(enum scsi_devinfo_key key)
{
 struct list_head *lh, *lh_next;
 struct scsi_dev_info_list_table *devinfo_table =
  scsi_devinfo_lookup_by_key(key);

 if (IS_ERR(devinfo_table))
  /* no such list */
  return -22 /* Invalid argument */;

 /* remove from the master list */
 list_del(&devinfo_table->node);

 for (lh = (&devinfo_table->scsi_dev_info_list)->next, lh_next = lh->next; !list_is_head(lh, (&devinfo_table->scsi_dev_info_list)); lh = lh_next, lh_next = lh->next) {
  struct scsi_dev_info_list *devinfo;

  devinfo = ({ void *__mptr = (void *)(lh); _Static_assert(__builtin_types_compatible_p(typeof(*(lh)), typeof(((struct scsi_dev_info_list *)0)->dev_info_list)) || __builtin_types_compatible_p(typeof(*(lh)), typeof(void)), "pointer type mismatch in container_of()"); ((struct scsi_dev_info_list *)(__mptr - __builtin_offsetof(struct scsi_dev_info_list, dev_info_list))); });

  kfree(devinfo);
 }
 kfree(devinfo_table);

 return 0;
}
extern typeof(scsi_dev_info_remove_list) scsi_dev_info_remove_list; extern const char __kstrtab_scsi_dev_info_remove_list[]; extern const char __kstrtabns_scsi_dev_info_remove_list[]; asm("	.section \"__ksymtab_strings\",\"aMS\",%progbits,1	\n" "__kstrtab_" "scsi_dev_info_remove_list" ":					\n" "	.asciz 	\"" "scsi_dev_info_remove_list" "\"					\n" "__kstrtabns_" "scsi_dev_info_remove_list" ":					\n" "	.asciz 	\"" "" "\"					\n" "	.previous						\n"); static void * __attribute__((__used__)) __attribute__((__section__(".discard.addressable"))) __UNIQUE_ID___addressable_scsi_dev_info_remove_list437 = (void *)&scsi_dev_info_remove_list; asm("	.section \"___ksymtab" "" "+" "scsi_dev_info_remove_list" "\", \"a\"	\n" "	.balign	4					\n" "__ksymtab_" "scsi_dev_info_remove_list" ":				\n" "	.long	" "scsi_dev_info_remove_list" "- .				\n" "	.long	__kstrtab_" "scsi_dev_info_remove_list" "- .			\n" "	.long	__kstrtabns_" "scsi_dev_info_remove_list" "- .			\n" "	.previous					\n");

/**
 * scsi_init_devinfo - set up the dynamic device list.
 *
 * Description:
 *	Add command line entries from scsi_dev_flags, then add
 *	scsi_static_device_list entries to the scsi device info list.
 */
int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) scsi_init_devinfo(void)
{



 int error, i;

 error = scsi_dev_info_add_list(SCSI_DEVINFO_GLOBAL, ((void *)0));
 if (error)
  return error;

 error = scsi_dev_info_list_add_str(scsi_dev_flags);
 if (error)
  goto out;

 for (i = 0; scsi_static_device_list[i].vendor; i++) {
  error = scsi_dev_info_list_add(1 /* compatibile */,
    scsi_static_device_list[i].vendor,
    scsi_static_device_list[i].model,
    ((void *)0),
    scsi_static_device_list[i].flags);
  if (error)
   goto out;
 }
# 880 "drivers/scsi/scsi_devinfo.c"
 out:
 if (error)
  scsi_exit_devinfo();
 return error;
}
